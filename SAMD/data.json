[{"idx":0,"sentence":"Massive misinformation spread over Internet has many negative impacts on our lives. ","offset":0,"pro":0,"labels":"GAP"},{"idx":0,"sentence":"While spreading a claim is easy, investigating its veracity is hard and time consuming, ","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":0,"sentence":"Therefore, we urgently need systems to help human factcheckers. ","offset":2,"pro":0.3333333333333333,"labels":"BAC"},{"idx":0,"sentence":"However, available data resources to develop effective systems are limited and the vast majority of them is for English. ","offset":3,"pro":0.5,"labels":"GAP"},{"idx":0,"sentence":"In this work, we introduce TrClaim-19, which is the very first labeled dataset for Turkish checkworthy claims. ","offset":4,"pro":0.6666666666666666,"labels":"PUR"},{"idx":0,"sentence":"The rationales we collected suggest that claims’ topics and their possible negative impacts are the main factors affecting their check-worthiness.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":1,"sentence":"Language models (LMs) trained on large quantities of text have been claimed to acquire abstract linguistic representations.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"Our work tests the robustness of these abstractions by focusing on the ability of LMs to learn interactions between different linguistic representations. ","offset":1,"pro":0.25,"labels":"PUR"},{"idx":1,"sentence":" We compared both transformer and long short-term memory LMs","offset":2,"pro":0.5,"labels":"MTD"},{"idx":1,"sentence":"to find that, contrary to humans, implicit causality only influences LM behavior for reference, not syntax, despite model representations that encode the necessary discourse information. ","offset":3,"pro":0.75,"labels":"RST"},{"idx":2,"sentence":"To communicate with new partners in new contexts, humans rapidly form new linguistic conventions. ","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"but are not able to flexibly and interactively adapt those conventions on the fly as humans do.","offset":1,"pro":0.25,"labels":"GAP"},{"idx":2,"sentence":"We introduce an interactive repeated reference task as a benchmark for models of adaptation in communication and propose a regularized continual learning framework that allows an artificial agent initialized with a generic language model to more accurately and efficiently communicate with a partner over time. ","offset":2,"pro":0.5,"labels":"PUR"},{"idx":2,"sentence":"e evaluate this framework through simulations on COCO and in real-time reference game experiments with human partners.","offset":3,"pro":0.75,"labels":"CTN"},{"idx":3,"sentence":"A problem in automatically generated stories for image sequences is that they use overly generic vocabulary and phrase structure and fail to match the distributional characteristics of human-generated text.","offset":0,"pro":0,"labels":"GAP"},{"idx":3,"sentence":"We address this problem","offset":1,"pro":0.2,"labels":"PUR"},{"idx":3,"sentence":"by introducing explicit representations for objects and their relations by extracting scene graphs from the images. ","offset":2,"pro":0.4,"labels":"MTD"},{"idx":3,"sentence":"We apply metrics that account for the diversity of words and phrases of generated stories as well as for reference to narratively-salient image features ","offset":3,"pro":0.6,"labels":"MTD"},{"idx":3,"sentence":"Our experiments also indicate that our models obtain competitive results on reference-based metrics.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":4,"sentence":"This paper tackles the task of named entity recognition (NER) applied to digitized historical texts obtained from processing digital images of newspapers using optical character recognition (OCR) techniques. ","offset":0,"pro":0,"labels":"PUR"},{"idx":4,"sentence":"Moreover, historical variations can be present in aged documents, which can impact the performance of the NER process. ","offset":1,"pro":0.25,"labels":"GAP"},{"idx":4,"sentence":"Our findings show that the proposed model clearly improves the results on both historical datasets, and does not degrade the results for modern datasets.","offset":2,"pro":0.5,"labels":"CLN"},{"idx":4,"sentence":"we propose a model based on a hierarchical stack of Transformers to approach the NER task for historical data.","offset":3,"pro":0.75,"labels":"RST"},{"idx":5,"sentence":"Researchers have recently demonstrated that tying the neural weights between the input look-up table and the output classification layer can improve training and lower perplexity on sequence learning tasks such as language modelling.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"Such a procedure is possible due to the design of the softmax classification layer, which previous work has shown to comprise a viable set of semantic representations for the model vocabulary, and these these output embeddings are known to perform well on word similarity benchmarks.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":5,"sentence":"In this paper, we make meaningful comparisons between the input and output embeddings and other SOTA distributional models to gain a better understanding of the types of information they represent. ","offset":2,"pro":0.4,"labels":"PUR"},{"idx":5,"sentence":"These locally-optimal embeddings demonstrate excellent performance across all our evaluations.","offset":3,"pro":0.6,"labels":"RST"},{"idx":5,"sentence":"to create locally-optimal approximations for the intermediate representations from the language model.","offset":4,"pro":0.8,"labels":"PUR"},{"idx":6,"sentence":"Transformers are being used extensively across several sequence modeling tasks. ","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"However, our conceptual and theoretical understanding of their power and inherent limitations is still nascent. ","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":6,"sentence":"In this paper, we take a step towards answering these questions.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":6,"sentence":"We analyze the computational power as captured by Turing-completeness.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":6,"sentence":"We first provide an alternate and simpler proof to show that vanilla Transformers are Turing-complete and then we prove that Transformers with only positional masking and without any positional encoding are also Turing-complete. ","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":6,"sentence":"interestingly, we find that a particular type of residual connection is necessary. ","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":7,"sentence":"n historical linguistics, cognate detection is the task of determining whether sets of words have common etymological roots.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"Inspired by the comparative method used by human linguists, we develop a system for automated cognate detection that frames the task as an inference problem for a general statistical model consisting of observed data (potentially cognate pairs of words), latent variables (the cognacy status of pairs) and unknown global parameters (which sounds correspond between languages).","offset":1,"pro":0.2,"labels":"PUR"},{"idx":7,"sentence":"We then give a specific instance of such a model along with an expectationmaximisation algorithm to infer its parameters. ","offset":2,"pro":0.4,"labels":"PUR"},{"idx":7,"sentence":"finding its performance of our method to be comparable to the state of the art. ","offset":3,"pro":0.6,"labels":"RST"},{"idx":7,"sentence":"demonstrating various advantages it has over existing systems. ","offset":4,"pro":0.8,"labels":"RST"},{"idx":8,"sentence":"It can be difficult to separate abstract linguistic knowledge in recurrent neural networks (RNNs) from surface heuristics. ","offset":0,"pro":0,"labels":"GAP"},{"idx":8,"sentence":"For models to generalize abstract patterns in expected ways to unseen data, they must share representational features in predictable ways.","offset":1,"pro":0.25,"labels":"PUR"},{"idx":8,"sentence":"We use cumulative priming to test for representational overlap between disparate filler-gap constructions in English ","offset":2,"pro":0.5,"labels":"MTD"},{"idx":8,"sentence":"However, we find no evidence that the models learn any of the shared underlying grammatical constraints we tested. ","offset":3,"pro":0.75,"labels":"RST"},{"idx":9,"sentence":"Semantic parsing is one of the key components of natural language understanding systems. ","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"Many algorithms have been proposed to solve this problem, from conventional rulebased or statistical slot-filling systems to shiftreduce based neural parsers.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":9,"sentence":"For complex parsing tasks, the state-of-the-art method is based on autoregressive sequence to sequence models to generate the parse directly.","offset":2,"pro":0.3333333333333333,"labels":"BAC"},{"idx":9,"sentence":"This model is slow at inference time, generating parses in O(n) decoding steps (n is the length of the target sequence). ","offset":3,"pro":0.5,"labels":"GAP"},{"idx":9,"sentence":"In this paper, we propose a non-autoregressive parser which is based on the insertion transformer to overcome these two issues. ","offset":4,"pro":0.6666666666666666,"labels":"PUR"},{"idx":9,"sentence":"We test our approach on three well-known monolingual datasets: ATIS, SNIPS and TOP. ","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":0,"sentence":"Words can have multiple senses.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"Compositional distributional models of meaning have been argued to deal well with finer shades of meaning variation known as polysemy, but are not so well equipped to handle word senses that are etymologically unrelated, or homonymy.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":0,"sentence":"Moving from vectors to density matrices allows us to encode a probability distribution over different senses of a word, and can also be accommodated within a compositional distributional model of meaning.","offset":2,"pro":0.4,"labels":"BAC"},{"idx":0,"sentence":"In this paper we present three new neural models for learning density matrices from a corpus, and test their ability to discriminate between word senses on a range of compositional datasets. ","offset":3,"pro":0.6,"labels":"PUR"},{"idx":0,"sentence":"When paired with a particular composition method, our best model outperforms existing vector-based compositional models as well as strong sentence encoders","offset":4,"pro":0.8,"labels":"RST"},{"idx":1,"sentence":"The language acquisition literature shows that children do not build their lexicon by segmenting the spoken input into phonemes and then building up words from them, but rather adopt a top-down approach and start by segmenting word-like units and then break them down into smaller units","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"This suggests that the ideal way of learning a language is by starting from full semantic units","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":1,"sentence":"In this paper, we investigate if this is also the case for a neural model of Visually Grounded Speech trained on a speech-image retrieval task","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":1,"sentence":"We evaluated how well such a network is able to learn a reliable speech-to-image mapping when provided with phone, syllable, or word boundary information","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":1,"sentence":"We present a simple way to introduce such information into an RNN-based model and investigate which type of boundary is the most efficient","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":1,"sentence":"We also explore at which level of the network’s architecture such information should be introduced so as to maximise its performances","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":1,"sentence":"Finally, we show that using multiple boundary types at once in a hierarchical structure, by which low-level segments are used to recompose high-level segments, is beneficial and yields better results than using lowlevel or high-level segments in isolation","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":2,"sentence":"In this paper, we develop a method for grounding medical text into a physically meaningful and interpretable space corresponding to a human atlas","offset":0,"pro":0,"labels":"PUR"},{"idx":2,"sentence":"We build on text embedding architectures such as BERT and introduce a loss function that allows us to reason about the semantic and spatial relatedness of medical texts by learning a projection of the embedding into a 3D space representing the human body","offset":1,"pro":0.25,"labels":"MTD"},{"idx":2,"sentence":"We quantitatively and qualitatively demonstrate that our proposed method learns a context sensitive and spatially aware mapping, in both the inter-organ and intra-organ sense, using a large scale medical text dataset from the “Large-scale online biomedical semantic indexing” track of the 2020 BioASQ challenge","offset":2,"pro":0.5,"labels":"CLN"},{"idx":2,"sentence":"We extend our approach to a self-supervised setting, and find it to be competitive with a classification based method, and a fully supervised variant of approach","offset":3,"pro":0.75,"labels":"CLN"},{"idx":3,"sentence":"This paper is about learning word representations using grammatical type information","offset":0,"pro":0,"labels":"PUR"},{"idx":3,"sentence":"We use the syntactic types of Combinatory Categorial Grammar to develop multilinear representations, i.e. maps with n arguments, for words with different functional types","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":3,"sentence":"The multilinear maps of words compose with each other to form sentence representations","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":3,"sentence":"We extend the skipgram algorithm from vectors to multilinear maps to learn these representations and instantiate it on unary and binary maps for transitive verbs","offset":3,"pro":0.5,"labels":"MTD"},{"idx":3,"sentence":"These are evaluated on verb and sentence similarity and disambiguation tasks and a subset of the SICK relatedness dataset","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":3,"sentence":"Our model performs better than previous typedriven models and is competitive with state of the art representation learning methods such as BERT and neural sentence encoders","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":4,"sentence":"This article establishes that, unlike the legacy tf*idf representation, recent natural language representations (word embedding vectors) tend to exhibit a so-called concentration of measure phenomenon, in the sense that, as the representation size p and database size n are both large, their behavior is similar to that of large dimensional Gaussian random vectors","offset":0,"pro":0,"labels":"CLN"},{"idx":4,"sentence":"This phenomenon may have important consequences as machine learning algorithms for natural language data could be amenable to improvement","offset":1,"pro":0.3333333333333333,"labels":"IMP"},{"idx":4,"sentence":"thereby providing new theoretical insights into the field of natural language processing","offset":2,"pro":0.6666666666666666,"labels":"IMP"},{"idx":5,"sentence":"Previous work has shown that artificial neural agents naturally develop surprisingly nonefficient codes","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"This is illustrated by the fact that in a referential game involving a speaker and a listener neural networks optimizing accurate transmission over a discrete channel, the emergent messages fail to achieve an optimal length","offset":1,"pro":0.2,"labels":"BAC"},{"idx":5,"sentence":"Furthermore, frequent messages tend to be longer than infrequent ones, a pattern contrary to the Zipf Law of Abbreviation (ZLA) observed in all natural languages","offset":2,"pro":0.4,"labels":"BAC"},{"idx":5,"sentence":"Here, we show that near-optimal and ZLA-compatible messages can emerge, but only if both the speaker and the listener are modified","offset":3,"pro":0.6,"labels":"CLN"},{"idx":5,"sentence":"We hence introduce a new communication system, “LazImpa”, where the speaker is made increasingly lazy, i.e., avoids long messages, and the listener impatient, i.e., seeks to guess the intended content as soon as possible","offset":4,"pro":0.8,"labels":"CTN"},{"idx":6,"sentence":"Many tasks are considered to be ‘solved’ in the computational linguistics literature, but the corresponding algorithms operate in ways which are radically different from human cognition","offset":0,"pro":0,"labels":"GAP"},{"idx":6,"sentence":"I illustrate this by coming back to the notion of semantic competence, which includes basic linguistic skills encompassing both referential phenomena and generic knowledge, in particular a) the ability to denote, b) the mastery of the lexicon, or c) the ability to model one’s language use on others","offset":1,"pro":0.14285714285714285,"labels":"MTD"},{"idx":6,"sentence":"Even though each of those faculties has been extensively tested individually, there is still no computational model that would account for their joint acquisition under the conditions experienced by a human","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":6,"sentence":"In this paper, I focus on one particular aspect of this problem: the amount of linguistic data available to the child or machine","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":6,"sentence":"I show that given the first competence mentioned above (a denotation function), the other two can in fact be learned from very limited data (2.8M token), reaching state-of-theart performance","offset":4,"pro":0.5714285714285714,"labels":"CLN"},{"idx":6,"sentence":"I argue that both the nature of the data and the way it is presented to the system matter to acquisition","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":6,"sentence":" I show that given the first competence mentioned above (a denotation function), the other two can in fact be learned from very limited data (2.8M token), reaching state-of-theart performance.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":7,"sentence":"Annotation styles express guidelines that direct human annotators by explicitly stating the rules to follow when creating gold standard annotations of text corpora","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"These guidelines not only shape the gold standards they help create, but also influence the training and evaluation of Named Entity Linking (NEL) tools, since different annotation styles correspond to divergent views on the entities present in a document","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":7,"sentence":"Such divergence is particularly relevant for texts from the media domain containing references to creative works","offset":2,"pro":0.2857142857142857,"labels":"BAC"},{"idx":7,"sentence":"This paper presents a corpus of 1000 annotated documents from sources such as Wikipedia, TVTropes and WikiNews that are organized in ten partitions","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":7,"sentence":"Each document contains multiple gold standard annotations representing various annotation styles","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":7,"sentence":"The corpus is used to evaluate a series of Named Entity Linking tools in order to understand the impact of the differences in annotation styles on the reported accuracy when processing highly ambiguous entities such as names of creative works","offset":5,"pro":0.7142857142857143,"labels":"PUR"},{"idx":7,"sentence":"Relaxed annotation guidelines that include overlap styles, for instance, lead to better results across all tools","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":8,"sentence":"Vector space models of words have long been claimed to capture linguistic regularities as simple vector translations, but problems have been raised with this claim","offset":0,"pro":0,"labels":"GAP"},{"idx":8,"sentence":"We decompose and empirically analyze the classic arithmetic word analogy test, to motivate two new metrics that address the issues with the standard test, and which distinguish between class-wise offset concentration (similar directions between pairs of words drawn from different broad classes, such as France– London, China–Ottawa, . . . ) and pairing consistency (the existence of a regular transformation between correctly-matched pairs such as France:Paris::China:Beijing)","offset":1,"pro":0.3333333333333333,"labels":"PUR"},{"idx":8,"sentence":"We show that, while the standard analogy test is flawed, several popular word embeddings do nevertheless encode linguistic regularities","offset":2,"pro":0.6666666666666666,"labels":"CLN"},{"idx":9,"sentence":"What do people know when they know the meaning of words?","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"Word associations have been widely used to tap into lexical representations and their structure, as a way of probing semantic knowledge in humans","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":9,"sentence":"We investigate whether current word embedding spaces (contextualized and uncontextualized) can be considered good models of human lexical knowledge by studying whether they have comparable characteristics to human association spaces","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":9,"sentence":"We study the three properties of association rank, asymmetry of similarity and triangle inequality","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":9,"sentence":"We find that word embeddings are good models of some word associations properties","offset":4,"pro":0.5714285714285714,"labels":"CLN"},{"idx":9,"sentence":"They replicate well human associations between words, and, like humans, their context-aware variants show violations of the triangle inequality","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":9,"sentence":"While they do show asymmetry of similarities, their asymmetries do not map those of human association norms.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":0,"sentence":"he meaning of a word is closely linked to sociocultural factors that can change over time and location, resulting in corresponding meaning changes. Taking a global view of words and their meanings in a widely used language, such as English, may require us to capture more refined semantics for use in time-specific or location-aware situations, such as the study of cultural trends or language use","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"owever, popular vector representations for words do not adequately include temporal or spatial information.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":0,"sentence":"In this work, we present a model for learning word representation conditioned on time and location","offset":2,"pro":0.25,"labels":"PUR"},{"idx":0,"sentence":"n addition to capturing meaning changes over time and location, we require that the resulting word embeddings retain salient semantic and geometric properties","offset":3,"pro":0.375,"labels":"MTD"},{"idx":0,"sentence":"We train our model on timeand locationstamped corpora, ","offset":4,"pro":0.5,"labels":"MTD"},{"idx":0,"sentence":"show using both quantitative and qualitative evaluations that it can capture semantics across time and locations.","offset":5,"pro":0.625,"labels":"RST"},{"idx":0,"sentence":"We note that our model compares favorably with the state-of-the-art for time-specific embedding","offset":6,"pro":0.75,"labels":"CLN"},{"idx":0,"sentence":"nd serves as a new benchmark for location-specific embeddings","offset":7,"pro":0.875,"labels":"CTN"},{"idx":1,"sentence":"While neural networks with attention mechanisms have achieved superior performance on many natural language processing tasks","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"it remains unclear to which extent learned attention resembles human visual attention","offset":1,"pro":0.125,"labels":"GAP"},{"idx":1,"sentence":"In this paper, we propose a new method that leverages eye-tracking data to investigate the relationship between human visual attention and neural attention in machine reading comprehension.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":1,"sentence":"o this end, we introduce a novel 23 participant eye tracking dataset MQA-RC, in which participants read movie plots and answered pre-defined question","offset":3,"pro":0.375,"labels":"MTD"},{"idx":1,"sentence":"We compare state of the art networks based on long shortterm memory (LSTM), convolutional neural models (CNN) and XLNet Transformer architectures","offset":4,"pro":0.5,"labels":"MTD"},{"idx":1,"sentence":"We find that higher similarity to human attention and performance significantly correlates to the LSTM and CNN model","offset":5,"pro":0.625,"labels":"RST"},{"idx":1,"sentence":"However, we show this relationship does not hold true for the XLNet models – despite the fact that the XLNet performs best on this challenging task.","offset":6,"pro":0.75,"labels":"RST"},{"idx":1,"sentence":"Our results suggest that different architectures seem to learn rather different neural attention strategies and similarity of neural to human attention does not guarantee best performance","offset":7,"pro":0.875,"labels":"CLN"},{"idx":2,"sentence":"Linear logic and the linear λ-calculus have a long standing tradition in the study of natural language form and meaning","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"Among the proof calculi of linear logic, proof nets are of particular interest, offering an attractive geometric representation of derivations that is unburdened by the bureaucratic complications of conventional prooftheoretic formats. ","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":2,"sentence":"Building on recent advances in set-theoretic learning, we propose a neural variant of proof nets based on Sinkhorn networks","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":2,"sentence":"which allows us to translate parsing as the problem of extracting syntactic primitives and permuting them into alignment","offset":3,"pro":0.5,"labels":"CTN"},{"idx":2,"sentence":"Our methodology induces a batch-efficient, end-to-end differentiable architecture that actualizes a formally grounded yet highly efficient neuro-symbolic parse","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":2,"sentence":"We test our approach on Æthel, a dataset of typelogical derivations for written Dutch, where it manages to correctly transcribe raw text sentences into proofs and terms of the linear λ-calculus with an accuracy of as high as 70%","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":3,"sentence":"Pre-trained Transformer-based neural architectures have consistently achieved state-of-the-art performance in the Natural Language Inference (NLI) task.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"ince NLI examples encompass a variety of linguistic, logical, and reasoning phenomena, it remains unclear as to which specific concepts are learnt by the trained systems and where they can achieve strong generalization","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":3,"sentence":"o investigate this question, we propose a taxonomic hierarchy of categories that are relevant for the NLI task.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":3,"sentence":"We introduce TAXINLI, a new dataset, that has 10k examples from the MNLI dataset (Williams et al., 2018) with these taxonomic labels. ","offset":3,"pro":0.5,"labels":"PUR"},{"idx":3,"sentence":"hrough various experiments on TAXINLI, we observe that whereas for certain taxonomic categories SOTA neural models have achieved near perfect accuracies—a large jump over the previous models—some categories still remain difficul","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":3,"sentence":"Our work adds to the growing body of literature that shows the gaps in the current NLI systems and datasets through a systematic presentation and analysis of reasoning categories.","offset":5,"pro":0.8333333333333334,"labels":"CTN"},{"idx":4,"sentence":"Crime reporting is a prevalent form of journalism with the power to shape public perceptions and social policies","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"How does the language of these reports act on readers?","offset":1,"pro":0.125,"labels":"GAP"},{"idx":4,"sentence":"We seek to address this question with the SuspectGuilt Corpus of annotated crime stories from Englishlanguage newspapers in the U.S. For SuspectGui","offset":2,"pro":0.25,"labels":"PUR"},{"idx":4,"sentence":"annotators read short crime articles and provided text-level ratings concerning the guilt of the main suspect as well as span-level annotations indicating which parts of the story they felt most influenced their ratings","offset":3,"pro":0.375,"labels":"MTD"},{"idx":4,"sentence":"uspectGuilt thus provides a rich picture of how linguistic choices affect subjective guilt judgments","offset":4,"pro":0.5,"labels":"CTN"},{"idx":4,"sentence":"e use SuspectGuilt to train and assess predictive models which validate the usefulness of the corpus,","offset":5,"pro":0.625,"labels":"MTD"},{"idx":4,"sentence":"how that these models benefit from genre pretraining and joint supervision from the text-level ratings and spanlevel annotations","offset":6,"pro":0.75,"labels":"RST"},{"idx":4,"sentence":"Such models might be used as tools for understanding the societal effects of crime reportin","offset":7,"pro":0.875,"labels":"CLN"},{"idx":5,"sentence":"We present an analysis on the effect UPOS accuracy has on parsing performance","offset":0,"pro":0,"labels":"PUR"},{"idx":5,"sentence":"Results suggest that leveraging UPOS tags as features for neural parsers requires a prohibitively high tagging accuracy and that the use of gold tags offers a non-linear increase in performance, suggesting some sort of exceptionali","offset":1,"pro":0.3333333333333333,"labels":"RST"},{"idx":5,"sentence":"We also investigate what aspects of predicted UPOS tags impact parsing accuracy the most, highlighting some potentially meaningful linguistic facets of the problem","offset":2,"pro":0.6666666666666666,"labels":"PUR"},{"idx":6,"sentence":"We present a method for classifying syntactic errors in learner language, namely errors whose correction alters the morphosyntactic structure of a sentence","offset":0,"pro":0,"labels":"PUR"},{"idx":6,"sentence":"The methodology builds on the established Universal Dependencies syntactic representation scheme, and provides complementary information to other error-classification systems","offset":1,"pro":0.25,"labels":"CTN"},{"idx":6,"sentence":"Unlike existing error classification methods, our method is applicable across languages, which we showcase by producing a detailed picture of syntactic errors in learner English and learner Russian","offset":2,"pro":0.5,"labels":"CLN"},{"idx":6,"sentence":"We further demonstrate the utility of the methodology for analyzing the outputs of leading Grammatical Error Correction (GEC) systems.","offset":3,"pro":0.75,"labels":"CTN"},{"idx":7,"sentence":"Sentence encoders map sentences to real valued vectors for use in downstream application","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"o peek into these representations-e.g., to increase interpretability of their results—probing tasks have been designed which query them for linguistic knowledge","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":7,"sentence":"However, designing probing tasks for lesser-resourced languages is tricky, because these often lack large-scale annotated data or (high-quality) dependency parsers as a prerequisite of probing task design in Englis","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":7,"sentence":"To investigate how to probe sentence embeddings in such cases, we investigate sensitivity of probing task results to structural design choices, conducting the first such large scale study.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":7,"sentence":"e show that design choices like size of the annotated probing dataset and type of classifier used for evaluation do (sometimes substantially) influence probing outcome","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":7,"sentence":"We then probe embeddings in a multilingual setup with design choices that lie in a ‘stable region’, as we identify for English, and find that results on English do not transfer to other languages","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":7,"sentence":"Fairer and more comprehensive sentence-level probing evaluation should thus be carried out on multiple languages in the future","offset":6,"pro":0.8571428571428571,"labels":"IMP"},{"idx":8,"sentence":"Semantic relations are core to how humans understand and express concepts in the real world using language. Recently, there has been a thread of research aimed at modeling these relations by learning vector representations from text corpora","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"Most of these approaches focus strictly on leveraging the co-occurrences of relationship word pairs within sentences","offset":1,"pro":0.2,"labels":"BAC"},{"idx":8,"sentence":"In this paper, we investigate the hypothesis that examples of a lexical relation in a corpus are fundamental to a neural word embedding’s ability to complete analogies involving the relation","offset":2,"pro":0.4,"labels":"PUR"},{"idx":8,"sentence":"Our experiments, in which we remove all known examples of a relation from training corpora, show only marginal degradation in analogy completion performance involving the removed relation","offset":3,"pro":0.6,"labels":"RST"},{"idx":8,"sentence":"This finding enhances our understanding of neural word embeddings, showing that co-occurrence information of a particular semantic relation is the not the main source of their structural regularit","offset":4,"pro":0.8,"labels":"CLN"},{"idx":9,"sentence":"One-anaphora has figured prominently in theoretical linguistic literature","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"but computational linguistics research on the phenomenon is sparse","offset":1,"pro":0.125,"labels":"GAP"},{"idx":9,"sentence":"ot only that, the long standing linguistic controversy between the determinative and the nominal anaphoric element one has propagated in the limited body of computational work on one-anaphora resolution, making this task harder than it is.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":9,"sentence":"n the present paper, we resolve this","offset":3,"pro":0.375,"labels":"PUR"},{"idx":9,"sentence":"by drawing from an adequate linguistic analysis of the word one in different syntactic environments onc","offset":4,"pro":0.5,"labels":"MTD"},{"idx":9,"sentence":"We prepare an annotated corpus marking actual instances of one-anaphora with their textual antecedents","offset":5,"pro":0.625,"labels":"MTD"},{"idx":9,"sentence":"use the annotations to experiment with state-of-the art neural models for one-anaphora resolut","offset":6,"pro":0.75,"labels":"MTD"},{"idx":9,"sentence":"Apart from presenting a strong neural baseline for this task, we contribute a gold-standard corpus, which is, to the best of our knowledge, the biggest resource on one-anaphora till date","offset":7,"pro":0.875,"labels":"CTN"},{"idx":0,"sentence":"In this work, we analyze how human gaze during reading comprehension is conditioned on the given reading comprehension question, and whether this signal can be beneficial for machine reading comprehension","offset":0,"pro":0,"labels":"PUR"},{"idx":0,"sentence":"To this end, we collect a new eye-tracking dataset with a large number of participants engaging in a multiple choice reading comprehension task","offset":1,"pro":0.2,"labels":"MTD"},{"idx":0,"sentence":"Our analysis of this data reveals increased fixation times over parts of the text that are most relevant for answering the question","offset":2,"pro":0.4,"labels":"CLN"},{"idx":0,"sentence":"Motivated by this finding, we propose making automated reading comprehension more human-like by mimicking human information-seeking reading behavior during reading comprehension","offset":3,"pro":0.6,"labels":"MTD"},{"idx":0,"sentence":"We demonstrate that this approach leads to performance gains on multiple choice question answering in English for a state-of-the-art reading comprehension model","offset":4,"pro":0.8,"labels":"CLN"},{"idx":1,"sentence":"We present a new summarisation task, taking scientific articles and producing journal tableof-contents entries in the chemistry domain","offset":0,"pro":0,"labels":"PUR"},{"idx":1,"sentence":"These are oneor two-sentence author-written summaries that present the key findings of a paper","offset":1,"pro":0.25,"labels":"BAC"},{"idx":1,"sentence":"This is a first look at this summarisation task with an open access publication corpus consisting of titles and abstracts, as input texts, and short author-written advertising blurbs, as the ground truth","offset":2,"pro":0.5,"labels":"RST"},{"idx":1,"sentence":"We introduce the dataset and evaluate it with state-of-the-art summarisation methods","offset":3,"pro":0.75,"labels":"MTD"},{"idx":2,"sentence":"Recurrent Neural Networks (RNNs) have been shown to capture various aspects of syntax from raw linguistic input","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"In most previous experiments, however, learning happens over unrealistic corpora, which do not reflect the type and amount of data a child would be exposed to","offset":1,"pro":0.2,"labels":"GAP"},{"idx":2,"sentence":"This paper remedies this state of affairs by training a Long Short-Term Memory network (LSTM) over a realistically sized subset of child-directed input","offset":2,"pro":0.4,"labels":"PUR"},{"idx":2,"sentence":"The behaviour of the network is analysed over time using a novel methodology which consists in quantifying the level of grammatical abstraction in the model’s generated output (its ‘babbling’), compared to the language it has been exposed to","offset":3,"pro":0.6,"labels":"MTD"},{"idx":2,"sentence":"We show that the LSTM indeed abstracts new structures as learning proceeds","offset":4,"pro":0.8,"labels":"CLN"},{"idx":3,"sentence":"How can people communicate successfully while keeping resource costs low in the face of ambiguity?","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"We present a principled theoretical analysis comparing two strategies for disambiguation in communication: (i) pragmatic reasoning, where communicators reason about each other, and (ii) other-initiated repair, where communicators signal and resolve trouble interactively.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":3,"sentence":"Using agent-based simulations and computational complexity analyses, we compare the efficiency of these strategies in terms of communicative success, computation cost and interaction cost.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":3,"sentence":"We show that agents with a simple repair mechanism can increase efficiency, compared to pragmatic agents, by reducing their computational burden at the cost of longer interactions.","offset":3,"pro":0.6,"labels":"CLN"},{"idx":3,"sentence":"We also find that efficiency is highly contingent on the mechanism, highlighting the importance of explicit formalisation and computational rigour.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":4,"sentence":"Classical accounts of child language learning invoke memory limits as a pressure to discover sparse, language-like representations of speech","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"while more recent proposals stress the importance of prediction for language learning","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":4,"sentence":"In this study, we propose a broadcoverage unsupervised neural network model to test memory and prediction as sources of signal by which children might acquire language directly from the perceptual stream.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":4,"sentence":"Our model embodies several likely properties of real-time human cognition: it is strictly incremental, it encodes speech into hierarchically organized labeled segments, it allows interactive top-down and bottom-up information flow, it attempts to model its own sequence of latent representations, and its objective function only recruits local signals that are plausibly supported by human working memory capacity. ","offset":3,"pro":0.5,"labels":"MTD"},{"idx":4,"sentence":"We show that much phonemic structure is learnable from unlabeled speech on the basis of these local signals.","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":4,"sentence":"We further show that remembering the past and predicting the future both contribute to the linguistic content of acquired representations, and that these contributions are at least partially complementary.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":5,"sentence":"The CoNLL-2003 corpus for Englishlanguage named entity recognition (NER) is one of the most influential corpora for NER model research. ","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"A large number of publications, including many landmark works, have used this corpus as a source of ground truth for NER tasks.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":5,"sentence":"In this paper, we examine this corpus and identify over 1300 incorrect labels (out of 35089 in the corpus).","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":5,"sentence":"In particular, the number of incorrect labels in the test fold is comparable to the number of errors that state-of-the-art models make when running inference over this corpus.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":5,"sentence":"We describe the process by which we identified these incorrect labels, using novel variants of techniques from semi-supervised learning.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":5,"sentence":"We also summarize the types of errors that we found, and we revisit several recent results in NER in light of the corrected data.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":5,"sentence":"Finally, we show experimentally that our corrections to the corpus have a positive impact on three state-ofthe-art models.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":6,"sentence":"This paper investigates contextual language models, which produce token representations, as a resource for lexical semantics at the word or type level.","offset":0,"pro":0,"labels":"PUR"},{"idx":6,"sentence":"We construct multi-prototype word embeddings from bert-base-uncased (Devlin et al., 2018)","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":6,"sentence":"These embeddings retain contextual knowledge that is critical for some type-level tasks, while being less cumbersome and less subject to outlier effects than exemplar models.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":6,"sentence":"Similarity and relatedness estimation, both type-level tasks, benefit from this contextual knowledge, indicating the context-sensitivity of these processes.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":6,"sentence":"Our findings provide important insight into the interpretability of BERT: layer 7 approximates semantic similarity, while the final layer (11) approximates relatedness","offset":4,"pro":0.6666666666666666,"labels":"CTN"},{"idx":6,"sentence":"BERT’s token level knowledge also allows the testing of a type-level hypothesis about lexical abstractness, demonstrating the relationship between token-level behavior and type-level concreteness ratings.","offset":5,"pro":0.8333333333333334,"labels":"IMP"},{"idx":7,"sentence":"Some have argued that word orders which are more difficult to process should be rarer cross-linguistically.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"Our current study fails to replicate the results of Maurits, Navarro, and Perfors (2010), who used an entropy-based Uniform Information Density (UID) measure to moderately predict the Greenbergian typology of transitive word orders.","offset":1,"pro":0.25,"labels":"GAP"},{"idx":7,"sentence":"We additionally report an inability of three measures of processing difficulty — entropy-based UID, surprisal-based UID, and pointwise mutual information — to correctly predict the correct typological distribution, using transitive constructions from 20 languages in the Universal Dependencies project (version 2.5).","offset":2,"pro":0.5,"labels":"PUR"},{"idx":7,"sentence":"However, our conclusions are limited by data sparsity.","offset":3,"pro":0.75,"labels":"RST"},{"idx":8,"sentence":"This work presents a detailed analysis of translation errors perceived by readers as comprehensibility and/or adequacy issues. ","offset":0,"pro":0,"labels":"PUR"},{"idx":8,"sentence":"Of all major adequacy errors, 30% were fully comprehensible, thus fully misleading the reader to accept the incorrect information.","offset":1,"pro":0.2,"labels":"RST"},{"idx":8,"sentence":"Another 25% of major adequacy errors were perceived as almost comprehensible, thus being potentially misleading. ","offset":2,"pro":0.4,"labels":"RST"},{"idx":8,"sentence":"Further analysis of misleading translations revealed that the most frequent error types are ambiguity, mistranslation, noun phrase error, word-by-word translation, untranslated word, subject-verb agreement, and spelling error in the source text.","offset":3,"pro":0.6,"labels":"CLN"},{"idx":8,"sentence":"However, none of these error types appears exclusively in misleading translations, but are also frequent in fully incorrect (incomprehensible inadequate) and discarded correct (incomprehensible adequate) translations. ","offset":4,"pro":0.8,"labels":"CLN"},{"idx":9,"sentence":"Grammatical gender is assigned to nouns differently in different languages.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"Are all factors that influence gender assignment idiosyncratic to languages or are there any that are universal? ","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":9,"sentence":"In both experiments, we predict the gender of nouns in language X using a classifier trained on the nouns of language Y, and take the classifier’s accuracy as a measure of transferability of gender systems.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":9,"sentence":"First, we show that for 22 Indo-European languages the transferability decreases as the phylogenetic distance increases.","offset":3,"pro":0.42857142857142855,"labels":"CLN"},{"idx":9,"sentence":"This correlation supports the claim that some gender assignment factors are idiosyncratic, and as the languages diverge, the proportion of shared inherited idiosyncrasies diminishes.","offset":4,"pro":0.5714285714285714,"labels":"CLN"},{"idx":9,"sentence":"Second, we show that when the classifier is trained on two Afro-Asiatic languages and tested on the same 22 Indo-European languages (or vice versa), its performance is still significantly above the chance baseline, thus showing that universal factors exist and, moreover, can be captured by word embeddings","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":9,"sentence":"When the classifier is tested across families and on inanimate nouns only, the performance is still above baseline, indicating that the universal factors are not limited to biological sex.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":0,"sentence":"Temporal knowledge graphs store the dynamics of entities and relations during a time period. ","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"However, typical temporal knowledge graphs often suffer from incomplete dynamics with missing facts in real-world scenarios.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":0,"sentence":" Hence, modeling temporal knowledge graphs to complete the missing facts is important. ","offset":2,"pro":0.3333333333333333,"labels":"BAC"},{"idx":0,"sentence":"In this paper, we tackle the temporal knowledge graph completion task by proposing \\textbf{TempCaps}, which is a \\textbf{Caps}ule network-based embedding model for \\textbf{Temp}oral knowledge graph completion. ","offset":3,"pro":0.5,"labels":"PUR"},{"idx":0,"sentence":"TempCaps models temporal knowledge graphs by introducing a novel dynamic routing aggregator inspired by Capsule Networks. Specifically, TempCaps builds entity embeddings by dynamically routing retrieved temporal relation and neighbor information.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":0,"sentence":"Experimental results demonstrate that TempCaps reaches state-of-the-art performance for temporal knowledge graph completion. Additional analysis also shows that TempCaps is efficient.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":1,"sentence":"It consists of a generator trained to extract spans from an input sentence, and a discriminator trained to determine whether a span comes from the generator, or from the gazetteer.","offset":0,"pro":0,"labels":"MTD"},{"idx":1,"sentence":"We evaluate the method on English newswire data and compare it against supervised, weakly-supervised, and unsupervised methods. ","offset":1,"pro":0.25,"labels":"MTD"},{"idx":1,"sentence":"Our results suggest that the model can generate spans that overlap well, but an additional filtering mechanism is required.","offset":2,"pro":0.5,"labels":"CLN"},{"idx":1,"sentence":"In other cases, it generates spans that are valid but differ from the benchmark.","offset":3,"pro":0.75,"labels":"RST"},{"idx":2,"sentence":"Topic models are some of the most popular ways to represent textual data in an interpret-able manner. ","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"Recently, advances in deep generative models, specifically auto-encoding variational Bayes (AEVB), have led to the introduction of unsupervised neural topic models, which leverage deep generative models as opposed to traditional statistics-based topic models. ","offset":1,"pro":0.25,"labels":"BAC"},{"idx":2,"sentence":"We extend upon these neural topic models by introducing the Label-Indexed Neural Topic Model (LI-NTM), which is, to the extent of our knowledge, the first effective upstream semi-supervised neural topic model.","offset":2,"pro":0.5,"labels":"PUR"},{"idx":2,"sentence":"We find that LI-NTM outperforms existing neural topic models in document reconstruction benchmarks, with the most notable results in low labeled data regimes and for data-sets with informative labels; ","offset":3,"pro":0.75,"labels":"RST"},{"idx":3,"sentence":"We propose the neural string edit distance model for string-pair matching and string transduction based on learnable string edit distance.","offset":0,"pro":0,"labels":"PUR"},{"idx":3,"sentence":"We modify the original expectation-maximization learned edit distance algorithm into a differentiable loss function, ","offset":1,"pro":0.125,"labels":"MTD"},{"idx":3,"sentence":"allowing us to integrate it into a neural network providing a contextual representation of the input. ","offset":2,"pro":0.25,"labels":"RST"},{"idx":3,"sentence":"We evaluate on cognate detection, transliteration, and grapheme-to-phoneme conversion,","offset":3,"pro":0.375,"labels":"MTD"},{"idx":3,"sentence":"and show that we can trade off between performance and interpretability in a single framework. ","offset":4,"pro":0.5,"labels":"RST"},{"idx":3,"sentence":"Using static embeddings and a slightly different loss function, ","offset":5,"pro":0.625,"labels":"MTD"},{"idx":3,"sentence":"we force interpretability, at the expense of an accuracy drop.","offset":6,"pro":0.75,"labels":"RST"},{"idx":3,"sentence":"we match the performance of state-of-the-art string-pair matching models.","offset":7,"pro":0.875,"labels":"RST"},{"idx":4,"sentence":"Transformers{'} quadratic complexity with respect to the input sequence length has motivated a body of work on efficient sparse approximations to softmax. ","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"however this approach still requires quadratic computation.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":4,"sentence":"We experiment with three variants of our method, based on distances, quantization, and clustering, on two tasks: machine translation (attention in the decoder) and masked language modeling (encoder-only).","offset":2,"pro":0.4,"labels":"MTD"},{"idx":4,"sentence":"Our work provides a new angle to study model efficiency ","offset":3,"pro":0.6,"labels":"CTN"},{"idx":4,"sentence":"This allows for detailed comparison between different models along their Pareto curves, ","offset":4,"pro":0.8,"labels":"CTN"},{"idx":5,"sentence":"Zero-shot relation extraction (ZSRE) aims to predict target relations that cannot be observed during training. ","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":" This study proposes a new model incorporating discriminative embedding learning for both sentences and semantic relations.","offset":1,"pro":0.25,"labels":"PUR"},{"idx":5,"sentence":"In addition, a self-adaptive comparator network is used to judge whether the relationship between a sentence and a relation is consistent.","offset":2,"pro":0.5,"labels":"PUR"},{"idx":5,"sentence":"Experimental results on two benchmark datasets showed that the proposed method significantly outperforms the state-of-the-art methods.","offset":3,"pro":0.75,"labels":"RST"},{"idx":6,"sentence":"While both extractive and generative readers have been successfully applied to the Question Answering (QA) task, little attention has been paid toward the systematic comparison of them.","offset":0,"pro":0,"labels":"GAP"},{"idx":6,"sentence":"Characterizing the strengths and weaknesses of the two readers is crucial not only for making a more informed reader selection in practice but also for developing a deeper understanding to foster further research on improving readers in a principled manner. ","offset":1,"pro":0.2,"labels":"BAC"},{"idx":6,"sentence":"To be aligned with the state-of-the-art, we explore nine transformer-based large pre-trained language models (PrLMs) as backbone architectures.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":6,"sentence":"Furthermore, we organize our findings under two main categories: (1) keeping the architecture invariant, and (2) varying the underlying PrLMs. ","offset":3,"pro":0.6,"labels":"MTD"},{"idx":6,"sentence":"We also study the effect of multi-task learning on the two types of readers varying the underlying PrLMs and perform qualitative and quantitative diagnosis to provide further insights into future directions in modeling better readers.","offset":4,"pro":0.8,"labels":"PUR"},{"idx":7,"sentence":"However, most domain adaptation methods focus on fine-tuning or training the entire or part of the model on every new domain, which can be costly. ","offset":0,"pro":0,"labels":"GAP"},{"idx":7,"sentence":"A drawback of these retrieval-augmented models, however, is that they tend to be substantially slower. ","offset":1,"pro":0.3333333333333333,"labels":"GAP"},{"idx":7,"sentence":"We adapt the methods recently proposed by He et al. (2021) for language modeling, and introduce a simple but effective caching strategy that avoids performing retrieval when similar contexts have been seen before. ","offset":2,"pro":0.6666666666666666,"labels":"MTD"},{"idx":8,"sentence":"We propose a novel framework to conduct field extraction from forms with unlabeled data.","offset":0,"pro":0,"labels":"PUR"},{"idx":8,"sentence":"To bootstrap the training process, we develop a rule-based method for mining noisy pseudo-labels from unlabeled forms.","offset":1,"pro":0.25,"labels":"MTD"},{"idx":8,"sentence":"Using the supervisory signal from the pseudo-labels, we extract a discriminative token representation from a transformer-based model by modeling the interaction between text in the form. ","offset":2,"pro":0.5,"labels":"MTD"},{"idx":8,"sentence":"Experimental results demonstrate the effectiveness of our framework.","offset":3,"pro":0.75,"labels":"CLN"},{"idx":9,"sentence":"Recently neural network based approaches to knowledge-intensive NLP tasks, such as question answering, started to rely heavily on the combination of neural retrievers and readers. ","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"Our results show that PCA is an easy solution that requires very little data and is only slightly worse than autoencoders, which are less stable.","offset":1,"pro":0.2,"labels":"RST"},{"idx":9,"sentence":"All methods are sensitive to pre- and post-processing and data should always be centered and normalized both before and after dimension reduction.","offset":2,"pro":0.4,"labels":"RST"},{"idx":9,"sentence":"Finally, we show that it is possible to combine PCA with using 1bit per dimension. ","offset":3,"pro":0.6,"labels":"RST"},{"idx":9,"sentence":"On HotpotQA we systematically investigate reducing the size of the KB index ","offset":4,"pro":0.8,"labels":"PUR"},{"idx":10,"sentence":"We present MozoLM, an open-source language model microservice package intended for use in AAC text-entry applications, with a particular focus on the design principles of the library. ","offset":0,"pro":0,"labels":"PUR"},{"idx":10,"sentence":"Some simulation experiments demonstrating the benefits of personalized language model ensembling via the library are presented.","offset":1,"pro":0.5,"labels":"RST"},{"idx":11,"sentence":"Our information-theoretic analysis of ColorCode simulations shows that it is efficient in extracting information from the user, even in the presence of errors, achieving nearly optimal error correction. ","offset":0,"pro":0,"labels":"RST"},{"idx":12,"sentence":"Robitaille (2010) wrote {`}if all technology companies have accessibility in their mind then people with disabilities won{'}t be left behind.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"Current technology has come a long way from where it stood decades ago","offset":1,"pro":0.25,"labels":"BAC"},{"idx":12,"sentence":"however, researchers and manufacturers often do not include people with disabilities in the design process and tend to accommodate them after the fact. ","offset":2,"pro":0.5,"labels":"GAP"},{"idx":12,"sentence":"We believe end users should be part of the design process and that by bringing together experts and users, we can bridge the research/practice gap.","offset":3,"pro":0.75,"labels":"CLN"},{"idx":13,"sentence":"Conversations between a clinician and a patient, in natural conditions, are valuable sources of information for medical follow-up. ","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"Yet, it is not clear which model is the most efficient to detect and identify the speaker turns, especially for individuals with speech disorders. ","offset":1,"pro":0.2,"labels":"GAP"},{"idx":13,"sentence":"We designed and trained end-to-end neural network architectures to directly tackle this task from the raw signal and evaluate each approach under the same metric. ","offset":2,"pro":0.4,"labels":"MTD"},{"idx":13,"sentence":" Experimental results are reported on naturalistic clinical conversations between Psychologists and Interviewees, at different stages of Huntington{'}s disease, displaying a large panel of speech disorders. ","offset":3,"pro":0.6,"labels":"MTD"},{"idx":13,"sentence":"Finally, we observed that we could extract clinical markers directly from the automatic systems, highlighting the clinical relevance of our methods.","offset":4,"pro":0.8,"labels":"RST"},{"idx":14,"sentence":"In this study we compare two approaches (neural machine translation and edit-based) and the use of synthetic data for the task of translating normalised Swiss German ASR output into correct written Standard German for subtitles, with a special focus on syntactic differences. ","offset":0,"pro":0,"labels":"PUR"},{"idx":15,"sentence":"In this preliminary study, we present the French version of a translation system using the Arasaac pictographs and we investigate the strategies used by speech therapists to translate into pictographs. ","offset":0,"pro":0,"labels":"PUR"},{"idx":16,"sentence":"This paper outlines the ethical implications of text simplification within the framework of assistive systems. ","offset":0,"pro":0,"labels":"PUR"},{"idx":16,"sentence":"When using the latter as a motivation for research, it is important that the subsequent ethical implications be carefully considered. ","offset":1,"pro":0.3333333333333333,"labels":"CLN"},{"idx":16,"sentence":"as well as suggesting directions for future research and discussion based on the concerns raised.","offset":2,"pro":0.6666666666666666,"labels":"IMP"},{"idx":17,"sentence":"AI models underlying assistive technologies have been shown to contain biased stereotypes, including racial, gender, and disability biases.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"We build on this work to present a psychology-based stereotype assessment of the representation of disability, deafness, and blindness in BERT using the Stereotype Content Model. ","offset":1,"pro":0.5,"labels":"PUR"},{"idx":18,"sentence":"Conversational assistants are ubiquitous among the general population,","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"however, these systems have not had an impact on people with disabilities, or speech and language disorders, for whom basic day-to-day communication and social interaction is a huge struggle. ","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":18,"sentence":"To enable this population, we build a system that can represent them in a social conversation and generate responses that can be controlled by the users using cues/keywords.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":18,"sentence":"Our evaluation and user study shows that keyword-control on end-to-end response generation models is powerful and can enable and empower users with degenerative disorders to carry out their day-to-day communication.","offset":3,"pro":0.5,"labels":"CLN"},{"idx":18,"sentence":"We build models that can speed up this communication by suggesting relevant cues in the dialog response context.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":18,"sentence":"We also introduce a keyword-loss to lexically constrain the model response output. ","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":19,"sentence":"This paper describes three areas of assistive technology development which deploy the resources and speech technology for Irish (Gaelic), newly emerging from the ABAIR initiative.","offset":0,"pro":0,"labels":"PUR"},{"idx":19,"sentence":"Each of these is at a different stage of development and poses unique challenges: these are dis-cussed along with the approaches adopted to address them. ","offset":1,"pro":0.25,"labels":"MTD"},{"idx":19,"sentence":" Firstly, the sociolinguistic context and the needs of the community are essential considerations in setting priorities. Secondly, development needs to be language sensitive.","offset":2,"pro":0.5,"labels":"RST"},{"idx":19,"sentence":"The need for skilled researchers with a deep knowledge of Irish structure is illustrated in the case of (ii) and (iii), where aspects of Irish linguistic structure (phonological, morphological and grammatical) and the striking differences from English pose challenges for systems aimed at bilingual Irish-English users. ","offset":3,"pro":0.75,"labels":"RST"},{"idx":0,"sentence":"Detecting emotion in text allows social and computational scientists to study how people behave and react to online events","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"However, developing these tools for different languages requires data that is not always available. ","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":0,"sentence":"his paper collects the available emotion detection datasets across 19 languages. ","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":0,"sentence":"We train a multilingual emotion prediction model for social media data, XLM-EMO. ","offset":3,"pro":0.5,"labels":"MTD"},{"idx":0,"sentence":"The model shows competitive performance in a zero-shot setting, suggesting it is helpful in the context of low-resource languages.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":0,"sentence":"We release our model to the community so that interested researchers can directly use it.","offset":5,"pro":0.8333333333333334,"labels":"CTN"},{"idx":1,"sentence":"Over the years, the review helpfulness prediction task has been the subject of several works,","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"but remains being a challenging issue in Natural Language Processing, as results vary a lot depending on the domain, on the adopted features and on the chosen classification strategy. ","offset":1,"pro":0.2,"labels":"GAP"},{"idx":1,"sentence":"his paper attempts to evaluate the impact of content features and classification methods for two different domains. ","offset":2,"pro":0.4,"labels":"PUR"},{"idx":1,"sentence":"n particular, we run our experiments for a low resource language {--} Portuguese {--}, trying to establish a benchmark for this language.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":1,"sentence":"We show that simple features and classical classification methods are powerful for the task of helpfulness prediction, but are largely outperformed by a convolutional neural network-based solution","offset":4,"pro":0.8,"labels":"RST"},{"idx":2,"sentence":"This paper presents the results that were obtained from WASSA 2022 shared task on predicting empathy, emotion, and personality in reaction to news stories. ","offset":0,"pro":0,"labels":"PUR"},{"idx":2,"sentence":"Participants were given access to a dataset comprising empathic reactions to news stories where harm is done to a person, group, or other","offset":1,"pro":0.14285714285714285,"labels":"MTD"},{"idx":2,"sentence":"We summarize the methods and resources used by the participating teams","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":2,"sentence":"Participation was encouraged in four tracks: predicting empathy and distress scores, predicting emotion categories, predicting personality and predicting interpersonal reactivity.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":2,"sentence":"n total, 14 teams participated in the shared task.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":2,"sentence":"hese reactions consist of essays and Batson{'}s empathic concern and personal distress scores","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":2,"sentence":"he dataset was further extended in WASSA 2021 shared task to include news articles, person-level demographic information (e.g. age, gender), personality information, and Ekman{'}s six basic emotions at essay level","offset":6,"pro":0.8571428571428571,"labels":"MTD"},{"idx":3,"sentence":"Our system, IUCL, participated in the WASSA 2022 Shared Task on Empathy Detection and Emotion Classification.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"ur main goal in building this system is to investigate how the use of demographic attributes influences performance. ","offset":1,"pro":0.3333333333333333,"labels":"PUR"},{"idx":3,"sentence":"Our systems that use both text and demographic data are less competitive","offset":2,"pro":0.6666666666666666,"labels":"MTD"},{"idx":4,"sentence":"Emotion is the essential attribute of human beings. Perceiving and understanding emotions in a human-like manner is the most central part of developing emotional intelligence. ","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"his paper describes the contribution of the LingJing team{'}s method to the Workshop on Computational Approaches to Subjectivity, Sentiment {\\&} Social Media Analysis (WASSA) 2022 shared task on Emotion Classification. ","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":4,"sentence":"This paper describes the continual pre-training method for the masked language model (MLM) to enhance the DeBERTa pre-trained language model.","offset":2,"pro":0.3333333333333333,"labels":"IMP"},{"idx":4,"sentence":"Several training strategies are designed to further improve the final downstream performance including the data augmentation with the supervised transfer, child-tuning training, and the late fusion method.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":4,"sentence":"Extensive experiments on the emotional classification dataset show that the proposed method outperforms other state-of-the-art methods, demonstrating our method{'}s effectiveness.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":4,"sentence":"Moreover, our submission ranked Top-1 with all metrics in the evaluation phase for the Emotion Classification task","offset":5,"pro":0.8333333333333334,"labels":"IMP"},{"idx":5,"sentence":"This paper describes the participation of the SINAI research group at WASSA 2022 (Empathy and Personality Detection and Emotion Classification)","offset":0,"pro":0,"labels":"PUR"},{"idx":5,"sentence":"Our team ranked 2nd in the first track and 3rd in the second track.","offset":1,"pro":0.2,"labels":"RST"},{"idx":5,"sentence":"Specifically, we participate in Track 1 (Empathy and Distress predictions) and Track 2 (Emotion classification). ","offset":2,"pro":0.4,"labels":"MTD"},{"idx":5,"sentence":"For Track 1, a Transformer multi-output regression model is proposed.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":5,"sentence":"For Track 2, we aim to explore recent techniques based on Zero-Shot Learning models including a Natural Language Inference model and GPT-3, using them in an ensemble manner with a fine-tune RoBERTa model","offset":4,"pro":0.8,"labels":"MTD"},{"idx":6,"sentence":"This Track 2 sub-task focuses on building models which can predict a multi-class emotion label based on essays from news articles where a person, group or another entity is affected. ","offset":0,"pro":0,"labels":"PUR"},{"idx":6,"sentence":"We observe better results than our baseline models and achieve an accuracy of 0.619 and a macro F1 score of 0.520 on the emotion classification task.","offset":1,"pro":0.5,"labels":"RST"},{"idx":7,"sentence":"his paper describes our contribution to the WASSA 2022 shared task which handles this crucial task of emotion detection. ","offset":0,"pro":0,"labels":"PUR"},{"idx":7,"sentence":"e have to identify the following emotions: sadness, surprise, neutral, anger, fear, disgust, joy based on a given essay text. ","offset":1,"pro":0.3333333333333333,"labels":"MTD"},{"idx":7,"sentence":"Our codebase (https://bit.ly/WASSA{\\_}shared{\\_}task) and our WandB project (https://wandb.ai/acl{\\_}wassa{\\_}pictxmanipal/acl{\\_}wassa) is publicly available.","offset":2,"pro":0.6666666666666666,"labels":"MTD"},{"idx":8,"sentence":"Computational comprehension and identifying emotional components in language have been critical in enhancing human-computer connection in recent years.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"he WASSA 2022 Shared Task introduced four tracks and released a dataset of news stories: Track-1 for Empathy and Distress Prediction, Track-2 for Emotion classification, Track-3 for Personality prediction, and Track-4 for Interpersonal Reactivity Index prediction at the essay level","offset":1,"pro":0.25,"labels":"BAC"},{"idx":8,"sentence":"This paper describes our participation in the WASSA 2022 shared task on the tasks mentioned above. We developed multi-task deep learning methods to address Tracks 1 and 2 and machine learning models for Track 3 and 4. ","offset":2,"pro":0.5,"labels":"PUR"},{"idx":8,"sentence":"e ranked 8th, 11th, 2nd and 2nd for tracks 1, 2, 3, and 4 respectively","offset":3,"pro":0.75,"labels":"CLN"},{"idx":9,"sentence":"This paper describes the contribution of team PHG to the WASSA 2022 shared task on Empathy Prediction and Emotion Classification. ","offset":0,"pro":0,"labels":"PUR"},{"idx":9,"sentence":" distress score and the type of emotion associated with the person who had reacted to the essay written in response to a newspaper article.","offset":1,"pro":0.3333333333333333,"labels":"BAC"},{"idx":9,"sentence":"We have used the RoBERTa model for training and top of which few layers are added to finetune the transformer. We also use few machine learning techniques to augment as well as upsample the data. ","offset":2,"pro":0.6666666666666666,"labels":"MTD"},{"idx":10,"sentence":"This paper describes the LingJing team{'}s method to the Workshop on Computational Approaches to Subjectivity, Sentiment {\\&} Social Media Analysis (WASSA) 2022 shared task on Personality Prediction (PER) and Reactivity Index Prediction (IRI","offset":0,"pro":0,"labels":"PUR"},{"idx":10,"sentence":"Extensive experiments are performed, which shows the effectiveness of the proposed method. ","offset":1,"pro":0.2,"labels":"MTD"},{"idx":10,"sentence":"We ranked 1-st on both sub-tasks","offset":2,"pro":0.4,"labels":"CLN"},{"idx":10,"sentence":"n this paper, we adopt the prompt-based method with the pre-trained language model to accomplish these tasks","offset":3,"pro":0.6,"labels":"MTD"},{"idx":10,"sentence":"Specifically, the prompt is designed to provide knowledge of the extra personalized information for enhancing the pre-trained model. ","offset":4,"pro":0.8,"labels":"MTD"},{"idx":11,"sentence":"n this work, we tested different learning strategies, like ensemble learning and multi-task learning, as well as several large language models, but our primary focus was on analysing and extracting emotion-intensive features from both the essays in the training data and the news articles, to better predict empathy and distress scores from the perspective of discourse and sentiment analysis. ","offset":0,"pro":0,"labels":"MTD"},{"idx":11,"sentence":"Our best submissions achieve an average Pearson correlation score of 0.518 for the empathy prediction task and an F1 score of 0.571 for the emotion prediction task,","offset":1,"pro":0.3333333333333333,"labels":"RST"},{"idx":11,"sentence":"indicating that using these schemes to extract emotion-intensive information can help improve model performance","offset":2,"pro":0.6666666666666666,"labels":"CLN"},{"idx":12,"sentence":"Our proposed system achieved a Macro F1 score of 0.585 and ranked one out of thirteen teams","offset":0,"pro":0,"labels":"RST"},{"idx":12,"sentence":"e participated in track 2 for predicting emotion at the essay level.","offset":1,"pro":0.25,"labels":"MTD"},{"idx":12,"sentence":"We propose an ensemble approach that leverages the linguistic knowledge of the RoBERTa, BART-large, and RoBERTa model finetuned on the GoEmotions dataset.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":12,"sentence":"Each brings in its unique advantage, as we discuss in the paper.","offset":3,"pro":0.75,"labels":"MTD"},{"idx":13,"sentence":"We build a system that leverages adapters, a light weight and efficient method for leveraging large language models to perform the task Em- pathy and Distress prediction tasks for WASSA 2022. ","offset":0,"pro":0,"labels":"PUR"},{"idx":13,"sentence":"We make our experimental code publicly available","offset":1,"pro":0.5,"labels":"CTN"},{"idx":14,"sentence":"Twitter has slowly but surely established itself as a forum for disseminating, analysing and promoting NLP research. ","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"he trend of researchers promoting work not yet peer-reviewed (preprints) by posting concise summaries presented itself as an opportunity to collect and combine multiple modalities of data. ","offset":1,"pro":0.2,"labels":"GAP"},{"idx":14,"sentence":"n scope of this paper, we (1) construct a dataset of Twitter threads in which researchers promote NLP preprints and (2) evaluate whether it is possible to predict the popularity of a thread based on the content of the Twitter thread, paper content and user metadata","offset":2,"pro":0.4,"labels":"PUR"},{"idx":14,"sentence":"We experimentally show that it is possible to predict popularity of threads promoting research based on their content, and that predictive performance depends on modelling textual input","offset":3,"pro":0.6,"labels":"RST"},{"idx":14,"sentence":"indicating that the dataset could present value for related areas of NLP research such as citation recommendation and abstractive summarization","offset":4,"pro":0.8,"labels":"CLN"},{"idx":15,"sentence":"Unsupervised approaches mainly focus on training a generator to rewrite input sentences","offset":0,"pro":0,"labels":"GAP"},{"idx":15,"sentence":"n this work, we assume that text styles are determined by only a small proportion of words; therefore, rewriting sentences via generative models may be unnecessary","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":15,"sentence":"The results of experiments comparing models indicate that our proposed model exceeds end-to-end baselines in terms of accuracy on both sentiment and style transfer tasks with comparable or better content preservation.","offset":2,"pro":0.3333333333333333,"labels":"CLN"},{"idx":15,"sentence":"As an alternative, we consider style transfer as a sequence tagging task.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":15,"sentence":"Specifically, we use edit operations (i.e., deletion, insertion and substitution) to tag words in an input sentence. ","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":15,"sentence":"Finally, the optimal path in the conditional random field is used as the output","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":16,"sentence":"Distinct from domain-agnostic politeness constructs, in specific domains such as online stores, booking platforms, and others, agents need to be capable of adopting highly specific vocabulary, with significant impact on lexical and grammatical aspects of utterances.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"Then, the challenge is on improving utterances{'} politeness while preserving the actual content, an utterly central requirement to achieve the task goal. ","offset":1,"pro":0.25,"labels":"GAP"},{"idx":16,"sentence":"e extend existing generative and rewriting politeness approaches, towards overcoming domain-shifting issues, and enabling the transfer of politeness patterns to a novel domain.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":16,"sentence":"Both automatic and human evaluation is conducted on customer-store interactions, over the fashion domain, from which contribute with insightful and experimentally supported lessons regarding the improvement of politeness in task-specific dialog agents","offset":3,"pro":0.75,"labels":"MTD"},{"idx":17,"sentence":"Another, less costly alternative, which leads to potentially more noisy and biased data, is to rely on labels inferred from publicly available information in the profiles of the users, for instance self-reported diagnoses or test results.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"In this paper, we explore a third strategy, namely to directly use a corpus of items from validated psychometric tests as training data.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":17,"sentence":"Items from psychometric tests often consist of sentences from an I-perspective (e.g., {`}I make friends easily.{'}). Such corpora of test items constitute {`}small data{'}, but their availability for many concepts is a rich resource. ","offset":2,"pro":0.4,"labels":"BAC"},{"idx":17,"sentence":"Our evaluation on a publicly available Twitter corpus shows a comparable performance to in-domain training for 4/5 personality traits with T5-based data augmentation","offset":3,"pro":0.6,"labels":"RST"},{"idx":17,"sentence":"Machine-learned models for author profiling in social media often rely on data acquired via self-reporting-based psychometric tests (questionnaires) filled out by social media users.","offset":4,"pro":0.8,"labels":"BAC"},{"idx":18,"sentence":"ncorporating stronger syntactic biases into neural language models (LMs) is a long-standing goal,","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"Our best model shows the advantage over sequential/overparameterized LMs,","offset":1,"pro":0.1111111111111111,"labels":"CLN"},{"idx":18,"sentence":"suggesting the positive effect of syntax injection in a multilingual setting","offset":2,"pro":0.2222222222222222,"labels":"CLN"},{"idx":18,"sentence":"Our experiments highlight the importance of choosing the right tree formalism, and provide insights into making an informed decision.","offset":3,"pro":0.3333333333333333,"labels":"CTN"},{"idx":18,"sentence":"However, this raises the question of ","offset":4,"pro":0.4444444444444444,"labels":"GAP"},{"idx":18,"sentence":"We examine the effect on LM performance across nine conversion methods and five languages through seven types of syntactic tests","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":18,"sentence":"We investigate this question","offset":6,"pro":0.6666666666666666,"labels":"PUR"},{"idx":18,"sentence":"by training recurrent neural network grammars (RNNGs) using various conversion methods, and evaluating them empirically in a multilingual setting","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":18,"sentence":"but research in this area often focuses on modeling English text, where constituent treebanks are readily available. ","offset":8,"pro":0.8888888888888888,"labels":"BAC"},{"idx":19,"sentence":"his study introduces a novel approach to the joint extraction of entities and relations by stacking convolutional neural networks (CNNs) on pretrained language models. ","offset":0,"pro":0,"labels":"PUR"},{"idx":19,"sentence":"ADE datasets","offset":1,"pro":0.2,"labels":"RST"},{"idx":19,"sentence":"Even when freezing pretrained language model parameters, the proposed method showed a stable performance, whereas the compared methods suffered from significant decreases in performance.","offset":2,"pro":0.4,"labels":"RST"},{"idx":19,"sentence":"This observation indicates that the parameters of the pretrained encoder may incorporate dependencies among the entity and relation labels during fine-tuning","offset":3,"pro":0.6,"labels":"CLN"},{"idx":19,"sentence":"egarding each table as an image and each cell in a table as an image pixel, we apply two-dimensional CNNs to the tables to capture local dependencies and predict the cell labels. ","offset":4,"pro":0.8,"labels":"MTD"},{"idx":0,"sentence":"Neural machine learning models can successfully model language that is similar to their training distribution, but they are highly susceptible to degradation under distribution shift, which occurs in many practical applications when processing out-of-domain (OOD) sentence","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"This has been attributed to {``}shortcut learning{''}'':'' relying on weak correlations over arbitrary large contexts","offset":1,"pro":0.25,"labels":"BAC"},{"idx":0,"sentence":"We propose a method based on OOD detection with Random Network Distillation to allow an autoregressive language model to automatically disregard OOD context during inference, smoothly transitioning towards a less expressive but more robust model as the data becomes more OOD, while retaining its full context capability when operating in-distribution. ","offset":2,"pro":0.5,"labels":"PUR"},{"idx":0,"sentence":"We apply our method to a GRU architecture, demonstrating improvements on multiple language modeling (LM) datasets","offset":3,"pro":0.75,"labels":"CTN"},{"idx":1,"sentence":"Simultaneous training of a multi-task learning network on different domains or tasks is not always straightforward. It could lead to inferior performance or generalization compared to the corresponding single-task networks. An effective training scheduling method is deemed necessary to maximize the benefits of multi-task learning. ","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"raditional schedulers follow a heuristic or prefixed strategy, ignoring the relation of the tasks, their sample complexities, and the state of the emergent shared features","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":1,"sentence":"We proposed a deep Q-Learning Scheduler (QLS) that monitors the state of the tasks and the shared features using a novel histogram of task uncertainty, and through trial-and-error, learns an optimal policy for task schedulin","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":1,"sentence":" Extensive experiments on multi-domain and multi-task settings with various task difficulty profiles have been conducted,","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":1,"sentence":"he proposed method is benchmarked against other schedulers, ","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":1,"sentence":"superior performance has been demonstrated","offset":5,"pro":0.7142857142857143,"labels":"CTN"},{"idx":1,"sentence":"results are discussed.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":2,"sentence":"However, recent work showed that in comparison to unimodal (visual) networks, CLIP{'}s multimodal training does not benefit generalization (e.g. few-shot or transfer learning) for standard visual classification tasks such as object, street numbers or animal recognition","offset":0,"pro":0,"labels":"GAP"},{"idx":2,"sentence":"Here, we hypothesize that CLIP{'}s improved unimodal generalization abilities may be most prominent in domains that involve human-centric concepts (cultural, social, aesthetic, affective...); this is because CLIP{'}s training dataset is mainly composed of image annotations made by humans for other humans.","offset":1,"pro":0.14285714285714285,"labels":"MTD"},{"idx":2,"sentence":"o evaluate this,","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":2,"sentence":" we use 3 tasks that require judging human-centric concepts{''}:'' sentiment analysis on tweets, genre classification on books or movies","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":2,"sentence":"We introduce and publicly release a new multimodal dataset for movie genre classification. We compare CLIP{'}s visual stream against two visually trained networks and CLIP{'}s textual stream against two linguistically trained networks, as well as multimodal combinations of these networks.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":2,"sentence":"We show that CLIP generally outperforms other networks, whether using one or two modalities","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":2,"sentence":"We conclude that CLIP{'}s multimodal training is beneficial for both unimodal and multimodal tasks that require classification of human-centric concepts.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":3,"sentence":"We choose random points in the hyperbolic disc and claim that these points are already word representation","offset":0,"pro":0,"labels":"MTD"},{"idx":3,"sentence":"However, it is yet to be uncovered which point corresponds to which word of the human language of interest","offset":1,"pro":0.25,"labels":"GAP"},{"idx":3,"sentence":"This correspondence can be approximately established","offset":2,"pro":0.5,"labels":"PUR"},{"idx":3,"sentence":"using a pointwise mutual information between words and recent alignment techniques","offset":3,"pro":0.75,"labels":"MTD"},{"idx":4,"sentence":" However, their performance in low-resource scenarios, where such data is not available, remains an open question","offset":0,"pro":0,"labels":"GAP"},{"idx":4,"sentence":"e introduce an encoder evaluation framework, and use it to systematically compare the performance of state-of-the-art pre-trained representations on the task of low-resource NER","offset":1,"pro":0.2,"labels":"MTD"},{"idx":4,"sentence":"We analyze a wide range of encoders pre-trained with different strategies, model architectures, intermediate-task fine-tuning, and contrastive learning","offset":2,"pro":0.4,"labels":"MTD"},{"idx":4,"sentence":"Our experimental results across ten benchmark NER datasets in English and German show that encoder performance varies significantly","offset":3,"pro":0.6,"labels":"RST"},{"idx":4,"sentence":"suggesting that the choice of encoder for a specific low-resource scenario needs to be carefully evaluated.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":5,"sentence":"Task-adaptive pre-training (TAPT) alleviates the lack of labelled data and provides performance lift by adapting unlabelled data to down","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"Unfortunately, existing adaptations mainly involve deterministic rules that cannot generalize wel","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":5,"sentence":"Here, we propose Clozer, a sequence-tagging based cloze answer extraction method used in TAPT that is extendable for adaptation on any cloze-style machine reading comprehension (MRC) downstream tasks.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":5,"sentence":"e experiment on multiple-choice cloze-style MRC tasks,","offset":3,"pro":0.5,"labels":"MTD"},{"idx":5,"sentence":"show that Clozer performs significantly better compared to the oracle and state-of-the-art in escalating TAPT effectiveness in lifting model performance","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":5,"sentence":"prove that Clozer is able to recognize the gold answers independently of any heuristics.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":6,"sentence":"Multilingual language models were shown to allow for nontrivial transfer across scripts and languages","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"In this work, we study the structure of the internal representations that enable this transfer","offset":1,"pro":0.2,"labels":"PUR"},{"idx":6,"sentence":"We focus on the representations of gender distinctions as a practical case study, and examine the extent to which the gender concept is encoded in shared subspaces across different languages.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":6,"sentence":"Our analysis shows that gender representations consist of several prominent components that are shared across languages, alongside language-specific components.","offset":3,"pro":0.6,"labels":"RST"},{"idx":6,"sentence":"he existence of language-independent and language-specific components provides an explanation for an intriguing empirical observation we make{''}:'' while gender classification transfers well across languages, interventions for gender removal trained on a single language do not transfer easily to others","offset":4,"pro":0.8,"labels":"CLN"},{"idx":7,"sentence":"Although deep neural networks have achieved state-of-the-art performance in various machine learning tasks, adversarial examples, constructed by adding small non-random perturbations to correctly classified inputs, successfully fool highly expressive deep classifi","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"Approaches to adversarial attacks in natural language tasks have boomed in the last five years using character-level, word-level, phrase-level, or sentence-level textual perturbation","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":7,"sentence":"While there is some work in NLP on defending against such attacks through proactive methods, like adversarial training, there is to our knowledge no effective general reactive approaches to defence via detection of textual adversarial examples such as is found in the image processing literature. ","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":7,"sentence":" this paper, we propose two new reactive methods for NLP to fill this gap, which unlike the few limited application baselines from NLP are based entirely on distribution characteristics of learned representations{''}:'' we adapt one from the image processing literature (Local Intrinsic Dimensionality (LID)), and propose a novel one (MultiDistance Representation Ensemble Method (MDRE)","offset":3,"pro":0.5,"labels":"PUR"},{"idx":7,"sentence":"Adapted LID and MDRE obtain state-of-the-art results on character-level, word-level, and phrase-level attacks on the IMDB dataset as well as on the later two with respect to the MultiNLI dataset","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":7,"sentence":"r future research, we publish our code","offset":5,"pro":0.8333333333333334,"labels":"CTN"},{"idx":8,"sentence":"Subword tokenization is a commonly used input pre-processing step in most recent NLP models","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"However, it limits the models{'} ability to leverage end-to-end task learning","offset":1,"pro":0.1,"labels":"GAP"},{"idx":8,"sentence":"Its frequency-based vocabulary creation compromises tokenization in low-resource languages, leading models to produce suboptimal representations","offset":2,"pro":0.2,"labels":"GAP"},{"idx":8,"sentence":"dditionally, the dependency on a fixed vocabulary limits the subword models{'} adaptability across languages an","offset":3,"pro":0.3,"labels":"GAP"},{"idx":8,"sentence":"n this work, we propose a vocabulary-free neural tokenizer by distilling segmentation information from heuristic-based subword tokenization","offset":4,"pro":0.4,"labels":"PUR"},{"idx":8,"sentence":"We pre-train our character-based tokenizer by processing unique words from multilingual corpus, thereby extensively increasing word diversity across languages","offset":5,"pro":0.5,"labels":"MTD"},{"idx":8,"sentence":"nlike the predefined and fixed vocabularies in subword methods, our tokenizer allows end-to-end task learning, resulting in optimal task-specific tokenization","offset":6,"pro":0.6,"labels":"MTD"},{"idx":8,"sentence":"he experimental results show that replacing the subword tokenizer with our neural tokenizer consistently improves performance on multilingual (NLI) and code-switching (sentiment analysis) tasks, with larger gains in low-resource languages","offset":7,"pro":0.7,"labels":"RST"},{"idx":8,"sentence":"Additionally, our neural tokenizer exhibits a robust performance on downstream tasks when adversarial noise is present (typos and misspelling)","offset":8,"pro":0.8,"labels":"RST"},{"idx":8,"sentence":"urther increasing the initial improvements over statistical subword tokenizers","offset":9,"pro":0.9,"labels":"CLN"},{"idx":9,"sentence":"here is growing evidence that pretrained language models improve task-specific fine-tuning even where the task examples are radically different from those seen in trainin","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"We study an extreme case of transfer learning by providing a systematic exploration of how much transfer occurs when models are denied any information about word identity via random scrambling","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":9,"sentence":"In four classification tasks and two sequence labeling tasks, we evaluate LSTMs using GloVe embeddings, BERT, and baseline models","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":9,"sentence":"Among these models, we find that only BERT shows high rates of transfer into our scrambled domains, and for classification but not sequence labeling task","offset":3,"pro":0.5,"labels":"RST"},{"idx":9,"sentence":"Our analyses seek to explain why transfer succeeds for some tasks but not others, to isolate the separate contributions of pretraining versus fine-tuning, to show that the fine-tuning process is not merely learning to unscramble the scrambled inputs, and to quantify the role of word frequenc","offset":4,"pro":0.6666666666666666,"labels":"PUR"},{"idx":9,"sentence":"Furthermore, our results suggest that current benchmarks may overestimate the degree to which current models actually understand language","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":10,"sentence":"emporal knowledge graph completion (TKGC) has become a popular approach for reasoning over the event and temporal knowledge graphs, targeting the completion of knowledge with accurate but missing information","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"n this context, tensor decomposition has successfully modeled interactions between entities and relations. Their effectiveness in static knowledge graph completion motivates us to introduce Time-LowFER, a family of parameter-efficient and time-aware extensions of the low-rank tensor factorization model LowFER","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":10,"sentence":"Noting several limitations in current approaches to","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":10,"sentence":"e propose a cycle-aware time-encoding scheme for time features, which is model-agnostic and offers a more generalized representation of time","offset":3,"pro":0.5,"labels":"PUR"},{"idx":10,"sentence":"e implement our methods in a unified temporal knowledge graph embedding framework, focusing on time-sensitive data proces","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":10,"sentence":"he experiments show that our proposed methods perform on par or better than the state-of-the-art semantic matching models on two benchmarks","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":11,"sentence":"Pre-trained language models have brought significant improvements in performance in a variety of natural language processing tasks. Most existing models performing state-of-the-art results have shown their approaches in the separate perspectives of data processing, pre-training tasks, neural network modeling, or fine-tunin","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"In this paper, we demonstrate how the approaches affect performance individually, and that the language model performs the best results on a specific question answering task when those approaches are jointly considered in pre-training models","offset":1,"pro":0.25,"labels":"PUR"},{"idx":11,"sentence":"n particular, we propose an extended pre-training task, and a new neighbor-aware mechanism that attends neighboring tokens more to capture the richness of context for pre-training language modeling. ","offset":2,"pro":0.5,"labels":"PUR"},{"idx":11,"sentence":"ur best model achieves new state-of-the-art results of 95.7{\\%} F1 and 90.6{\\%} EM on SQuAD 1.1 and also outperforms existing pre-trained language models such as RoBERTa, ALBERT, ELECTRA, and XLNet on the SQuAD 2.0 benchmar","offset":3,"pro":0.75,"labels":"RST"},{"idx":12,"sentence":"Our model combines two original contributions{''}:'' A multi-modal fast-learning feature fusion (FLF) block and a mechanism that uses self-attended language features to separately guide neural attention on both static and dynamic visual features extracted from individual video frames and short video clips.","offset":0,"pro":0,"labels":"PUR"},{"idx":12,"sentence":"When trained from scratch, VLCN achieves competitive results with the state of the art on both MSVD-QA and MSRVTT-QA with 38.06{\\%} and 36.01{\\%} test accuracies, respectively. ","offset":1,"pro":0.25,"labels":"RST"},{"idx":12,"sentence":"rough an ablation study","offset":2,"pro":0.5,"labels":"MTD"},{"idx":12,"sentence":"e further show that FLF improves generalization across different VideoQA datasets and performance for question types that are notoriously challenging in current datasets, such as long questions that require deeper reasoning as well as questions with rare answer","offset":3,"pro":0.75,"labels":"CLN"},{"idx":13,"sentence":"State-of-the-art machine learning models are prone to adversarial attacks{''}","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"Maliciously crafted inputs to fool the model into making a wrong prediction, often with high confidence. ","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":13,"sentence":"hile defense strategies have been extensively explored in the computer vision domain, research in natural language processing still lacks techniques to make models resilient to adversarial text inputs. ","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":13,"sentence":"We adapt a technique from computer vision to detect word-level attacks targeting text classifiers","offset":3,"pro":0.5,"labels":"PUR"},{"idx":13,"sentence":"his method relies on training an adversarial detector leveraging Shapley additive explanations and outperforms the current state-of-the-art on two benchmarks.","offset":4,"pro":0.6666666666666666,"labels":"CTN"},{"idx":13,"sentence":"rthermore, we prove the detector requires only a low amount of training samples and, in some cases, generalizes to different datasets without needing to retrain.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":14,"sentence":"but is slow and difficult to scale due to the large number of floating point calculations.","offset":0,"pro":0,"labels":"GAP"},{"idx":14,"sentence":"This paper demonstrates that by combining pre-existing lower bounds with binary encoded word vectors, the metric can be rendered highly efficient in terms of computation time and memory while still maintaining accuracy on several textual similarity tasks","offset":1,"pro":0.5,"labels":"CLN"},{"idx":15,"sentence":"he quality of our representations, and their preservation across languages, are evaluated in similarity comparison tasks, achieving competitive results.","offset":0,"pro":0,"labels":"MTD"},{"idx":15,"sentence":"ermore, we show that our structural-based representations can be combined with existing methods f","offset":1,"pro":0.5,"labels":"CLN"},{"idx":16,"sentence":"Entity linking disambiguates mentions by mapping them to entities in a knowledge graph (KG). One important question in today{'}s research is how to extend neural entity linking systems to new domains.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":" this paper, we aim at a system that enables linking mentions to entities from a general-domain KG and a domain-specific KG at the same time","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":16,"sentence":"n particular, we represent the entities of different KGs in a joint vector space and address the questions of which data is best suited for creating and fine-tuning that space, and whether fine-tuning harms performance on the general domain","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":16,"sentence":"We find that a combination of data from both the general and the special domain is most helpful","offset":3,"pro":0.5,"labels":"RST"},{"idx":16,"sentence":"The first is especially necessary for avoiding performance loss on the general domain","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":16,"sentence":"While additional supervision on entities that appear in both KGs performs best in an intrinsic evaluation of the vector space, it has less impact on the downstream task of entity linking.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":17,"sentence":"xt retrieval has been widely-used in many online applications to help users find relevant information from a text collectio","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":" this paper, we study a new attack scenario against text retrieval to evaluate its robustness to adversarial attacks under the black-box setting, in which attackers want their own texts to always get high relevance scores with different users{'} input queries and thus be retrieved frequently and can receive large amounts of impressions for profits","offset":1,"pro":0.125,"labels":"PUR"},{"idx":17,"sentence":"onsidering that most current attack methods only simply follow certain fixed optimization rule","offset":2,"pro":0.25,"labels":"GAP"},{"idx":17,"sentence":"e propose a novel text rewriting attack (TRAttack) method with learning ability from the multi-armed bandit mechanism","offset":3,"pro":0.375,"labels":"PUR"},{"idx":17,"sentence":"xtensive experiments conducted on simulated victim environments demonstrate that TRAttack can yield texts that have higher relevance scores with different given users{'} queries than those generated by current state-of-the-art attack methods","offset":4,"pro":0.5,"labels":"MTD"},{"idx":17,"sentence":"e also evaluate TRAttack on Tencent Cloud{'}s and Baidu Cloud{'}s commercially-available text retrieval API","offset":5,"pro":0.625,"labels":"MTD"},{"idx":17,"sentence":"the rewritten adversarial texts successfully get high relevance ","offset":6,"pro":0.75,"labels":"RST"},{"idx":17,"sentence":"which shows the practical potential of our method and the risk of text retrieval system","offset":7,"pro":0.875,"labels":"CLN"},{"idx":18,"sentence":"We use data for English and German","offset":0,"pro":0,"labels":"MTD"},{"idx":18,"sentence":"how that concreteness and abstractness can be determined independently and turn out to be completely op","offset":1,"pro":0.25,"labels":"RST"},{"idx":18,"sentence":"Various methods can be used to determine the direction of concreteness, always resulting in roughly the same vecto","offset":2,"pro":0.5,"labels":"MTD"},{"idx":18,"sentence":"Though concreteness is a central aspect of the meaning of words and can be detected clearly in embedding spaces, it seems not as easy to subtract or add concreteness to words to obtain other words or word senses like e.g. can be done with a semantic property like gende","offset":3,"pro":0.75,"labels":"CLN"},{"idx":19,"sentence":"t{'}s better to say {``}I can{'}t answer{''} than to answer incorrectly. This selective prediction ability is crucial for NLP systems to be reliably deployed in real-world applications","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"Prior work has shown that existing selective prediction techniques fail to perform well, especially in the out-of-domain settin","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":19,"sentence":"his work, we propose a method that improves probability estimates of models by calibrating them using prediction confidence and difficulty score of instance","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":19,"sentence":"ing these two signals, we first annotate held-out instances and then train a calibrator to predict the likelihood of correctness of the model{'}s predictio","offset":3,"pro":0.5,"labels":"MTD"},{"idx":19,"sentence":"e instantiate our method with Natural Language Inference (NLI) and Duplicate Detection (DD) tasks and evaluate it in both In-Domain (IID) and Out-of-Domain (OOD) setting","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":19,"sentence":" (IID, OOD) settings, we show that the representations learned by our calibrator result in an improvement of (15.81{\\%}, 5.64{\\%}) and (6.19{\\%}, 13.9{\\%}) over {`}MaxProb{'} -a selective prediction baseline- on NLI and DD tasks respe","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":0,"sentence":"Continuous generative models proved their usefulness in high-dimensional data, such as image and audio generation.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"However, continuous models for text generation have received limited attention from the community.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":0,"sentence":"In this work, we study continuous text generation using Transformers for neural machine translation (NMT).","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":0,"sentence":"We argue that the choice of embeddings is crucial for such models, so we aim to focus on one particular aspect{''}:'' target representation via embeddings.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":0,"sentence":"We explore pretrained embeddings and also introduce knowledge transfer from the discrete Transformer model using embeddings in Euclidean and non-Euclidean spaces.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":0,"sentence":"Our results on the WMT Romanian-English and English-Turkish benchmarks show such transfer leads to the best-performing continuous model","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":1,"sentence":"Pretrained multilingual encoders enable zero-shot cross-lingual transfer, but often produce unreliable models that exhibit high performance variance on the target language. ","offset":0,"pro":0,"labels":"GAP"},{"idx":1,"sentence":"We postulate that this high variance results from zero-shot cross-lingual transfer solving an under-specified optimization problem.","offset":1,"pro":0.25,"labels":"PUR"},{"idx":1,"sentence":"Additionally, we show that zero-shot solution lies in non-flat region of target language error generalization surface, causing the high variance.","offset":2,"pro":0.5,"labels":"CLN"},{"idx":1,"sentence":"We show that any linear-interpolated model between the source language monolingual model and source + target bilingual model has equally low source language generalization error, yet the target language generalization error reduces smoothly and linearly as we move from the monolingual to bilingual model, suggesting that the model struggles to identify good solutions for both source and target languages using the source language alone.","offset":3,"pro":0.75,"labels":"CLN"},{"idx":2,"sentence":"Recent advances in the development of style representations have increasingly used training objectives from authorship verification (AV){''}:'' Do two texts have the same author? The assumption underlying the AV training task (same author approximates same writing style) enables self-supervised and, thus, extensive training.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"However, a good performance on the AV task does not ensure good {``}general-purpose{''} style representations. ","offset":1,"pro":0.2,"labels":"GAP"},{"idx":2,"sentence":"We introduce a variation of the AV training task that controls for content using conversation or domain labels.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":2,"sentence":"We evaluate whether known style dimensions are represented and preferred over content information through an original variation to the recently proposed STEL framework.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":2,"sentence":"We find that representations trained by controlling for conversation are better than representations trained with domain or no content control at representing style independent from content.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":3,"sentence":"A popular approach to decrease the need for costly manual annotation of large data sets is weak supervision, which introduces problems of noisy labels, coverage and bias.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"Methods for overcoming these problems have either relied on discriminative models, trained with cost functions specific to weak supervision, and more recently, generative models, trying to model the output of the automatic annotation process.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":3,"sentence":"In this work, we explore a novel direction of generative modeling for weak supervision{''}:'' Instead of modeling the output of the annotation process (the labeling function matches), we generatively model the input-side data distributions (the feature space) covered by labeling functions.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":3,"sentence":"Specifically, we estimate a density for each weak labeling source, or labeling function, by using normalizing flows.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":3,"sentence":"An integral part of our method is the flow-based modeling of multiple simultaneously matching labeling functions, and therefore phenomena such as labeling function overlap and correlations are captured.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":3,"sentence":"We analyze the effectiveness and modeling capabilities on various commonly used weak supervision data sets,","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":3,"sentence":"and show that weakly supervised normalizing flows compare favorably to standard weak supervision baselines.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":4,"sentence":"We survey four benchmark dataset sharing platforms: HuggingFace, PaperswithCode, Tensorflow, and Pytorch to diagnose the current practices of how the dataset is shared which metadata is shared and omitted.","offset":0,"pro":0,"labels":"MTD"},{"idx":4,"sentence":"To be specific, drawing on the concept of data curation which considers the future reuse when the data is made public, we advance the direction that benchmark dataset sharing platforms should take into consideration. ","offset":1,"pro":0.25,"labels":"MTD"},{"idx":4,"sentence":"We believe the problem of missing a discussion around social impact in the dataset sharing platforms has to do with the failed agreement on who should be in charge.","offset":2,"pro":0.5,"labels":"CLN"},{"idx":4,"sentence":"We propose that the benchmark dataset should develop social impact metadata and data curator should take a role in managing the social impact metadata.","offset":3,"pro":0.75,"labels":"IMP"},{"idx":5,"sentence":"Natural language processing (NLP) systems are often used for adversarial tasks such as detecting spam, abuse, hate speech, and fake news.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"Properly evaluating such systems requires dynamic evaluation that searches for weaknesses in the model, rather than a static test set.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":5,"sentence":"Prior work has evaluated such models on both manually and automatically generated examples, but both approaches have limitations: manually constructed examples are time-consuming to create and are limited by the imagination and intuition of the creators, while automatically constructed examples are often ungrammatical or labeled inconsistently.","offset":2,"pro":0.4,"labels":"GAP"},{"idx":5,"sentence":"We propose to combine human and AI expertise in generating adversarial examples, benefiting from humans{'} expertise in language and automated attacks{'} ability to probe the target system more quickly and thoroughly. ","offset":3,"pro":0.6,"labels":"PUR"},{"idx":5,"sentence":"Preliminary results from our own experimentation suggest that human-AI hybrid attacks are more effective than either human-only or AI-only attacks. ","offset":4,"pro":0.8,"labels":"CLN"},{"idx":6,"sentence":"A user-generated text on social media enables health workers to keep track of information, identify possible outbreaks, forecast disease trends, monitor emergency cases, and ascertain disease awareness and response to official health correspondence.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"This exchange of health information on social media has been regarded as an attempt to enhance public health surveillance (PHS). ","offset":1,"pro":0.2,"labels":"BAC"},{"idx":6,"sentence":"However, there are no PLMs for social media tasks involving PHS. ","offset":2,"pro":0.4,"labels":"GAP"},{"idx":6,"sentence":"We compared and benchmarked the performance of PHS-BERT on 25 datasets from different social medial platforms related to 7 different PHS tasks. ","offset":3,"pro":0.6,"labels":"MTD"},{"idx":6,"sentence":"By making PHS-BERT available, we aim to facilitate the community to reduce the computational cost and introduce new baselines for future works across various PHS-related tasks.","offset":4,"pro":0.8,"labels":"CTN"},{"idx":7,"sentence":"Relation classification models are conventionally evaluated using only a single measure, e.g., micro-F1, macro-F1 or AUC.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"In this work, we analyze weighting schemes, such as micro and macro, for imbalanced datasets. ","offset":1,"pro":0.3333333333333333,"labels":"PUR"},{"idx":7,"sentence":"We show that reporting results of different weighting schemes better highlights strengths and weaknesses of a model.","offset":2,"pro":0.6666666666666666,"labels":"CLN"},{"idx":8,"sentence":"Recent improvements in automatic news summarization fundamentally rely on large corpora of news articles and their summaries. ","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"Apart from more generic noise, we identify straplines as a form of text scraped from news websites that commonly turn out not to be summaries.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":8,"sentence":"The presence of these non-summaries threatens the validity of scraped corpora as benchmarks for news summarization.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":8,"sentence":"We have annotated extracts from two news sources that form part of the Newsroom corpus (Grusky et al., 2018), labeling those which were straplines, those which were summaries, and those which were both.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":8,"sentence":"We present a rule-based strapline detection method that achieves good performance on a manually annotated test set.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":8,"sentence":"Automatic evaluation indicates that removing straplines and noise from the training data of a news summarizer results in higher quality summaries, with improvements as high as 7 points ROUGE score.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":9,"sentence":"Measuring the performance of natural language processing models is challenging.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"Traditionally used metrics, such as BLEU and ROUGE, originally devised for machine translation and summarization, have been shown to suffer from low correlation with human judgment and a lack of transferability to other tasks and languages.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":9,"sentence":"In the past 15 years, a wide range of alternative metrics have been proposed.","offset":2,"pro":0.2857142857142857,"labels":"BAC"},{"idx":9,"sentence":"However, it is unclear to what extent this has had an impact on NLP benchmarking efforts.","offset":3,"pro":0.42857142857142855,"labels":"GAP"},{"idx":9,"sentence":"Here we provide the first large-scale cross-sectional analysis of metrics used for measuring performance in natural language processing. ","offset":4,"pro":0.5714285714285714,"labels":"PUR"},{"idx":9,"sentence":"Our results suggest that the large majority of natural language processing metrics currently used have properties that may result in an inadequate reflection of a models{'} performance.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":9,"sentence":"Furthermore, we found that ambiguities and inconsistencies in the reporting of metrics may lead to difficulties in interpreting and comparing model performances, impairing transparency and reproducibility in NLP research.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":10,"sentence":"Although recent Massively Multilingual Language Models (MMLMs) like mBERT and XLMR support around 100 languages, most existing multilingual NLP benchmarks provide evaluation data in only a handful of these languages with little linguistic diversity. ","offset":0,"pro":0,"labels":"GAP"},{"idx":10,"sentence":"We propose that the recent work done in Performance Prediction for NLP tasks can serve as a potential solution in fixing benchmarking in Multilingual NLP by utilizing features related to data and language typology to estimate the performance of an MMLM on different languages.","offset":1,"pro":0.25,"labels":"PUR"},{"idx":10,"sentence":"We compare performance prediction with translating test data with a case study on four different multilingual datasets,","offset":2,"pro":0.5,"labels":"MTD"},{"idx":10,"sentence":"and observe that these methods can provide reliable estimates of the performance that are often on-par with the translation based approaches, without the need for any additional translation as well as evaluation costs.","offset":3,"pro":0.75,"labels":"RST"},{"idx":11,"sentence":"Behavioural testing{---}verifying system capabilities by validating human-designed input-output pairs{---}is an alternative evaluation method of natural language processing systems proposed to address the shortcomings of the standard approach: computing metrics on held-out data. ","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"With this in mind, we explore behaviour-aware learning by examining several fine-tuning schemes using HateCheck, a suite of functional tests for hate speech detection systems.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":11,"sentence":"To address potential pitfalls of training on data originally intended for evaluation, ","offset":2,"pro":0.4,"labels":"PUR"},{"idx":11,"sentence":"The fine-tuning procedure led to improvements in the classification accuracy of held-out functionalities and identity groups, ","offset":3,"pro":0.6,"labels":"RST"},{"idx":11,"sentence":"However, performance on held-out functionality classes and i.i.d. hate speech detection data decreased, which indicates that generalisation occurs mostly across functionalities from the same class and that the procedure led to overfitting to the HateCheck data distribution.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":12,"sentence":"Meaning is context-dependent, but many properties of language (should) remain the same even if we transform the context. ","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"We introduce language invariant properties: i.e., properties that should not change when we transform text, and how they can be used to quantitatively evaluate the robustness of transformation algorithms.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":12,"sentence":"Language invariant properties can be used to define novel benchmarks to evaluate text transformation methods.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":12,"sentence":"In our work we use translation and paraphrasing as examples, but our findings apply more broadly to any transformation.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":12,"sentence":"Our results indicate that many NLP transformations change properties. ","offset":4,"pro":0.8,"labels":"CLN"},{"idx":13,"sentence":"Large-scale pre-trained language models have shown remarkable results in diverse NLP applications.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"However, these performance gains have been accompanied by a significant increase in computation time and model size, stressing the need to develop new or complementary strategies to increase the efficiency of these models. ","offset":1,"pro":0.25,"labels":"GAP"},{"idx":13,"sentence":"DACT-BERT adds an adaptive computational mechanism to BERT{'}s regular processing pipeline, which controls the number of Transformer blocks that need to be executed at inference time.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":13,"sentence":"By doing this, the model learns to combine the most appropriate intermediate representations for the task at hand. ","offset":3,"pro":0.75,"labels":"PUR"},{"idx":14,"sentence":"Transformer-based Natural Language Processing models have become the standard for hate speech detection.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"However, the unconscious use of these techniques for such a critical task comes with negative consequences. ","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":14,"sentence":"These findings have prompted efforts to explain classifiers, mainly using attribution methods.","offset":2,"pro":0.2857142857142857,"labels":"BAC"},{"idx":14,"sentence":"In this paper, we provide the first benchmark study of interpretability approaches for hate speech detection. ","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":14,"sentence":"Further, we compare generated attributions to attention analysis.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":14,"sentence":"We find that only two algorithms provide faithful explanations aligned with human expectations.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":14,"sentence":"Gradient-based methods and attention, however, show inconsistent outputs, making their value for explanations questionable for hate speech detection tasks.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":15,"sentence":"With many real-world applications of Natural Language Processing (NLP) comprising of long texts, there has been a rise in NLP benchmarks that measure the accuracy of models that can handle longer input sequences. ","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"In this work, we perform a systematic study of this accuracy vs. efficiency trade-off on two widely used long-sequence models - Longformer-Encoder-Decoder (LED) and Big Bird - during fine-tuning and inference on four datasets from the SCROLLS benchmark.","offset":1,"pro":0.14285714285714285,"labels":"MTD"},{"idx":15,"sentence":"To study how this trade-off differs across hyperparameter settings,","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":15,"sentence":"we compare the models across four sequence lengths (1024, 2048, 3072, 4096) and two model sizes (base and large) under a fixed resource budget. ","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":15,"sentence":"For summarization, we find that increasing model size is more energy efficient than increasing sequence length for higher accuracy.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":15,"sentence":"However, this comes at the cost of a large drop in inference speed.","offset":5,"pro":0.7142857142857143,"labels":"GAP"},{"idx":15,"sentence":"For question answering, we find that smaller models are both more efficient and more accurate due to the larger training batch sizes possible under a fixed resource budget.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":16,"sentence":"A major issue in open-domain dialogue generation is the agent{'}s tendency to generate repetitive and generic responses.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"The lack in response diversity has been addressed in recent years via the use of latent variable models, such as the Conditional Variational Auto-Encoder (CVAE), which typically involve learning a latent Gaussian distribution over potential response intents. ","offset":1,"pro":0.2,"labels":"BAC"},{"idx":16,"sentence":"Other approaches proposed to improve response diversity also largely entail a significant increase in training complexity. ","offset":2,"pro":0.4,"labels":"GAP"},{"idx":16,"sentence":"The RL Transformer does not require any additional enhancements to the training process or loss function.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":16,"sentence":"Empirical results show that, when it comes to response diversity, the RL Transformer achieved comparable performance compared to latent variable models.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":17,"sentence":"Pre-trained Transformer-based models were reported to be robust in intent classification.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"In this work, we first point out the importance of in-domain out-of-scope detection in few-shot intent recognition tasks and then illustrate the vulnerability of pre-trained Transformer-based models against samples that are in-domain but out-of-scope (ID-OOS). ","offset":1,"pro":0.3333333333333333,"labels":"PUR"},{"idx":17,"sentence":"and empirically show that pre-trained models do not perform well on both ID-OOS examples and general out-of-scope examples, especially on fine-grained few-shot intent detection tasks.","offset":2,"pro":0.6666666666666666,"labels":"CLN"},{"idx":18,"sentence":"Retailing combines complicated communication skills and strategies to reach an agreement between buyer and seller with identical or different goals.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"In each transaction a good seller finds an optimal solution by considering his/her own profits while simultaneously considering whether the buyer{'}s needs have been met.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":18,"sentence":"We present a rich dataset of buyer-seller bargaining in a simulated marketplace in which each agent values goods and utility separately.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":18,"sentence":"Various attributes (preference, quality, and profit) are initially hidden from one agent with respect to its role; during the conversation, both sides may reveal, fake, or retain the information uncovered to come to a final decision through natural language. ","offset":3,"pro":0.5,"labels":"MTD"},{"idx":18,"sentence":"An automatic evaluation shows that our approach results in more optimal transactions than human does.","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":18,"sentence":"We also show that our framework controls the falsehoods generated by seller agents.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":19,"sentence":"Existing research studies on cross-sentence relation extraction in long-form multi-party conversations aim to improve relation extraction without considering the explainability of such methods.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"This work addresses that gap by focusing on extracting explanations that indicate that a relation exists while using only partially labeled explanations.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":19,"sentence":"We propose our model-agnostic framework, D-REX, a policy-guided semi-supervised algorithm that optimizes for explanation quality and relation extraction simultaneously.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":19,"sentence":"We frame relation extraction as a re-ranking task and include relation- and entity-specific explanations as an intermediate step of the inference process. ","offset":3,"pro":0.6,"labels":"MTD"},{"idx":19,"sentence":"Finally, our evaluations show that D-REX is simple yet effective and improves relation extraction performance of strong baseline models by 1.2-4.7{\\%}.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":0,"sentence":"Existing studies on semantic parsing focus on mapping a natural-language utterance to a logical form (LF) in one turn. ","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"However, because natural language may contain ambiguity and variability, this is a difficult challenge.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":0,"sentence":" In this work, we investigate an interactive semantic parsing framework that explains the predicted LF step by step in natural language and enables the user to make corrections through natural-language feedback for individual steps.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":0,"sentence":"We focus on question answering over knowledge bases (KBQA) as an instantiation of our framework, aiming to increase the transparency of the parsing process and help the user trust the final answer.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":0,"sentence":"We construct INSPIRED, a crowdsourced dialogue dataset derived from the ComplexWebQuestions dataset. ","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":0,"sentence":"Our experiments show that this framework has the potential to greatly improve overall parse accuracy. ","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":0,"sentence":"The results demonstrate that our framework promises to be effective across such models.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":1,"sentence":"Personalized news recommendation is an essential technique to help users find interested news.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"Accurately matching user's interests and candidate news is the key to news recommendation.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":1,"sentence":"Most existing methods learn a single user embedding from user's historical behaviors to represent the reading interest.","offset":2,"pro":0.2857142857142857,"labels":"BAC"},{"idx":1,"sentence":"However, user interest is usually diverse and may not be adequately modeled by a single user embedding.","offset":3,"pro":0.42857142857142855,"labels":"GAP"},{"idx":1,"sentence":"In this paper, we propose a poly attention scheme to learn multiple interest vectors for each user, which encodes the different aspects of user interest. ","offset":4,"pro":0.5714285714285714,"labels":"PUR"},{"idx":1,"sentence":" Moreover, we design a category-aware attention weighting strategy that incorporates the news category information as explicit interest signals into the attention mechanism.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":1,"sentence":"Extensive experiments on the MIND news recommendation benchmark demonstrate that our approach significantly outperforms existing state-of-the-art methods.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":2,"sentence":"Knowledge-enhanced methods have bridged the gap between human beings and machines in generating dialogue responses.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"However, most previous works solely seek knowledge from a single source, and thus they often fail to obtain available knowledge because of the insufficient coverage of a single knowledge source. ","offset":1,"pro":0.25,"labels":"GAP"},{"idx":2,"sentence":"This paper proposes a novel approach Knowledge Source Aware Multi-Head Decoding, KSAM, to infuse multi-source knowledge into dialogue generation more efficiently. ","offset":2,"pro":0.5,"labels":"PUR"},{"idx":2,"sentence":"Experiments on a Chinese multi-source knowledge-aligned dataset demonstrate the superior performance of KSAM against various competitive approaches.","offset":3,"pro":0.75,"labels":"CLN"},{"idx":3,"sentence":"We present a playbook for responsible dataset creation for polyglossic, multidialectal languages.","offset":0,"pro":0,"labels":"MTD"},{"idx":3,"sentence":"his work is informed by a study on Arabic annotation of social media content.","offset":1,"pro":0.5,"labels":"MTD"},{"idx":4,"sentence":"This is due to learning spurious correlations between words that are not necessarily relevant to hateful language, and hate speech labels from the training corpus. ","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"While this has been demonstrated to improve the generalizability of classifiers, the coverage of such methods is limited and the dictionaries require regular manual updates from human experts. ","offset":1,"pro":0.25,"labels":"GAP"},{"idx":4,"sentence":"using attribution methods with dynamic refinement of the list of terms that need to be regularized during training.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":4,"sentence":"Our approach is flexible and improves the cross-corpora performance over previous work independently and in combination with pre-defined dictionaries.","offset":3,"pro":0.75,"labels":"CLN"},{"idx":5,"sentence":"Users interacting with voice assistants today need to phrase their requests in a very specific manner to elicit an appropriate response.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"This limits the user experience, and is partly due to the lack of reasoning capabilities of dialogue platforms and the hand-crafted rules that require extensive labor. ","offset":1,"pro":0.2,"labels":"GAP"},{"idx":5,"sentence":" In this work, we propose a novel method to incorporate the knowledge reasoning capability into dialog systems in a more scalable and generalizable manner. ","offset":2,"pro":0.4,"labels":"PUR"},{"idx":5,"sentence":"To the best of our knowledge, this is the first work to have transformer models generate responses by reasoning over differentiable knowledge graphs. ","offset":3,"pro":0.6,"labels":"CTN"},{"idx":5,"sentence":"Empirical results show that this method can effectively and efficiently incorporate a knowledge graph into a dialogue system with fully-interpretable reasoning paths.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":6,"sentence":"Keyphrase extraction (KPE) automatically extracts phrases in a document that provide a concise summary of the core content, which benefits downstream information retrieval and NLP tasks. ","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"They suffer performance degradation on long documents due to discrepancy between sequence lengths which causes mismatch between representations of keyphrase candidates and the document.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":6,"sentence":"In this work, we propose a novel unsupervised embedding-based KPE approach, Masked Document Embedding Rank (MDERank), to address this problem","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":6,"sentence":"by leveraging a mask strategy and ranking candidates by the similarity between embeddings of the source document and the masked document. ","offset":3,"pro":0.5,"labels":"MTD"},{"idx":6,"sentence":"Comprehensive evaluations on six KPE benchmarks demonstrate that the proposed MDERank outperforms state-of-the-art unsupervised KPE approach by average 1.80 F1@15 improvement. ","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":6,"sentence":"by proposing a novel self-supervised contrastive learning method, which is more compatible to MDERank than vanilla BERT.","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":7,"sentence":"but it is hard to answer how the change of the encoded linguistic information will affect task performance. ","offset":0,"pro":0,"labels":"GAP"},{"idx":7,"sentence":" Its key idea is to obtain a set of models which are Pareto-optimal in terms of both objectives.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":7,"sentence":"From this viewpoint, we propose a method to optimize the Pareto-optimal models by formalizing it as a multi-objective optimization problem. ","offset":2,"pro":0.4,"labels":"MTD"},{"idx":7,"sentence":"Experimental results demonstrate that the proposed method is better than a baseline method.","offset":3,"pro":0.6,"labels":"RST"},{"idx":7,"sentence":"Our empirical findings suggest that some syntactic information is helpful for NLP tasks whereas encoding more syntactic information does not necessarily lead to better performance, because the model architecture is also an important factor.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":8,"sentence":"The automation of extracting argument structures faces a pair of challenges on (1) encoding long-term contexts to facilitate comprehensive understanding, and (2) improving data efficiency since constructing high-quality argument structures is time-consuming. ","offset":0,"pro":0,"labels":"GAP"},{"idx":8,"sentence":"To tackle the difficulty of data annotation, we examine two complementary methods: (i) transfer learning to leverage existing annotated data to boost model performance in a new target domain, and (ii) active learning to strategically identify a small amount of samples for annotation. ","offset":1,"pro":0.25,"labels":"MTD"},{"idx":8,"sentence":"With extensive experiments, we show that our simple-yet-effective acquisition strategies yield competitive results against three strong comparisons.","offset":2,"pro":0.5,"labels":"RST"},{"idx":8,"sentence":"Combined with transfer learning, substantial F1 score boost (5-25) can be further achieved during the early iterations of active learning across domains.","offset":3,"pro":0.75,"labels":"RST"},{"idx":9,"sentence":"However, existing tasks to assess LMs' efficacy as KBs do not adequately consider multiple large-scale updates. ","offset":0,"pro":0,"labels":"GAP"},{"idx":9,"sentence":"-in which multiple large-scale updates are made to LMs,","offset":1,"pro":0.2,"labels":"MTD"},{"idx":9,"sentence":"and the performance is measured with respect to the success in adding and updating knowledge while retaining existing knowledge. ","offset":2,"pro":0.4,"labels":"MTD"},{"idx":9,"sentence":"We find that our method is 4x more effective in terms of updates/forgets ratio, compared to a fine-tuning baseline.","offset":3,"pro":0.6,"labels":"RST"},{"idx":9,"sentence":"Experiments conducted on zsRE QA and NQ datasets show that our method outperforms existing approaches.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":10,"sentence":"Medical images are widely used in clinical decision-making, where writing radiology reports is a potential application that can be enhanced by automatic solutions to alleviate physicians' workload.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"In general, radiology report generation is an image-text task, where cross-modal mappings between images and texts play an important role in generating high-quality reports.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":10,"sentence":"Although previous studies attempt to facilitate the alignment via the co-attention mechanism under supervised settings, they suffer from lacking valid and accurate correspondences due to no annotation of such alignment. ","offset":2,"pro":0.25,"labels":"GAP"},{"idx":10,"sentence":"In detail, a shared memory is used to record the mappings between visual and textual information,","offset":3,"pro":0.375,"labels":"MTD"},{"idx":10,"sentence":"and the proposed reinforced algorithm is performed to learn the signal from the reports to guide the cross-modal alignment even though such reports are not directly related to how images and texts are mapped.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":10,"sentence":"Experimental results on two English radiology report datasets, i.e., IU X-Ray and MIMIC-CXR, show the effectiveness of our approach, where the state-of-the-art results are achieved.","offset":5,"pro":0.625,"labels":"RST"},{"idx":10,"sentence":"We further conduct human evaluation and case study","offset":6,"pro":0.75,"labels":"MTD"},{"idx":10,"sentence":"which confirm the validity of the reinforced algorithm in our approach.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":11,"sentence":"Deep learning has demonstrated performance advantages in a wide range of natural language processing tasks, including neural machine translation (NMT).","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"Transformer NMT models are typically strengthened by deeper encoder layers,","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":11,"sentence":"but deepening their decoder layers usually results in failure. ","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":11,"sentence":"Inspired by this discovery, we then propose approaches to improving it, with respect to model structure and model training, to make the deep decoder practical in NMT. ","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":11,"sentence":"For model training, we propose a collapse reducing training approach to improve the stability and effectiveness of deep-decoder training. ","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":11,"sentence":"The results showed that deepening the NMT model by increasing the number of decoder layers successfully prevented the deepened decoder from degrading to an unconditional language model.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":11,"sentence":"In contrast to prior work on deepening an NMT model on the encoder, our method can deepen the model on both the encoder and decoder at the same time, resulting in a deeper model and improved performance.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":12,"sentence":"For capturing the variety of code mixing in, and across corpus, Language ID (LID) tags based measures (CMI) have been proposed.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"Syntactical variety/patterns of code-mixing and their relationship vis-a-vis computational model's performance is under explored.","offset":1,"pro":0.25,"labels":"BAC"},{"idx":12,"sentence":"We train SoTA en-hi PoS tagger, accuracy of 93.4%, to reliably compute PoS tags on a corpus, ","offset":2,"pro":0.5,"labels":"MTD"},{"idx":12,"sentence":"and compare datasets using the measure.","offset":3,"pro":0.75,"labels":"MTD"},{"idx":13,"sentence":"A pressing challenge in current dialogue systems is to successfully converse with users on topics with information distributed across different modalities.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"Previous work in multiturn dialogue systems has primarily focused on either text or table information.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":13,"sentence":"We present a new dialogue dataset, HybriDialogue, which consists of crowdsourced natural conversations grounded on both Wikipedia text and tables.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":13,"sentence":"We propose retrieval, system state tracking, and dialogue response generation tasks for our dataset and conduct baseline experiments for each. ","offset":3,"pro":0.6,"labels":"MTD"},{"idx":13,"sentence":"demonstrating the importance of building stronger dialogue systems that can reason over the complex setting of informationseeking dialogue grounded on tables and text.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":14,"sentence":"Existing benchmarking corpora provide concordant pairs of full and abridged versions of Web, news or professional content.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"To date, all summarization datasets operate under a one-size-fits-all paradigm that may not reflect the full range of organic summarization needs.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":14,"sentence":"Several recently proposed models (e.g., plug and play language models) have the capacity to condition the generated summaries on a desired range of themes.","offset":2,"pro":0.3333333333333333,"labels":"BAC"},{"idx":14,"sentence":"These capacities remain largely unused and unevaluated as there is no dedicated dataset that would support the task of topic-focused summarization.","offset":3,"pro":0.5,"labels":"GAP"},{"idx":14,"sentence":"based on the well-known CNN/Dailymail dataset, and annotated via online crowd-sourcing. ","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":14,"sentence":"We evaluate a representative range of existing techniques and analyze the effectiveness of different prompting methods.","offset":5,"pro":0.8333333333333334,"labels":"PUR"},{"idx":15,"sentence":"and users signal their political affiliations in multiple ways-from self-declarations to community participation. ","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"Here, we test this assumption of political users","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":15,"sentence":"and show that commonly-used political-inference models do not generalize,","offset":2,"pro":0.2857142857142857,"labels":"RST"},{"idx":15,"sentence":"indicating heterogeneous types of political users. ","offset":3,"pro":0.42857142857142855,"labels":"CLN"},{"idx":15,"sentence":"Across a 14-year longitudinal analysis, ","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":15,"sentence":"Controlling for multiple factors, political users are more toxic on the platform and inter-party interactions are even more toxic-but not all political users behave this way.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":15,"sentence":"Last, we identify a subset of political users who repeatedly flip affiliations, ","offset":6,"pro":0.8571428571428571,"labels":"MTD"},{"idx":16,"sentence":"In this position paper, we describe our perspective on how meaningful resources for lower-resourced languages should be developed in connection with the speakers of those languages.","offset":0,"pro":0,"labels":"PUR"},{"idx":16,"sentence":"identifying shortcomings that limit their usefulness.","offset":1,"pro":0.2,"labels":"RST"},{"idx":16,"sentence":"We explore the contents of the names stored in Wikidata for a few lower-resourced languages ","offset":2,"pro":0.4,"labels":"MTD"},{"idx":16,"sentence":"We discuss quality issues present in WikiAnn and evaluate whether it is a useful supplement to hand-annotated data. ","offset":3,"pro":0.6,"labels":"MTD"},{"idx":16,"sentence":"We conclude with recommended guidelines for resource development.","offset":4,"pro":0.8,"labels":"CTN"},{"idx":17,"sentence":"However, these models still lack the robustness to achieve general adoption. ","offset":0,"pro":0,"labels":"GAP"},{"idx":17,"sentence":" We propose a combination of multitask training, data augmentation and contrastive learning to achieve better and more robust QE performance.","offset":1,"pro":0.25,"labels":"PUR"},{"idx":17,"sentence":"We show that our method improves QE performance significantly in the MLQE challenge and the robustness of QE models when tested in the Parallel Corpus Mining setup.","offset":2,"pro":0.5,"labels":"RST"},{"idx":17,"sentence":"We increase the accuracy in PCM by more than 0.80, making it on par with state-of-the-art PCM methods that use millions of sentence pairs to train their models. ","offset":3,"pro":0.75,"labels":"RST"},{"idx":18,"sentence":"In this paper, we bring a new way of digesting news content","offset":0,"pro":0,"labels":"PUR"},{"idx":18,"sentence":"by introducing the task of segmenting a news article into multiple sections and generating the corresponding summary to each section. ","offset":1,"pro":0.25,"labels":"MTD"},{"idx":18,"sentence":"First, we create and make available a dataset, SegNews, consisting of 27k news articles with sections and aligned heading-style section summaries. ","offset":2,"pro":0.5,"labels":"CTN"},{"idx":18,"sentence":" Experimental results on SegNews demonstrate that our model can outperform several state-of-the-art sequence-to-sequence generation models for this new task.","offset":3,"pro":0.75,"labels":"CLN"},{"idx":19,"sentence":"Though nearest neighbor Machine Translation (kNN-MT) (CITATION) has proved to introduce significant performance boosts over standard neural MT systems, it is prohibitively slow since it uses the entire reference corpus as the datastore for the nearest neighbor search. ","offset":0,"pro":0,"labels":"GAP"},{"idx":19,"sentence":"kNN-MT is thus two-orders slower than vanilla MT models, making it hard to be applied to real-world applications, especially online services.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":19,"sentence":"In this work, we propose Fast kNN-MT to address this issue. ","offset":2,"pro":0.2222222222222222,"labels":"PUR"},{"idx":19,"sentence":"for each word in a source sentence, Fast kNN-MT first selects its nearest token-level neighbors, which is limited to tokens that are the same as the query token.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":19,"sentence":"Then at each decoding step, in contrast to using the entire corpus as the datastore, the search space is limited to target tokens corresponding to the previously selected reference source tokens.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":19,"sentence":"This strategy avoids search through the whole datastore for nearest neighbors and drastically improves decoding efficiency.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":19,"sentence":"Without loss of performance, Fast kNN-MT is two-orders faster than kNN-MT, ","offset":6,"pro":0.6666666666666666,"labels":"RST"},{"idx":19,"sentence":"Fast kNN-MT enables the practical use of kNN-MT systems in real-world MT applications.","offset":7,"pro":0.7777777777777778,"labels":"CLN"},{"idx":19,"sentence":"The code is available at url{https://github.com/ShannonAI/fast-knn-nmt.","offset":8,"pro":0.8888888888888888,"labels":"CTN"},{"idx":20,"sentence":"Prior work on controllable text generation has focused on learning how to control language models through trainable decoding, smart-prompt design, or fine-tuning based on a desired objective. ","offset":0,"pro":0,"labels":"BAC"},{"idx":20,"sentence":"Accordingly, we explore a different approach altogether: ","offset":1,"pro":0.2,"labels":"PUR"},{"idx":20,"sentence":"Experiments show that there exist steering vectors, which, when added to the hidden states of the language model, generate a target sentence nearly perfectly (greater 99 BLEU) for English sentences from a variety of domains. ","offset":2,"pro":0.4,"labels":"RST"},{"idx":20,"sentence":"We find that distances between steering vectors reflect sentence similarity when evaluated on a textual similarity benchmark (STS-B), ","offset":3,"pro":0.6,"labels":"RST"},{"idx":20,"sentence":"Finally, we present an analysis of the intrinsic properties of the steering vectors. ","offset":4,"pro":0.8,"labels":"MTD"},{"idx":21,"sentence":"Generalising to unseen domains is under-explored and remains a challenge in neural machine translation. ","offset":0,"pro":0,"labels":"BAC"},{"idx":21,"sentence":" that learns to combine domain-specific parameters. ","offset":1,"pro":0.3333333333333333,"labels":"MTD"},{"idx":21,"sentence":"Empirical results on three language pairs show that our proposed fusion method outperforms other baselines up to +0.8 BLEU score on average.","offset":2,"pro":0.6666666666666666,"labels":"RST"},{"idx":22,"sentence":"We study this question","offset":0,"pro":0,"labels":"PUR"},{"idx":22,"sentence":"by conducting extensive empirical analysis that shed light on important features of successful instructional prompts. ","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":22,"sentence":"Some examples include decomposing a complex task instruction into multiple simpler tasks or itemizing instructions into sequential steps. ","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":22,"sentence":"For example, the same reframed prompts boost few-shot performance of GPT3-series and GPT2-series by 12.5% and 6.7% respectively averaged over all tasks.","offset":3,"pro":0.5,"labels":"RST"},{"idx":22,"sentence":"Furthermore, reframed instructions reduce the number of examples required to prompt LMs in the few-shot setting. ","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":22,"sentence":"Compared with original instructions, our reframed instructions lead to significant improvements across LMs with different sizes.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":23,"sentence":" We propose a simple approach to reorder the documents according to their relative importance before concatenating and summarizing them. ","offset":0,"pro":0,"labels":"PUR"},{"idx":23,"sentence":"Experiments show that our approach outperforms previous state-of-the-art methods with more complex architectures.","offset":1,"pro":0.5,"labels":"CLN"},{"idx":24,"sentence":"Here, we propose human language modeling (HuLM), a hierarchical extension to the language modeling problem where by a human- level exists to connect sequences of documents (e.g. social media messages) and capture the notion that human language is moderated by changing human states. ","offset":0,"pro":0,"labels":"PUR"},{"idx":24,"sentence":"and demonstrate it's effectiveness in terms of both language modeling (perplexity) for social media and fine-tuning for 4 downstream tasks spanning document- and user-levels. ","offset":1,"pro":0.5,"labels":"MTD"},{"idx":25,"sentence":"These methods modify input samples with prompt sentence pieces, and decode label tokens to map samples to corresponding labels. ","offset":0,"pro":0,"labels":"BAC"},{"idx":25,"sentence":"Since slot tagging samples are multiple consecutive words in a sentence, the prompting methods have to enumerate all n-grams token spans to find all the possible slots, which greatly slows down the prediction.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":25,"sentence":"To tackle this, we introduce an inverse paradigm for prompting. ","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":25,"sentence":" Such inverse prompting only requires a one-turn prediction for each slot type and greatly speeds up the prediction. ","offset":3,"pro":0.5,"labels":"MTD"},{"idx":25,"sentence":"from which the model learns to refine predictions by considering the relations between different slot types.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":25,"sentence":"We find, somewhat surprisingly, the proposed method not only predicts faster but also significantly improves the effect (improve over 6.1 F1-scores on 10-shot setting) and achieves new state-of-the-art performance.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":26,"sentence":"Due to the noisy nature of brain recordings, existing work has simplified brain-to-word decoding as a binary classification task which is to discriminate a brain signal between its corresponding word and a wrong one.","offset":0,"pro":0,"labels":"BAC"},{"idx":26,"sentence":"This pairwise classification task, however, cannot promote the development of practical neural decoders for two reasons. ","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":26,"sentence":"Second, a perfect pairwise decoder cannot guarantee the performance on direct classification. ","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":26,"sentence":"which is to predict the target word encoded in the neural image with a context as prompt. ","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":26,"sentence":"that leverages the pre-trained language model to predict the target word.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":26,"sentence":"To validate our method, we perform experiments on more than 20 participants from two brain imaging datasets. ","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":26,"sentence":"This result indicates that our model can serve as a state-of-the-art baseline for the CMC task.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":27,"sentence":"Language models excel at generating coherent text, ","offset":0,"pro":0,"labels":"BAC"},{"idx":27,"sentence":"However, these models can be biased in multiple ways, including the unfounded association of male and female genders with gender-neutral professions.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":27,"sentence":"Therefore, knowledge distillation without any fairness constraints may preserve or exaggerate the teacher model's biases onto the distilled model.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":27,"sentence":"To this end, we present a novel approach to mitigate gender disparity in text generation","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":27,"sentence":"by learning a fair model during knowledge distillation.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":27,"sentence":"We evaluate gender polarity across professions in open-ended text generated from the resulting distilled and finetuned GPT-2 models ","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":27,"sentence":"Finally, we observe that language models that reduce gender polarity in language generation do not improve embedding fairness or downstream classification fairness.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":28,"sentence":"We propose a probabilistic approach to select a subset of a target domain representative keywords from a candidate set, contrasting with a context domain. ","offset":0,"pro":0,"labels":"PUR"},{"idx":28,"sentence":"To contrast the target domain and the context domain, we adapt the two-component mixture model concept to generate a distribution of candidate keywords. ","offset":1,"pro":0.25,"labels":"MTD"},{"idx":28,"sentence":"To support the representativeness of the selected keywords towards the target domain, we introduce an optimization algorithm for selecting the subset from the generated candidate distribution. ","offset":2,"pro":0.5,"labels":"MTD"},{"idx":28,"sentence":" Finally, extensive experiments on multiple domains demonstrate the superiority of our approach over other baselines for the tasks of keyword summary generation and trending keywords selection.","offset":3,"pro":0.75,"labels":"CLN"},{"idx":29,"sentence":"Pre-trained models have achieved excellent performance on the dialogue task.","offset":0,"pro":0,"labels":"BAC"},{"idx":29,"sentence":"However, for the continual increase of online chit-chat scenarios, directly fine-tuning these models for each of the new tasks not only explodes the capacity of the dialogue system on the embedded devices but also causes knowledge forgetting on pre-trained models and knowledge interference among diverse dialogue tasks.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":29,"sentence":"First, we introduce the adapter module into pre-trained models for learning new dialogue tasks.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":29,"sentence":"As the only trainable module, it is beneficial for the dialogue system on the embedded devices to acquire new dialogue skills with negligible additional parameters. ","offset":3,"pro":0.6,"labels":"MTD"},{"idx":29,"sentence":" Empirical evaluation and analysis indicate that our framework obtains comparable performance under deployment-friendly model capacity.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":0,"sentence":"Understanding causal narratives communicated in clinical notes can help make strides towards personalized healthcare. ","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"Extracted causal information from clinical notes can be combined with structured EHR data such as patients' demographics, diagnoses, and medications. ","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":0,"sentence":"n this work, we propose annotation guidelines, develop an annotated corpus and provide baseline scores to identify types and direction of causal relations between a pair of biomedical concepts in clinical notes; communicated implicitly or explicitly, identified either in a single sentence or across multiple sentences","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":0,"sentence":"We annotate a total of 2714 de-identified examples sampled from the 2018 n2c2 shared task dataset and train four different language model based architectures.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":0,"sentence":"Annotation based on our guidelines achieved a high inter-annotator agreement i.e. Fleiss' kappa (kappa) score of 0.72, and our model for identification of causal relations achieved a macro F1 score of 0.56 on the test data.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":0,"sentence":"The high inter-annotator agreement for clinical text shows the quality of our annotation guidelines while the provided baseline F1 score sets the direction for future research towards understanding narratives in clinical texts.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":1,"sentence":"However, these models are often huge and produce large sentence embeddings. ","offset":0,"pro":0,"labels":"GAP"},{"idx":1,"sentence":" In this paper, we propose Homomorphic Projective Distillation (HPD) to learn compressed sentence embeddings.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":1,"sentence":"Our method augments a small Transformer encoder model with learnable projection layers to produce compact representations while mimicking a large pre-trained language model to retain the sentence representation quality. ","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":1,"sentence":" Experiments show that our method achieves 2.7-4.5 points performance gain on STS tasks compared with previous best representations of the same size","offset":3,"pro":0.5,"labels":"RST"},{"idx":1,"sentence":" In SR tasks, our method improves retrieval speed (8.2{mbox{times) and memory usage (8.0{mbox{times) compared with state-of-the-art large models.","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":1,"sentence":"Our implementation is available at url{https://github.com/XuandongZhao/HPD.","offset":5,"pro":0.8333333333333334,"labels":"CTN"},{"idx":2,"sentence":"We study event understanding as a critical step towards visual commonsense tasks","offset":0,"pro":0,"labels":"PUR"},{"idx":2,"sentence":"Meanwhile, we argue that current object-based event understanding is purely likelihood-based, leading to incorrect event prediction, due to biased correlation between events and objects","offset":1,"pro":0.25,"labels":"PUR"},{"idx":2,"sentence":"We propose to mitigate such biases with do-calculus, proposed in causality research, but overcoming its limited robustness, by an optimized aggregation with association-based prediction","offset":2,"pro":0.5,"labels":"MTD"},{"idx":2,"sentence":"We show the effectiveness of our approach, intrinsically by comparing our generated events with ground-truth event annotation, and extrinsically by downstream commonsense tasks.","offset":3,"pro":0.75,"labels":"MTD"},{"idx":3,"sentence":"Current Question Answering over Knowledge Graphs (KGQA) task mainly focuses on performing answer reasoning upon KGs with binary facts.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"However, it neglects the n-ary facts, which contain more than two entities.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":3,"sentence":"We propose two feasible improvements: 1) upgrade the basic reasoning unit from entity or relation to fact, and 2) upgrade the reasoning structure from chain to tree. ","offset":2,"pro":0.4,"labels":"PUR"},{"idx":3,"sentence":" FacTree transforms the question into a fact tree and performs iterative fact reasoning on the fact tree to infer the correct answer.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":3,"sentence":"Experimental results on the n-ary KGQA dataset we constructed and two binary KGQA benchmarks demonstrate the effectiveness of FacTree compared with state-of-the-art methods.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":4,"sentence":"We introduce a method for improving the structural understanding abilities of language models.","offset":0,"pro":0,"labels":"PUR"},{"idx":4,"sentence":"Unlike previous approaches that finetune the models with task-specific augmentation,","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":4,"sentence":"we pretrain language models to generate structures from the text on a collection of task-agnostic corpora.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":4,"sentence":"Our structure pretraining enables zero-shot transfer of the learned knowledge that models have about the structure tasks.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":4,"sentence":"We study the performance of this approach on 28 datasets, spanning 10 structure prediction tasks including open information extraction, joint entity and relation extraction, named entity recognition, relation classification, semantic role labeling, event extraction, coreference resolution, factual probe, intent detection, and dialogue state tracking.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":4,"sentence":"We further enhance the pretraining with the task-specific training sets. ","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":4,"sentence":"Our code and datasets will be made publicly available.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":5,"sentence":"The current performance of discourse models is very low on texts outside of the training distribution's coverage, diminishing the practical utility of existing models","offset":0,"pro":0,"labels":"GAP"},{"idx":5,"sentence":"here is need for a measure that can inform us to what extent our model generalizes from the training to the test sample when these samples may be drawn from distinct distributions. ","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":5,"sentence":"hile this can be estimated via distribution shift, we argue that this does not directly correlate with change in the observed error of a classifier (i.e. error-gap). ","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":5,"sentence":"hus, we propose to use a statistic from the theoretical domain adaptation literature which can be directly tied to error-gap. ","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":5,"sentence":"Our results not only motivate our proposal and help us to understand its limitations, but also provide insight on the properties of discourse models and datasets which improve performance in domain adaptation.","offset":4,"pro":0.5714285714285714,"labels":"IMP"},{"idx":5,"sentence":"For instance, we find that non-news datasets are slightly easier to transfer to than news datasets when the training and test sets are very different.","offset":5,"pro":0.7142857142857143,"labels":"IMP"},{"idx":5,"sentence":"Our code and an associated Python package are available to allow practitioners to make more informed model and dataset choices.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":6,"sentence":"In this paper, we address the problem of the absence of organized benchmarks in the Turkish language. ","offset":0,"pro":0,"labels":"PUR"},{"idx":6,"sentence":"As a solution, we present Mukayese, a set of NLP benchmarks for the Turkish language that contains several NLP tasks.","offset":1,"pro":0.25,"labels":"PUR"},{"idx":6,"sentence":"We work on one or more datasets for each benchmark and present two or more baselines. ","offset":2,"pro":0.5,"labels":"MTD"},{"idx":6,"sentence":"All datasets and baselines are available under: https://github.com/alisafaya/mukayese","offset":3,"pro":0.75,"labels":"CTN"},{"idx":7,"sentence":"This challenge is magnified in natural language processing, where no general rules exist for data augmentation due to the discrete nature of natural language.","offset":0,"pro":0,"labels":"GAP"},{"idx":7,"sentence":"We tackle this challenge by presenting a Virtual augmentation Supported Contrastive Learning of sentence representations (VaSCL). ","offset":1,"pro":0.25,"labels":"PUR"},{"idx":7,"sentence":"Leveraging the large training batch size of contrastive learning, we approximate the neighborhood of an instance via its K-nearest in-batch neighbors in the representation space. ","offset":2,"pro":0.5,"labels":"MTD"},{"idx":7,"sentence":"We access the performance of VaSCL on a wide range of downstream tasks and set a new state-of-the-art for unsupervised sentence representation learning.","offset":3,"pro":0.75,"labels":"CLN"},{"idx":8,"sentence":"However, the computational patterns of FFNs are still unclear. In this work, we study the computational patterns of FFNs and observe that most inputs only activate a tiny ratio of neurons of FFNs.","offset":0,"pro":0,"labels":"GAP"},{"idx":8,"sentence":"This phenomenon is similar to the sparsity of the human brain, which drives research on functional partitions of the human brain.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":8,"sentence":"Specifically, MoEfication consists of two phases: (1) splitting the parameters of FFNs into multiple functional partitions as experts, and (2) building expert routers to decide which experts will be used for each input","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":8,"sentence":"The source code of this paper can be obtained from https://github.com/thunlp/MoEfication.","offset":3,"pro":0.5,"labels":"CTN"},{"idx":8,"sentence":"Experimental results show that MoEfication can conditionally use 10% to 30% of FFN parameters while maintaining over 95% original performance for different models on various downstream tasks.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":8,"sentence":"Besides, MoEfication brings two advantages: (1) it significantly reduces the FLOPS of inference, i.e., 2x speedup with 25% of FFN parameters, and (2) it provides a fine-grained perspective to study the inner mechanism of FFNs.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":9,"sentence":"These approaches, however, exploit general dialogic corpora (e.g., Reddit) and thus presumably fail to reliably embed domain-specific knowledge useful for concrete downstream TOD domains","offset":0,"pro":0,"labels":"GAP"},{"idx":9,"sentence":"In this work, we investigate the effects of domain specialization of pretrained language models (PLMs) for TOD","offset":1,"pro":0.2,"labels":"PUR"},{"idx":9,"sentence":"Within our DS-TOD framework, we first automatically extract salient domain-specific terms, and then use them to construct DomainCC and DomainReddit - resources that we leverage for domain-specific pretraining, based on (i) masked language modeling (MLM) and (ii) response selection (RS) objectives, respectively. ","offset":2,"pro":0.4,"labels":"MTD"},{"idx":9,"sentence":"Our experiments with prominent TOD tasks - dialog state tracking (DST) and response retrieval (RR) - encompassing five domains from the MultiWOZ benchmark demonstrate the effectiveness of DS-TOD.","offset":3,"pro":0.6,"labels":"RST"},{"idx":9,"sentence":"Moreover, we show that the light-weight adapter-based specialization (1) performs comparably to full fine-tuning in single domain setups and (2) is particularly suitable for multi-domain specialization, where besides advantageous computational footprint, it can offer better TOD performance.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":10,"sentence":"Recently, the problem of robustness of pre-trained language models (PrLMs) has received increasing research interest. ","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"However, we find that the adversarial samples that PrLMs fail are mostly non-natural and do not appear in reality. ","offset":1,"pro":0.25,"labels":"GAP"},{"idx":10,"sentence":"We also investigate two applications of the anomaly detector: (1) In data augmentation, we employ the anomaly detector to force generating augmented data that are distinguished as non-natural, which brings larger gains to the accuracy of PrLMs. ","offset":2,"pro":0.5,"labels":"MTD"},{"idx":10,"sentence":"It can be used to defend all types of attacks and achieves higher accuracy on both adversarial samples and compliant samples than other defense frameworks.","offset":3,"pro":0.75,"labels":"IMP"},{"idx":11,"sentence":"We present a comprehensive study of sparse attention patterns in Transformer models. ","offset":0,"pro":0,"labels":"PUR"},{"idx":11,"sentence":"Then we compare the widely used local attention pattern and the less-well-studied global attention pattern, demonstrating that global patterns have several unique advantages. ","offset":1,"pro":0.2,"labels":"MTD"},{"idx":11,"sentence":"Rather than choosing a fixed attention pattern, the adaptive axis attention method identifies important tokens-for each task and model layer-and focuses attention on those.","offset":2,"pro":0.4,"labels":"RST"},{"idx":11,"sentence":"Drawing on this insight, we propose a novel Adaptive Axis Attention method, which learns-during fine-tuning-different attention patterns for each Transformer layer depending on the downstream task.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":11,"sentence":"It does not require pre-training to accommodate the sparse patterns and demonstrates competitive and sometimes better performance against fixed sparse attention patterns that require resource-intensive pre-training.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":12,"sentence":"Most research on question answering focuses on the pre-deployment stage; i.e., building an accurate model for deployment.I","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"We collect a retrieval-based QA dataset, FeedbackQA, which contains interactive feedback from users.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":12,"sentence":"We collect this dataset by deploying a base QA system to crowdworkers who then engage with the system and provide feedback on the quality of its answers.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":12,"sentence":"We train a neural model with this feedback data that can generate explanations and re-score answer candidates.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":12,"sentence":"The generated explanations also help users make informed decisions about the correctness of answers.","offset":4,"pro":0.8,"labels":"IMP"},{"idx":13,"sentence":"However, contemporary NLI models are still limited in interpreting mathematical knowledge written in Natural Language, even though mathematics is an integral part of scientific argumentation for many disciplines.","offset":0,"pro":0,"labels":"GAP"},{"idx":13,"sentence":"One of the fundamental requirements towards mathematical language understanding, is the creation of models able to meaningfully represent variables.","offset":1,"pro":0.3333333333333333,"labels":"BAC"},{"idx":13,"sentence":"ecent research has formalised the variable typing task, a benchmark for the understanding of abstract mathematical types and variables in a sentence. I","offset":2,"pro":0.6666666666666666,"labels":"BAC"},{"idx":14,"sentence":"We start with an iterative framework in which an input sentence is revised using explicit edit operations, ","offset":0,"pro":0,"labels":"MTD"},{"idx":14,"sentence":"This allows us to combine the advantages of generative and revision-based approaches: paraphrasing captures complex edit operations, and the use of explicit edit operations in an iterative manner provides controllability and interpretability.","offset":1,"pro":0.3333333333333333,"labels":"MTD"},{"idx":14,"sentence":"We demonstrate these advantages of GRS compared to existing methods on the Newsela and ASSET datasets","offset":2,"pro":0.6666666666666666,"labels":"CLN"},{"idx":15,"sentence":"Morphologically-rich polysynthetic languages present a challenge for NLP systems due to data sparsity, and a common strategy to handle this issue is to apply subword segmentation. ","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"Then, we compare the morphologically inspired segmentation methods against Byte-Pair Encodings (BPEs) as inputs for machine translation (MT) when translating to and from Spanish. ","offset":1,"pro":0.3333333333333333,"labels":"MTD"},{"idx":15,"sentence":" Finally, we contribute two new morphological segmentation datasets for Raramuri and Shipibo-Konibo, and a parallel corpus for Raramuri-Spanish.","offset":2,"pro":0.6666666666666666,"labels":"CTN"},{"idx":16,"sentence":"We introduce distributed NLI, a new NLU task with a goal to predict the distribution of human judgements for natural language inference.","offset":0,"pro":0,"labels":"PUR"},{"idx":16,"sentence":"We show that by applying additional distribution estimation methods, namely, Monte Carlo (MC) Dropout, Deep Ensemble, Re-Calibration, and Distribution Distillation, models can capture human judgement distribution more effectively than the softmax baseline. ","offset":1,"pro":0.25,"labels":"MTD"},{"idx":16,"sentence":"We showcase the common errors for MC Dropout and Re-Calibration.","offset":2,"pro":0.5,"labels":"CLN"},{"idx":16,"sentence":"Finally, we give guidelines on the usage of these methods with different levels of data availability and encourage future work on modeling the human opinion distribution for language reasoning.","offset":3,"pro":0.75,"labels":"IMP"},{"idx":17,"sentence":"First, we survey recent developments in computational morphology with a focus on low-resource languages. ","offset":0,"pro":0,"labels":"MTD"},{"idx":17,"sentence":"We perform an empirical study on a truly unsupervised version of the paradigm completion task and show tha","offset":1,"pro":0.25,"labels":"MTD"},{"idx":17,"sentence":"while existing state-of-the-art models bridged by two newly proposed models we devise perform reasonably, there is still much room for improvement","offset":2,"pro":0.5,"labels":"CLN"},{"idx":17,"sentence":"he stakes are high: solving this task will increase the language coverage of morphological resources by a number of magnitudes.","offset":3,"pro":0.75,"labels":"IMP"},{"idx":18,"sentence":" Recent advances in word embeddings have proven successful in learning entity representations from short texts","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":" but fall short on longer documents because they do not capture full book-level information.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":18,"sentence":"To overcome the weakness of such text-based embeddings, we propose two novel methods for representing characters","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":18,"sentence":"We test the quality of these character embeddings using a new benchmark suite to evaluate character representations, encompassing 12 different tasks.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":18,"sentence":"We show that our representation techniques combined with text-based embeddings lead to the best character representations, outperforming text-based embeddings in four tasks.","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":18,"sentence":"Our dataset and evaluation script will be made publicly available to stimulate additional work in this area.","offset":5,"pro":0.8333333333333334,"labels":"CTN"},{"idx":19,"sentence":"achine reading comprehension (MRC) has drawn a lot of attention as an approach for assessing the ability of systems to understand natural language","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":" Usually systems focus on selecting the correct answer to a question given a contextual paragraph","offset":1,"pro":0.08333333333333333,"labels":"BAC"},{"idx":19,"sentence":"However, for many applications of multiple-choice MRC systems there are two additional considerations","offset":2,"pro":0.16666666666666666,"labels":"GAP"},{"idx":19,"sentence":" For multiple-choice exams there is often a negative marking scheme; ","offset":3,"pro":0.25,"labels":"GAP"},{"idx":19,"sentence":"here is a penalty for an incorrect answer. In terms of an MRC system this means that the system is required to have an idea of the uncertainty in the predicted answer. ","offset":4,"pro":0.3333333333333333,"labels":"GAP"},{"idx":19,"sentence":"This paper investigates both of these issues","offset":5,"pro":0.4166666666666667,"labels":"PUR"},{"idx":19,"sentence":"by making use of predictive uncertainty.","offset":6,"pro":0.5,"labels":"MTD"},{"idx":19,"sentence":"Whether the system should propose an answer is a direct application of answer uncertainty. ","offset":7,"pro":0.5833333333333334,"labels":"MTD"},{"idx":19,"sentence":"The simplest is to explicitly build a system on data that includes this option. ","offset":8,"pro":0.6666666666666666,"labels":"MTD"},{"idx":19,"sentence":"If the system is not sufficiently confident it will select NOA. ","offset":9,"pro":0.75,"labels":"MTD"},{"idx":19,"sentence":"A high-performance MRC system is used to evaluate whether answer uncertainty can be applied in these situations","offset":10,"pro":0.8333333333333334,"labels":"MTD"},{"idx":19,"sentence":" It is shown that uncertainty does allow questions that the system is not confident about to be detected. ","offset":11,"pro":0.9166666666666666,"labels":"CLN"},{"idx":20,"sentence":"Being able to reliably estimate self-disclosure - a key component of friendship and intimacy - from language is important for many psychology studies.","offset":0,"pro":0,"labels":"BAC"},{"idx":20,"sentence":"ccuracy of predicted message-level self-disclosure of the best-performing model (","offset":1,"pro":0.14285714285714285,"labels":"RST"},{"idx":20,"sentence":" is much higher than the respective across data set accuracy (mean Pearson's r=0.32), due to both variations in the corpora (e.g., medical vs. general topics) and labeling instructions (target variables: self-disclosure, emotional disclosure, intimacy)","offset":2,"pro":0.2857142857142857,"labels":"RST"},{"idx":20,"sentence":"We develop a multi-task model that yields better results, with an average Pearson's r of 0.37 for out-of-corpora prediction.","offset":3,"pro":0.42857142857142855,"labels":"CLN"},{"idx":20,"sentence":"However, some lexical features, such as expression of negative emotions and use of first person personal pronouns such as I' reliably predict self-disclosure across corpora","offset":4,"pro":0.5714285714285714,"labels":"CLN"},{"idx":20,"sentence":"e build single-task models on five self-disclosure corpora, ","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":20,"sentence":"ut find that these models generalize poorly; the within-domain","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":0,"sentence":" Large-scale pre-trained language models have demonstrated strong knowledge representation abilit","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"However, recent studies suggest that even though these giant models contain rich simple commonsense knowledge (e.g., bird can fly and fish can swim.), they often struggle with complex commonsense knowledge that involves multiple eventualities (verb-centric phrases, e.g., identifying the relationship between Jim yells at Bob and Bob is upse","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":0,"sentence":"o address this issue, in this paper, we propose to help pre-trained language models better incorporate complex commonsense knowledge","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":0,"sentence":"Unlike direct fine-tuning approaches, we do not focus on a specific task and instead propose a general language model named CoCoLM","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":0,"sentence":"Through the careful training over a large-scale eventuality knowledge graph ASER,","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":0,"sentence":"we successfully teach pre-trained language models (i.e., BERT and RoBERTa) rich multi-hop commonsense knowledge among eventualiti","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":0,"sentence":"Experiments on multiple commonsense tasks that require the correct understanding of eventualities demonstrate the effectiveness of CoCoL","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":1,"sentence":"A critical bottleneck in supervised machine learning is the need for large amounts of labeled data which is expensive and time-consuming to obtai","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"lthough a small amount of labeled data cannot be used to train a model, it can be used effectively for the generation of humaninterpretable labeling functions","offset":1,"pro":0.125,"labels":"BAC"},{"idx":1,"sentence":"These LFs, in turn, have been used to generate a large amount of additional noisy labeled data in a paradigm that is now commonly referred to as data programmin","offset":2,"pro":0.25,"labels":"BAC"},{"idx":1,"sentence":"Previous methods of generating LFs do not attempt to use the given labeled data further to train a model, thus missing opportunities for improving performance. ","offset":3,"pro":0.375,"labels":"GAP"},{"idx":1,"sentence":"dditionally, since the LFs are generated automatically, they are likely to be noisy, and naively aggregating these LFs can lead to suboptimal results","offset":4,"pro":0.5,"labels":"GAP"},{"idx":1,"sentence":"n this work, we propose an LF-based bi-level optimization framework WISDOM to solve these two critical limitations","offset":5,"pro":0.625,"labels":"PUR"},{"idx":1,"sentence":"ISDOM learns a joint model on the (same) labeled dataset used for LF induction along with any unlabeled data in a semi-supervised manner, and more critically, reweighs each LF according to its goodness, influencing its contribution to the semi-supervised loss using a robust bi-level optimization algorithm","offset":6,"pro":0.75,"labels":"MTD"},{"idx":1,"sentence":"We show that WISDOM significantly outperforms prior approaches on several text classification datasets","offset":7,"pro":0.875,"labels":"CLN"},{"idx":2,"sentence":"he emotion cause pair extraction (ECPE) task aims to extract emotions and causes as pairs from document","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"We observe that the relative distance distribution of emotions and causes is extremely imbalanced in the typical ECPE datase","offset":1,"pro":0.125,"labels":"GAP"},{"idx":2,"sentence":" Existing methods have set a fixed size window to capture relations between neighboring clause","offset":2,"pro":0.25,"labels":"BAC"},{"idx":2,"sentence":"However, they neglect the effective semantic connections between distant clauses, leading to poor generalization ability towards position-insensitive data.","offset":3,"pro":0.375,"labels":"GAP"},{"idx":2,"sentence":"To alleviate the problem, we propose a novel Multi-Granularity Semantic Aware Graph model (MGSAG) to incorporate fine-grained and coarse-grained semantic features jointly, without regard to distance limitation","offset":4,"pro":0.5,"labels":"PUR"},{"idx":2,"sentence":" In particular, we first explore semantic dependencies between clauses and keywords extracted from the document that convey fine-grained semantic features, obtaining keywords enhanced clause representation","offset":5,"pro":0.625,"labels":"MTD"},{"idx":2,"sentence":"Besides, a clause graph is also established to model coarse-grained semantic relations between clauses. ","offset":6,"pro":0.75,"labels":"MTD"},{"idx":2,"sentence":"xperimental results indicate that MGSAG surpasses the existing state-of-the-art ECPE models. Especially, MGSAG outperforms other models significantly in the condition of position-insensitive","offset":7,"pro":0.875,"labels":"RST"},{"idx":3,"sentence":"Predicate entailment detection is a crucial task for question-answering from text, where previous work has explored unsupervised learning of entailment graphs from typed open relation triple","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":" In this paper, we present the first pipeline for building Chinese entailment graphs, which involves a novel high-recall open relation extraction (ORE) method and the first Chinese fine-grained entity typing dataset under the FIGER type ontology","offset":1,"pro":0.25,"labels":"PUR"},{"idx":3,"sentence":"Through experiments on the Levy-Holt dataset","offset":2,"pro":0.5,"labels":"MTD"},{"idx":3,"sentence":"we verify the strength of our Chinese entailment graph, and reveal the cross-lingual complementarity: on the parallel Levy-Holt dataset, an ensemble of Chinese and English entailment graphs outperforms both monolingual graphs, and raises unsupervised SOTA by 4.7 AUC points","offset":3,"pro":0.75,"labels":"CLN"},{"idx":4,"sentence":"hile previous studies tackle the problem from different aspects, the essence of paraphrase generation is to retain the key semantics of the source sentence and rewrite the rest of the conten","offset":0,"pro":0,"labels":"GAP"},{"idx":4,"sentence":"Inspired by this observation, we propose a novel two-stage model, PGKPR, for paraphrase generation with keyword and part-of-speech reconstruction","offset":1,"pro":0.1111111111111111,"labels":"PUR"},{"idx":4,"sentence":"The rationale is to capture simultaneously the possible keywords of a source sentence and the relations between them to facilitate the rewriting","offset":2,"pro":0.2222222222222222,"labels":"BAC"},{"idx":4,"sentence":"n the first stage, we identify the possible keywords using a prediction attribution technique, where the words obtaining higher attribution scores are more likely to be the keywords","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":4,"sentence":" the second stage, we train a transformer-based model via multi-task learning for paraphrase generation.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":4,"sentence":"e novel learning task is the reconstruction of the keywords and part-of-speech tags, respectively, from a perturbed sequence of the source sentence.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":4,"sentence":"The learned encodings are then decoded to generate the paraphrase","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":4,"sentence":"e conduct the experiments on two commonly-used datasets, ","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":4,"sentence":"demonstrate the superior performance of PGKPR over comparative models on multiple evaluation metrics.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":5,"sentence":"Chinese Spelling Correction (CSC) is a task to detect and correct misspelled characters in Chinese texts. CSC is challenging since many Chinese characters are visually or phonologically similar but with quite different semantic meanings. Many recent works use BERT-based language models to directly correct each character of the input sentence","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"However, these methods can be sub-optimal since they correct every character of the sentence only by the context which is easily negatively affected by the misspelled characters","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":5,"sentence":"ome other works propose to use an error detector to guide the correction by masking the detected errors","offset":2,"pro":0.3333333333333333,"labels":"BAC"},{"idx":5,"sentence":"evertheless, these methods dampen the visual or phonological features from the misspelled characters which could be critical for correction","offset":3,"pro":0.5,"labels":"GAP"},{"idx":5,"sentence":"n this work, we propose a novel general detector-corrector multi-task framework where the corrector uses BERT to capture the visual and phonological features from each character in the raw sentence and uses a late fusion strategy to fuse the hidden states of the corrector with that of the detector to minimize the negative impact from the misspelled characters","offset":4,"pro":0.6666666666666666,"labels":"PUR"},{"idx":5,"sentence":"omprehensive experiments on benchmarks demonstrate that our proposed method can significantly outperform the state-of-the-art methods in the CSC task","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":6,"sentence":"The task of converting a natural language question into an executable SQL query, known as text-to-SQL, is an important branch of semantic parsin","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"The state-of-the-art graph-based encoder has been successfully used in this task but does not model the question syntax wel","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":6,"sentence":"n this paper, we propose S^2SQL, injecting Syntax to question-Schema graph encoder for Text-to-SQL parsers, which effectively leverages the syntactic dependency information of questions in text-to-SQL to improve the performance","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":6,"sentence":"We also employ the decoupling constraint to induce diverse relational edge embedding","offset":3,"pro":0.5,"labels":"MTD"},{"idx":6,"sentence":"hich further improves the network's performance","offset":4,"pro":0.6666666666666666,"labels":"PUR"},{"idx":6,"sentence":"periments on the Spider and robustness setting Spider-Syn demonstrate that the proposed approach outperforms all existing methods when pre-training models are used, resulting in a performance ranks first on the Spider leaderboard","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":7,"sentence":"his paper presents the first multi-objective transformer model for generating open cloze tests that exploits generation and discrimination capabilities to improve performanc","offset":0,"pro":0,"labels":"PUR"},{"idx":7,"sentence":"Our model is further enhanced by tweaking its loss function and applying a post-processing re-ranking algorithm that improves overall test structure.","offset":1,"pro":0.25,"labels":"MTD"},{"idx":7,"sentence":"xperiments using automatic and human evaluation show that our approach can achieve up to 82% accuracy according to experts, outperforming previous work and baseline","offset":2,"pro":0.5,"labels":"RST"},{"idx":7,"sentence":"e also release a collection of high-quality open cloze tests along with sample system output and human annotations that can serve as a future benchmark","offset":3,"pro":0.75,"labels":"CTN"},{"idx":8,"sentence":"We introduce a method for unsupervised parsing that relies on bootstrapping classifiers to identify if a node dominates a specific span in a sentence.","offset":0,"pro":0,"labels":"PUR"},{"idx":8,"sentence":"ere are two types of classifiers, an inside classifier that acts on a span, and an outside classifier that acts on everything outside of a given spa","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":8,"sentence":"hrough self-training and co-training with the two classifier","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":8,"sentence":"we show that the interplay between them helps improve the accuracy of both, and as a result, effectively par","offset":3,"pro":0.42857142857142855,"labels":"RST"},{"idx":8,"sentence":"A seed bootstrapping technique prepares the data to train these classifiers","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":8,"sentence":"Our analyses further validate that such an approach in conjunction with weak supervision using prior branching knowledge of a known language (left/right-branching) and minimal heuristics injects strong inductive bias into the parser, achieving 63.1 F_1 on the English (PTB) test set","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":8,"sentence":"In addition, we show the effectiveness of our architecture by evaluating on treebanks for Chinese (CTB) and Japanese (KTB) and achieve new state-of-the-art results.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":9,"sentence":"Transformer-based language models usually treat texts as linear sequence","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"However, most texts also have an inherent hierarchical structure, i.e., parts of a text can be identified using their position in this hierarch","offset":1,"pro":0.125,"labels":"GAP"},{"idx":9,"sentence":"In addition, section titles usually indicate the common topic of their respective sentences","offset":2,"pro":0.25,"labels":"GAP"},{"idx":9,"sentence":"e propose a novel approach to formulate, extract, encode and inject hierarchical structure information explicitly into an extractive summarization model based on a pre-trained, encoder-only Transformer language model (HiStruct+ model), which improves SOTA ROUGEs for extractive summarization on PubMed and arXiv substantiall","offset":3,"pro":0.375,"labels":"PUR"},{"idx":9,"sentence":"sing various experimental settings on three datasets (i.e., CNN/DailyMail, PubMed a","offset":4,"pro":0.5,"labels":"MTD"},{"idx":9,"sentence":"our HiStruct+ model outperforms a strong baseline collectively, which differs from our model only in that the hierarchical structure information is not injecte","offset":5,"pro":0.625,"labels":"RST"},{"idx":9,"sentence":"t is also observed that the more conspicuous hierarchical structure the dataset has, the larger improvements our method gain","offset":6,"pro":0.75,"labels":"CLN"},{"idx":9,"sentence":"he ablation study demonstrates that the hierarchical position information is the main contributor to our model's SOTA performance","offset":7,"pro":0.875,"labels":"CLN"},{"idx":10,"sentence":"Several studies have explored various advantages of multilingual pre-trained models (such as multilingual BERT) in capturing shared linguistic knowledge","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"However, less attention has been paid to their limitations.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":10,"sentence":"n this paper, we investigate the multilingual BERT for two known issues of the monolingual models: anisotropic embedding space and outlier dimensions","offset":2,"pro":0.25,"labels":"PUR"},{"idx":10,"sentence":"We show that, unlike its monolingual counterpart, the multilingual BERT model exhibits no outlier dimension in its representations while it has a highly anisotropic space.","offset":3,"pro":0.375,"labels":"RST"},{"idx":10,"sentence":"here are a few dimensions in the monolingual BERT with high contributions to the anisotropic distributio","offset":4,"pro":0.5,"labels":"BAC"},{"idx":10,"sentence":"However, we observe no such dimensions in the multilingual BER","offset":5,"pro":0.625,"labels":"RST"},{"idx":10,"sentence":"ur experimental results demonstrate that increasing the isotropy of multilingual space can significantly improve its representation power and performance, similarly to what had been observed for monolingual CWRs on semantic similarity task","offset":6,"pro":0.75,"labels":"RST"},{"idx":10,"sentence":"r analysis indicates that, despite having different degenerated directions, the embedding spaces in various languages tend to be partially similar with respect to their structure","offset":7,"pro":0.875,"labels":"CLN"},{"idx":11,"sentence":"xisting knowledge-grounded dialogue systems typically use finetuned versions of a pretrained language model (LM) and large-scale knowledge bases","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"These models typically fail to generalize on topics outside of the knowledge base, and require maintaining separate potentially large checkpoints each time finetuning is needed","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":11,"sentence":"In this paper, we aim to address these limitations by leveraging the inherent knowledge stored in the pretrained LM as well as its powerful generation abilit","offset":2,"pro":0.2222222222222222,"labels":"PUR"},{"idx":11,"sentence":"We propose a multi-stage prompting approach to generate knowledgeable responses from a single pretrained L","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":11,"sentence":"e first prompt the LM to generate knowledge based on the dialogue contex","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":11,"sentence":"hen, we further prompt it to generate responses based on the dialogue context and the previously generated knowledg","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":11,"sentence":"esults show that our knowledge generator outperforms the state-of-the-art retrieval-based model by 5.8% when combining knowledge relevance and correctnes","offset":6,"pro":0.6666666666666666,"labels":"RST"},{"idx":11,"sentence":"n addition, our multi-stage prompting outperforms the finetuning-based dialogue model in terms of response knowledgeability and engagement by up to 10% and 5%, respectivel","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":11,"sentence":"Furthermore, we scale our model up to 530 billion parameters and demonstrate that larger LMs improve the generation correctness score by up to 10%, and response relevance, knowledgeability and engagement by up to 1","offset":8,"pro":0.8888888888888888,"labels":"RST"},{"idx":12,"sentence":"pen-domain question answering has been used in a wide range of applications, such as web search and enterprise search, which usually takes clean texts extracted from various formats of documents (e.g., web pages, PDFs, or Word documents) as the information source.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"However, designing different text extraction approaches is time-consuming and not scalab","offset":1,"pro":0.1,"labels":"GAP"},{"idx":12,"sentence":"In order to reduce human cost and improve the scalability of QA systems, we propose and study an Open-domain Document Visual Question Answering (Open-domain DocVQA) task, which requires answering questions based on a collection of document images directly instead of only document texts, utilizing layouts and visual features additionally","offset":2,"pro":0.2,"labels":"PUR"},{"idx":12,"sentence":"owards this end","offset":3,"pro":0.3,"labels":"PUR"},{"idx":12,"sentence":"we introduce the first Chinese Open-domain DocVQA dataset called DuReader_{vis, containing about 15K question-answering pairs and 158K document images from the Baidu search engi","offset":4,"pro":0.4,"labels":"MTD"},{"idx":12,"sentence":"The extensive experiments demonstrate that the dataset is challengi","offset":5,"pro":0.5,"labels":"CLN"},{"idx":12,"sentence":"dditionally, we propose a simple approach that incorporates the layout and visual f","offset":6,"pro":0.6,"labels":"PUR"},{"idx":12,"sentence":"the experimental results show the effectiveness of the proposed approa","offset":7,"pro":0.7,"labels":"CLN"},{"idx":12,"sentence":"The dataset and code will be publicly available at","offset":8,"pro":0.8,"labels":"CTN"},{"idx":12,"sentence":"There are three main challenges in DuReader_{vis: (1) long document understanding, (2) noisy texts, and (3) multi-span answer extractio","offset":9,"pro":0.9,"labels":"MTD"},{"idx":13,"sentence":"elations between words are governed by hierarchical structure rather than linear orderin","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":" Sequence-to-sequence (seq2seq) models, despite their success in downstream NLP applications, often fail to generalize in a hierarchy-sensitive manner when performing syntactic transformations-for example, transforming declarative sentences into questio","offset":1,"pro":0.125,"labels":"GAP"},{"idx":13,"sentence":"However, syntactic evaluations of seq2seq models have only observed models that were not pre-trained on natural language data before being trained to perform syntactic transformations, in spite of the fact that pre-training has been found to induce hierarchical linguistic generalizations in language model","offset":2,"pro":0.25,"labels":"GAP"},{"idx":13,"sentence":" other words, the syntactic capabilities of seq2seq models may have been greatly understate","offset":3,"pro":0.375,"labels":"GAP"},{"idx":13,"sentence":"e address this gap using the pre-trained seq2seq models T5 and BART, as well as their multilingual variants mT5 and mBAR","offset":4,"pro":0.5,"labels":"MTD"},{"idx":13,"sentence":" evaluate whether they generalize hierarchically on two transformations in two languages: question formation and passivization in English and German","offset":5,"pro":0.625,"labels":"MTD"},{"idx":13,"sentence":"e find that pre-trained seq2seq models generalize hierarchically when performing syntactic transformations, whereas models trained from scratch on syntactic transformations do not. ","offset":6,"pro":0.75,"labels":"RST"},{"idx":13,"sentence":"is result presents evidence for the learnability of hierarchical syntactic information from non-annotated natural language text while also demonstrating that seq2seq models are capable of syntactic generalization, though only after exposure to much more language data than human learners receive","offset":7,"pro":0.875,"labels":"CLN"},{"idx":14,"sentence":"o fill the gap, we curate a large-scale multi-turn human-written conversation corpus, and create the first Chinese commonsense conversation knowledge graph which incorporates both social commonsense knowledge and dialog flow information","offset":0,"pro":0,"labels":"PUR"},{"idx":14,"sentence":"e develop a graph-conversation matching approach, and benchmark two graph-grounded conversational tasks.","offset":1,"pro":0.25,"labels":"RST"},{"idx":14,"sentence":"All the resources in this work will be released to foster future resear","offset":2,"pro":0.5,"labels":"CTN"},{"idx":14,"sentence":"o show the potential of our graph","offset":3,"pro":0.75,"labels":"PUR"},{"idx":15,"sentence":"fter a period of decrease, interest in word alignments is increasing again for their usefulness in domains such as typological research, cross-lingual annotation projection and machine translatio","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"Generally, alignment algorithms only use bitext and do not make use of the fact that many parallel corpora are multiparallel. ","offset":1,"pro":0.1,"labels":"GAP"},{"idx":15,"sentence":"irst, we create a multiparallel word alignment graph, joining all bilingual word alignment pairs in one graph","offset":2,"pro":0.2,"labels":"MTD"},{"idx":15,"sentence":"We show that community detection algorithms can provide valuable information for multiparallel word alignment","offset":3,"pro":0.3,"labels":"RST"},{"idx":15,"sentence":"ur method outperforms previous work on three word alignment datasets and on a downstream task","offset":4,"pro":0.4,"labels":"CLN"},{"idx":15,"sentence":"ur GNN approach (i) utilizes information about the meaning, position and language of the input wor","offset":5,"pro":0.5,"labels":"MTD"},{"idx":15,"sentence":"y considering all language pairs togeth","offset":6,"pro":0.6,"labels":"MTD"},{"idx":15,"sentence":"ere, we compute high-quality wor","offset":7,"pro":0.7,"labels":"PUR"},{"idx":15,"sentence":"ext, we use graph neural networks (GNNs","offset":8,"pro":0.8,"labels":"MTD"},{"idx":15,"sentence":"o exploit","offset":9,"pro":0.9,"labels":"PUR"},{"idx":16,"sentence":"we propose the sentiment word aware multimodal refinement model (SWRM), which can dynamically refine the erroneous sentiment words by leveraging multimodal sentiment clue","offset":0,"pro":0,"labels":"PUR"},{"idx":16,"sentence":"Specifically, we first use the sent","offset":1,"pro":0.1,"labels":"MTD"},{"idx":16,"sentence":"to obtain the most possible position of the sentiment word in the tex","offset":2,"pro":0.2,"labels":"PUR"},{"idx":16,"sentence":"hen utilize the multimodal sentiment word refinement module","offset":3,"pro":0.3,"labels":"MTD"},{"idx":16,"sentence":"dynamically refine the sentiment word embeddi","offset":4,"pro":0.4,"labels":"PUR"},{"idx":16,"sentence":"he refined embeddings are taken as the textual inputs of the multimodal feature fusion mod","offset":5,"pro":0.5,"labels":"MTD"},{"idx":16,"sentence":"predict the sentiment labe","offset":6,"pro":0.6,"labels":"PUR"},{"idx":16,"sentence":"We conduct extensive experiments on the real-world datasets including MOSI-Speechbrain, MOSI-IBM, and MOSI-iFlytek ","offset":7,"pro":0.7,"labels":"MTD"},{"idx":16,"sentence":"esults demonstrate the effectiveness of our model, which surpasses the current state-of-the-art models on ","offset":8,"pro":0.8,"labels":"CLN"},{"idx":16,"sentence":"ermore, our approach can be adapted for other multimodal feature fusion models eas","offset":9,"pro":0.9,"labels":"CTN"},{"idx":17,"sentence":"edical code prediction from clinical notes aims at automatically associating medical codes with the clinical note","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"Rare code problem, the medical codes with low occurrences, is prominent in medical code predictio","offset":1,"pro":0.1,"labels":"GAP"},{"idx":17,"sentence":"Recent studies employ deep neural networks and the external knowledge to tackle it.","offset":2,"pro":0.2,"labels":"BAC"},{"idx":17,"sentence":"owever, such approaches lack interpretability which is a vital issue in medical applicatio","offset":3,"pro":0.3,"labels":"GAP"},{"idx":17,"sentence":"Moreover, due to the lengthy and noisy clinical notes, such approaches fail to achieve satisfactory results","offset":4,"pro":0.4,"labels":"GAP"},{"idx":17,"sentence":"erefore, in this paper, we propose a novel framework based on medical concept driven attention to incorporate external knowledge for explainable medical code prediction","offset":5,"pro":0.5,"labels":"PUR"},{"idx":17,"sentence":" specific, both the clinical notes and Wikipedia documents are aligned into topic space to extract medical concepts using topic modelin","offset":6,"pro":0.6,"labels":"MTD"},{"idx":17,"sentence":"hen, the medical concept-driven attention mechanism is applied ","offset":7,"pro":0.7,"labels":"MTD"},{"idx":17,"sentence":"o uncover the medical code related concepts which provide explanations for medical code pred","offset":8,"pro":0.8,"labels":"PUR"},{"idx":17,"sentence":"Experimental results on the benchmark dataset show the superiority of the proposed framework over several state-of-the-art baseline","offset":9,"pro":0.9,"labels":"CLN"},{"idx":18,"sentence":"nsupervised constrained text generation aims to generate text under a given set of constraints without any supervised data.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"rrent state-of-the-art methods stochastically sample edit positions and actions, which may cause unnecessary search steps","offset":1,"pro":0.125,"labels":"GAP"},{"idx":18,"sentence":"cifically, PMCTG extends perturbed masking techniqu","offset":2,"pro":0.25,"labels":"MTD"},{"idx":18,"sentence":" effectively search for the most incongruent token to ed","offset":3,"pro":0.375,"labels":"PUR"},{"idx":18,"sentence":"Then it introduces four multi-aspect scoring functio","offset":4,"pro":0.5,"labels":"MTD"},{"idx":18,"sentence":" select edit action to further reduce search difficu","offset":5,"pro":0.625,"labels":"PUR"},{"idx":18,"sentence":"Since PMCTG does not require supervised data, it could be applied to different generation tasks","offset":6,"pro":0.75,"labels":"RST"},{"idx":18,"sentence":"We show that under the unsupervised setting, PMCTG achieves new state-of-the-art results in two representative tasks, namely keywords- to-sentence generation and paraphrasin","offset":7,"pro":0.875,"labels":"CLN"},{"idx":19,"sentence":"aph-based methods, which decompose the score of a dependency tree into scores of dependency arcs, are popular in dependency parsing for decad","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"ecently, (CITATION) propose a headed-span-based method that decomposes the score of a dependency tree into scores of headed spa","offset":1,"pro":0.1,"labels":"BAC"},{"idx":19,"sentence":"They show improvement over first-order graph-based method","offset":2,"pro":0.2,"labels":"BAC"},{"idx":19,"sentence":"However, their method does not score dependency arcs at all, and dependency arcs are implicitly induced by their cubic-time algorithm, which is possibly sub-optimal since modeling dependency arcs is intuitively usefu","offset":3,"pro":0.3,"labels":"GAP"},{"idx":19,"sentence":"n this work, we aim to combine graph-based and headed-span-based methods, incorporating both arc scores and headed span scores into our mo","offset":4,"pro":0.4,"labels":"PUR"},{"idx":19,"sentence":" show a direct way to combine with O(n^4) parsing complexit","offset":5,"pro":0.5,"labels":"MTD"},{"idx":19,"sentence":"o decrease complexi","offset":6,"pro":0.6,"labels":"PUR"},{"idx":19,"sentence":" show two O(n^3) dynamic programming algorithms to combine first- and second-order graph-based and headed-span-based meth","offset":7,"pro":0.7,"labels":"MTD"},{"idx":19,"sentence":"ur experiments on PTB, CTB, and UD show that combining first-order graph-based and headed-span-based methods is effective","offset":8,"pro":0.8,"labels":"RST"},{"idx":19,"sentence":"e also confirm the effectiveness of second-order graph-based parsing in the deep learning age, however, we observe marginal or no improvement when combining second-order graph-based and headed-span-based methods ","offset":9,"pro":0.9,"labels":"CLN"},{"idx":20,"sentence":"ode switching (CS) refers to the phenomenon of interchangeably using words and phrases from different","offset":0,"pro":0,"labels":"BAC"},{"idx":20,"sentence":"CS can pose significant accuracy challenges to NLP, due to the often monolingual nature of the underlying system","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":20,"sentence":"focus on CS in the context of English/Spanish conversations for the task of speech translation (ST), generating and evaluating both transcript and translatio","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":20,"sentence":" evaluate model performance on this task","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":20,"sentence":"we create a novel ST corpus derived from existing public data sets","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":20,"sentence":" explore various ST architectures across two dimensions:","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":20,"sentence":"e show that our ST architectures, and especially our bidirectional end-to-end architecture, perform well on CS speech, even when no CS training data is use","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":21,"sentence":"cent interest in entity linking has focused in the zero-shot scenario, where at test time the entity mention to be labelled is never seen during training, or may belong to a different domain from the source do","offset":0,"pro":0,"labels":"BAC"},{"idx":21,"sentence":"Current work leverage pre-trained BERT with the implicit assumption that it bridges the gap between the source and target domain distribut","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":21,"sentence":"-tuned BERT has a considerable underperformance at zero-shot when applied ","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":21,"sentence":"We solve this problem","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":21,"sentence":"y proposing a Transformational Biencoder that incorporates a transformation into BERT to perform a zero-shot transfer from the source domain during train","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":21,"sentence":"s like previous work, we rely on negative entities to encourage our model to discriminate the golden entities during trainin","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":21,"sentence":"o generate these negative entitie","offset":6,"pro":0.6666666666666666,"labels":"PUR"},{"idx":21,"sentence":"we propose a simple but effective strategy that takes the domain of the golden entity into perspecti","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":21,"sentence":"ur experimental results on the benchmark dataset Zeshel show effectiveness of our approach and achieve new state-of-the-ar","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":22,"sentence":"he Lottery Ticket Hypothesis suggests that for any over-parameterized model, a small subnetwork exists to achieve competitive performance compared to the backbone architectu","offset":0,"pro":0,"labels":"BAC"},{"idx":22,"sentence":"n this paper, we study whether there is a winning lottery ticket for pre-trained language models, which allow the practitioners to fine-tune the parameters in the ticket but achieve good downstream performanc","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":22,"sentence":" achieve this, ","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":22,"sentence":"e regularize the fine-tuning process with L1 distance and explore the subnetwork structure (what we refer to as the dominant winning ticket","offset":3,"pro":0.5,"labels":"MTD"},{"idx":22,"sentence":"mpirically, we show that (a) the dominant winning ticket can achieve performance that is comparable with that of the full-parameter model, (b) the dominant winning ticket is transferable across different tasks, (c) and the dominant winning ticket has a natural structure within each para","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":22,"sentence":"Strikingly, we find that a dominant winning ticket that takes up 0.05% of the parameters can already achieve satisfactory performance, indicating that the PLM is significantly reducible during fine-tun","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":23,"sentence":"Thai N-NER consists of 264,798 mentions, 104 classes, and a maximum depth of 8 layers obtained from 4,894 documents in the domains of news articles and restaurant reviews","offset":0,"pro":0,"labels":"BAC"},{"idx":23,"sentence":"r work, to the best of our knowledge, presents the largest non-English N-NER dataset and the first non-English one with fine-grained classes","offset":1,"pro":0.125,"labels":"CTN"},{"idx":23,"sentence":"o understand the new challenges our proposed dataset brings to the f","offset":2,"pro":0.25,"labels":"PUR"},{"idx":23,"sentence":"e conduct an experimental study on (i) cutting edge N-NER models with the state-of-the-art accuracy in English and (ii) baseline methods based on well-known language model architectu","offset":3,"pro":0.375,"labels":"MTD"},{"idx":23,"sentence":"om the experimental results, we obtained two key findin","offset":4,"pro":0.5,"labels":"RST"},{"idx":23,"sentence":"These findings suggest that further investigation is required to make a multilingual N-NER solution that works well across different languag","offset":5,"pro":0.625,"labels":"CLN"},{"idx":23,"sentence":"irst, all models produced poor F1 scores in the tail region of the class distribution","offset":6,"pro":0.75,"labels":"RST"},{"idx":23,"sentence":"There is little or no performance improvement provided by these models with respect to the baseline methods with our Thai datas","offset":7,"pro":0.875,"labels":"RST"},{"idx":24,"sentence":"he retriever-reader pipeline has shown promising performance in open-domain QA but suffers from a very slow inference speed","offset":0,"pro":0,"labels":"BAC"},{"idx":24,"sentence":"cently proposed question retrieval models tackle this problem by indexing question-answer pairs and searching for similar questions","offset":1,"pro":0.125,"labels":"BAC"},{"idx":24,"sentence":"ese models have shown a significant increase in inference speed, but at the cost of lower QA performance compared to the retriever-reader mo","offset":2,"pro":0.25,"labels":"GAP"},{"idx":24,"sentence":"is paper proposes a two-step question retrieval model, SQuID (Sequential Question-Indexed Dense retrieval) and distant supervision for trainin","offset":3,"pro":0.375,"labels":"PUR"},{"idx":24,"sentence":"QuID uses two bi-encoders for question retrieval","offset":4,"pro":0.5,"labels":"MTD"},{"idx":24,"sentence":"he first-step retriever selects top-k similar questions, and the second-step retriever finds the most similar question from the top-k questio","offset":5,"pro":0.625,"labels":"MTD"},{"idx":24,"sentence":"We evaluate the performance and the computational efficiency of SQ","offset":6,"pro":0.75,"labels":"MTD"},{"idx":24,"sentence":"he results show that SQuID significantly increases the performance of existing question retrieval models with a negligible loss on inference spee","offset":7,"pro":0.875,"labels":"RST"},{"idx":25,"sentence":"nalysis of vision-and-language models has revealed their brittleness under linguistic phenomena such as paraphrasing, negation, textual entailment, and word substitutions with synonyms or anto","offset":0,"pro":0,"labels":"BAC"},{"idx":25,"sentence":"hile data augmentation techniques have been designed to mitigate against these failure modes, methods that can integrate this knowledge into the training pipeline remain under-explored.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":25,"sentence":"his paper, we present SDRO, a model-agnostic method that utilizes a set linguistic transformations in a distributed robust optimization setting, along with an ensembling technique to leverage these transformations during inference","offset":2,"pro":0.4,"labels":"PUR"},{"idx":25,"sentence":"xperiments on benchmark datasets with images (NLVR^2) and video (VIOLIN) demonstrate performance improvements as well as robustness to adversarial attack","offset":3,"pro":0.6,"labels":"RST"},{"idx":25,"sentence":"Experiments on binary VQA explore the generalizability of this method to other V{&L task","offset":4,"pro":0.8,"labels":"RST"},{"idx":26,"sentence":"mmonsense inference poses a unique challenge to reason and generate the physical, social, and causal conditions of a given even","offset":0,"pro":0,"labels":"BAC"},{"idx":26,"sentence":"xisting approaches to commonsense inference utilize commonsense transformers, which are large-scale language models that learn commonsense knowledge graph","offset":1,"pro":0.125,"labels":"BAC"},{"idx":26,"sentence":"owever, they suffer from a lack of coverage and expressive diversity of the graphs, resulting in a degradation of the representation quali","offset":2,"pro":0.25,"labels":"GAP"},{"idx":26,"sentence":" this paper, we focus on addressing missing relations in commonsense knowledge graphs, and propose a novel contrastive learning framework called SOLA","offset":3,"pro":0.375,"labels":"PUR"},{"idx":26,"sentence":"Our framework contrasts sets of semantically similar and dissimilar events, learning richer inferential knowledge compared to existing approache","offset":4,"pro":0.5,"labels":"RST"},{"idx":26,"sentence":"mpirical results demonstrate the efficacy of SOLAR in commonsense inference of diverse commonsense knowledge grap","offset":5,"pro":0.625,"labels":"RST"},{"idx":26,"sentence":"pecifically, SOLAR outperforms the state-of-the-art commonsense transformer on commonsense inference with ConceptNet by 1.84% on average among 8 automatic evaluation metri","offset":6,"pro":0.75,"labels":"RST"},{"idx":26,"sentence":"In-depth analysis of SOLAR sheds light on the effects of the missing relations utilized in learning commonsense knowledge graphs","offset":7,"pro":0.875,"labels":"CLN"},{"idx":27,"sentence":"atural Language Inference (NLI) datasets contain examples with highly ambiguous labels due to its subjectivit","offset":0,"pro":0,"labels":"BAC"},{"idx":27,"sentence":"Several recent efforts have been made to acknowledge and embrace the existence of ambiguity, and explore how to capture the human disagreement distribution.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":27,"sentence":" contrast with directly learning from gold ambiguity labels, relying on special resou","offset":2,"pro":0.3333333333333333,"labels":"BAC"},{"idx":27,"sentence":"we argue that the model has naturally captured the human ambiguity distribution as long as it's calibrated, i.e. the predictive probability can reflect the true correctness likelihoo","offset":3,"pro":0.5,"labels":"PUR"},{"idx":27,"sentence":"Our experiments show that when model is well-calibrated, either by label smoothing or temperature scaling, it can obtain competitive performance as prior work, on both divergence scores between predictive probability and the true human opinion distribution, and the accura","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":27,"sentence":"This reveals the overhead of collecting gold ambiguity labels can be cut, by broadly solving how to calibrate the NLI networ","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":28,"sentence":"o maximize the accuracy and increase the overall acceptance of text classifiers,","offset":0,"pro":0,"labels":"PUR"},{"idx":28,"sentence":"e propose a framework for the efficient, in-operation moderation of classifiers' outpu","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":28,"sentence":"ur framework focuses on use cases in which F1-scores of modern Neural Networks classifiers (ca. 90%) are still inapplicable in practic","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":28,"sentence":"e suggest a semi-automated approach that uses prediction uncertainties to pass unconfident, probably incorrect classifications to human moderato","offset":3,"pro":0.5,"labels":"MTD"},{"idx":28,"sentence":"o minimize the workloa","offset":4,"pro":0.6666666666666666,"labels":"PUR"},{"idx":28,"sentence":"we limit the human moderated data to the point where the accuracy gains saturate and further human effort does not lead to substantial improvements","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":29,"sentence":"It has been the norm for a long time to evaluate automated summarization tasks using the popular ROUGE m","offset":0,"pro":0,"labels":"BAC"},{"idx":29,"sentence":" Although several studies in the past have highlighted the limitations of ROUGE, researchers have struggled to reach a consensus on a better alternative until toda","offset":1,"pro":0.125,"labels":"GAP"},{"idx":29,"sentence":"ne major limitation of the traditional ROUGE metric is the lack of semantic understanding (relies on direct overlap of n-grams","offset":2,"pro":0.25,"labels":"GAP"},{"idx":29,"sentence":"n this paper, we exclusively focus on the extractive summarization task and propose a semantic-aware nCG (normalized cumulative gain)-based evaluation metric (called Sem-nCG) for evaluating thi","offset":3,"pro":0.375,"labels":"PUR"},{"idx":29,"sentence":"One fundamental contribution of the paper is that it demonstrates how we can generate more reliable semantic-aware ground truths for evaluating extractive summarization tasks without any additional human interventio","offset":4,"pro":0.5,"labels":"CTN"},{"idx":29,"sentence":"o the best of our knowledge, this work is the first of its kind","offset":5,"pro":0.625,"labels":"CTN"},{"idx":29,"sentence":"have conducted extensive experiments with this new metric using the widely used CNN/DailyMail dataset","offset":6,"pro":0.75,"labels":"MTD"},{"idx":29,"sentence":"xperimental results show that the new Sem-nCG metric is indeed semantic-aware, shows higher correlation with human judgement (more reliable) and yields a large number of disagreements with the original ROUGE metric (suggesting that ROUGE often leads to inaccurate conclusions also verified by huma","offset":7,"pro":0.875,"labels":"CLN"},{"idx":0,"sentence":"  The extreme multi-label classification (XMC) task aims at tagging content with a subset of labels from an extremely large label set.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"The label vocabulary is typically defined in advance by domain experts and assumed to capture all necessary tags. ","offset":1,"pro":0.2,"labels":"BAC"},{"idx":0,"sentence":"However in real world scenarios this label set, although large, is often incomplete and experts frequently need to refine it. T","offset":2,"pro":0.4,"labels":"GAP"},{"idx":0,"sentence":"Hence, in addition to not having training data for some labels-as is the case in zero-shot classification-models need to invent some labels on-thefly.","offset":3,"pro":0.6,"labels":"CLN"},{"idx":0,"sentence":"We propose GROOV, a fine-tuned seq2seq model for OXMC that generates the set of labels as a flat sequence and is trained using a novel loss independent of predicted label order. ","offset":4,"pro":0.8,"labels":"MTD"},{"idx":1,"sentence":"Few-shot named entity recognition (NER) systems aim at recognizing novel-class named entities based on only a few labeled examples.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"In this paper, we present a decomposed meta-learning approach which addresses the problem of few-shot NER by sequentially tackling few-shot span detection and few-shot entity typing using meta-learning. ","offset":1,"pro":0.25,"labels":"MTD"},{"idx":1,"sentence":" For few-shot entity typing, we propose MAML-ProtoNet, i.e., MAML-enhanced prototypical networks to find a good embedding space that can better distinguish text span representations from different entity classes.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":1,"sentence":"Extensive experiments on various benchmarks show that our approach achieves superior performance over prior methods.","offset":3,"pro":0.75,"labels":"RST"},{"idx":2,"sentence":"Generating natural and informative texts has been a long-standing problem in NLP. Much effort has been dedicated into incorporating pre-trained language models (PLMs) with various open-world knowledge, such as knowledge graphs or wiki pages.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"To address the problem, we propose augmenting TExt Generation via Task-specific and Open-world Knowledge (TegTok) in a unified framework","offset":1,"pro":0.25,"labels":"PUR"},{"idx":2,"sentence":"Our model selects knowledge entries from two types of knowledge sources through dense retrieval and then injects them into the input encoding and output decoding stages respectively on the basis of PLMs. ","offset":2,"pro":0.5,"labels":"MTD"},{"idx":2,"sentence":"Experiments on two text generation tasks of dialogue generation and question generation, and on two datasets show that our method achieves better performance than various baseline models.","offset":3,"pro":0.75,"labels":"CLN"},{"idx":3,"sentence":"Emotion recognition in conversation (ERC) aims to analyze the speaker's state and identify their emotion in the conversation.","offset":0,"pro":0,"labels":"PUR"},{"idx":3,"sentence":"In order to extract multi-modal information and the emotional tendency of the utterance effectively, we propose a new structure named Emoformer to extract multi-modal emotion vectors from different modalities and fuse them with sentence vector to be an emotion capsule.","offset":1,"pro":0.25,"labels":"MTD"},{"idx":3,"sentence":" In order to extract multi-modal information and the emotional tendency of the utterance effectively, we propose a new structure named Emoformer to extract multi-modal emotion vectors from different modalities and fuse them with sentence vector to be an emotion capsule. ","offset":2,"pro":0.5,"labels":"PUR"},{"idx":3,"sentence":"Through the experiments with two benchmark datasets, our model shows better performance than the existing state-of-the-art models.","offset":3,"pro":0.75,"labels":"RST"},{"idx":4,"sentence":"Logical reasoning of text requires identifying critical logical structures in the text and performing inference over them. ","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"In this paper, we not only put forward a logic-driven context extension framework but also propose a logic-driven data augmentation algorithm. ","offset":1,"pro":0.2,"labels":"PUR"},{"idx":4,"sentence":"The latter augments literally similar but logically different instances and incorporates contrastive learning to better capture logical information, especially logical negative and conditional relationships.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":4,"sentence":"We conduct experiments on two benchmark datasets, ReClor and LogiQA. The results show that our method achieves state-of-the-art performance on both datasets, and even surpasses human performance on the ReClor dataset.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":4,"sentence":" respectively to extract logical expressions as elementary reasoning units, symbolically infer the implicit expressions following equivalence laws and extend the context to validate the options","offset":4,"pro":0.8,"labels":"PUR"},{"idx":5,"sentence":"Toxic span detection is the task of recognizing offensive spans in a text snippet. ","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"In this work, we introduce a novel multi-task framework for toxic span detection in which the model seeks to simultaneously predict offensive words and opinion phrases to leverage their inter-dependencies and improve the performance. ","offset":1,"pro":0.2,"labels":"MTD"},{"idx":5,"sentence":" encourage the consistency of the model predictions across similar inputs for toxic span detection.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":5,"sentence":"Our extensive experiments demonstrate the effectiveness of the proposed model compared to strong baselines.","offset":3,"pro":0.6,"labels":"RST"},{"idx":5,"sentence":"the task of recognizing spans responsible for the toxicity of a text is not explored yet.","offset":4,"pro":0.8,"labels":"GAP"},{"idx":6,"sentence":"Relational triple extraction is a critical task for constructing knowledge graphs.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":" However, they usually suffered from ignoring relational reasoning patterns, thus failed to extract the implicitly implied triples. ","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":6,"sentence":"In this paper, we propose a unified framework to learn the relational reasoning patterns for this task","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":6,"sentence":"To identify multi-hop reasoning paths, we construct a relational graph from the sentence (text-to-graph generation) and apply multi-layer graph convolutions to it. ","offset":3,"pro":0.5,"labels":"PUR"},{"idx":6,"sentence":"we propose to understand the unlabeled conceptual expressions by reconstructing the sentence from the relational graph (graph-to-text generation) in a self-supervised manner. Experimental results on several benchmark datasets demonstrate the effectiveness of our method.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":6,"sentence":"Moreover, the type inference logic through the paths can be captured with the sentence's supplementary relational expressions that represent the real-world conceptual meanings of the paths' composite relations.","offset":5,"pro":0.8333333333333334,"labels":"BAC"},{"idx":7,"sentence":"Despite the success of prior works in sentence-level EAE, the document-level setting is less explored. In particular, whereas syntactic structures of sentences have been shown to be effective for sentence-level EAE, prior document-level EAE models totally ignore syntactic structures for documents.","offset":0,"pro":0,"labels":"GAP"},{"idx":7,"sentence":"Hence, in this work, we study the importance of syntactic structures in document-level EAE. ","offset":1,"pro":0.14285714285714285,"labels":"RST"},{"idx":7,"sentence":", we propose to employ Optimal Transport (OT) to induce ","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":7,"sentence":"we propose a novel regularization technique to explicitly constrain the contributions of unrelated context ","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":7,"sentence":"We perform extensive experiments on the benchmark document-level EAE dataset RAMS","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":7,"sentence":"Moreover, our experiments on the ACE 2005 dataset reveals the effectiveness of the proposed model in the sentence-level EAE by establishing new state-of-the-art results.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":7,"sentence":"Furthermore, we propose a novel regularization technique to explicitly constrain the contributions of unrelated context words in the final prediction for EAE.","offset":6,"pro":0.8571428571428571,"labels":"MTD"},{"idx":8,"sentence":"Augmentation of task-oriented dialogues has followed standard methods used for plain-text such as back-translation, word-level manipulation, and paraphrasing despite its richly annotated structure","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"In this work, we introduce an augmentation framework that utilizes belief state annotations to match turns from various dialogues and form new synthetic dialogues in a bottom-up manner.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":8,"sentence":"Unlike other augmentation strategies, it operates with as few as five examples","offset":2,"pro":0.4,"labels":"BAC"},{"idx":8,"sentence":"Our augmentation strategy yields significant improvements when both adapting a DST model to a new domain, and when adapting a language model to the DST task, on evaluations with TRADE and TOD-BERT models","offset":3,"pro":0.6,"labels":"RST"},{"idx":8,"sentence":"Further analysis shows that our model performs better on seen values during training, and it is also more robust to unseen values.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":9,"sentence":"Document-level Relation Extraction (DocRE) is a more challenging task compared to its sentence-level counterpart. ","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":" In this paper, we propose a semi-supervised framework for DocRE with three novel components. Firstly, we use an axial attention module for learning the interdependency among entity-pairs, which improves the performance on two-hop relations. Secondly, we propose an adaptive focal loss to tackle the class imbalance problem of DocRE. Lastly, we use knowledge distillation to overcome the differences between human annotated data and distantly supervised data. We conducted experiments on two DocRE datasets. Our model consistently outperforms strong baselines and its performance exceeds the previous SOTA by 1.36 F1 and 1.46 Ign{_F1 score on the DocRED leaderboard.","offset":1,"pro":0.5,"labels":"MTD"},{"idx":10,"sentence":"Our experimental results reveal that the approach works well, and can be useful to selectively predict answers when question answering systems are posed with unanswerable or out-of-the-training distribution questions.","offset":0,"pro":0,"labels":"RST"},{"idx":10,"sentence":"We show that calibrating such complex systems which contain discrete retrieval and deep reading components is challenging and current calibration techniques fail to scale to these settings.","offset":1,"pro":0.5,"labels":"GAP"},{"idx":11,"sentence":" resulting in inferior performance both in clean accuracy and adversarial robustness. ","offset":0,"pro":0,"labels":"RST"},{"idx":11,"sentence":"We propose to train text classifiers by a sample reweighting method in which the example weights are learned to minimize the loss of a validation set mixed with the clean examples and their adversarial ones in an online learning manner. ","offset":1,"pro":0.5,"labels":"PUR"},{"idx":12,"sentence":"We present state-of-the-art results on morphosyntactic tagging across different varieties of Arabic using fine-tuned pre-trained transformer language models. ","offset":0,"pro":0,"labels":"MTD"},{"idx":12,"sentence":" We explore different training setups for fine-tuning pre-trained transformer language models, including training data size, the use of external linguistic resources, and the use of annotated data from other dialects in a low-resource scenario.","offset":1,"pro":0.3333333333333333,"labels":"MTD"},{"idx":12,"sentence":"Our results show that strategic fine-tuning using datasets from other high-resource dialects is beneficial for a low-resource dialect.","offset":2,"pro":0.6666666666666666,"labels":"RST"},{"idx":13,"sentence":"However, it is still a mystery how PLMs generate the results correctly: relying on effective clues or shortcut patterns? ","offset":0,"pro":0,"labels":"GAP"},{"idx":13,"sentence":"We check the words that have three typical associations with the missing words: knowledge-dependent, positionally close, and highly co-occurred.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":13,"sentence":" Accordingly, we conclude that the PLMs capture the factual knowledge ineffectively because of depending on the inadequate associations.","offset":2,"pro":0.4,"labels":"CLN"},{"idx":13,"sentence":"by a causal-inspired analysis that quantitatively measures and evaluates the word-level patterns that PLMs depend on to generate the missing words.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":13,"sentence":"Our analysis shows: (1) PLMs generate the missing factual words more by the positionally close and highly co-occurred words than the knowledge-dependent words;","offset":4,"pro":0.8,"labels":"RST"},{"idx":14,"sentence":"Popular language models (LMs) struggle to capture knowledge about rare tail facts and entities. ","offset":0,"pro":0,"labels":"PUR"},{"idx":14,"sentence":" We observe proposed methods typically start with a base LM and data that has been annotated with entity metadata, then change the model, by modifying the architecture or introducing auxiliary loss terms to better capture entity knowledge.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":14,"sentence":"We propose metadata shaping, a method which inserts substrings corresponding to the readily available entity metadata, e.g. types and descriptions, into examples at train and inference time based on mutual information","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":14,"sentence":"Despite its simplicity, metadata shaping is quite effective. ","offset":3,"pro":0.5,"labels":"GAP"},{"idx":14,"sentence":"We further show the gains are on average 4.4x larger for the slice of examples containing tail vs. popular entities","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":14,"sentence":"In this work, we question this typical process and ask to what extent can we match the quality of model modifications, with a simple alternative: using a base LM and only changing the data.","offset":5,"pro":0.8333333333333334,"labels":"PUR"},{"idx":15,"sentence":"We study how to enhance text representation via textual commonsense.","offset":0,"pro":0,"labels":"PUR"},{"idx":15,"sentence":"We point out that commonsense has the nature of domain discrepancy. Namely, commonsense has different data formats and is domain-independent from the downstream task. ","offset":1,"pro":0.14285714285714285,"labels":"MTD"},{"idx":15,"sentence":"A typical method of introducing textual knowledge is continuing pre-training over the commonsense corpus","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":15,"sentence":"However, it will cause catastrophic forgetting to the downstream task due to the domain discrepancy. In addition, previous methods of directly using textual descriptions as extra input information cannot apply to large-scale commonsense.","offset":3,"pro":0.42857142857142855,"labels":"GAP"},{"idx":15,"sentence":" In order to effectively incorporate the commonsense, we proposed OK-Transformer (Out-of-domain Knowledge enhanced Transformer).","offset":4,"pro":0.5714285714285714,"labels":"PUR"},{"idx":15,"sentence":"OK-Transformer effectively integrates commonsense descriptions and enhances them to the target text representation. In addition, OK-Transformer can adapt to the Transformer-based language models (e.g. BERT, RoBERTa) for free, without pre-training on large-scale unsupervised corpora.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":15,"sentence":"We have verified the effectiveness of OK-Transformer in multiple applications such as commonsense reasoning, general text classification, and low-resource commonsense settings.","offset":6,"pro":0.8571428571428571,"labels":"MTD"},{"idx":16,"sentence":"Recent researches show that multi-criteria resources and n-gram features are beneficial to Chinese Word Segmentation (CWS).","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"However, these methods rely heavily on such additional information mentioned above and focus less on the model itself.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":16,"sentence":"We thus propose a novel neural framework, named Weighted self Distillation for Chinese word segmentation (WeiDC).","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":16,"sentence":"The framework, which only requires unigram features, adopts self-distillation technology with four hand-crafted weight modules and two teacher models configurations.","offset":3,"pro":0.5,"labels":"BAC"},{"idx":16,"sentence":"Experiment results show that WeiDC can make use of character features to learn contextual knowledge and successfully achieve state-of-the-art or competitive performance in terms of strictly closed test settings on SIGHAN Bakeoff benchmark datasets.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":16,"sentence":"Moreover, further experiments and analyses also demonstrate the robustness of WeiDC. Source codes of this paper are available on Github.","offset":5,"pro":0.8333333333333334,"labels":"IMP"},{"idx":17,"sentence":"The vast majority of text transformation techniques in NLP are inherently limited in their ability to expand input space coverage due to an implicit constraint to preserve the original class label.","offset":0,"pro":0,"labels":"GAP"},{"idx":17,"sentence":"In this work, we propose the notion of sibylvariance (SIB) to describe the broader set of transforms that relax the label-preserving constraint, knowably vary the expected class, and lead to significantly more diverse input distributions.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":17,"sentence":"To explore the role of sibylvariance within NLP, we implemented 41 text transformations, including several novel techniques like Concept2Sentence and SentMix. ","offset":2,"pro":0.4,"labels":"MTD"},{"idx":17,"sentence":"Our experiments on six benchmark datasets strongly support the efficacy of sibylvariance for generalization performance, defect detection, and adversarial robustness.","offset":3,"pro":0.6,"labels":"CLN"},{"idx":17,"sentence":"We offer a unified framework to organize all data transformations, including two types of SIB: (1) Transmutations convert one discrete kind into another, (2) Mixture Mutations blend two or more classes together.","offset":4,"pro":0.8,"labels":"MTD"},{"idx":18,"sentence":"Domain Adaptation (DA) of Neural Machine Translation (NMT) model often relies on a pre-trained general NMT model which is adapted to the new domain on a sample of in-domain parallel data. ","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"We propose a Domain adaptation Learning Curve prediction (DaLC) model that predicts prospective DA performance","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":18,"sentence":"based on in-domain monolingual samples in the source language.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":18,"sentence":"Our model relies on the NMT encoder representations combined with various instance and corpus-level features. ","offset":3,"pro":0.5,"labels":"MTD"},{"idx":18,"sentence":"Finally, we perform in-depth analyses of the results highlighting the limitations of our approach, and provide directions for future research.","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":18,"sentence":"It is however a desirable functionality that could help MT practitioners to make an informed decision before investing resources in dataset creation.","offset":5,"pro":0.8333333333333334,"labels":"GAP"},{"idx":19,"sentence":"Training giant models from scratch for each complex task is resource- and data-inefficient.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"We design a synthetic benchmark, CommaQA, with three complex reasoning tasks (explicit, implicit, numeric) designed to be solved by communicating with existing QA agents. ","offset":1,"pro":0.2,"labels":"MTD"},{"idx":19,"sentence":"We show that black-box models struggle to learn this task from scratch (accuracy under 50%) even with access to each agent's knowledge and gold facts supervision. ","offset":2,"pro":0.4,"labels":"RST"},{"idx":19,"sentence":"We will release CommaQA, along with a compositional generalization test split, to advance research in this direction.","offset":3,"pro":0.6,"labels":"CTN"},{"idx":19,"sentence":"However, we show that the challenge of learning to solve complex tasks by communicating with existing agents without relying on any auxiliary supervision or data still remains highly elusive.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":20,"sentence":"In multimodal machine learning, additive late-fusion is a straightforward approach to combine the feature representations from different modalities, in which the final prediction can be formulated as the sum of unimodal predictions.","offset":0,"pro":0,"labels":"BAC"},{"idx":20,"sentence":"While it has been found that certain late-fusion models can achieve competitive performance with lower computational costs compared to complex multimodal interactive models, how to effectively search for a good late-fusion model is still an open question.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":20,"sentence":"Moreover, for different modalities, the best unimodal models may work under significantly different learning rates due to the nature of the modality and the computational flow of the model; thus, selecting a global learning rate for late-fusion models can result in a vanishing gradient for some modalities.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":20,"sentence":"To help address these issues, we propose a Modality-Specific Learning Rate (MSLR) method to effectively build late-fusion multimodal models from fine-tuned unimodal models.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":20,"sentence":"We investigate three different strategies to assign learning rates to different modalities.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":20,"sentence":"Our experiments show that MSLR outperforms global learning rates on multiple tasks and settings, and enables the models to effectively learn each modality.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":21,"sentence":"Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis task that aims to align aspects and corresponding sentiments for aspect-specific sentiment polarity inference. ","offset":0,"pro":0,"labels":"BAC"},{"idx":21,"sentence":"Recently, exploiting dependency syntax information with graph neural networks has been the most popular trend.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":21,"sentence":"In this paper, to alleviate this problem, we propose a Bi-Syntax aware Graph Attention Network (BiSyn-GAT+).","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":21,"sentence":"Despite its success, methods that heavily rely on the dependency tree pose challenges in accurately modeling the alignment of the aspects and their words indicative of sentiment, since the dependency tree may provide noisy signals of unrelated associations (e.g., the conj relation between great and dreadful in Figure 2).","offset":3,"pro":0.5,"labels":"GAP"},{"idx":21,"sentence":"Specifically, BiSyn-GAT+ fully exploits the syntax information (e.g., phrase segmentation and hierarchical structure) of the constituent tree of a sentence to model the sentiment-aware context of every single aspect (called intra-context) and the sentiment relations across aspects (called inter-context) for learning.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":21,"sentence":"Experiments on four benchmark datasets demonstrate that BiSyn-GAT+ outperforms the state-of-the-art methods consistently.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":22,"sentence":"IndicBART utilizes the orthographic similarity between Indic scripts to improve transfer learning between similar Indic languages. ","offset":0,"pro":0,"labels":"MTD"},{"idx":22,"sentence":"Our experiments on NMT and extreme summarization show that a model specific to related languages like IndicBART is competitive with large pre-trained models like mBART50 despite being significantly smaller.","offset":1,"pro":0.25,"labels":"RST"},{"idx":22,"sentence":"It also performs well on very low-resource translation scenarios where languages are not included in pre-training or fine-tuning.","offset":2,"pro":0.5,"labels":"RST"},{"idx":22,"sentence":"Script sharing, multilingual training, and better utilization of limited model capacity contribute to the good performance of the compact IndicBART model.","offset":3,"pro":0.75,"labels":"CLN"},{"idx":23,"sentence":"While T5 achieves impressive performance on language tasks, it is unclear how to produce sentence embeddings from encoder-decoder models.","offset":0,"pro":0,"labels":"GAP"},{"idx":23,"sentence":"We investigate three methods to construct Sentence-T5 (ST5) models: two utilize only the T5 encoder and one using the full T5 encoder-decoder.","offset":1,"pro":0.125,"labels":"MTD"},{"idx":23,"sentence":"We establish a new sentence representation transfer benchmark, SentGLUE, which extends the SentEval toolkit to nine tasks from the GLUE benchmark.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":23,"sentence":"Our encoder-only models outperform the previous best models on both SentEval and SentGLUE transfer tasks, including semantic textual similarity (STS).","offset":3,"pro":0.375,"labels":"RST"},{"idx":23,"sentence":"Scaling up ST5 from millions to billions of parameters shown to consistently improve performance.","offset":4,"pro":0.5,"labels":"RST"},{"idx":23,"sentence":"Finally, our encoder-decoder method achieves a new state-of-the-art on STS when using sentence embeddings.","offset":5,"pro":0.625,"labels":"CLN"},{"idx":23,"sentence":"Sentence embeddings are broadly useful for language processing tasks.","offset":6,"pro":0.75,"labels":"BAC"},{"idx":23,"sentence":"We provide the first exploration of sentence embeddings from text-to-text transformers (T5) including the effects of scaling up sentence encoders to 11B parameters.","offset":7,"pro":0.875,"labels":"BAC"},{"idx":24,"sentence":"Relation extraction (RE) is an important natural language processing task that predicts the relation between two given entities, where a good understanding of the contextual information is essential to achieve an outstanding model performance.","offset":0,"pro":0,"labels":"BAC"},{"idx":24,"sentence":"Among different types of contextual information, the auto-generated syntactic information (namely, word dependencies) has shown its effectiveness for the task.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":24,"sentence":"However, most existing studies require modifications to the existing baseline architectures (e.g., adding new components, such as GCN,","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":24,"sentence":"on the top of an encoder) to leverage the syntactic information.","offset":3,"pro":0.42857142857142855,"labels":"GAP"},{"idx":24,"sentence":"To offer an alternative solution, we propose to leverage syntactic information to improve RE by training a syntax-induced encoder on auto-parsed data through dependency masking.","offset":4,"pro":0.5714285714285714,"labels":"PUR"},{"idx":24,"sentence":"Specifically, the syntax-induced encoder is trained by recovering the masked dependency connections and types in first, second, and third orders, which significantly differs from existing studies that train language models or word embeddings by predicting the context words along the dependency paths.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":24,"sentence":"Experimental results on two English benchmark datasets, namely, ACE2005EN and SemEval 2010 Task 8 datasets, demonstrate the effectiveness of our approach for RE, where our approach outperforms strong baselines and achieve state-of-the-art results on both datasets.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":25,"sentence":"While fine-tuning pre-trained models for downstream classification is the conventional paradigm in NLP, often task-specific nuances may not get captured in the resultant models.","offset":0,"pro":0,"labels":"GAP"},{"idx":25,"sentence":"Specifically, for tasks that take two inputs and require the output to be invariant of the order of the inputs, inconsistency is often observed in the predicted labels or confidence scores.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":25,"sentence":"and apply a consistency loss function to alleviate inconsistency in symmetric classification.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":25,"sentence":"Our results show an improved consistency in predictions for three paraphrase detection datasets without a significant drop in the accuracy scores.","offset":3,"pro":0.6,"labels":"RST"},{"idx":25,"sentence":"We examine the classification performance of six datasets (both symmetric and non-symmetric) to showcase the strengths and limitations of our approach.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":26,"sentence":"Recent years have seen a surge of interest in improving the generation quality of commonsense reasoning tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":26,"sentence":"Nevertheless, these approaches have seldom investigated diversity in the GCR tasks, which aims to generate alternative explanations for a real-world situation or predict all possible outcomes. ","offset":1,"pro":0.25,"labels":"GAP"},{"idx":26,"sentence":"In this paper, we propose MoKGE, a novel method that diversifies the generative reasoning by a mixture of expert (MoE) strategy on commonsense knowledge graphs (KG). ","offset":2,"pro":0.5,"labels":"PUR"},{"idx":26,"sentence":"Empirical experiments demonstrated that MoKGE can significantly improve the diversity while achieving on par performance on accuracy on two GCR benchmarks, based on both automatic and human evaluations.","offset":3,"pro":0.75,"labels":"RST"},{"idx":27,"sentence":"Pre-trained language models (PLMs) aim to learn universal language representations by conducting self-supervised training tasks on large-scale corpora.","offset":0,"pro":0,"labels":"BAC"},{"idx":27,"sentence":"Since PLMs capture word semantics in different contexts, the quality of word representations highly depends on word frequency, which usually follows a heavy-tailed distributions in the pre-training corpus. ","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":27,"sentence":"In this work, we focus on enhancing language model pre-training by leveraging definitions of the rare words in dictionaries (e.g., Wiktionary).","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":27,"sentence":"To incorporate a rare word definition as a part of input, we fetch its definition from the dictionary and append it to the end of the input text sequence.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":27,"sentence":"In addition to training with the masked language modeling objective, we propose two novel self-supervised pre-training tasks on word and sentence-level alignment between input text sequence and rare word definitions to enhance language modeling representation with dictionary.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":27,"sentence":"We evaluate the proposed Dict-BERT model on the language understanding benchmark GLUE and eight specialized domain benchmark datasets.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":27,"sentence":"Extensive experiments demonstrate that Dict-BERT can significantly improve the understanding of rare words and boost model performance on various NLP downstream tasks.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":28,"sentence":"We conduct a feasibility study into the applicability of answer-unaware question generation models to textbook passages.","offset":0,"pro":0,"labels":"PUR"},{"idx":28,"sentence":"We show that a significant portion of errors in such systems arise from asking irrelevant or un-interpretable questions and that such errors can be ameliorated by providing summarized input.","offset":1,"pro":0.25,"labels":"RST"},{"idx":28,"sentence":"We find that giving these models human-written summaries instead of the original text results in a significant increase in acceptability of generated questions (33% rightarrow 83%) as determined by expert annotators.","offset":2,"pro":0.5,"labels":"CLN"},{"idx":28,"sentence":"We also find that, in the absence of human-written summaries, automatic summarization can serve as a good middle ground.","offset":3,"pro":0.75,"labels":"CLN"},{"idx":29,"sentence":"We study the challenge of learning causal reasoning over procedural text to answer What if... questions when external commonsense knowledge is required.","offset":0,"pro":0,"labels":"PUR"},{"idx":29,"sentence":"We propose a novel multi-hop graph reasoning model to 1) efficiently extract a commonsense subgraph with the most relevant information from a large knowledge graph; 2) predict the causal answer by reasoning over the representations obtained from the commonsense subgraph and the contextual interactions between the questions and context.","offset":1,"pro":0.3333333333333333,"labels":"MTD"},{"idx":29,"sentence":"We evaluate our model on WIQA benchmark and achieve state-of-the-art performance compared to the recent models.","offset":2,"pro":0.6666666666666666,"labels":"RST"},{"idx":0,"sentence":"Knowledge graph integration typically suffers from the widely existing dangling entities that cannot find alignment cross knowledge graphs (KGs)","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":" The dangling entity set is unavailable in most real-world scenarios, and manually mining the entity pairs that consist of entities with the same meaning is labor-consuming.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":0,"sentence":"In this paper, we propose a novel accurate Unsupervised method for joint Entity alignment (EA) and Dangling entity detection (DED), called UED.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":0,"sentence":"The UED mines the literal semantic information to generate pseudo entity pairs and globally guided alignment information for EA and then utilizes the EA results to assist the DED","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":0,"sentence":"We construct a medical cross-lingual knowledge graph dataset, MedED, providing data for both the EA and DED tasks.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":0,"sentence":"Extensive experiments demonstrate that in the EA task, UED achieves EA results comparable to those of state-of-the-art supervised EA baselines and outperforms the current state-of-the-art EA methods by combining supervised EA data","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":0,"sentence":"For the DED task, UED obtains high-quality results without supervision.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":1,"sentence":"Most work targeting multilinguality, for example, considers only accuracy; most work on fairness or interpretability considers only English; and so on.","offset":0,"pro":0,"labels":"GAP"},{"idx":1,"sentence":"Such one-dimensionality of most research means we are only exploring a fraction of the NLP research search space.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":1,"sentence":"We provide historical and recent examples of how the square one bias has led researchers to draw false conclusions or make unwise choices, point to promising yet unexplored directions on the research manifold, and make practical recommendations to enable more multi-dimensional research.","offset":2,"pro":0.2222222222222222,"labels":"MTD"},{"idx":1,"sentence":"We open-source the results of our annotations to enable further analysis.","offset":3,"pro":0.3333333333333333,"labels":"CTN"},{"idx":1,"sentence":"The prototypical NLP experiment trains a standard architecture on labeled English data and optimizes for accuracy","offset":4,"pro":0.4444444444444444,"labels":"BAC"},{"idx":1,"sentence":"without accounting for other dimensions such as fairness, interpretability, or computational efficiency","offset":5,"pro":0.5555555555555556,"labels":"GAP"},{"idx":1,"sentence":"We show through a manual classification of recent NLP research papers","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":1,"sentence":"this is indeed the case and refer to it as the square one experimental setup","offset":7,"pro":0.7777777777777778,"labels":"GAP"},{"idx":1,"sentence":"We observe that NLP research often goes beyond the square one setup, e.g, focusing not only on accuracy, but also on fairness or interpretability, but typically only along a single dimension.","offset":8,"pro":0.8888888888888888,"labels":"MTD"},{"idx":2,"sentence":"Metamorphic testing has recently been used to check the safety of neural NLP models. Its main advantage is that it does not rely on a ground truth to generate test cases.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"We propose three new classes of metamorphic relations, which address the properties of systematicity, compositionality and transitivity.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":2,"sentence":"Unlike robustness, our relations are defined over multiple source inputs, thus increasing the number of test cases that we can produce by a polynomial factor.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":2,"sentence":"With them, we test the internal consistency of state-of-the-art NLP models","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":2,"sentence":"and show that they do not always behave according to their expected linguistic properties","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":2,"sentence":"Lastly, we introduce a novel graphical notation that efficiently summarises the inner structure of metamorphic relations.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":2,"sentence":"However, existing studies are mostly concerned with robustness-like metamorphic relations, limiting the scope of linguistic properties they can test.","offset":6,"pro":0.8571428571428571,"labels":"GAP"},{"idx":3,"sentence":"Many tasks in text-based computational social science (CSS) involve the classification of political statements into categories based on a domain-specific codebook","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"he typically skewed distribution of fine-grained categories, however, results in a challenging classification problem on the NLP side.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":3,"sentence":"This paper proposes to make use of the hierarchical relations among categories typically present in such codebooks:e.g., markets and taxation are both subcategories of economy, while borders is a subcategory of security.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":3,"sentence":"We use these ontological relations as prior knowledge to establish additional constraints on the learned model, thusimproving performance overall and in particular for infrequent categories.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":3,"sentence":"We evaluate several lightweight variants of this intuition by extending state-of-the-art transformer-based textclassifiers on two datasets and multiple languages.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":3,"sentence":"We find the most consistent improvement for an approach based on regularization.","offset":5,"pro":0.625,"labels":"RST"},{"idx":3,"sentence":"In order to be useful for CSS analysis","offset":6,"pro":0.75,"labels":"PUR"},{"idx":3,"sentence":"these categories must be fine-grained","offset":7,"pro":0.875,"labels":"GAP"},{"idx":4,"sentence":"The recent large-scale vision-language pre-training (VLP) of dual-stream architectures (e.g., CLIP) with a tremendous amount of image-text pair data, has shown its superiority on various multimodal alignment tasks","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"Despite its success, the resulting models are not capable of multimodal generative tasks due to the weak text encoder.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":4,"sentence":"To tackle this problem, we propose to augment the dual-stream VLP model with a textual pre-trained language model (PLM) via vision-language knowledge distillation (VLKD), enabling the capability for multimodal generation. ","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":4,"sentence":"VLKD is pretty data- and computation-efficient compared to the pre-training from scratch","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":4,"sentence":"Experimental results show that the resulting model has strong zero-shot performance on multimodal generation tasks, such as open-ended visual question answering and image captioning.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":4,"sentence":"For example, it achieves 44.5% zero-shot accuracy on the VQAv2 dataset, surpassing the previous state-of-the-art zero-shot model with 7times fewer parameters.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":4,"sentence":"Furthermore, the original textual language understanding and generation ability of the PLM is maintained after VLKD, which makes our model versatile for both multimodal and unimodal tasks.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":5,"sentence":"Most existing approaches to Visual Question Answering (VQA) answer questions directly","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"however, people usually decompose a complex question into a sequence of simple sub questions and finally obtain the answer to the original question after answering the sub question sequence(SQS).","offset":1,"pro":0.1,"labels":"GAP"},{"idx":5,"sentence":"Questioner raises the sub questions using an extending HRED model, and Oracle answers them one-by-one.","offset":2,"pro":0.2,"labels":"MTD"},{"idx":5,"sentence":"An Adaptive Chain Visual Reasoning Model (ACVRM) for Answerer is also proposed, where the question-answer pair is used to update the visual representation sequentially.","offset":3,"pro":0.3,"labels":"MTD"},{"idx":5,"sentence":"To perform supervised learning for each model","offset":4,"pro":0.4,"labels":"PUR"},{"idx":5,"sentence":"we introduce a well-designed method to build a SQS for each question on VQA 2.0 and VQA-CP v2 datasets.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":5,"sentence":"Experimental results show that our method achieves state-of-the-art on VQA-CP v2.","offset":6,"pro":0.6,"labels":"RST"},{"idx":5,"sentence":"Further analyses show that SQSs help build direct semantic connections between questions and images, provide question-adaptive variable-length reasoning chains, and with explicit interpretability as well as error traceability.","offset":7,"pro":0.7,"labels":"CTN"},{"idx":5,"sentence":"By simulating the process","offset":8,"pro":0.8,"labels":"MTD"},{"idx":5,"sentence":"this paper proposes a conversation-based VQA (Co-VQA) framework, which consists of three components: Questioner, Oracle, and Answerer.","offset":9,"pro":0.9,"labels":"PUR"},{"idx":6,"sentence":"Previous works usually adopt heuristic metrics such as the entropy of internal outputs to measure instance difficulty, which suffers from generalization and threshold-tuning.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"In contrast, learning to exit, or learning to predict instance difficulty is a more appealing way.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":6,"sentence":"Though some effort has been devoted to employing such learn-to-exit modules, it is still unknown whether and how well the instance difficulty can be learned.","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":6,"sentence":"As a response, we first conduct experiments on the learnability of instance difficulty, which demonstrates that modern neural models perform poorly on predicting instance difficulty.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":6,"sentence":"Different from previous methods, HashEE requires no internal classifiers nor extra parameters, and therefore is more efficient.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":6,"sentence":"HashEE can be used in various tasks (including language understanding and generation) and model architectures such as seq2seq models.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":6,"sentence":"Experimental results on classification, regression, and generation tasks demonstrate that HashEE can achieve higher performance with fewer FLOPs and inference time compared with previous state-of-the-art early exiting methods.","offset":6,"pro":0.6666666666666666,"labels":"RST"},{"idx":6,"sentence":"Based on this observation","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":6,"sentence":"we propose a simple-yet-effective Hash-based Early Exiting approach HashEE) that replaces the learn-to-exit modules with hash functions to assign each token to a fixed exiting layer.","offset":8,"pro":0.8888888888888888,"labels":"PUR"},{"idx":7,"sentence":"The biaffine parser of (CITATION) was successfully extended to semantic dependency parsing (SDP) (CITATION)","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"Its performance on graphs is surprisingly high given that, without the constraint of producing a tree, all arcs for a given sentence are predicted independently from each other (modulo a shared representation of tokens).","offset":1,"pro":0.2,"labels":"BAC"},{"idx":7,"sentence":"To circumvent such an independence of decision, while retaining the O(n^2) complexity and highly parallelizable architecture, we propose to use simple auxiliary tasks that introduce some form of interdependence between arcs.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":7,"sentence":"Experiments on the three English acyclic datasets of SemEval-2015 task 18 (CITATION), and on French deep syntactic cyclic graphs (CITATION) show modest but systematic performance gains on a near-state-of-the-art baseline using transformer-based contextualized representations.","offset":3,"pro":0.6,"labels":"RST"},{"idx":7,"sentence":"This provides a simple and robust method to boost SDP performance","offset":4,"pro":0.8,"labels":"CTN"},{"idx":8,"sentence":"Syntactic information has been proved to be useful for transformer-based pre-trained language models.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"Previous studies often rely on additional syntax-guided attention components to enhance the transformer, which require more parameters and additional syntactic parsing in downstream tasks.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":8,"sentence":"This increase in complexity severely limits the application of syntax-enhanced language model in a wide range of scenarios.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":8,"sentence":"In order to inject syntactic knowledge effectively and efficiently into pre-trained language models, we propose a novel syntax-guided contrastive learning method which does not change the transformer architecture.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":8,"sentence":"Based on constituency and dependency structures of syntax trees, we design phrase-guided and tree-guided contrastive objectives, and optimize them in the pre-training stage, so as to help the pre-trained language model to capture rich syntactic knowledge in its representations.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":8,"sentence":"Experimental results show that our contrastive method achieves consistent improvements in a variety of tasks, including grammatical error detection, entity tasks, structural probing and GLUE.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":8,"sentence":"Detailed analysis further verifies that the improvements come from the utilization of syntactic information, and the learned attention weights are more explainable in terms of linguistics.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":9,"sentence":"Class imbalance and drift can sometimes be mitigated by resampling the training data to simulate (or compensate for) a known target distribution","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"but what if the target distribution is determined by unknown future events?","offset":1,"pro":0.2,"labels":"GAP"},{"idx":9,"sentence":"Reframing group-robust algorithms as adaptation algorithms under concept drift","offset":2,"pro":0.4,"labels":"MTD"},{"idx":9,"sentence":"The effect is more pronounced the larger the label set.","offset":3,"pro":0.6,"labels":"RST"},{"idx":9,"sentence":"we find that Invariant Risk Minimization and Spectral Decoupling outperform sampling-based approaches to class imbalance and concept drift, and lead to much better performance on minority classes.","offset":4,"pro":0.8,"labels":"RST"},{"idx":10,"sentence":"Prompt-based learning, which exploits knowledge from pre-trained language models by providing textual prompts and designing appropriate answer-category mapping methods, has achieved impressive successes on few-shot text classification and natural language inference (NLI)","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"Because of the diverse linguistic expression, there exist many answer tokens for the same category.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":10,"sentence":"However, both manual answer design and automatic answer search constrain answer space and therefore hardly achieve ideal performance.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":10,"sentence":"To address this issue, we propose an answer space clustered prompting model (ASCM) together with a synonym initialization method (SI) which automatically categorizes all answer tokens in a semantic-clustered embedding space.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":10,"sentence":"We also propose a stable semi-supervised method named stair learning (SL) that orderly distills knowledge from better models to weaker models.","offset":4,"pro":0.6666666666666666,"labels":"PUR"},{"idx":10,"sentence":"Extensive experiments demonstrate that our ASCM+SL significantly outperforms existing state-of-the-art techniques in few-shot settings.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":11,"sentence":"We present a literature and empirical survey that critically assesses the state of the art in character-level modeling for machine translation (MT).","offset":0,"pro":0,"labels":"PUR"},{"idx":11,"sentence":"Despite evidence in the literature that character-level systems are comparable with subword systems,","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":11,"sentence":"they are virtually never used in competitive setups in WMT competitions.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":11,"sentence":"We empirically show that even with recent modeling innovations in character-level natural language processing, character-level MT systems still struggle to match their subword-based counterparts.","offset":3,"pro":0.5,"labels":"RST"},{"idx":11,"sentence":"Character-level MT systems show neither better domain robustness, nor better morphological generalization, despite being often so motivated.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":11,"sentence":"However, we are able to show robustness towards source side noise and that translation quality does not degrade with increasing beam size at decoding time.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":12,"sentence":"Recent work shows that existing models memorize procedures from context and rely on shallow heuristics to solve MWPs.","offset":0,"pro":0,"labels":"GAP"},{"idx":12,"sentence":"In this paper, we look at this issue and argue that the cause is a lack of overall understanding of MWP patterns.","offset":1,"pro":0.125,"labels":"PUR"},{"idx":12,"sentence":"We first investigate how a neural network understands patterns only from semantics, and observe that, if the prototype equations are the same, most problems get closer representations and those representations apart from them or close to other prototypes tend to produce wrong solutions","offset":2,"pro":0.25,"labels":"MTD"},{"idx":12,"sentence":" Inspired by it, we propose a contrastive learning approach, where the neural network perceives the divergence of patterns.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":12,"sentence":"We collect contrastive examples by converting the prototype equation into a tree and seeking similar tree structures.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":12,"sentence":"The solving model is trained with an auxiliary objective on the collected examples, resulting in the representations of problems with similar prototypes being pulled closer.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":12,"sentence":"We conduct experiments on the Chinese dataset Math23k and the English dataset MathQA.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":12,"sentence":"Our method greatly improves the performance in monolingual and multilingual settings.","offset":7,"pro":0.875,"labels":"CTN"},{"idx":13,"sentence":"Recent advances in multimodal vision and language modeling have predominantly focused on the English language, mostly due to the lack of multilingual multimodal datasets to steer modeling efforts.","offset":0,"pro":0,"labels":"GAP"},{"idx":13,"sentence":"In this work, we address this gap and provide xGQA, a new multilingual evaluation benchmark for the visual question answering task.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":13,"sentence":"We extend the established English GQA dataset to 7 typologically diverse languages, enabling us to detect and explore crucial challenges in cross-lingual visual question answering.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":13,"sentence":"We further propose new adapter-based approaches to adapt multimodal transformer-based models to become multilingual, and-vice versa-multilingual models to become multimodal.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":13,"sentence":"Our proposed methods outperform current state-of-the-art multilingual multimodal models (e.g., M3P) in zero-shot cross-lingual settings, but the accuracy remains low across the board","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":13,"sentence":"a performance drop of around 38 accuracy points in target languages showcases the difficulty of zero-shot cross-lingual transfer for this task.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":13,"sentence":"Our results suggest that simple cross-lingual transfer of multimodal models yields latent multilingual multimodal misalignment, calling for more sophisticated methods for vision and multilingual language modeling.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":14,"sentence":"We propose to use about one hour of annotated data to design an automatic speech recognition system for each language.","offset":0,"pro":0,"labels":"PUR"},{"idx":14,"sentence":"We evaluate how much data is needed","offset":1,"pro":0.2,"labels":"MTD"},{"idx":14,"sentence":"Moreover, our experiments show that multilingual self-supervised models are not necessarily the most efficient for Creole languages.","offset":2,"pro":0.4,"labels":"RST"},{"idx":14,"sentence":"We investigate the exploitation of self-supervised models for two Creole languages with few resources: Gwadloup{'eyen and Morisien.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":14,"sentence":"Automatic language processing tools are almost non-existent for these two languages.","offset":4,"pro":0.8,"labels":"BAC"},{"idx":15,"sentence":"When directly using existing text generation datasets for controllable generation, we are facing the problem of not having the domain knowledge and thus the aspects that could be controlled are limited.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"A typical example is when using CNN/Daily Mail dataset for controllable text summarization, there is no guided information on the emphasis of summary sentences.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":15,"sentence":"A more useful text generator should leverage both the input text and the control signal to guide the generation, which can only be built with deep understanding of the domain knowledge.","offset":2,"pro":0.2,"labels":"BAC"},{"idx":15,"sentence":"Motivated by this vision, our paper introduces a new text generation dataset, named MReD.","offset":3,"pro":0.3,"labels":"PUR"},{"idx":15,"sentence":"Our new dataset consists of 7,089 meta-reviews and all its 45k meta-review sentences are manually annotated with one of the 9 carefully defined categories, including abstract, strength, decision, etc.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":15,"sentence":"We present experimental results on start-of-the-art summarization models","offset":5,"pro":0.5,"labels":"MTD"},{"idx":15,"sentence":"and propose methods for structure-controlled generation with both extractive and abstractive models using our annotated data.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":15,"sentence":"By exploring various settings and analyzing the model behavior with respect to the control signal,","offset":7,"pro":0.7,"labels":"MTD"},{"idx":15,"sentence":"we demonstrate the challenges of our proposed task and the values of our dataset MReD.","offset":8,"pro":0.8,"labels":"RST"},{"idx":15,"sentence":"Meanwhile, MReD also allows us to have a better understanding of the meta-review domain.","offset":9,"pro":0.9,"labels":"CTN"},{"idx":16,"sentence":"Subword regularizations use multiple subword segmentations during training to improve the robustness of neural machine translation models","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"In previous subword regularizations, we use multiple segmentations in the training process but use only one segmentation in the inference.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":16,"sentence":"The proposed strategy approximates the marginalized likelihood by using multiple segmentations including the most plausible segmentation and several sampled segmentations","offset":2,"pro":0.4,"labels":"MTD"},{"idx":16,"sentence":"Because the proposed strategy aggregates predictions from several segmentations, we can regard it as a single model ensemble that does not require any additional cost for training.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":16,"sentence":"xperimental results show that the proposed strategy improves the performance of models trained with subword regularization in low-resource machine translation tasks","offset":4,"pro":0.8,"labels":"RST"},{"idx":17,"sentence":"The filtering and/or selection of training data is one of the core aspects to be considered when building a strong machine translation system","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"In the same year the WMT introduced a shared task on parallel corpus filtering, which went on to be repeated in the following years, and resulted in many different filtering approaches being proposed.","offset":1,"pro":0.25,"labels":"BAC"},{"idx":17,"sentence":"We observe that most of these types of noise can be detected with an accuracy of over 90% by modern filtering systems when operating in a well studied high resource setting.","offset":2,"pro":0.5,"labels":"RST"},{"idx":17,"sentence":" investigate whether state-of-the-art filtering systems are capable of removing all the suggested noise types","offset":3,"pro":0.75,"labels":"MTD"},{"idx":18,"sentence":"Due to the limitations of the model structure and pre-training objectives, existing vision-and-language generation models cannot utilize pair-wise images and text through bi-directional generation.","offset":0,"pro":0,"labels":"GAP"},{"idx":18,"sentence":"In this paper, we propose DU-VLG, a framework which unifies vision-and-language generation as sequence generation problems.","offset":1,"pro":0.125,"labels":"PUR"},{"idx":18,"sentence":"DU-VLG is trained with novel dual pre-training tasks: multi-modal denoising autoencoder tasks and modality translation tasks.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":18,"sentence":"To bridge the gap between image understanding and generation","offset":3,"pro":0.375,"labels":"PUR"},{"idx":18,"sentence":"we further design a novel commitment loss","offset":4,"pro":0.5,"labels":"MTD"},{"idx":18,"sentence":"We compare pre-training objectives on image captioning and text-to-image generation datasets.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":18,"sentence":"Results show that DU-VLG yields better performance than variants trained with uni-directional generation objectives or the variant without the commitment loss.","offset":6,"pro":0.75,"labels":"RST"},{"idx":18,"sentence":"We also obtain higher scores compared to previous state-of-the-art systems on three vision-and-language generation tasks.","offset":7,"pro":0.875,"labels":"RST"},{"idx":19,"sentence":"Distant supervision assumes that any sentence containing the same entity pairs reflects identical relationships.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"Previous works of distantly supervised relation extraction (DSRE) task generally focus on sentence-level or bag-level de-noising techniques independently, neglecting the explicit interaction with cross levels.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":19,"sentence":"In this paper, we propose a hierarchical contrastive learning Framework for Distantly Supervised relation extraction (HiCLRE) to reduce noisy sentences, which integrate the global structural information and local fine-grained interaction.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":19,"sentence":"Specifically, we propose a three-level hierarchical learning framework to interact with cross levels, generating the de-noising context-aware representations via adapting the existing multi-head self-attention, named Multi-Granularity Recontextualization","offset":3,"pro":0.5,"labels":"MTD"},{"idx":19,"sentence":"Meanwhile, pseudo positive samples are also provided in the specific level for contrastive learning via a dynamic gradient-based data augmentation strategy, named Dynamic Gradient Adversarial Perturbation.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":19,"sentence":"Experiments demonstrate that HiCLRE significantly outperforms strong baselines in various mainstream DSRE datasets","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":20,"sentence":"Neural machine translation (NMT) has obtained significant performance improvement over the recent years.","offset":0,"pro":0,"labels":"BAC"},{"idx":20,"sentence":"However, NMT models still face various challenges including fragility and lack of style flexibility.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":20,"sentence":"Moreover, current methods for instance-level constraints are limited in that they are either constraint-specific or model-specific","offset":2,"pro":0.4,"labels":"GAP"},{"idx":20,"sentence":"To this end, we propose prompt-driven neural machine translation to incorporate prompts for enhancing translation control and enriching flexibility. ","offset":3,"pro":0.6,"labels":"PUR"},{"idx":20,"sentence":"Through human evaluation, we further show the flexibility of prompt control and the efficiency in human-in-the-loop translation.","offset":4,"pro":0.8,"labels":"RST"},{"idx":21,"sentence":"Dialogue agents can leverage external textual knowledge to generate responses of a higher quality.","offset":0,"pro":0,"labels":"BAC"},{"idx":21,"sentence":"To our best knowledge, most existing works on knowledge grounded dialogue settings assume that the user intention is always answerable.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":21,"sentence":"Unfortunately, this is impractical as there is no guarantee that the knowledge retrievers could always retrieve the desired knowledge.","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":21,"sentence":"Therefore, this is crucial to incorporate fallback responses to respond to unanswerable contexts appropriately while responding to the answerable contexts in an informative manner.","offset":3,"pro":0.3333333333333333,"labels":"GAP"},{"idx":21,"sentence":"We propose a novel framework that automatically generates a control token with the generator to bias the succeeding response towards informativeness for answerable contexts and fallback for unanswerable contexts in an end-to-end manner.","offset":4,"pro":0.4444444444444444,"labels":"PUR"},{"idx":21,"sentence":"Since no existing knowledge grounded dialogue dataset considers this aim, we augment the existing dataset with unanswerable contexts to conduct our experiments.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":21,"sentence":"Automatic and human evaluation results indicate that naively incorporating fallback responses with controlled text generation still hurts informativeness for answerable context.","offset":6,"pro":0.6666666666666666,"labels":"RST"},{"idx":21,"sentence":"In contrast, our proposed framework effectively mitigates this problem while still appropriately presenting fallback responses to unanswerable contexts.","offset":7,"pro":0.7777777777777778,"labels":"CTN"},{"idx":21,"sentence":"Such a framework also reduces the extra burden of the additional classifier and the overheads introduced in the previous works, which operates in a pipeline manner.","offset":8,"pro":0.8888888888888888,"labels":"CTN"},{"idx":22,"sentence":"Humans are able to perceive, understand and reason about causal events.","offset":0,"pro":0,"labels":"BAC"},{"idx":22,"sentence":"Developing models with similar physical and causal understanding capabilities is a long-standing goal of artificial intelligence.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":22,"sentence":"As a step towards this direction, we introduce CRAFT, a new video question answering dataset that requires causal reasoning about physical forces and object interactions.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":22,"sentence":"It contains 58K video and question pairs that are generated from 10K videos from 20 different virtual environments, containing various objects in motion that interact with each other and the scene.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":22,"sentence":"Additionally, inspired by the Force Dynamics Theory in cognitive linguistics, we introduce a new causal question category that involves understanding the causal interactions between objects through notions like cause, enable, and prevent","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":22,"sentence":"Our results show that even though the questions in CRAFT are easy for humans, the tested baseline models, including existing state-of-the-art methods, do not yet deal with the challenges posed in our benchmark.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":23,"sentence":"Predicting the subsequent event for an existing event context is an important but challenging task, as it requires understanding the underlying relationship between events.","offset":0,"pro":0,"labels":"BAC"},{"idx":23,"sentence":"However, the sparsity of event graph may restrict the acquisition of relevant graph information, and hence influence the model performance.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":23,"sentence":"To address this issue, we consider automatically building of event graph using a BERT model","offset":2,"pro":0.4,"labels":"PUR"},{"idx":23,"sentence":"To this end, we incorporate an additional structured variable into BERT to learn to predict the event connections in the training process","offset":3,"pro":0.6,"labels":"MTD"},{"idx":23,"sentence":"Hence, in the test process, the connection relationship for unseen events can be predicted by the structured variable.","offset":4,"pro":0.8,"labels":"MTD"},{"idx":24,"sentence":"Most of the open-domain dialogue models tend to perform poorly in the setting of long-term human-bot conversations","offset":0,"pro":0,"labels":"BAC"},{"idx":24,"sentence":"The possible reason is that they lack the capability of understanding and memorizing long-term dialogue history information","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":24,"sentence":"To address this issue, we present a novel task of Long-term Memory Conversation (LeMon) and then build a new dialogue dataset DuLeMon and a dialogue generation framework with Long-Term Memory (LTM) mechanism (called PLATO-LTM)","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":24,"sentence":"This LTM mechanism enables our system to accurately extract and continuously update long-term persona memory without requiring multiple-session dialogue datasets for model training","offset":3,"pro":0.5,"labels":"MTD"},{"idx":24,"sentence":"To our knowledge, this is the first attempt to conduct real-time dynamic management of persona information of both parties, including the user and the bot.","offset":4,"pro":0.6666666666666666,"labels":"CTN"},{"idx":24,"sentence":"Results on DuLeMon indicate that PLATO-LTM can significantly outperform baselines in terms of long-term dialogue consistency, leading to better dialogue engagingness.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":25,"sentence":"Word embeddings are powerful dictionaries, which may easily capture language variations","offset":0,"pro":0,"labels":"BAC"},{"idx":25,"sentence":"However, these dictionaries fail to give sense to rare words, which are surprisingly often covered by traditional dictionaries","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":25,"sentence":"In this paper, we propose to use definitions retrieved in traditional dictionaries to produce word embeddings for rare words.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":25,"sentence":"For this purpose, we introduce two methods: Definition Neural Network (DefiNNet) and Define BERT (DefBERT).","offset":3,"pro":0.5,"labels":"MTD"},{"idx":25,"sentence":"In our experiments, DefiNNet and DefBERT significantly outperform state-of-the-art as well as baseline methods devised for producing embeddings of unknown words.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":25,"sentence":"Then, definitions in traditional dictionaries are useful to build word embeddings for rare words.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":26,"sentence":"Existing news recommendation methods usually learn news representations solely based on news titles.","offset":0,"pro":0,"labels":"BAC"},{"idx":26,"sentence":"To sufficiently utilize other fields of news information such as category and entities, some methods treat each field as an additional feature and combine different feature vectors with attentive pooling","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":26,"sentence":"With the adoption of large pre-trained models like BERT in news recommendation, the above way to incorporate multi-field information may encounter challenges: the shallow feature encoding to compress the category and entity information is not compatible with the deep BERT encoding","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":26,"sentence":"In this paper, we propose a multi-task method to incorporate the multi-field information into BERT, which improves its news encoding capability.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":26,"sentence":"Besides, we modify the gradients of auxiliary tasks based on their gradient conflicts with the main task, which further boosts the model performance.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":26,"sentence":"Extensive experiments on the MIND news recommendation benchmark show the effectiveness of our approach.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":27,"sentence":"Cross-domain NER is a practical yet challenging problem since the data scarcity in the real-world scenario","offset":0,"pro":0,"labels":"BAC"},{"idx":27,"sentence":"A common practice is first to learn a NER model in a rich-resource general domain and then adapt the model to specific domains.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":27,"sentence":"Due to the mismatch problem between entity types across domains, the wide knowledge in the general domain can not effectively transfer to the target domain NER model.","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":27,"sentence":"To this end, we model the label relationship as a probability distribution and construct label graphs in both source and target label spaces.","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":27,"sentence":"To enhance the contextual representation with label structures","offset":4,"pro":0.4444444444444444,"labels":"PUR"},{"idx":27,"sentence":"we fuse the label graph into the word embedding output by BERT.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":27,"sentence":"By representing label relationships as graphs, we formulate cross-domain NER as a graph matching problem.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":27,"sentence":"Furthermore, the proposed method has good applicability with pre-training methods and is potentially capable of other cross-domain prediction tasks","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":27,"sentence":"Empirical results on four datasets show that our method outperforms a series of transfer learning, multi-task learning, and few-shot learning methods","offset":8,"pro":0.8888888888888888,"labels":"RST"},{"idx":28,"sentence":"Recently pre-trained multimodal models, such as CLIP, have shown exceptional capabilities towards connecting images and natural language","offset":0,"pro":0,"labels":"BAC"},{"idx":28,"sentence":"The textual representations in English can be desirably transferred to multilingualism and support downstream multimodal tasks for different languages.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":28,"sentence":"Nevertheless, the principle of multilingual fairness is rarely scrutinized: do multilingual multimodal models treat languages equally? Are their performances biased towards particular languages?","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":28,"sentence":"To answer these questions, we view language as the fairness recipient and introduce two new fairness notions, multilingual individual fairness and multilingual group fairness, for pre-trained multimodal models.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":28,"sentence":"Multilingual individual fairness requires that text snippets expressing similar semantics in different languages connect similarly to images, while multilingual group fairness requires equalized predictive performance across languages.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":28,"sentence":"We characterize the extent to which pre-trained multilingual vision-and-language representations are individually fair across languages.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":28,"sentence":"However, extensive experiments demonstrate that multilingual representations do not satisfy group fairness: (1) there is a severe multilingual accuracy disparity issue; (2) the errors exhibit biases across languages conditioning the group of people in the images, including race, gender and age.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":29,"sentence":"Traditionally, Latent Dirichlet Allocation (LDA) ingests words in a collection of documents to discover their latent topics using word-document co-occurrences.","offset":0,"pro":0,"labels":"BAC"},{"idx":29,"sentence":"Previous studies show that representing bigrams collocations in the input can improve topic coherence in English.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":29,"sentence":"However, it is unclear how to achieve the best results for languages without marked word boundaries such as Chinese and Thai.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":29,"sentence":"Here, we explore the use of retokenization based on chi-squared measures, t-statistics, and raw frequency to merge frequent token ngrams into collocations when preparing input to the LDA model.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":29,"sentence":"Based on the goodness of fit and the coherence metric","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":29,"sentence":"we show that topics trained with merged tokens result in topic keys that are clearer, more coherent, and more effective at distinguishing topics than those of unmerged models.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":0,"sentence":"Training the deep neural networks that dominate NLP requires large datasets.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"These are often collected automatically or via crowdsourcing, and may exhibit systematic biases or annotation artifacts.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":0,"sentence":"By the latter we mean spurious correlations between inputs and outputs that do not represent a generally held causal relationship between features and classes; models that exploit such correlations may appear to perform a given task well, but fail on out of sample data.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":0,"sentence":"In this paper, we evaluate use of different attribution methods for aiding identification of training data artifacts.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":0,"sentence":"We propose new hybrid approaches that combine saliency maps (which highlight important input features) with instance attribution methods (which retrieve training samples influential to a given prediction). ","offset":4,"pro":0.5,"labels":"MTD"},{"idx":0,"sentence":"We show that this proposed training-feature attribution can be used to efficiently uncover artifacts in training data when a challenging validation set is available.","offset":5,"pro":0.625,"labels":"CLN"},{"idx":0,"sentence":"We also carry out a small user study to evaluate whether these methods are useful to NLP researchers in practice, with promising results. ","offset":6,"pro":0.75,"labels":"MTD"},{"idx":0,"sentence":"We make code for all methods and experiments in this paper available","offset":7,"pro":0.875,"labels":"CTN"},{"idx":1,"sentence":"Named Entity Recognition (NER) systems often demonstrate great performance on in-distribution data,","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"but perform poorly on examples drawn from a shifted distribution.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":1,"sentence":"One way to evaluate the generalization ability of NER models is to use adversarial examples, on which the specific variations associated with named entities are rarely considered.","offset":2,"pro":0.2222222222222222,"labels":"BAC"},{"idx":1,"sentence":"To this end, we propose leveraging expert-guided heuristics to change the entity tokens and their surrounding contexts thereby altering their entity types as adversarial attacks.","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":1,"sentence":"Using expert-guided heuristics, we augmented the CoNLL 2003 test set and manually annotated it to construct a high-quality challenging set.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":1,"sentence":"We found that state-of-the-art NER systems trained on CoNLL 2003 training data drop performance dramatically on our challenging set.","offset":5,"pro":0.5555555555555556,"labels":"CLN"},{"idx":1,"sentence":"By training on adversarial augmented training examples and using mixup for regularization, ","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":1,"sentence":"we were able to significantly improve the performance on the challenging set as well as improve out-of-domain generalization which we evaluated by using OntoNotes data.","offset":7,"pro":0.7777777777777778,"labels":"CLN"},{"idx":1,"sentence":"We have publicly released our dataset and code at https://github.com/GT-SALT/Guided-Adversarial-Augmentation.","offset":8,"pro":0.8888888888888888,"labels":"CTN"},{"idx":2,"sentence":"We study the problem of few shot learning for named entity recognition.","offset":0,"pro":0,"labels":"PUR"},{"idx":2,"sentence":"Specifically, we leverage the semantic information in the names of the labels as a way of giving the model additional signal and enriched priors. ","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":2,"sentence":"We propose a neural architecture that consists of two BERT encoders, one to encode the document and its tokens and another one to encode each of the labels in natural language format.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":2,"sentence":"Our model learns to match the representations of named entities computed by the first encoder with label representations computed by the second encoder.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":2,"sentence":"The label semantics signal is shown to support improved state-of-the-art results in multiple few shot NER benchmarks and on-par performance in standard benchmarks.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":2,"sentence":"Our model is especially effective in low resource settings.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":3,"sentence":"We propose an autoregressive entity linking model, that is trained with two auxiliary tasks, and learns to re-rank generated samples at inference time. ","offset":0,"pro":0,"labels":"PUR"},{"idx":3,"sentence":"Our proposed novelties address two weaknesses in the literature.","offset":1,"pro":0.1111111111111111,"labels":"CTN"},{"idx":3,"sentence":"First, a recent method proposes to learn mention detection and then entity candidate selection, but relies on predefined sets of candidates.","offset":2,"pro":0.2222222222222222,"labels":"CTN"},{"idx":3,"sentence":"We use encoder-decoder autoregressive entity linking in order to bypass this need, and propose to train mention detection as an auxiliary task instead. ","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":3,"sentence":"Second, previous work suggests that re-ranking could help correct prediction errors.","offset":4,"pro":0.4444444444444444,"labels":"BAC"},{"idx":3,"sentence":"We add a new, auxiliary task, match prediction, to learn re-ranking.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":3,"sentence":"Without the use of a knowledge base or candidate sets, our model sets a new state of the art in two benchmark datasets of entity linking: COMETA in the biomedical domain, and AIDA-CoNLL in the news domain.","offset":6,"pro":0.6666666666666666,"labels":"RST"},{"idx":3,"sentence":"We show through ablation studies that each of the two auxiliary tasks increases performance, and that re-ranking is an important factor to the increase. ","offset":7,"pro":0.7777777777777778,"labels":"CLN"},{"idx":3,"sentence":"Finally, our low-resource experimental results suggest that performance on the main task benefits from the knowledge learned by the auxiliary tasks, and not just from the additional training data.","offset":8,"pro":0.8888888888888888,"labels":"IMP"},{"idx":4,"sentence":"Interactive robots navigating photo-realistic environments need to be trained to effectively leverage and handle the dynamic nature of dialogue in addition to the challenges underlying vision-and-language navigation (VLN)","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"In this paper, we present VISITRON, a multi-modal Transformer-based navigator better suited to the interactive regime inherent to Cooperative Vision-and-Dialog Navigation (CVDN).","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":4,"sentence":"VISITRON is trained to: i) identify and associate object-level concepts and semantics between the environment and dialogue history, ii) identify when to interact vs. navigate via imitation learning of a binary classification head. ","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":4,"sentence":"We perform extensive pre-training and fine-tuning ablations with VISITRON to gain empirical insights and improve performance on CVDN. ","offset":3,"pro":0.5,"labels":"MTD"},{"idx":4,"sentence":"VISITRON's ability to identify when to interact leads to a natural generalization of the game-play mode introduced by Roman et al. (2020) for enabling the use of such models in different environments.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":4,"sentence":"VISITRON is competitive with models on the static CVDN leaderboard and attains state-of-the-art performance on the Success weighted by Path Length (SPL) metric.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":5,"sentence":" However, which approaches work best across tasks or even if they consistently outperform the simplest baseline MaxProb remains to be explored. ","offset":0,"pro":0,"labels":"GAP"},{"idx":5,"sentence":"To this end, we systematically study selective prediction in a large-scale setup of 17 datasets across several NLP tasks.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":5,"sentence":"Through comprehensive experiments under in-domain (IID), out-of-domain (OOD), and adversarial (ADV) settings,","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":5,"sentence":"we show that despite leveraging additional resources (held-out data/computation), none of the existing approaches consistently and considerably outperforms MaxProb in all three settings.","offset":3,"pro":0.42857142857142855,"labels":"RST"},{"idx":5,"sentence":"Furthermore, their performance does not translate well across tasks.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":5,"sentence":"For instance, Monte-Carlo Dropout outperforms all other approaches on Duplicate Detection datasets but does not fare well on NLI datasets, especially in the OOD setting.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":5,"sentence":"Thus, we recommend that future selective prediction approaches should be evaluated across tasks and settings for reliable estimation of their capabilities.","offset":6,"pro":0.8571428571428571,"labels":"IMP"},{"idx":6,"sentence":"Transformer-based models achieve impressive performance on numerous Natural Language Inference (NLI) benchmarks when trained on respective training datasets.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"However, in certain cases, training samples may not be available or collecting them could be time-consuming and resource-intensive.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":6,"sentence":"In this work, we address the above challenge and present an explorative study on unsupervised NLI, a paradigm in which no human-annotated training samples are available. ","offset":2,"pro":0.25,"labels":"PUR"},{"idx":6,"sentence":"We investigate it under three settings: PH, P, and NPH that differ in the extent of unlabeled data available for learning.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":6,"sentence":"As a solution, we propose a procedural data generation approach that leverages a set of sentence transformations to collect PHL (Premise, Hypothesis, Label) triplets for training NLI models, bypassing the need for human-annotated training data.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":6,"sentence":"Comprehensive experiments with several NLI datasets show that the proposed approach results in accuracies of up to 66.75%, 65.9%, 65.39% in PH, P, and NPH settings respectively, outperforming all existing unsupervised baselines.","offset":5,"pro":0.625,"labels":"RST"},{"idx":6,"sentence":"Furthermore, fine-tuning our model with as little as asciitilde0.1% of the human-annotated training dataset (500 instances) leads to 12.2% higher accuracy than the model trained from scratch on the same 500 instances.","offset":6,"pro":0.75,"labels":"RST"},{"idx":6,"sentence":" Supported by this superior performance, we conclude with a recommendation for collecting high-quality task-specific data.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":7,"sentence":"Scaling dialogue systems to a multitude of domains, tasks and languages relies on costly and time-consuming data annotation for different domain-task-language configurations. ","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"The annotation efforts might be substantially reduced by the methods that generalise well in zero- and few-shot scenarios, and also effectively leverage external unannotated data sources (e.g., Web-scale corpora)","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":7,"sentence":"We propose two methods to this aim, offering improved dialogue natural language understanding (NLU) across multiple languages: 1) Multi-SentAugment, and 2) LayerAgg.","offset":2,"pro":0.2222222222222222,"labels":"PUR"},{"idx":7,"sentence":"Multi-SentAugment is a self-training method which augments available (typically few-shot) training data with similar (automatically labelled) in-domain sentences from large monolingual Web-scale corpora.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":7,"sentence":"LayerAgg learns to select and combine useful semantic information scattered across different layers of a Transformer model (e.g., mBERT);","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":7,"sentence":"it is especially suited for zero-shot scenarios as semantically richer representations should strengthen the model's cross-lingual capabilities.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":7,"sentence":"Applying the two methods with state-of-the-art NLU models obtains consistent improvements across two standard multilingual NLU datasets covering 16 diverse languages.","offset":6,"pro":0.6666666666666666,"labels":"RST"},{"idx":7,"sentence":"The gains are observed in zero-shot, few-shot, and even in full-data scenarios.","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":7,"sentence":"The results also suggest that the two methods achieve a synergistic effect: the best overall performance in few-shot setups is attained when the methods are used together.","offset":8,"pro":0.8888888888888888,"labels":"IMP"},{"idx":8,"sentence":"We propose a novel approach that jointly utilizes the labels and elicited rationales for text classification to speed up the training of deep learning models with limited training data.","offset":0,"pro":0,"labels":"PUR"},{"idx":8,"sentence":"We define and optimize a ranking-constrained loss function that combines cross-entropy loss with ranking losses as rationale constraints.","offset":1,"pro":0.25,"labels":"MTD"},{"idx":8,"sentence":"We evaluate our proposed rationale-augmented learning approach on three human-annotated datasets,","offset":2,"pro":0.5,"labels":"MTD"},{"idx":8,"sentence":"and show that our approach provides significant improvements over classification approaches that do not utilize rationales as well as other state-of-the-art rationale-augmented baselines.","offset":3,"pro":0.75,"labels":"CLN"},{"idx":9,"sentence":"Content is created for a well-defined purpose, often described by a metric or signal represented in the form of structured information. ","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"The relationship between the goal (metrics) of target content and the content itself is non-trivial.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":9,"sentence":"While large-scale language models show promising text generation capabilities, guiding the generated text with external metrics is challenging.","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":9,"sentence":"These metrics and content tend to have inherent relationships and not all of them may be of consequence.","offset":3,"pro":0.3333333333333333,"labels":"GAP"},{"idx":9,"sentence":"We introduce CaM-Gen: Causally aware Generative Networks guided by user-defined target metrics incorporating the causal relationships between the metric and content features.","offset":4,"pro":0.4444444444444444,"labels":"PUR"},{"idx":9,"sentence":"We leverage causal inference techniques to identify causally significant aspects of a text that lead to the target metric and then explicitly guide generative models towards these by a feedback mechanism.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":9,"sentence":"We propose this mechanism for variational autoencoder and Transformer-based generative models. ","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":9,"sentence":"The proposed models beat baselines in terms of the target metric control while maintaining fluency and language quality of the generated text.","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":9,"sentence":"To the best of our knowledge, this is one of the early attempts at controlled generation incorporating a metric guide using causal inference.","offset":8,"pro":0.8888888888888888,"labels":"CTN"},{"idx":10,"sentence":"Pre-trained language models (e.g. BART) have shown impressive results when fine-tuned on large summarization datasets.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"However, little is understood about this fine-tuning process, including what knowledge is retained from pre-training time or how content selection and generation strategies are learnt across iterations.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":10,"sentence":"In this work, we analyze the training dynamics for generation models, focusing on summarization. ","offset":2,"pro":0.25,"labels":"PUR"},{"idx":10,"sentence":"Across different datasets (CNN/DM, XSum, MediaSum) and summary properties, such as abstractiveness and hallucination, we study what the model learns at different stages of its fine-tuning process.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":10,"sentence":"We find that a propensity to copy the input is learned early in the training process consistently across all datasets studied.","offset":4,"pro":0.5,"labels":"RST"},{"idx":10,"sentence":"On the other hand, factual errors, such as hallucination of unsupported facts, are learnt in the later stages, though this behavior is more varied across domains.","offset":5,"pro":0.625,"labels":"RST"},{"idx":10,"sentence":"Based on these observations, we explore complementary approaches for modifying training: first, disregarding high-loss tokens that are challenging to learn and second, disregarding low-loss tokens that are learnt very quickly in the latter stages of the training process.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":10,"sentence":"We show that these simple training modifications allow us to configure our model to achieve different goals, such as improving factuality or improving abstractiveness.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":11,"sentence":"We examine whether some countries are more richly represented in embedding space than others. ","offset":0,"pro":0,"labels":"PUR"},{"idx":11,"sentence":"We find that countries whose names occur with low frequency in training corpora are more likely to be tokenized into subwords, are less semantically distinct in embedding space, and are less likely to be correctly predicted: e.g., Ghana (the correct answer and in-vocabulary) is not predicted for, The country producing the most cocoa is [MASK]..","offset":1,"pro":0.16666666666666666,"labels":"RST"},{"idx":11,"sentence":"Although these performance discrepancies and representational harms are due to frequency, we find that frequency is highly correlated with a country's GDP","offset":2,"pro":0.3333333333333333,"labels":"RST"},{"idx":11,"sentence":"thus perpetuating historic power and wealth inequalities.","offset":3,"pro":0.5,"labels":"RST"},{"idx":11,"sentence":"We analyze the effectiveness of mitigation strategies","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":11,"sentence":"recommend that researchers report training word frequencies; and recommend future work for the community to define and design representational guarantees.","offset":5,"pro":0.8333333333333334,"labels":"CTN"},{"idx":12,"sentence":"It is well documented that NLP models learn social biases, ","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"but little work has been done on how these biases manifest in model outputs for applied tasks like question answering (QA). ","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":12,"sentence":"We introduce the Bias Benchmark for QA (BBQ), a dataset of question-sets constructed by the authors that highlight attested social biases against people belonging to protected classes along nine social dimensions relevant for U.S. English-speaking contexts.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":12,"sentence":"Our task evaluate model responses at two levels: (i) given an under-informative context, we test how strongly responses reflect social biases, and (ii) given an adequately informative context, we test whether the model's biases override a correct answer choice. ","offset":3,"pro":0.5,"labels":"MTD"},{"idx":12,"sentence":"We find that models often rely on stereotypes when the context is under-informative, meaning the model's outputs consistently reproduce harmful biases in this setting.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":12,"sentence":"Though models are more accurate when the context provides an informative answer, they still rely on stereotypes and average up to 3.4 percentage points higher accuracy when the correct answer aligns with a social bias than when it conflicts, with this difference widening to over 5 points on examples targeting gender for most models tested.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":13,"sentence":"Grapheme-to-Phoneme (G2P) has many applications in NLP and speech fields.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"Most existing work focuses heavily on languages with abundant training datasets, which limits the scope of target languages to less than 100 languages. ","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":13,"sentence":"This work attempts to apply zero-shot learning to approximate G2P models for all low-resource and endangered languages in Glottolog (about 8k languages). ","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":13,"sentence":"For any unseen target language, we first build the phylogenetic tree (i.e. language family tree) to identify top-k nearest languages for which we have training sets.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":13,"sentence":"Then we run models of those languages to obtain a hypothesis set, which we combine into a confusion network to propose a most likely hypothesis as an approximation to the target language.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":13,"sentence":"We test our approach on over 600 unseen languages and demonstrate it significantly outperforms baselines.","offset":5,"pro":0.8333333333333334,"labels":"IMP"},{"idx":14,"sentence":"Recent progress in NLP is driven by pretrained models leveraging massive datasets and has predominantly benefited the world's political and economic superpowers. ","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"Technologically underserved languages are left behind because they lack such resources.","offset":1,"pro":0.1,"labels":"GAP"},{"idx":14,"sentence":"Hundreds of underserved languages, nevertheless, have available data sources in the form of interlinear glossed text (IGT) from language documentation efforts.","offset":2,"pro":0.2,"labels":"GAP"},{"idx":14,"sentence":"IGT remains underutilized in NLP work, perhaps because its annotations are only semi-structured and often language-specific.","offset":3,"pro":0.3,"labels":"GAP"},{"idx":14,"sentence":"With this paper, we make the case that IGT data can be leveraged successfully provided that target language expertise is available.","offset":4,"pro":0.4,"labels":"PUR"},{"idx":14,"sentence":"We specifically advocate for collaboration with documentary linguists.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":14,"sentence":"Our paper provides a roadmap for successful projects utilizing IGT data: (1) It is essential to define which NLP tasks can be accomplished with the given IGT data and how these will benefit the speech community.","offset":6,"pro":0.6,"labels":"CTN"},{"idx":14,"sentence":"(2) Great care and target language expertise is required when converting the data into structured formats commonly employed in NLP.","offset":7,"pro":0.7,"labels":"CTN"},{"idx":14,"sentence":"(3) Task-specific and user-specific evaluation can help to ascertain that the tools which are created benefit the target language speech community.","offset":8,"pro":0.8,"labels":"CTN"},{"idx":14,"sentence":"We illustrate each step through a case study on developing a morphological reinflection system for the Tsimchianic language Gitksan.","offset":9,"pro":0.9,"labels":"CTN"},{"idx":15,"sentence":"Reading is integral to everyday life","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"and yet learning to read is a struggle for many young learners.","offset":1,"pro":0.1,"labels":"GAP"},{"idx":15,"sentence":"During lessons, teachers can use comprehension questions to increase engagement, test reading skills, and improve retention.","offset":2,"pro":0.2,"labels":"BAC"},{"idx":15,"sentence":"Historically such questions were written by skilled teachers, but recently language models have been used to generate comprehension questions.","offset":3,"pro":0.3,"labels":"GAP"},{"idx":15,"sentence":"However, many existing Question Generation (QG) systems focus on generating extractive questions from the text, and have no way to control the type of the generated question.","offset":4,"pro":0.4,"labels":"GAP"},{"idx":15,"sentence":"In this paper, we study QG for reading comprehension where inferential questions are critical and extractive techniques cannot be used. ","offset":5,"pro":0.5,"labels":"PUR"},{"idx":15,"sentence":"We propose a two-step model (HTA-WTA) that takes advantage of previous datasets, and can generate questions for a specific targeted comprehension skill.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":15,"sentence":"We propose a new reading comprehension dataset that contains questions annotated with story-based reading comprehension skills (SBRCS), allowing for a more complete reader assessment.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":15,"sentence":" Across several experiments, our results show that HTA-WTA outperforms multiple strong baselines on this new dataset.","offset":8,"pro":0.8,"labels":"RST"},{"idx":15,"sentence":"We show that the HTA-WTA model tests for strong SCRS by asking deep inferential questions.","offset":9,"pro":0.9,"labels":"CLN"},{"idx":16,"sentence":"Entity retrieval-retrieving information about entity mentions in a query-is a key step in open-domain tasks, such as question answering or fact checking. ","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"However, state-of-the-art entity retrievers struggle to retrieve rare entities for ambiguous mentions due to biases towards popular entities.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":16,"sentence":"Incorporating knowledge graph types during training could help overcome popularity biases, but there are several challenges: (1) existing type-based retrieval methods require mention boundaries as input, but open-domain tasks run on unstructured text, (2) type-based methods should not compromise overall performance, and (3) type-based methods should be robust to noisy and missing types. ","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":16,"sentence":"In this work, we introduce TABi, a method to jointly train bi-encoders on knowledge graph types and unstructured text for entity retrieval for open-domain tasks. ","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":16,"sentence":"TABi leverages a type-enforced contrastive loss to encourage entities and queries of similar types to be close in the embedding space. ","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":16,"sentence":"TABi improves retrieval of rare entities on the Ambiguous Entity Retrieval (AmbER) sets","offset":5,"pro":0.5555555555555556,"labels":"RST"},{"idx":16,"sentence":"while maintaining strong overall retrieval performance on open-domain tasks in the KILT benchmark compared to state-of-the-art retrievers. ","offset":6,"pro":0.6666666666666666,"labels":"RST"},{"idx":16,"sentence":"TABi is also robust to incomplete type systems, improving rare entity retrieval over baselines with only 5% type coverage of the training dataset.","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":16,"sentence":"We make our code publicly available.","offset":8,"pro":0.8888888888888888,"labels":"CTN"},{"idx":17,"sentence":"Large pretrained models enable transfer learning to low-resource domains for language generation tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"However, previous end-to-end approaches do not account for the fact that some generation sub-tasks, specifically aggregation and lexicalisation, can benefit from transfer learning in different extents.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":17,"sentence":"To exploit these varying potentials for transfer learning, we propose a new hierarchical approach for few-shot and zero-shot generation. ","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":17,"sentence":"Our approach consists of a three-moduled jointly trained architecture: the first module independently lexicalises the distinct units of information in the input as sentence sub-units (e.g. phrases), the second module recurrently aggregates these sub-units to generate a unified intermediate output, while the third module subsequently post-edits it to generate a coherent and fluent final text. ","offset":3,"pro":0.5,"labels":"MTD"},{"idx":17,"sentence":"We perform extensive empirical analysis and ablation studies on few-shot and zero-shot settings across 4 datasets.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":17,"sentence":"Automatic and human evaluation shows that the proposed hierarchical approach is consistently capable of achieving state-of-the-art results when compared to previous work.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":18,"sentence":"Recent advances in NLP often stem from large transformer-based pre-trained models, which rapidly grow in size and use more and more training data.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"Such models are often released to the public so that end users can fine-tune them on a task dataset.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":18,"sentence":"While it is common to treat pre-training data as public, it may still contain personally identifiable information (PII), such as names, phone numbers, and copyrighted material.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":18,"sentence":"Recent findings show that the capacity of these models allows them to memorize parts of the training data, ","offset":3,"pro":0.375,"labels":"BAC"},{"idx":18,"sentence":"and suggest differentially private (DP) training as a potential mitigation. ","offset":4,"pro":0.5,"labels":"BAC"},{"idx":18,"sentence":"While there is recent work on DP fine-tuning of NLP models, the effects of DP pre-training are less well understood: it is not clear how downstream performance is affected by DP pre-training, and whether DP pre-training mitigates some of the memorization concerns. ","offset":5,"pro":0.625,"labels":"GAP"},{"idx":18,"sentence":"We focus on T5 and show that by using recent advances in JAX and XLA we can train models with DP that do not suffer a large drop in pre-training utility, nor in training speed, and can still be fine-tuned to high accuracies on downstream tasks (e.g. GLUE).","offset":6,"pro":0.75,"labels":"CLN"},{"idx":18,"sentence":"Moreover, we show that T5's span corruption is a good defense against data memorization.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":19,"sentence":"Active learning is the iterative construction of a classification model through targeted labeling, enabling significant labeling cost savings. ","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"As most research on active learning has been carried out before transformer-based language models (transformers) became popular, despite its practical importance, comparably few papers have investigated how transformers can be combined with active learning to date.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":19,"sentence":"This can be attributed to the fact that using state-of-the-art query strategies for transformers induces a prohibitive runtime overhead, which effectively nullifies, or even outweighs the desired cost savings.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":19,"sentence":"For this reason, we revisit uncertainty-based query strategies, which had been largely outperformed before, but are particularly suited in the context of fine-tuning transformers.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":19,"sentence":"In an extensive evaluation, we connect transformers to experiments from previous research, assessing their performance on five widely used text classification benchmarks.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":19,"sentence":"For active learning with transformers, several other uncertainty-based approaches outperform the well-known prediction entropy query strategy,","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":19,"sentence":"thereby challenging its status as most popular uncertainty baseline in active learning for text classification.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":20,"sentence":"Considering the seq2seq architecture of Yin and Neubig (2018) for natural language to code translation, we identify four key components of importance: grammatical constraints, lexical preprocessing, input representations, and copy mechanisms. ","offset":0,"pro":0,"labels":"PUR"},{"idx":20,"sentence":"To study the impact of these components,","offset":1,"pro":0.25,"labels":"PUR"},{"idx":20,"sentence":"we use a state-of-the-art architecture that relies on BERT encoder and a grammar-based decoder for which a formalization is provided.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":20,"sentence":"The paper highlights the importance of the lexical substitution component in the current natural language to code systems.","offset":3,"pro":0.75,"labels":"CTN"},{"idx":21,"sentence":"Aspect-based sentiment analysis (ABSA) tasks aim to extract sentiment tuples from a sentence. ","offset":0,"pro":0,"labels":"BAC"},{"idx":21,"sentence":"Recent generative methods such as Seq2Seq models have achieved good performance by formulating the output as a sequence of sentiment tuples. ","offset":1,"pro":0.09090909090909091,"labels":"BAC"},{"idx":21,"sentence":"However, the orders between the sentiment tuples do not naturally exist and the generation of the current tuple should not condition on the previous ones.","offset":2,"pro":0.18181818181818182,"labels":"GAP"},{"idx":21,"sentence":"In this paper, we propose Seq2Path to generate sentiment tuples as paths of a tree.","offset":3,"pro":0.2727272727272727,"labels":"PUR"},{"idx":21,"sentence":"A tree can represent 1-to-n relations (e.g., an aspect term may correspond to multiple opinion terms) and the paths of a tree are independent and do not have orders. ","offset":4,"pro":0.36363636363636365,"labels":"MTD"},{"idx":21,"sentence":"For training, we treat each path as an independent target, and we calculate the average loss of the ordinary Seq2Seq model over paths.","offset":5,"pro":0.45454545454545453,"labels":"MTD"},{"idx":21,"sentence":"For inference, we apply beam search with constrained decoding.","offset":6,"pro":0.5454545454545454,"labels":"MTD"},{"idx":21,"sentence":"By introducing an additional discriminative token and applying a data augmentation technique, valid paths can be automatically selected.","offset":7,"pro":0.6363636363636364,"labels":"MTD"},{"idx":21,"sentence":"We conduct experiments on five tasks including AOPE, ASTE, TASD, UABSA, ACOS. ","offset":8,"pro":0.7272727272727273,"labels":"MTD"},{"idx":21,"sentence":"We evaluate our method on four common benchmark datasets including Laptop14, Rest14, Rest15, Rest16.","offset":9,"pro":0.8181818181818182,"labels":"MTD"},{"idx":21,"sentence":"Our proposed method achieves state-of-the-art results in almost all cases.","offset":10,"pro":0.9090909090909091,"labels":"RST"},{"idx":22,"sentence":"Neural networks are widely used in various NLP tasks for their remarkable performance. ","offset":0,"pro":0,"labels":"BAC"},{"idx":22,"sentence":"However, the complexity makes them difficult to interpret, i.e., they are not guaranteed right for the right reason.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":22,"sentence":"Besides the complexity, we reveal that the model pathology - the inconsistency between word saliency and model confidence, further hurts the interpretability. ","offset":2,"pro":0.25,"labels":"GAP"},{"idx":22,"sentence":"We show that the pathological inconsistency is caused by the representation collapse issue, which means that the representation of the sentences with tokens in different saliency reduced is somehow collapsed, and thus the important words cannot be distinguished from unimportant words in terms of model confidence changing.","offset":3,"pro":0.375,"labels":"BAC"},{"idx":22,"sentence":"In this paper, to mitigate the pathology and obtain more interpretable models, we propose Pathological Contrastive Training (PCT) framework, which adopts contrastive learning and saliency-based samples augmentation to calibrate the sentences representation. ","offset":4,"pro":0.5,"labels":"PUR"},{"idx":22,"sentence":"Combined with qualitative analysis, we also conduct extensive quantitative experiments and measure the interpretability with eight reasonable metrics. ","offset":5,"pro":0.625,"labels":"MTD"},{"idx":22,"sentence":"Experiments show that our method can mitigate the model pathology and generate more interpretable models while keeping the model performance.","offset":6,"pro":0.75,"labels":"CLN"},{"idx":22,"sentence":"Ablation study also shows the effectiveness.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":23,"sentence":"The popularity of pretrained language models in natural language processing systems calls for a careful evaluation of such models in down-stream tasks, which have a higher potential for societal impact.","offset":0,"pro":0,"labels":"BAC"},{"idx":23,"sentence":"The evaluation of such systems usually focuses on accuracy measures.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":23,"sentence":"Our findings in this paper call for attention to be paid to fairness measures as well.","offset":2,"pro":0.2,"labels":"PUR"},{"idx":23,"sentence":"Through the analysis of more than a dozen pretrained language models of varying sizes on two toxic text classification tasks (English),","offset":3,"pro":0.3,"labels":"MTD"},{"idx":23,"sentence":"we demonstrate that focusing on accuracy measures alone can lead to models with wide variation in fairness characteristics.","offset":4,"pro":0.4,"labels":"CLN"},{"idx":23,"sentence":"Specifically, we observe that fairness can vary even more than accuracy with increasing training data size and different random initializations.","offset":5,"pro":0.5,"labels":"CLN"},{"idx":23,"sentence":"At the same time, we find that little of the fairness variation is explained by model size, despite claims in the literature.","offset":6,"pro":0.6,"labels":"CLN"},{"idx":23,"sentence":"To improve model fairness without retraining,","offset":7,"pro":0.7,"labels":"PUR"},{"idx":23,"sentence":" we show that two post-processing methods developed for structured, tabular data can be successfully applied to a range of pretrained language models. ","offset":8,"pro":0.8,"labels":"CLN"},{"idx":23,"sentence":"Warning: This paper contains samples of offensive text.","offset":9,"pro":0.9,"labels":"MTD"},{"idx":24,"sentence":"Charts are very popular for analyzing data. ","offset":0,"pro":0,"labels":"BAC"},{"idx":24,"sentence":"When exploring charts, people often ask a variety of complex reasoning questions that involve several logical and arithmetic operations.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":24,"sentence":"They also commonly refer to visual features of a chart in their questions. ","offset":2,"pro":0.25,"labels":"BAC"},{"idx":24,"sentence":"However, most existing datasets do not focus on such complex reasoning questions as their questions are template-based and answers come from a fixed-vocabulary.","offset":3,"pro":0.375,"labels":"GAP"},{"idx":24,"sentence":"In this work, we present a large-scale benchmark covering 9.6K human-written questions as well as 23.1K questions generated from human-written chart summaries. ","offset":4,"pro":0.5,"labels":"PUR"},{"idx":24,"sentence":"To address the unique challenges in our benchmark involving visual and logical reasoning over charts,","offset":5,"pro":0.625,"labels":"PUR"},{"idx":24,"sentence":"we present two transformer-based models that combine visual features and the data table of the chart in a unified way to answer questions.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":24,"sentence":"While our models achieve the state-of-the-art results on the previous datasets as well as on our benchmark, the evaluation also reveals several challenges in answering complex reasoning questions.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":25,"sentence":"Many recent deep learning-based solutions have adopted the attention mechanism in various tasks in the field of NLP. ","offset":0,"pro":0,"labels":"BAC"},{"idx":25,"sentence":"However, the inherent characteristics of deep learning models and the flexibility of the attention mechanism increase the models' complexity, thus leading to challenges in model explainability. ","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":25,"sentence":"To address this challenge, we propose a novel practical framework by utilizing a two-tier attention architecture to decouple the complexity of explanation and the decision-making process.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":25,"sentence":"We apply it in the context of a news article classification task. ","offset":3,"pro":0.5,"labels":"MTD"},{"idx":25,"sentence":"The experiments on two large-scaled news corpora demonstrate that the proposed model can achieve competitive performance with many state-of-the-art alternatives and illustrate its appropriateness from an explainability perspective. ","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":25,"sentence":"We release the source code here.","offset":5,"pro":0.8333333333333334,"labels":"CTN"},{"idx":26,"sentence":"Traditional methods for named entity recognition (NER) classify mentions into a fixed set of pre-defined entity types. ","offset":0,"pro":0,"labels":"BAC"},{"idx":26,"sentence":"However, in many real-world scenarios, new entity types are incrementally involved. ","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":26,"sentence":"To investigate this problem, continual learning is introduced for NER. ","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":26,"sentence":"However, the existing method depends on the relevance between tasks and is prone to inter-type confusion.","offset":3,"pro":0.3333333333333333,"labels":"GAP"},{"idx":26,"sentence":"In this paper, we propose a novel two-stage framework Learn-and-Review (L{&R) for continual NER under the type-incremental setting to alleviate the above issues.","offset":4,"pro":0.4444444444444444,"labels":"PUR"},{"idx":26,"sentence":"Specifically, for the learning stage, we distill the old knowledge from teacher to a student on the current dataset. ","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":26,"sentence":"For the reviewing stage, we first generate synthetic samples of old types to augment the dataset. ","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":26,"sentence":"Then, we further distill new knowledge from the above student and old knowledge from the teacher to get an enhanced student on the augmented dataset. ","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":26,"sentence":"Experimental results show that L{&R outperforms the state-of-the-art method on CoNLL-03 and OntoNotes-5.0.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":27,"sentence":"Transcription is often reported as the bottleneck in endangered language documentation, requiring large efforts from scarce speakers and transcribers.","offset":0,"pro":0,"labels":"BAC"},{"idx":27,"sentence":"In general, automatic speech recognition (ASR) can be accurate enough to accelerate transcription only if trained on large amounts of transcribed data. ","offset":1,"pro":0.2,"labels":"BAC"},{"idx":27,"sentence":"Here we expand this body of work on speaker-dependent transcription by comparing four ASR approaches, notably recent transformer and pretrained multilingual models, on a common dataset of 11 languages. ","offset":2,"pro":0.4,"labels":"PUR"},{"idx":27,"sentence":"we also developed a phoneme recognition setup which handles morphologically complex languages and writing systems for which no pronunciation dictionary exists.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":27,"sentence":"For the 5 languages with between 100 and 192 minutes of training, we achieved a PER of 8.4% or less. ","offset":4,"pro":0.8,"labels":"RST"},{"idx":28,"sentence":"Although transformer-based Neural Language Models demonstrate impressive performance on a variety of tasks, their generalization abilities are not well understood.","offset":0,"pro":0,"labels":"GAP"},{"idx":28,"sentence":"They have been shown to perform strongly on subject-verb number agreement in a wide array of settings, suggesting that they learned to track syntactic dependencies during their training even without explicit supervision.","offset":1,"pro":0.25,"labels":"BAC"},{"idx":28,"sentence":"In this paper, we examine the extent to which BERT is able to perform lexically-independent subject-verb number agreement (NA) on targeted syntactic templates. ","offset":2,"pro":0.5,"labels":"PUR"},{"idx":28,"sentence":"Our results on nonce sentences suggest that the model generalizes well for simple templates, but fails to perform lexically-independent syntactic generalization when as little as one attractor is present.","offset":3,"pro":0.75,"labels":"CLN"},{"idx":29,"sentence":"Static and contextual multilingual embeddings have complementary strengths. ","offset":0,"pro":0,"labels":"BAC"},{"idx":29,"sentence":"We combine the strengths of static and contextual models to improve multilingual representations.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":29,"sentence":"We extract static embeddings for 40 languages from XLM-R, ","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":29,"sentence":"and then align them using VecMap. ","offset":3,"pro":0.5,"labels":"MTD"},{"idx":29,"sentence":"Then we apply a novel continued pre-training approach to XLM-R, leveraging the high quality alignment of our static embeddings to better align the representation space of XLM-R. ","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":29,"sentence":"We release the static embeddings and the continued pre-training code. ","offset":5,"pro":0.8333333333333334,"labels":"CTN"},{"idx":0,"sentence":"Data augmentation is a widely employed technique to alleviate the problem of data scarcity. ","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"In this work, we propose a prompting-based approach to generate labelled training data for intent classification with off-the-shelf language models (LMs) such as GPT-3.","offset":1,"pro":0.1111111111111111,"labels":"PUR"},{"idx":0,"sentence":"An advantage of this method is that no task-specific LM-fine-tuning for data generation is required;","offset":2,"pro":0.2222222222222222,"labels":"CTN"},{"idx":0,"sentence":"hence the method requires no hyper parameter tuning and is applicable even when the available training data is very scarce. ","offset":3,"pro":0.3333333333333333,"labels":"CTN"},{"idx":0,"sentence":"We evaluate the proposed method in a few-shot setting on four diverse intent classification tasks. ","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":0,"sentence":"We find that GPT-generated data significantly boosts the performance of intent classifiers when intents in consideration are sufficiently distinct from each other.","offset":5,"pro":0.5555555555555556,"labels":"CLN"},{"idx":0,"sentence":"In tasks with semantically close intents, we observe that the generated data is less helpful. ","offset":6,"pro":0.6666666666666666,"labels":"CLN"},{"idx":0,"sentence":"Our analysis shows that this is because GPT often generates utterances that belong to a closely-related intent instead of the desired one.","offset":7,"pro":0.7777777777777778,"labels":"CLN"},{"idx":0,"sentence":"We present preliminary evidence that a prompting-based GPT classifier could be helpful in filtering the generated data to enhance its quality.","offset":8,"pro":0.8888888888888888,"labels":"IMP"},{"idx":1,"sentence":"Personal attributes represent structured information about a person, such as their hobbies, pets, family, likes and dislikes. ","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"We introduce the tasks of extracting and inferring personal attributes from human-human dialogue, and analyze the linguistic demands of these tasks. ","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":1,"sentence":"To meet these challenges, ","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":1,"sentence":"we introduce a simple and extensible model that combines an autoregressive language model utilizing constrained attribute generation with a discriminative reranker. ","offset":3,"pro":0.5,"labels":"MTD"},{"idx":1,"sentence":"Our model outperforms strong baselines on extracting personal attributes as well as inferring personal attributes that are not contained verbatim in utterances and instead requires commonsense reasoning and lexical inferences, which occur frequently in everyday conversation. ","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":1,"sentence":"Finally, we demonstrate the benefit of incorporating personal attributes in social chit-chat and task-oriented dialogue settings.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":2,"sentence":"In conversational QA, models have to leverage information in previous turns to answer upcoming questions.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"Current approaches, such as Question Rewriting, struggle to extract relevant information as the conversation unwinds. ","offset":1,"pro":0.3333333333333333,"labels":"BAC"},{"idx":2,"sentence":"We show that CG offers a more efficient and human-like way to exploit conversational information compared to existing approaches, leading to improvements on Open Domain Conversational QA.","offset":2,"pro":0.6666666666666666,"labels":"CLN"},{"idx":3,"sentence":"At the heart of improving conversational AI is the open problem of how to evaluate conversations.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"Issues with automatic metrics are well known (Liu et al., 2016), with human evaluations still considered the gold standard. ","offset":1,"pro":0.25,"labels":"BAC"},{"idx":3,"sentence":"In this work we compare five different crowdworker-based human evaluation methods and find that different methods are best depending on the types of models compared, with no clear winner across the board.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":3,"sentence":"While this highlights the open problems in the area, our analysis leads to advice of when to use which one, and possible future directions.","offset":3,"pro":0.75,"labels":"IMP"},{"idx":4,"sentence":"These walks, in turn, provide an explanation of the flow of the conversation. ","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"Knowledge-grounded dialogue systems utilise external knowledge such as knowledge graphs to generate informative and appropriate responses. ","offset":1,"pro":0.25,"labels":"BAC"},{"idx":4,"sentence":"This fact selection can be formulated as path traversal over a knowledge graph conditioned on the dialogue context. ","offset":2,"pro":0.5,"labels":"BAC"},{"idx":4,"sentence":"Extensive evaluations showed that our model outperforms the state-of-the-art models on the OpenDialKG dataset on multiple metrics.","offset":3,"pro":0.75,"labels":"CLN"},{"idx":5,"sentence":"Large Transformer-based natural language understanding models have achieved state-of-the-art performance in dialogue systems. ","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"Few-shot learning and knowledge distillation techniques have been introduced to reduce the need for labeled data and computational resources, respectively.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":5,"sentence":"However, these techniques are incompatible because few-shot learning trains models using few data, whereas, knowledge distillation requires sufficient data to train smaller, yet competitive models that run on limited computational resources. ","offset":2,"pro":0.4,"labels":"GAP"},{"idx":5,"sentence":"Considering in-domain and cross-domain few-shot learning scenarios, we introduce an approach for distilling small models that generalize to new intent classes and domains using only a handful of labeled examples. ","offset":3,"pro":0.6,"labels":"MTD"},{"idx":5,"sentence":"Overall, our results in both few-shot scenarios confirm the generalization ability of the small distilled models while having lower computational costs.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":6,"sentence":"Language understanding in speech-based systems has attracted extensive interest from both academic and industrial communities in recent years with the growing demand for voice-based applications.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"Prior works focus on independent research by the automatic speech recognition (ASR) and natural language processing (NLP) communities, or on jointly modeling the speech and NLP problems focusing on a single dataset or single NLP task. ","offset":1,"pro":0.2,"labels":"BAC"},{"idx":6,"sentence":"MTL-SLT takes speech as input, and outputs transcription, intent, named entities, summaries, and answers to text queries, supporting the tasks of spoken language understanding, spoken summarization and spoken question answering respectively. ","offset":2,"pro":0.4,"labels":"MTD"},{"idx":6,"sentence":"We obtain state-of-the-art results on spoken language understanding tasks such as SLURP and ATIS.","offset":3,"pro":0.6,"labels":"RST"},{"idx":6,"sentence":"Spoken summarization results are reported on a new dataset: Spoken-Gigaword.","offset":4,"pro":0.8,"labels":"CTN"},{"idx":7,"sentence":"As humans, we experience the world with all our senses or modalities (sound, sight, touch, smell, and taste). ","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"Multimodal expressions are central to conversations; a rich set of modalities amplify and often compensate for each other. ","offset":1,"pro":0.25,"labels":"BAC"},{"idx":7,"sentence":"This paper motivates, defines, and mathematically formulates the multimodal conversational research objective. ","offset":2,"pro":0.5,"labels":"PUR"},{"idx":7,"sentence":"We survey state-of-the-art datasets and approaches for each research area and highlight their limiting assumptions. ","offset":3,"pro":0.75,"labels":"MTD"},{"idx":8,"sentence":"Human{--}computer conversation has long been an interest of artificial intelligence and natural language processing research. ","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"The goal of this work is threefold: (1) to provide an overview of recent advances in the field of open-domain dialogue, (2) to summarize issues related to ethics, bias, and fairness that the field has identified as well as typical errors of dialogue systems, and (3) to outline important future challenges.","offset":1,"pro":0.3333333333333333,"labels":"PUR"},{"idx":8,"sentence":"We hope that this work will be of interest to both new and experienced researchers in the area.","offset":2,"pro":0.6666666666666666,"labels":"IMP"},{"idx":9,"sentence":"In this work, we evaluate various existing dialogue relevance metrics, find strong dependency on the dataset, often with poor correlation with human scores of relevance, and propose modifications to reduce data requirements and domain sensitivity while improving correlation.","offset":0,"pro":0,"labels":"PUR"},{"idx":9,"sentence":"Our proposed metric achieves state-of-the-art performance on the HUMOD dataset while reducing measured sensitivity to dataset by 37{\\%}-66{\\%}. ","offset":1,"pro":0.25,"labels":"RST"},{"idx":9,"sentence":"Despite these limitations, we demonstrate competitive performance on four datasets from different domains.","offset":2,"pro":0.5,"labels":"CLN"},{"idx":9,"sentence":"Our code, including our metric and experiments, is open sourced.","offset":3,"pro":0.75,"labels":"CTN"},{"idx":10,"sentence":"While large pre-trained language models accumulate a lot of knowledge in their parameters, it has been demonstrated that augmenting it with non-parametric retrieval-based memory has a number of benefits ranging from improved accuracy to data efficiency for knowledge-focused tasks such as question answering. ","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"Our technique, RetroNLU, extends a sequence-to-sequence model architecture with a retrieval component, which is used to retrieve existing similar samples and present them as an additional context to the model.","offset":1,"pro":0.25,"labels":"MTD"},{"idx":10,"sentence":"In particular, we analyze two settings, where we augment an input with (a) retrieved nearest neighbor utterances (utterance-nn), and (b) ground-truth semantic parses of nearest neighbor utterances (semparse-nn).","offset":2,"pro":0.5,"labels":"MTD"},{"idx":10,"sentence":"Our technique outperforms the baseline method by 1.5{\\%} absolute macro-F1, especially at the low resource setting, matching the baseline model accuracy with only 40{\\%} of the complete data.","offset":3,"pro":0.75,"labels":"RST"},{"idx":11,"sentence":"Personality traits influence human actions and thoughts, which is manifested in day to day conversations. ","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"With the motivation of enabling stylistically configurable response generators, in this paper we experiment with end-to-end mechanisms to ground neural response generators based on both (i) interlocutor Big-5 personality traits, and (ii) discourse intent as stylistic control codes. ","offset":1,"pro":0.25,"labels":"PUR"},{"idx":11,"sentence":"and further assess the impact of using such features as control codes for response generation using automatic evaluation metrics, ablation studies and human judgement.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":11,"sentence":"Our experiments illustrate the effectiveness of this strategy resulting in improvements to existing benchmarks. ","offset":3,"pro":0.75,"labels":"CLN"},{"idx":12,"sentence":"Conversational Recommendation Systems recommend items through language based interactions with users.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"We also propose an alignment loss to further integrate KG entities into the response generation network.","offset":1,"pro":0.3333333333333333,"labels":"MTD"},{"idx":12,"sentence":"Experiments on the large-scale REDIAL dataset demonstrate that the proposed system consistently outperforms state-of-the-art baselines.","offset":2,"pro":0.6666666666666666,"labels":"CLN"},{"idx":13,"sentence":"Exemplar-based generative models for open-domain conversation produce responses based on the exemplars provided by the retriever, taking advantage of generative models and retrieval models.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"However, due to the one-to-many problem of the open-domain conversation, they often ignore the retrieved exemplars while generating responses or produce responses over-fitted to the retrieved exemplars. ","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":13,"sentence":"In the training phase, our training method first uses the gold response instead of dialogue context as a query to select exemplars that are semantically relevant to the gold response.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":13,"sentence":"And then, it eliminates the exemplars that lexically resemble the gold responses to alleviate the dependency of the generative models on that exemplars. ","offset":3,"pro":0.5,"labels":"MTD"},{"idx":13,"sentence":"Thus, our training method further utilizes the relevance scores between the given context and the exemplars to penalize the irrelevant exemplars.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":13,"sentence":"Extensive experiments demonstrate that our proposed training method alleviates the drawbacks of the existing exemplar-based generative models and significantly improves the performance in terms of appropriateness and informativeness.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":14,"sentence":"Recent GAN-based text-to-image generation models have advanced that they can generate photo-realistic images matching semantically with descriptions. ","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"There are two problems when constructing a multilingual text-to-image generation model: 1) language imbalance issue in text-to-image paired datasets and 2) generating images that have the same meaning but are semantically inconsistent with each other in texts expressed in different languages. ","offset":1,"pro":0.3333333333333333,"labels":"GAP"},{"idx":14,"sentence":"Experiments on relatively low-resource language text-image datasets show that the model has comparable generation quality as images generated by high-resource language text, ","offset":2,"pro":0.6666666666666666,"labels":"CLN"},{"idx":15,"sentence":"Algorithmic oppression is an urgent and persistent problem in speech and language technologies.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"Considering power relations embedded in datasets before compiling or using them to train or test speech and language technologies is essential to designing less harmful, more just technologies.","offset":1,"pro":0.3333333333333333,"labels":"BAC"},{"idx":15,"sentence":"This paper presents a reflective exercise to recognise and challenge gaps and the power relations they reveal in speech and language datasets by applying principles of Data Feminism and Design Justice, and building on work on dataset documentation and sociolinguistics.","offset":2,"pro":0.6666666666666666,"labels":"PUR"},{"idx":16,"sentence":"Deep learning holds great promise for detecting discriminatory language in the public sphere. ","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"In this paper, we investigate job advertisements in the Netherlands.","offset":1,"pro":0.3333333333333333,"labels":"PUR"},{"idx":16,"sentence":"We present a qualitative analysis of the benefits of the {`}old{'} approach based on regexes and investigate how neural embeddings could address its limitations.","offset":2,"pro":0.6666666666666666,"labels":"MTD"},{"idx":17,"sentence":"This paper makes the case for studying concreteness in language as a bridge that will allow language technology to support the understanding and improvement of ethnic inclusivity in job advertisements. ","offset":0,"pro":0,"labels":"PUR"},{"idx":17,"sentence":"Using an annotated dataset of Dutch-language job ads, ","offset":1,"pro":0.5,"labels":"MTD"},{"idx":18,"sentence":"Current language technology is ubiquitous and directly influences individuals{'} lives worldwide.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"Given the recent trend in AI on training and constantly releasing new and powerful large language models (LLMs), there is a need to assess their biases and potential concrete consequences. ","offset":1,"pro":0.25,"labels":"BAC"},{"idx":18,"sentence":"In this paper, we investigated a state-of-the-art template-based approach for measuring the harmfulness of English LLMs sentence completion when the subjects belong to the LGBTQIA+ community. ","offset":2,"pro":0.5,"labels":"PUR"},{"idx":18,"sentence":"Our results raise serious concerns about the applicability of these models in production environments.","offset":3,"pro":0.75,"labels":"IMP"},{"idx":19,"sentence":"Deaf and hard of hearing individuals regularly rely on captioning while watching live TV.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"Live TV captioning is evaluated by regulatory agencies using various caption evaluation metrics. ","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":19,"sentence":"There is a need to construct caption evaluation metrics that take the relative importance of words in transcript into account. ","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":19,"sentence":"We found that normalized contextualized word embeddings generated using BERT correlated better with manually annotated importance scores than word2vec-based word embeddings.","offset":3,"pro":0.42857142857142855,"labels":"CLN"},{"idx":19,"sentence":"We make available a pairing of word embeddings and their human-annotated importance scores.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":19,"sentence":"We also provide proof-of-concept utility by training word importance models,","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":19,"sentence":"achieving an F1-score of 0.57 in the 6-class word importance classification task.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":19,"sentence":"Hope Speech are positive terms that help to promote or criticise a point of view without hurting the user{'}s or community{'}s feelings.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"Non-Hope Speech, on the other side, includes expressions that are harsh, ridiculing, or demotivating. ","offset":1,"pro":0.1,"labels":"BAC"},{"idx":19,"sentence":"The goal of this article is to find the hope speech comments in a YouTube dataset. ","offset":2,"pro":0.2,"labels":"PUR"},{"idx":19,"sentence":"The datasets were created as part of the {``}LT-EDI-ACL 2022: Hope Speech Detection for Equality, Diversity, and Inclusion{''} shared task. ","offset":3,"pro":0.3,"labels":"MTD"},{"idx":19,"sentence":"The shared task dataset was proposed in Malayalam, Tamil, English, Spanish, and Kannada languages.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":19,"sentence":"In this paper, we worked at English-language YouTube comments. ","offset":5,"pro":0.5,"labels":"MTD"},{"idx":19,"sentence":"We employed several deep learning based models such as DNN (dense or fully connected neural network), CNN (Convolutional Neural Network), Bi-LSTM (Bidirectional Long Short Term Memory Network), and GRU(Gated Recurrent Unit) to identify the hopeful comments.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":19,"sentence":"We also used Stacked LSTM-CNN and Stacked LSTM-LSTM network to train the model. ","offset":7,"pro":0.7,"labels":"MTD"},{"idx":19,"sentence":"The best macro average F1-score 0.67 for development dataset was obtained using the DNN model. ","offset":8,"pro":0.8,"labels":"RST"},{"idx":19,"sentence":"The macro average F1-score of 0.67 was achieved for the classification done on the test data as well.","offset":9,"pro":0.9,"labels":"RST"},{"idx":0,"sentence":"Task Hope Speech Detection required models for the automatic identification of hopeful comments for equality, diversity, and inclusion. ","offset":0,"pro":0,"labels":"PUR"},{"idx":0,"sentence":"The 2022 edition of LT-EDI proposed two tasks in various languages.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":0,"sentence":"We targeted both tasks in English by using reinforced BERT-based approaches.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":0,"sentence":" On the basis of an active learning process, we trained a model on the dataset for Task $i$ and applied it to the dataset for Task $j$ to iteratively integrate new silver data for Task $i$. ","offset":3,"pro":0.6,"labels":"MTD"},{"idx":0,"sentence":" placing our team in the third and fourth positions out of 11 and 12 participating teams respectively.","offset":4,"pro":0.8,"labels":"RST"},{"idx":1,"sentence":"Social media has seen enormous growth in its users recently and knowingly or unknowingly the behavior of a person will be reflected in the comments she/he posts on social media.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"Users having the sign of depression may post negative or disturbing content seeking the attention of other users.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":1,"sentence":"Hence, social media data can be analysed to check whether the users{'} have the sign of depression and help them to get through the situation if required.","offset":2,"pro":0.3333333333333333,"labels":"BAC"},{"idx":1,"sentence":"However, as analyzing the increasing amount of social media data manually in laborious and error-prone, automated tools have to be developed for the same.","offset":3,"pro":0.5,"labels":"GAP"},{"idx":1,"sentence":"To address the issue of detecting the sign of depression content on social media, in this paper, we - team MUCS, describe an Ensemble of Machine Learning (ML) models and a Transfer Learning (TL) model submitted to {``}Detecting Signs of Depression from Social Media Text-LT-EDI@ACL 2022{''} (DepSign-LT-EDI@ACL-2022) shared task at Association for Computational Linguistics (ACL) 2022.","offset":4,"pro":0.6666666666666666,"labels":"PUR"},{"idx":1,"sentence":"Both frequency and text based features are used to train an Ensemble model and Bidirectional Encoder Representations from Transformers (BERT) fine-tuned with raw text is used to train the TL model.","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":2,"sentence":"Automatic speech recognition is a tool used to transform human speech into a written form.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"It is used in a variety of avenues, such as in voice commands, customer, service and more.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":2,"sentence":" It has been known to be of vital importance in making the lives of elderly and disabled people much easier.","offset":2,"pro":0.3333333333333333,"labels":"BAC"},{"idx":2,"sentence":"In this paper we describe an automatic speech recognition model, ","offset":3,"pro":0.5,"labels":"PUR"},{"idx":2,"sentence":"The best model for speech recognition in Tamil is determined by finding the word error rate of the data.","offset":4,"pro":0.6666666666666666,"labels":"BAC"},{"idx":2,"sentence":"This work explains the submission made by SSNCSE{\\_}NLP in the shared task organized by LT-EDI at ACL 2022. A word error rate of 39.4512 is achieved.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":3,"sentence":"With the increase of users on social media platforms, manipulating or provoking masses of people has become a piece of cake. ","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":" Hence, it is essential to have a system that automatically classifies the hatred content, especially on social media, to take it down.","offset":1,"pro":0.25,"labels":"GAP"},{"idx":3,"sentence":"This paper presents a simple modular pipeline classifier with BERT embeddings and attention mechanism to classify hope speech content in the Hope Speech Detection shared task for Equality, Diversity, and Inclusion-ACL 2022.","offset":2,"pro":0.5,"labels":"PUR"},{"idx":3,"sentence":"Our system submission ranks fourth with an F1-score of 0.84.","offset":3,"pro":0.75,"labels":"RST"},{"idx":4,"sentence":"Detecting signs ofdepression early on can help in the treatmentand prevention of extreme outcomes like suicide.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"This shared task has used datascraped from various social media sites andaims to develop models that detect signs andthe severity of depression effectively.","offset":1,"pro":0.3333333333333333,"labels":"GAP"},{"idx":4,"sentence":" by applyingenhanced BERT model trained for Wikipediadataset to the social media text and performtext classification.","offset":2,"pro":0.6666666666666666,"labels":"MTD"},{"idx":5,"sentence":"Social media is considered as a platform whereusers express themselves.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"The rise of social me-dia as one of humanity{'}s most important publiccommunication platforms presents a potentialprospect for early identification and manage-ment of mental illness.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":5,"sentence":"Depression is one suchillness that can lead to a variety of emotionaland physical problems.","offset":2,"pro":0.2222222222222222,"labels":"BAC"},{"idx":5,"sentence":"It is necessary to mea-sure the level of depression from the socialmedia text to treat them and to avoid the nega-tive consequences.","offset":3,"pro":0.3333333333333333,"labels":"BAC"},{"idx":5,"sentence":"Detecting levels of depres-sion is a challenging task since it involves themindset of the people which can change period-ically.","offset":4,"pro":0.4444444444444444,"labels":"BAC"},{"idx":5,"sentence":"The aim of the DepSign-LT-EDI@ACL-2022 shared task is to classify the social me-dia text into three levels of depression namely{``}Not Depressed{''}, {``}Moderately Depressed{''}, and{``}Severely Depressed{''}.","offset":5,"pro":0.5555555555555556,"labels":"PUR"},{"idx":5,"sentence":"This overview presentsa description on the task, the data set, method-ologies used and an analysis on the results ofthe submissions.","offset":6,"pro":0.6666666666666666,"labels":"PUR"},{"idx":5,"sentence":"The models that were submit-ted as a part of the shared task had used a va-riety of technologies from traditional machinelearning algorithms to deep learning models.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":5,"sentence":"Among the 31 teams whohad submitted their results for the shared task,the best macro F1-score of 0.583 was obtainedusing transformer based model.","offset":8,"pro":0.8888888888888888,"labels":"RST"},{"idx":6,"sentence":"This paper illustrates the overview of the sharedtask on automatic speech recognition in the Tamillanguage.","offset":0,"pro":0,"labels":"PUR"},{"idx":6,"sentence":"These utterances were collected from people when they communicatedin the public locations such as hospitals, markets, vegetable shop, etc.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":6,"sentence":"The speech corpusincludes utterances of male, female, and transgender and was split into training and testingdata.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":6,"sentence":"The given task was evaluated using WER(Word Error Rate).","offset":3,"pro":0.5,"labels":"MTD"},{"idx":6,"sentence":"The participants used thetransformer-based model for automatic speechrecognition.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":6,"sentence":"Different results using differentpre-trained transformer models are discussedin this overview paper.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":7,"sentence":"Depression is linked to the development of dementia.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"Social media usage has beenincreased among the people in recent days.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":7,"sentence":"Thetechnology advancements help the communityto express their views publicly. ","offset":2,"pro":0.3333333333333333,"labels":"BAC"},{"idx":7,"sentence":"As part ofthe shared task on detecting signs of depressionfrom social media text, a dataset has been providedby the organizers (Sampath et al.).","offset":3,"pro":0.5,"labels":"PUR"},{"idx":7,"sentence":"Weapplied different machine learning techniquessuch as Support Vector Machine, Random Forestand XGBoost classifier to classify the signsof depression.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":7,"sentence":"Experimental results revealedthat, the XGBoost model outperformed othermodels with the highest classification accuracyof 0.61{\\%} and an Macro F1 score of 0.54.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":8,"sentence":"Social media platforms have been provoking masses of people.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"The individual comments affect a prevalent way of thinking by moving away from preoccupation with discrimination, loneliness, or influence in building confidence, support, and good qualities.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":8,"sentence":"This paper aims to identify hope in these social media posts.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":8,"sentence":"Hope significantly impacts the well-being of people, as suggested by health professionals.","offset":3,"pro":0.42857142857142855,"labels":"BAC"},{"idx":8,"sentence":"It reflects the belief to achieve an objective, discovers a new path, or become motivated to formulate pathways.","offset":4,"pro":0.5714285714285714,"labels":"BAC"},{"idx":8,"sentence":"For Kannada, we use Multilingual BERT with 0.32 F1 score($5^{th}$)position.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":8,"sentence":"For non-English languages Malayalam, Spanish and Tamil we utilized XLM RoBERTA with 0.50, 0.81, 0.3 macro F1 score ($1^{st}$, $1^{st}$,$3^{rd}$ rank) respectively.","offset":6,"pro":0.8571428571428571,"labels":"MTD"},{"idx":9,"sentence":"It leads to offensive speech and causes severe social problems that can make online platforms toxic and unpleasant to LGBT+people, endeavoring to eliminate equality, diversity, and inclusion.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"In this paper, we present our classification system;","offset":1,"pro":0.25,"labels":"PUR"},{"idx":9,"sentence":"given comments, it predicts whether or not it contains any form of homophobia/transphobia with a Zero-Shot learning framework","offset":2,"pro":0.5,"labels":"MTD"},{"idx":9,"sentence":" Our system submission achieved 0.40, 0.85, 0.89 F1-score for Tamil and Tamil-English, English with ($1^{st}$, $1^{st}$,$8^{th}$) ranks respectively.","offset":3,"pro":0.75,"labels":"RST"},{"idx":10,"sentence":"Depression is a common illness involving sadness and lack of interest in all day-to-day activities.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"It is important to detect depression at an early stage as it is treated at an early stage to avoid consequences.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":10,"sentence":"In this paper, we present our system submission of ARGUABLY for DepSign-LT-EDI@ACL-2022.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":10,"sentence":"We aim to detect the signs of depression of a person from their social media postings wherein people share their feelings and emotions.","offset":3,"pro":0.6,"labels":"PUR"},{"idx":10,"sentence":"The proposed system is an ensembled voting model with fine-tuned BERT, RoBERTa, and XLNet. Given social media postings in English, the submitted system classify the signs of depression into three labels, namely {``}not depressed,{''} {``}moderately depressed,{''} and {``}severely depressed.{''}","offset":4,"pro":0.8,"labels":"MTD"},{"idx":11,"sentence":"The best systems for Tamil, English, and Tamil-English scored 0.570, 0.870, and 0.610, respectively, on average macro F1-score.","offset":0,"pro":0,"labels":"RST"},{"idx":11,"sentence":"Homophobia and transphobia are both toxic languages directed at LGBTQ+ individuals that are described as hate speech.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":11,"sentence":"This paper summarizes our findings on the {``}Homophobia and Transphobia Detection in social media comments{''} shared task held at LT-EDI 2022 - ACL 2022 1.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":11,"sentence":"This shared taskfocused on three sub-tasks for Tamil, English, and Tamil-English (code-mixed) languages.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":11,"sentence":"It received 10 systems for Tamil, 13 systems for English, and 11 systems for Tamil-English.","offset":4,"pro":0.8,"labels":"MTD"},{"idx":12,"sentence":"Hope Speech detection is the task of classifying a sentence as hope speech or non-hope speech given a corpus of sentences.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"Hope speech is any message or content that is positive, encouraging, reassuring, inclusive and supportive that inspires and engenders optimism in the minds of people.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":12,"sentence":"In contrast to identifying and censoring negative speech patterns, hope speech detection is focussed on recognising and promoting positive speech patterns online.","offset":2,"pro":0.25,"labels":"BAC"},{"idx":12,"sentence":"In this paper, we report an overview of the findings and results from the shared task on hope speech detection for Tamil, Malayalam, Kannada, English and Spanish languages conducted in the second workshop on Language Technology for Equality, Diversity and Inclusion (LT-EDI-2022) organised as a part of ACL 2022.","offset":3,"pro":0.375,"labels":"RST"},{"idx":12,"sentence":"The participants were provided with annotated training {\\&} development datasets and unlabelled test datasets in all the five languages.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":12,"sentence":"The goal of the shared task is to classify the given sentences into one of the two hope speech classes.","offset":5,"pro":0.625,"labels":"PUR"},{"idx":12,"sentence":"The performances of the systems submitted by the participants were evaluated in terms of micro-F1 score and weighted-F1 score.","offset":6,"pro":0.75,"labels":"RST"},{"idx":12,"sentence":"The datasets for this challenge are openly available","offset":7,"pro":0.875,"labels":"RST"},{"idx":13,"sentence":"When mapping a natural language instruction to a sequence of actions, it is often useful toidentify sub-tasks in the instruction.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"We present the A2LCTC (Action-to-Language Connectionist Temporal Classification) algorithm to automatically discover a sub-task segmentation of an action sequence.","offset":1,"pro":0.3333333333333333,"labels":"MTD"},{"idx":13,"sentence":"We experiment with the ALFRED dataset and show that A2LCTC accurately finds the sub-task structures.","offset":2,"pro":0.6666666666666666,"labels":"RST"},{"idx":14,"sentence":"This work addresses the lack of feature attribution approaches that also take into account the sentence structure.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"by proposing GrammarSHAP{---}a model-agnostic explainer leveraging the sentence{'}s constituency parsing to generate hierarchical importance scores.","offset":1,"pro":0.3333333333333333,"labels":"MTD"},{"idx":14,"sentence":"We extend the SHAP framework","offset":2,"pro":0.6666666666666666,"labels":"RST"},{"idx":15,"sentence":"This presents a challenge for building trust in machine learning systems.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"We take inspiration from real-world situations where difficult questions are answered by considering opposing sides (see Irving et al., 2018).","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":15,"sentence":"For multiple-choice QA examples, we build a dataset of single arguments for both a correct and incorrect answer option in a debate-style set-up as an initial step in training models to produce explanations for two candidate answers.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":15,"sentence":"We use long contexts{---}humans familiar with the context write convincing explanations for pre-selected correct and incorrect answers, and we test if those explanations allow humans who have not read the full context to more accurately determine the correct answer.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":15,"sentence":"We do not find that explanations in our set-up improve human accuracy, but a baseline condition shows that providing human-selected text snippets does improve accuracy.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":15,"sentence":"We use these findings to suggest ways of improving the debate set up for future data collection efforts.","offset":5,"pro":0.8333333333333334,"labels":"IMP"},{"idx":16,"sentence":"Many methods now exist for conditioning models on task instructions and user-provided explanations for individual data points","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":" These methods show great promise for improving task performance of language models beyond what can be achieved by learning from individual (x,y) pairs.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":16,"sentence":"In this paper, we (1) provide a formal framework for characterizing approaches to learning from explanation data, and (2) we propose a synthetic task for studying how models learn from explanation data.","offset":2,"pro":0.3333333333333333,"labels":"RST"},{"idx":16,"sentence":"In the first direction, we give graphical models for the available modeling approaches, in which explanation data can be used as model inputs, as targets, or as a prior. In the second direction, we introduce a carefully designed synthetic task with several properties making it useful for studying a model{'}s ability to learn from explanation data.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":16,"sentence":"Each data point in this binary classification task is accompanied by a string that is essentially an answer to the \\textit{why} question: {``}why does data point x have label y?{''}","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":16,"sentence":"We aim to encourage research into this area by identifying key considerations for the modeling problem and providing an empirical testbed for theories of how models can best learn from explanation data.","offset":5,"pro":0.8333333333333334,"labels":"PUR"},{"idx":17,"sentence":"Training a model with access to human explanations can improve data efficiency and model performance on in- and out-of-domain data.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"Adding to these empirical findings, similarity with the process of human learning makes learning from explanations a promising way to establish a fruitful human-machine interaction.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":17,"sentence":"Several methods have been proposed for improving natural language processing (NLP) models with human explanations, that rely on different explanation types and mechanism for integrating these explanations into the learning process.","offset":2,"pro":0.3333333333333333,"labels":"BAC"},{"idx":17,"sentence":"These methods are rarely compared with each other, making it hard for practitioners to choose the best combination of explanation type and integration mechanism for a specific use-case.","offset":3,"pro":0.5,"labels":"BAC"},{"idx":17,"sentence":"and discuss different factors that can inform the decision of which method to choose for a specific use-case.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":17,"sentence":"In this paper, we give an overview of different methods for learning from human explanations,","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":18,"sentence":"The documents selection covers ten domains of interest to cultural historians in the olfactory domain and includes texts published between 1620 to 1920, allowing a diachronic analysis of smell descriptions.","offset":0,"pro":0,"labels":"MTD"},{"idx":18,"sentence":"With this work, we aim to foster the development of olfactory information extraction approaches as well as the analysis of changes in smell descriptions over time.","offset":1,"pro":0.5,"labels":"PUR"},{"idx":19,"sentence":"Languages around the world employ classifier systems as a method of semantic organization and categorization.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"These systems are rife with variability, violability, and ambiguity, and are prone to constant change over time.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":19,"sentence":"We explicitly model change in classifier systems as the population-level outcome of child language acquisition over time in order to shed light on the factors that drive change to classifier systems.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":19,"sentence":"Our research consists of two parts: a contrastive corpus study of Cantonese and Mandarin child-directed speech to determine the role that ambiguity and homophony avoidance may play in classifier learning and change followed by a series of population-level learning simulations of an abstract classifier system.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":19,"sentence":"We find that acquisition without reference to ambiguity avoidance is sufficient to drive broad trends in classifier change and suggest an additional role for adults and discourse factors in classifier death.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":0,"sentence":"Depression is a common mental illness that involves sadness and lack of interest in all day-to-day activities. ","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"The task is to classify the social media text as signs of depression into three labels namely {``}not depressed{''}, {``}moderately depressed{''}, and {``}severely depressed{''}.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":0,"sentence":"We have build a system using Deep Learning Model {``}Transformers{''}. ","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":0,"sentence":"Transformers provides thousands of pretrained models to perform tasks on different modalities such as text, vision, and audio. ","offset":3,"pro":0.5,"labels":"MTD"},{"idx":0,"sentence":"The multi-class classification model used in our system is based on the ALBERT model.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":0,"sentence":"In the shared task ACL 2022, Our team SSN{\\_}MLRG3 obtained a Macro F1 score of 0.473.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":1,"sentence":"In this paper, we report the solution of the team BERT 4EVER for the LT-EDI-2022 shared task2: Homophobia/Transphobia Detection in social media comments in ACL 2022, which aims to classify Youtube comments into one of the following categories: no,moderate, or severe depression.","offset":0,"pro":0,"labels":"PUR"},{"idx":1,"sentence":"We model the problem as a text classification task and a text generation task and respectively propose two different models for the tasks.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":1,"sentence":"To combine the knowledge learned from these two different models,","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":1,"sentence":"we softly fuse the predicted probabilities of the models above and then select the label with the highest probability as the final output.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":1,"sentence":"In addition, multiple augmentation strategies are leveraged to improve the model generalization capability, such as back translation and adversarial training.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":1,"sentence":"Experimental results demonstrate the effectiveness of the proposed models and two augmented strategies.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":2,"sentence":"Hope is an inherent part of human life and essential for improving the quality of life. ","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"Hope increases happiness and reduces stress and feelings of helplessness. ","offset":1,"pro":0.2,"labels":"BAC"},{"idx":2,"sentence":"Hope speech is the desired outcome for better and can be studied using text from various online sources where people express their desires and outcomes. ","offset":2,"pro":0.4,"labels":"BAC"},{"idx":2,"sentence":"In this paper, we address a deep-learning approach with a combination of linguistic and psycho-linguistic features for hope-speech detection. ","offset":3,"pro":0.6,"labels":"PUR"},{"idx":2,"sentence":"We report our best results submitted to LT-EDI-2022 which ranked 2nd and 3rd in English and Spanish respectively.","offset":4,"pro":0.8,"labels":"RST"},{"idx":3,"sentence":"Social media platforms play a major role in our day-to-day life and are considered as a virtual friend by many users, who use the social media to share their feelings all day.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"Many a time, the content which is shared by users on social media replicate their internal life. ","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":3,"sentence":"Nowadays people love to share their daily life incidents like happy or unhappy moments and their feelings in social media and it makes them feel complete and it has become a habit for many users. ","offset":2,"pro":0.2222222222222222,"labels":"BAC"},{"idx":3,"sentence":"Social media provides a new chance to identify the feelings of a person through their posts. ","offset":3,"pro":0.3333333333333333,"labels":"BAC"},{"idx":3,"sentence":"The aim of the shared task is to develop a model in which the system is capable of analyzing the grammatical markers related to onset and permanent symptoms of depression.","offset":4,"pro":0.4444444444444444,"labels":"PUR"},{"idx":3,"sentence":"We as a team participated in the shared task Detecting Signs of Depression from Social Media Text at LT-EDI 2022- ACL 2022 and we have proposed a model which predicts depression from English social media posts using the data set shared for the task. ","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":3,"sentence":"The prediction is done based on the labels Moderate, Severe and Not Depressed. ","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":3,"sentence":"We have implemented this using different transformer models like DistilBERT, RoBERTa and ALBERT by which we were able to achieve a Macro F1 score of 0.337, 0.457 and 0.387 respectively. ","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":3,"sentence":"Our code is publicly available in the github","offset":8,"pro":0.8888888888888888,"labels":"CTN"},{"idx":4,"sentence":"The world is filled with serious problems like political {\\&} religious conflicts, wars, pandemics, and offensive hate speech is the last thing we desire. ","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"Our task was to classify a text into {`}Hope Speech{'} and {`}Non-Hope Speech{'}. ","offset":1,"pro":0.1111111111111111,"labels":"PUR"},{"idx":4,"sentence":"We searched for datasets acquired from YouTube comments that offer support, reassurance, inspiration, and insight, and the ones that don{'}t. ","offset":2,"pro":0.2222222222222222,"labels":"MTD"},{"idx":4,"sentence":"The datasets were provided to us by the LTEDI organizers in English, Tamil, Spanish, Kannada, and Malayalam. ","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":4,"sentence":"To successfully identify and classify them, ","offset":4,"pro":0.4444444444444444,"labels":"PUR"},{"idx":4,"sentence":"we employed several machine learning transformer models such as m-BERT, MLNet, BERT, XLMRoberta, and XLM{\\_}MLM.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":4,"sentence":"The observed results indicate that the BERT and m-BERT have obtained the best results among all the other techniques, ","offset":6,"pro":0.6666666666666666,"labels":"CLN"},{"idx":4,"sentence":"gaining a weighted F1- score of 0.92, 0.71, 0.76, 0.87, and 0.83 for English, Tamil, Spanish, Kannada, and Malayalam respectively.","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":4,"sentence":"This paper depicts our work for the Shared Task on Hope Speech Detection for Equality, Diversity, and Inclusion at LTEDI 2021.","offset":8,"pro":0.8888888888888888,"labels":"CTN"},{"idx":5,"sentence":"Language should be accommodating of equality and diversity as a fundamental aspect of communication. ","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"The language of internet users has a big impact on peer users all over the world. ","offset":1,"pro":0.1,"labels":"BAC"},{"idx":5,"sentence":"On virtual platforms such as Facebook, Twitter, and YouTube, people express their opinions in different languages. ","offset":2,"pro":0.2,"labels":"BAC"},{"idx":5,"sentence":"People respect others{'} accomplishments, pray for their well-being, and cheer them on when they fail.","offset":3,"pro":0.3,"labels":"BAC"},{"idx":5,"sentence":"Such motivational remarks are hope speech remarks. ","offset":4,"pro":0.4,"labels":"BAC"},{"idx":5,"sentence":"Simultaneously, a group of users encourages discrimination against women, people of color, people with disabilities, and other minorities based on gender, race, sexual orientation, and other factors.","offset":5,"pro":0.5,"labels":"BAC"},{"idx":5,"sentence":"To recognize hope speech from YouTube comments, the current study offers an ensemble approach that combines a support vector machine, logistic regression, and random forest classifiers. ","offset":6,"pro":0.6,"labels":"PUR"},{"idx":5,"sentence":"Extensive testing was carried out to discover the best features for the aforementioned classifiers. ","offset":7,"pro":0.7,"labels":"MTD"},{"idx":5,"sentence":"In the support vector machine and logistic regression classifiers, char-level TF-IDF features were used, whereas in the random forest classifier, word-level features were used. ","offset":8,"pro":0.8,"labels":"MTD"},{"idx":5,"sentence":"The proposed ensemble model performed significantly well among English, Spanish, Tamil, Malayalam, and Kannada YouTube comments.","offset":9,"pro":0.9,"labels":"RST"},{"idx":6,"sentence":"Hope is considered significant for the wellbeing,recuperation and restoration of humanlife by health professionals. ","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"Hope speech reflectsthe belief that one can discover pathwaysto their desired objectives and become rousedto utilise those pathways. ","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":6,"sentence":"Hope speech offerssupport, reassurance, suggestions, inspirationand insight. Hate speech is a prevalent practicethat society has to struggle with everyday.","offset":2,"pro":0.2857142857142857,"labels":"BAC"},{"idx":6,"sentence":"The freedom of speech and ease of anonymitygranted by social media has also resulted inincitement to hatred. ","offset":3,"pro":0.42857142857142855,"labels":"BAC"},{"idx":6,"sentence":"In this paper, we workto identify and promote positive and supportivecontent on these platforms.","offset":4,"pro":0.5714285714285714,"labels":"PUR"},{"idx":6,"sentence":"We work withseveral machine learning models to classify socialmedia comments as hope speech or nonhopespeech in English. ","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":6,"sentence":"This paper portraysour work for the Shared Task on Hope SpeechDetection for Equality, Diversity, and Inclusionat LT-EDI-ACL 2022.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":7,"sentence":"The DepSign-LT-EDI-ACL2022 shared task focuses on early prediction of severity of depression over social media posts.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"The BioNLP group at Department of Data Science and Engineering in Indian Institute of Science Education and Research Bhopal (IISERB) has participated in this challenge and submitted three runs based on three different text mining models. ","offset":1,"pro":0.1111111111111111,"labels":"PUR"},{"idx":7,"sentence":"The severity of depression were categorized into three classes, viz., no depression, moderate, and severe and the data to build models were released as part of this shared task.","offset":2,"pro":0.2222222222222222,"labels":"MTD"},{"idx":7,"sentence":"The objective of this work is to identify relevant features from the given social media texts for effective text classification.","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":7,"sentence":"As part of our investigation, we explored features derived from text data using document embeddings technique and simple bag of words model following different weighting schemes. ","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":7,"sentence":"Subsequently, adaptive boosting, logistic regression, random forest and support vector machine (SVM) classifiers were used to identify the scale of depression from the given texts. ","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":7,"sentence":"The experimental analysis on the given validation data show that the SVM classifier using the bag of words model following term frequency and inverse document frequency weighting scheme outperforms the other models for identifying depression.","offset":6,"pro":0.6666666666666666,"labels":"CLN"},{"idx":7,"sentence":"However, this framework could not achieve a place among the top ten runs of the shared task.","offset":7,"pro":0.7777777777777778,"labels":"CLN"},{"idx":7,"sentence":"This paper describes the potential of the proposed framework as well as the possible reasons behind mediocre performance on the given data.","offset":8,"pro":0.8888888888888888,"labels":"IMP"},{"idx":8,"sentence":"Over the years, there has been a slow but steady change in the attitude of society towards different kinds of sexuality.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"However, on social media platforms, where people have the license to be anonymous, toxic comments targeted at homosexuals, transgenders and the LGBTQ+ community are not uncommon. ","offset":1,"pro":0.2,"labels":"GAP"},{"idx":8,"sentence":"Detection of homophobic comments on social media can be useful in making the internet a safer place for everyone. ","offset":2,"pro":0.4,"labels":"GAP"},{"idx":8,"sentence":"For this task, we used a combination of word embeddings and SVM Classifiers as well as some BERT-based transformers. ","offset":3,"pro":0.6,"labels":"MTD"},{"idx":8,"sentence":"We achieved a weighted F1-score of 0.93 on the English dataset, 0.75 on the Tamil dataset and 0.87 on the Tamil-English Code-Mixed dataset.","offset":4,"pro":0.8,"labels":"RST"},{"idx":9,"sentence":"In this paper we present our approach for detecting signs of depression from social media text.","offset":0,"pro":0,"labels":"PUR"},{"idx":9,"sentence":"Our model relies on word unigrams, part-of-speech tags, readabilitiy measures and the use of first, second or third person and the number of words. ","offset":1,"pro":0.25,"labels":"MTD"},{"idx":9,"sentence":"Our best model obtained a macro F1-score of 0.439 and ranked 25th, out of 31 teams. ","offset":2,"pro":0.5,"labels":"RST"},{"idx":9,"sentence":"We further take advantage of the interpretability of the Logistic Regression model and we make an attempt to interpret the model coefficients with the hope that these will be useful for further research on the topic.","offset":3,"pro":0.75,"labels":"MTD"},{"idx":10,"sentence":"Depression is a mental illness that negatively affects a person{'}s well-being and can, if left untreated, lead to serious consequences such as suicide. ","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"In the last decade, social media has become one of the most common places to express one{'}s feelings.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":10,"sentence":"Hence, there is a possibility of text processing and applying machine learning techniques to detect possible signs of depression. ","offset":2,"pro":0.3333333333333333,"labels":"BAC"},{"idx":10,"sentence":"We explore three different approaches to solve the challenge: fine-tuning BERT model, leveraging AutoML for the construction of features and classifier selection and finally,","offset":3,"pro":0.5,"labels":"MTD"},{"idx":10,"sentence":"we explore latent spaces derived from the combination of textual and knowledge-based representations. ","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":10,"sentence":"Our best solution, based on knowledge graph and textual representations, was 4.9{\\%} behind the best model in terms of Macro F1, and only 1.9{\\%} behind in terms of Recall.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":11,"sentence":"In this paper, we describe our approach for the task of homophobia and transphobia detection in English social media comments. ","offset":0,"pro":0,"labels":"PUR"},{"idx":11,"sentence":"Given the high class imbalance, we propose a solution based on data augmentation and ensemble modeling.","offset":1,"pro":0.3333333333333333,"labels":"MTD"},{"idx":11,"sentence":"We fine-tuned different large language models (BERT, RoBERTa, and HateBERT) and used the weighted majority vote on their predictions.","offset":2,"pro":0.6666666666666666,"labels":"MTD"},{"idx":12,"sentence":"Depression is a common and serious mental illness that early detection can improve the patient{'}s symptoms and make depression easier to treat. ","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"The goal of DepSign is to classify the signs of depression into three labels namely {``}not depressed{''}, {``}moderately depressed{''}, and {``}severely depressed{''} based on social media{'}s posts.","offset":1,"pro":0.25,"labels":"PUR"},{"idx":12,"sentence":"In this paper, we propose a predictive ensemble model that utilizes the fine-tuned contextualized word embedding, ALBERT, DistilBERT, RoBERTa, and BERT base model.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":12,"sentence":"We show that our model outperforms the baseline models in all considered metrics and achieves an F1 score of 54{\\%} and accuracy of 61{\\%}, ranking 5th on the leader-board for the DepSign task.","offset":3,"pro":0.75,"labels":"CLN"},{"idx":13,"sentence":"Hateful and offensive content on social media platforms can have negative effects on users and can make online communities more hostile towards certain people and hamper equality, diversity and inclusion.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"In this paper, we describe our approach to classify homophobia and transphobia in social media comments. ","offset":1,"pro":0.3333333333333333,"labels":"PUR"},{"idx":13,"sentence":"Our model ranked 2nd for English, 8th for Tamil and 10th for Tamil-English.","offset":2,"pro":0.6666666666666666,"labels":"RST"},{"idx":14,"sentence":"This paper presents our winning solution for the Shared Task on Detecting Signs of Depression from Social Media Text at LT-EDI-ACL2022. ","offset":0,"pro":0,"labels":"PUR"},{"idx":14,"sentence":"We based our solution on transformer-based language models. ","offset":1,"pro":0.2,"labels":"MTD"},{"idx":14,"sentence":"Then, using the prepared corpus, we trained our own language model called DepRoBERTa (RoBERTa for Depression Detection). ","offset":2,"pro":0.4,"labels":"MTD"},{"idx":14,"sentence":"The third solution was to use the ensemble averaging, which turned out to be the best solution.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":14,"sentence":"It achieved a macro-averaged F1-score of 0.583. ","offset":4,"pro":0.8,"labels":"RST"},{"idx":15,"sentence":"Depression is a common mental disorder that severely affects the quality of life, and can lead to suicide. ","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"This is why it is vital to detect signs of depression in time. ","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":15,"sentence":"Transformers have achieved state-of-the-art performance on a variety of similar text classification tasks. ","offset":2,"pro":0.3333333333333333,"labels":"BAC"},{"idx":15,"sentence":"Because of this, in this paper, we examine the effect of balancing a depression detection dataset using data augmentation.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":15,"sentence":"In particular, we use abstractive summarization techniques for data augmentation. ","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":15,"sentence":"Our results show that when increasing the multiplicity of the minority classes to the right degree, this data augmentation method can in fact improve classification scores on the task.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":16,"sentence":"One of the important tasks is homophobia/transphobia detection. ","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"A machine learning-based model has been designed and various classification algorithms have been implemented for automatic detection of homophobia in YouTube comments. ","offset":1,"pro":0.3333333333333333,"labels":"MTD"},{"idx":16,"sentence":"Support Vector Machines has been used to develop the proposed model and our submission reported 0.91, 0.92, 0.88 weighted f1-score for English, Tamil and Tamil-English datasets respectively.","offset":2,"pro":0.6666666666666666,"labels":"RST"},{"idx":17,"sentence":"This paper describes team giniUs{'} submission to the Hope Speech Detection for Equality, Diversity and Inclusion Shared Task organised by LT-EDI ACL 2022. ","offset":0,"pro":0,"labels":"PUR"},{"idx":17,"sentence":"Our best result on the leaderboard achieve a weighted F1 score of 0.86 and a Macro F1 score of 0.51 for English. ","offset":1,"pro":0.3333333333333333,"labels":"RST"},{"idx":17,"sentence":"We have open-sourced our code implementations on GitHub to facilitate easy reproducibility by the scientific community.","offset":2,"pro":0.6666666666666666,"labels":"CTN"},{"idx":18,"sentence":"DepSign-LT-EDI@ACL-2022 aims to ascer-tain the signs of depression of a person fromtheir messages and posts on social mediawherein people share their feelings and emo-tions. ","offset":0,"pro":0,"labels":"PUR"},{"idx":18,"sentence":"To achieve this objective, ","offset":1,"pro":0.3333333333333333,"labels":"PUR"},{"idx":18,"sentence":"This solu-tion from team SSN{\\_}MLRG1 achieves 58.5{\\%}accuracy on the DepSign-LT-EDI@ACL-2022test set.","offset":2,"pro":0.6666666666666666,"labels":"RST"},{"idx":19,"sentence":"Depression is a common and serious medical illness that negatively affects how you feel, the way you think, and how you act. ","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"Detecting depression is essential as it must be treated early to avoid painful consequences. ","offset":1,"pro":0.125,"labels":"BAC"},{"idx":19,"sentence":"Nowadays, people are broadcasting how they feel via posts and comments. ","offset":2,"pro":0.25,"labels":"BAC"},{"idx":19,"sentence":"Using social media, we can extract many comments related to depression and use NLP techniques to train and detect depression.","offset":3,"pro":0.375,"labels":"BAC"},{"idx":19,"sentence":"This work presents the submission of the DepressionOne team at LT-EDI-2022 for the shared task, detecting signs of depression from social media text. ","offset":4,"pro":0.5,"labels":"PUR"},{"idx":19,"sentence":"The depression data is small and unbalanced.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":19,"sentence":"Thus, we have used oversampling and undersampling methods such as SMOTE and RandomUnderSampler to represent the data. ","offset":6,"pro":0.75,"labels":"MTD"},{"idx":19,"sentence":"Later, we used machine learning methods to train and detect the signs of depression.\n\n","offset":7,"pro":0.875,"labels":"MTD"},{"idx":0,"sentence":" In this paper, we aim to introduce a Cognitive Linguistics perspective into a computational analysis of near-synonyms.","offset":0,"pro":0,"labels":"PUR"},{"idx":0,"sentence":" We focus on a single set of Dutch near-synonyms, vernielen and vernietigen, roughly translated as {`}to destroy{'}, replicating the analysis from Geeraerts (1997) with distributional models.","offset":1,"pro":0.3333333333333333,"labels":"MTD"},{"idx":0,"sentence":"Our analysis, which tracks the meaning of both words in a corpus of 16th-20th century prose data, shows that both lexical items have undergone semantic change, led by differences in their prototypical semantic core.","offset":2,"pro":0.6666666666666666,"labels":"RST"},{"idx":1,"sentence":"In this paper, we present What is Done is Done (WiDiD), an incremental approach to semantic shift detection based on incremental clustering techniques and contextual embedding methods to capture the changes over the meanings of a target word along a diachronic corpus.","offset":0,"pro":0,"labels":"MTD"},{"idx":1,"sentence":" In WiDiD, the word contexts observed in the past are consolidated as a set of clusters that constitute the {``}memory{''} of the word meanings observed so far. ","offset":1,"pro":0.3333333333333333,"labels":"MTD"},{"idx":1,"sentence":"Such a memory is exploited as a basis for subsequent word observations, so that the meanings observed in the present are stratified over the past ones.","offset":2,"pro":0.6666666666666666,"labels":"RST"},{"idx":2,"sentence":" However, language units may be complex organizations in themselves, e.g. in the case of schematic constructions, featuring a free slot.","offset":0,"pro":0,"labels":"GAP"},{"idx":2,"sentence":"Such a slot is filled by words forming a set or {`}paradigm{'} and engaging in inter-related dynamics within this constructional environment.","offset":1,"pro":0.25,"labels":"BAC"},{"idx":2,"sentence":"To tackle this complexity, a simple computational method is offered to automatically characterize their interactions, and visualize them through networks of cooperation and competition.","offset":2,"pro":0.5,"labels":"PUR"},{"idx":2,"sentence":" Applying this method to the French paradigm of quantifiers, I show that this method efficiently captures phenomena regarding the evolving organization of constructional paradigms, in particular the constitution of competing clusters of fillers that promote different semantic strategies overall.","offset":3,"pro":0.75,"labels":"RST"},{"idx":3,"sentence":"In this work, we explore whether large pre-trained contextualised language models, a common tool for lexical semantic change detection, are sensitive to such morphosyntactic changes.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"To this end, we first compare the performance of grammatical profiles against that of a multilingual neural language model (XLM-R) on 10 datasets, covering 7 languages, and then combine the two approaches in ensembles to assess their complementarity.","offset":1,"pro":0.14285714285714285,"labels":"MTD"},{"idx":3,"sentence":"Our results show that ensembling grammatical profiles with XLM-R improves semantic change detection performance for most datasets and languages.","offset":2,"pro":0.2857142857142857,"labels":"RST"},{"idx":3,"sentence":" This indicates that language models do not fully cover the fine-grained morphological and syntactic signals that are explicitly represented in grammatical profiles.","offset":3,"pro":0.42857142857142855,"labels":"IMP"},{"idx":3,"sentence":"An interesting exception are the test sets where the time spans under analysis are much longer than the time gap between them (for example, century-long spans with a one-year gap between them).","offset":4,"pro":0.5714285714285714,"labels":"IMP"},{"idx":3,"sentence":"Morphosyntactic change is slow so grammatical profiles do not detect in such cases.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":3,"sentence":"In contrast, language models, thanks to their access to lexical information, are able to detect fast topical changes.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":4,"sentence":"In this paper, we describe a BERT model trained on the Eighteenth Century Collections Online (ECCO) dataset of digitized documents.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"The ECCO dataset poses unique modelling challenges due to the presence of Optical Character Recognition (OCR) artifacts.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":4,"sentence":"We establish the performance of the BERT model on a publication year prediction task against linear baseline models and human judgement,","offset":2,"pro":0.4,"labels":"MTD"},{"idx":4,"sentence":"finding the BERT model to be superior to both and able to date the works, on average, with less than 7 years absolute error. ","offset":3,"pro":0.6,"labels":"RST"},{"idx":4,"sentence":"We also explore how language change over time affects the model by analyzing the features the model uses for publication year predictions as given by the Integrated Gradients model explanation method.","offset":4,"pro":0.8,"labels":"MTD"},{"idx":5,"sentence":"The tree model is well known for expressing the historic evolution of languages.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"This model has been considered as a method of describing genetic relationships between languages. ","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":5,"sentence":"Nevertheless, some researchers question the model{'}s ability to predict the proximity between two languages, since it represents genetic relatedness rather than linguistic resemblance.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":5,"sentence":"Defining other language proximity models has been an active research area for many years.","offset":3,"pro":0.42857142857142855,"labels":"BAC"},{"idx":5,"sentence":" In this paper we explore a part-of-speech model for defining proximity between languages using a multilingual language model that was fine-tuned on the task of cross-lingual part-of-speech tagging.","offset":4,"pro":0.5714285714285714,"labels":"BAC"},{"idx":5,"sentence":"We train the model on one language and evaluate it on another; the measured performance is then used to define the proximity between the two languages.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":5,"sentence":"By further developing the model, we show that it can reconstruct some parts of the tree model.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":6,"sentence":"Despite these increased efforts, there are not many easy-to-use and fast approaches for the task of phonological reconstruction.","offset":0,"pro":0,"labels":"GAP"},{"idx":6,"sentence":"Here we present a new framework that combines state-of-the-art techniques for automated sequence comparison with novel techniques for phonetic alignment analysis and sound correspondence pattern detection to allow for the supervised reconstruction of word forms in ancestral languages.","offset":1,"pro":0.25,"labels":"MTD"},{"idx":6,"sentence":"We test the method on a new dataset covering six groups from three different language families.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":6,"sentence":"The results show that our method yields promising results while at the same time being not only fast but also easy to apply and expand.","offset":3,"pro":0.75,"labels":"RST"},{"idx":7,"sentence":"In this work, we study semantic change of such items using multilingual word embeddings, both static and contextualised.","offset":0,"pro":0,"labels":"MTD"},{"idx":7,"sentence":"We underline caveats identified while building and evaluating these embeddings. We release both said embeddings and a newly-built historical words lexicon, containing typed relations between words of varied Romance languages.","offset":1,"pro":0.5,"labels":"BAC"},{"idx":8,"sentence":"While several methods for detecting semantic change have been proposed, such studies are limited to a few languages, where evaluation datasets are available.","offset":0,"pro":0,"labels":"GAP"},{"idx":8,"sentence":"This paper presents the first dataset for evaluating Chinese semantic change in contexts preceding and following the Reform and Opening-up, covering a 50-year period in Modern Chinese.","offset":1,"pro":0.3333333333333333,"labels":"BAC"},{"idx":8,"sentence":" Following the DURel framework, we collected 6,000 human judgments for the dataset. We also reported the performance of alignment-based word embedding models on this evaluation dataset, achieving high and significant correlation scores.","offset":2,"pro":0.6666666666666666,"labels":"MTD"},{"idx":9,"sentence":"Our comparison is based on character n-grams on the one hand and PoS n-grams on the other and we show that these two lead to different distances.","offset":0,"pro":0,"labels":"MTD"},{"idx":9,"sentence":"Particularly in the PoS-based distances, one can observe all of the 21st century Low Saxon dialects shifting towards the modern majority languages.","offset":1,"pro":0.5,"labels":"RST"},{"idx":10,"sentence":"Languages can respond to external events in various ways - the creation of new words or named entities, additional senses might develop for already existing words or the valence of words can change.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"In this work, we explore the semantic shift of the Dutch words {``}natie{''} ({``}nation{''}), {``}volk{''} ({``}people{''}) and {``}vaderland{''} ({``}fatherland{''}) over a period that is known for the rise of nationalism in Europe: 1700-1880.","offset":1,"pro":0.125,"labels":"MTD"},{"idx":10,"sentence":"The semantic change is measured by means of Dynamic Bernoulli Word Embeddings which allow for comparison between word embeddings over different time slices.","offset":2,"pro":0.25,"labels":"BAC"},{"idx":10,"sentence":"The word embeddings were generated based on Dutch fiction literature divided over different decades.","offset":3,"pro":0.375,"labels":"BAC"},{"idx":10,"sentence":"From the analysis of the absolute drifts, it appears that the word {``}natie{''} underwent a relatively small drift.","offset":4,"pro":0.5,"labels":"BAC"},{"idx":10,"sentence":"However, the drifts of {``}vaderland{'}{''} and {``}volk{''}{'} show multiple peaks, culminating around the turn of the nineteenth century.","offset":5,"pro":0.625,"labels":"GAP"},{"idx":10,"sentence":"To verify whether this semantic change can indeed be attributed to nationalistic movements, a detailed analysis of the nearest neighbours of the target words is provided.","offset":6,"pro":0.75,"labels":"PUR"},{"idx":10,"sentence":"From the analysis, it appears that {``}natie{''}, {``}volk{''} and {``}vaderlan{''}{'} became more nationalistically-loaded over time.","offset":7,"pro":0.875,"labels":"RST"},{"idx":11,"sentence":"We use this dataset to evaluate traditional and more recent unsupervised approaches to lexical semantic change that make use of contextualized word representations based on the BERT neural language model to obtain representations of word usages.","offset":0,"pro":0,"labels":"MTD"},{"idx":11,"sentence":"We argue that previous models that encode local representations of words cannot capture global context shifts such as the context shift of face masks since the pandemic outbreak.","offset":1,"pro":0.2,"labels":"RST"},{"idx":11,"sentence":"We experiment with neural topic models to track context shifts of words.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":11,"sentence":"We show that this approach can reveal textual associations of words that go beyond their lexical meaning representation.","offset":3,"pro":0.6,"labels":"RST"},{"idx":11,"sentence":"We discuss future work and how to proceed capturing the pragmatic aspect of meaning change as opposed to lexical semantic change.","offset":4,"pro":0.8,"labels":"BAC"},{"idx":12,"sentence":"The use of word embeddings is an important NLP technique for extracting meaningful conclusions from corpora of human text.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"One important question that has been raised about word embeddings is the degree of gender bias learned from corpora","offset":1,"pro":0.2,"labels":"BAC"},{"idx":12,"sentence":"Bolukbasi et al. (2016) proposed an important technique for quantifying gender bias in word embeddings that, at its heart, is lexically based and relies on sets of highly gendered word pairs (e.g., mother/father and madam/sir) and a list of professions words (e.g., doctor and nurse).","offset":2,"pro":0.4,"labels":"BAC"},{"idx":12,"sentence":"In this paper, we document problems that arise with this method to quantify gender bias in diachronic corpora.","offset":3,"pro":0.6,"labels":"BAC"},{"idx":12,"sentence":"We further document complications in languages such as Arabic, where many words are highly polysemous/homonymous, especially female professions words.","offset":4,"pro":0.8,"labels":"MTD"},{"idx":13,"sentence":"We create the first dataset of Spanish words manually annotated by semantic change using the DURel framewok (Schlechtweg et al., 2018).","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"The task is divided in two phases: 1) graded change discovery, and 2) binary change detection.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":13,"sentence":"In addition to introducing a new language for this task, the main novelty with respect to the previous tasks consists in predicting and evaluating changes for all vocabulary words in the corpus.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":13,"sentence":"Six teams participated in phase 1 and seven teams in phase 2 of the shared task, and the best system obtained a Spearman rank correlation of 0.735 for phase 1 and an F1 score of 0.735 for phase 2.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":13,"sentence":"We describe the systems developed by the competing teams, highlighting the techniques that were particularly useful.","offset":4,"pro":0.8,"labels":"RST"},{"idx":14,"sentence":"Our approach is based on generating lexical substitutes that describe old and new senses of a given word.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"This approach achieves the second best result in sense loss and sense gain detection subtasks.","offset":1,"pro":0.25,"labels":"MTD"},{"idx":14,"sentence":"By observing those substitutes that are specific for only one time period, one can understand which senses were obtained or lost.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":14,"sentence":"This allows providing more detailed information about semantic change to the user and makes our method interpretable.","offset":3,"pro":0.75,"labels":"RST"},{"idx":15,"sentence":" We basically try to replicate the annotation of the dataset for the shared task, but replacing human annotators with a neural network.","offset":0,"pro":0,"labels":"PUR"},{"idx":15,"sentence":"In the graded change discovery subtask, our solution has achieved the 2nd best result according to all metrics.","offset":1,"pro":0.16666666666666666,"labels":"RST"},{"idx":15,"sentence":"However, in the optional sense gain detection subtask we have outperformed all other participants.","offset":2,"pro":0.3333333333333333,"labels":"RST"},{"idx":15,"sentence":"In the main binary change detection subtask, our F1-score is 0.655 compared to 0.716 of the best submission, corresponding to the 5th place.","offset":3,"pro":0.5,"labels":"RST"},{"idx":15,"sentence":"During the post-evaluation experiments we compared different ways to prepare WiC data in Spanish for fine-tuning.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":15,"sentence":"We have found that it helps leaving only examples annotated as 1 (unrelated senses) and 4 (identical senses) rather than using 2x more examples including intermediate annotations.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":16,"sentence":"For binary change detection, we frame the task as a word sense disambiguation (WSD) problem. ","offset":0,"pro":0,"labels":"MTD"},{"idx":16,"sentence":"We assume that the word semantics have changed if a sense is observed in only one of the two corpora, or the relative change for any sense exceeds a tuned threshold.","offset":1,"pro":0.2,"labels":"CLN"},{"idx":16,"sentence":"For graded change discovery, we follow the design of CIRCE (P{\\\"o}msl and Lyapin, 2020) by combining both static and contextual embeddings.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":16,"sentence":"For contextual embeddings, we use XLM-RoBERTa instead of BERT, and train the model to predict a masked token instead of the time period.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":16,"sentence":"Our language-independent methods achieve results that are close to the best-performing systems in the shared task.},","offset":4,"pro":0.8,"labels":"RST"},{"idx":17,"sentence":"We participated in both tasks (graded discovery and binary change, including sense gain and sense loss) and proposed models based on word embedding distances combined with hand-crafted linguistic features, including polysemy, number of neological synonyms, and relation to cognates in English.","offset":0,"pro":0,"labels":"MTD"},{"idx":17,"sentence":"We find that models that include linguistically informed features combined using weights assigned manually by experts lead to promising results.","offset":1,"pro":0.5,"labels":"RST"},{"idx":18,"sentence":"This paper describes the methods used for lexical semantic change discovery in Spanish.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"We tried the method based on BERT embeddings with clustering, the method based on grammatical profiles and the grammatical profiles method enhanced with permutation tests.","offset":1,"pro":0.3333333333333333,"labels":"MTD"},{"idx":18,"sentence":"Our best submission for graded discovery was the 3rd best result, while for binary detection it was the 2nd place (precision) and the 7th place (both F1-score and recall). Our highest precision for binary detection was 0.75 and it was achieved due to improving grammatical profiling with permutation tests.","offset":2,"pro":0.6666666666666666,"labels":"RST"},{"idx":19,"sentence":"The contextualized embeddings obtained from neural networks pre-trained as Language Models (LM) or Masked Language Models (MLM) are not well suitable for solving the Lexical Semantic Change Detection (LSCD) task because they are more sensitive to changes in word forms rather than word meaning, a property previously known as the word form bias or orthographic bias.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"Unlike many other NLP tasks, it is also not obvious how to fine-tune such models for LSCD.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":19,"sentence":"In order to conclude if there are any differences between senses of a particular word in two corpora, a human annotator or a system shall analyze many examples containing this word from both corpora.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":19,"sentence":"The existing LSCD datasets contain up to 100 words that are labeled according to their semantic change, which is hardly enough for fine-tuning.","offset":3,"pro":0.375,"labels":"BAC"},{"idx":19,"sentence":"To solve these problems we fine-tune the XLM-R MLM as part of a gloss-based WSD system on a large WSD dataset in English.","offset":4,"pro":0.5,"labels":"PUR"},{"idx":19,"sentence":"Then we employ zero-shot cross-lingual transferability of XLM-R to build the contextualized embeddings for examples in Spanish.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":19,"sentence":"In order to obtain the graded change score for each word, we calculate the average distance between our improved contextualized embeddings of its old and new occurrences.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":19,"sentence":"Our solution has shown the best results among all other participants in all subtasks except for the optional sense gain detection subtask.","offset":7,"pro":0.875,"labels":"RST"},{"idx":0,"sentence":"We advance the state-of-the-art in unsupervised abstractive dialogue summarization by utilizing multi-sentence compression graphs. ","offset":0,"pro":0,"labels":"PUR"},{"idx":0,"sentence":"Starting from well-founded assumptions about word graphs, we present simple but reliable path-reranking and topic segmentation schemes. ","offset":1,"pro":0.2,"labels":"MTD"},{"idx":0,"sentence":"Robustness of our method is demonstrated on datasets across multiple domains, including meetings, interviews, movie scripts, and day-to-day conversations.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":0,"sentence":"We also identify possible avenues to augment our heuristic-based system with deep learning. ","offset":3,"pro":0.6,"labels":"MTD"},{"idx":0,"sentence":"We open-source our code, to provide a strong, reproducible baseline for future research into unsupervised dialogue summarization.","offset":4,"pro":0.8,"labels":"CTN"},{"idx":1,"sentence":"With millions of documented recoveries from COVID-19 worldwide, various long-term sequelae have been observed in a large group of survivors. ","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"This paper is aimed at systematically analyzing user-generated conversations on Twitter that are related to long-term COVID symptoms for a better understanding of the Long COVID health consequences.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":1,"sentence":"Using an interactive information extraction tool built especially for this purpose,","offset":2,"pro":0.4,"labels":"MTD"},{"idx":1,"sentence":"we extracted key information from the relevant tweets and analyzed the user-reported Long COVID symptoms with respect to their demographic and geographical characteristics. ","offset":3,"pro":0.6,"labels":"MTD"},{"idx":1,"sentence":"The results of our analysis are expected to improve the public awareness on long-term COVID-19 sequelae and provide important insights to public health authorities.","offset":4,"pro":0.8,"labels":"CTN"},{"idx":2,"sentence":"Classification of posts in social media such as Twitter is difficult due to the noisy and short nature of texts.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"Sequence classification models based on recurrent neural networks (RNN) are popular for classifying posts that are sequential in nature. ","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":2,"sentence":"RNNs assume the hidden representation dynamics to evolve in a discrete manner and do not consider the exact time of the posting.","offset":2,"pro":0.3333333333333333,"labels":"BAC"},{"idx":2,"sentence":"In this work, we propose to use recurrent neural ordinary differential equations (RNODE) for social media post classification which consider the time of posting and allow the computation of hidden representation to evolve in a time-sensitive continuous manner.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":2,"sentence":"In addition, we propose a novel model, Bi-directional RNODE (Bi-RNODE), which can consider the information flow in both the forward and backward directions of posting times to predict the post label. ","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":2,"sentence":"Our experiments demonstrate that RNODE and Bi-RNODE are effective for the problem of stance classification of rumours in social media.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":3,"sentence":"Authors of posts in social media communicate their emotions and what causes them with text and images. ","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"While there is work on emotion and stimulus detection for each modality separately, it is yet unknown if the modalities contain complementary emotion information in social media.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":3,"sentence":"We aim at filling this research gap and contribute a novel, annotated corpus of English multimodal Reddit posts. ","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":3,"sentence":"On this resource, we develop models to automatically detect the relation between image and text, an emotion stimulus category and the emotion class. ","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":3,"sentence":"We evaluate if these tasks require both modalities and find for the image{--}text relations, that text alone is sufficient for most categories (complementary, illustrative, opposing): the information in the text allows to predict if an image is required for emotion understanding. ","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":3,"sentence":"The emotions of anger and sadness are best predicted with a multimodal model, while text alone is sufficient for disgust, joy, and surprise. ","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":3,"sentence":"Stimuli depicted by objects, animals, food, or a person are best predicted by image-only models, while multimodal mod- els are most effective on art, events, memes, places, or screenshots.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":4,"sentence":"COVID-19 has disproportionately threatened minority communities in the U.S, not only in health but also in societal impact. ","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"However, social scientists and policymakers lack critical data to capture the dynamics of the anti-Asian hate trend and to evaluate its scale and scope. ","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":4,"sentence":"We introduce new datasets from Twitter related to anti-Asian hate sentiment before and during the pandemic. ","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":4,"sentence":"Relying on Twitter{'}s academic API, we retrieve hateful and counter-hate tweets from the Twitter Historical Database. ","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":4,"sentence":"To build contextual understanding and collect related racial cues, we also collect instances of heated arguments, often political, but not necessarily hateful, discussing Chinese issues. ","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":4,"sentence":"We then use the state-of-the-art hate speech classifiers to discern whether these tweets express hatred. ","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":4,"sentence":"These datasets can be used to study hate speech, general anti-Asian or Chinese sentiment, and hate linguistics by social scientists as well as to evaluate and build hate speech or sentiment analysis classifiers by computational scholars.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":5,"sentence":"In many real-world machine learning applications, samples belong to a set of domains e.g., for product reviews each review belongs to a product category. ","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"In this paper, we study multi-domain imbalanced learning (MIL), the scenario that there is imbalance not only in classes but also in domains. ","offset":1,"pro":0.2,"labels":"PUR"},{"idx":5,"sentence":"In the MIL setting, different domains exhibit different patterns and there is a varying degree of similarity and divergence among domains posing opportunities and challenges for transfer learning especially when faced with limited or insufficient training data.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":5,"sentence":"We propose a novel domain-aware contrastive knowledge transfer method called DCMI to (1) identify the shared domain knowledge to encourage positive transfer among similar domains (in particular from head domains to tail domains); (2) isolate the domain-specific knowledge to minimize the negative transfer from dissimilar domains. ","offset":3,"pro":0.6,"labels":"MTD"},{"idx":5,"sentence":"We evaluated the performance of DCMI on three different datasets showing significant improvements in different MIL scenarios.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":6,"sentence":"People associate affective meanings to words - {``}death{''} is scary and sad while {``}party{''} is connotated with surprise and joy. ","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"This raises the question if the association is purely a product of the learned affective imports inherent to semantic meanings, or is also an effect of other features of words, e.g., morphological and phonological patterns. ","offset":1,"pro":0.1,"labels":"BAC"},{"idx":6,"sentence":"We approach this question with an annotation-based analysis leveraging nonsense words. ","offset":2,"pro":0.2,"labels":"PUR"},{"idx":6,"sentence":"Specifically, we conduct a best-worst scaling crowdsourcing study in which participants assign intensity scores for joy, sadness, anger, disgust, fear, and surprise to 272 non-sense words and, for comparison of the results to previous work, to 68 real words. ","offset":3,"pro":0.3,"labels":"MTD"},{"idx":6,"sentence":"Based on this resource, we develop character-level and phonology-based intensity regressors. ","offset":4,"pro":0.4,"labels":"MTD"},{"idx":6,"sentence":"We evaluate them on both nonsense words and real words (making use of the NRC emotion intensity lexicon of 7493 words), across six emotion categories. ","offset":5,"pro":0.5,"labels":"MTD"},{"idx":6,"sentence":"The analysis of our data reveals that some phonetic patterns show clear differences between emotion intensities.","offset":6,"pro":0.6,"labels":"CLN"},{"idx":6,"sentence":"For instance, s as a first phoneme contributes to joy, sh to surprise, p as last phoneme more to disgust than to anger and fear.","offset":7,"pro":0.7,"labels":"CLN"},{"idx":6,"sentence":"In the modelling experiments, a regressor trained on real words from the NRC emotion intensity lexicon shows a higher performance (r = 0.17) than regressors that aim at learning the emotion connotation purely from nonsense words. ","offset":8,"pro":0.8,"labels":"RST"},{"idx":6,"sentence":"We conclude that humans do associate affective meaning to words based on surface patterns, but also based on similarities to existing words ({``}juy{''} to {``}joy{''}, or {``}flike{''} to {``}like{''}).","offset":9,"pro":0.9,"labels":"CLN"},{"idx":7,"sentence":"In this paper, we present the SentEMO platform, a tool that provides aspect-based sentiment analysis and emotion detection of unstructured text data such as reviews, emails and customer care conversations.","offset":0,"pro":0,"labels":"PUR"},{"idx":7,"sentence":"Currently, models have been trained for five domains and one general domain and are implemented in a pipeline approach, where the output of one model serves as the input for the next. ","offset":1,"pro":0.25,"labels":"MTD"},{"idx":7,"sentence":"The results are presented in three dashboards, allowing companies to gain more insights into what stakeholders think of their products and services.","offset":2,"pro":0.5,"labels":"CTN"},{"idx":7,"sentence":"The SentEMO platform is available at https://sentemo.ugent.be","offset":3,"pro":0.75,"labels":"CTN"},{"idx":8,"sentence":"Deep Neural Networks (DNN) models have achieved acceptable performance in sentiment prediction of written text.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"However, the output of these machine learning (ML) models cannot be natively interpreted.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":8,"sentence":"In this paper, we study how the sentiment polarity predictions by DNNs can be explained and compare them to humans{'} explanations.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":8,"sentence":"We crowdsource a corpus of Personal Narratives and ask human judges to annotate them with polarity and select the corresponding token chunks - the Emotion Carriers (EC) - that convey narrators{'} emotions in the text. ","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":8,"sentence":"The interpretations of ML neural models are carried out through Integrated Gradients method and we compare them with human annotators{'} interpretations. ","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":8,"sentence":"The results of our comparative analysis indicate that while the ML model mostly focuses on the explicit appearance of emotions-laden words (e.g. happy, frustrated), ","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":8,"sentence":"the human annotator predominantly focuses the attention on the manifestation of emotions through ECs that denote events, persons, and objects which activate narrator{'}s emotional state.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":9,"sentence":"This is challenging when the model lacks background knowledge about the target.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"Here, we show how background knowledge from Wikipedia can help enhance the performance on stance detection. ","offset":1,"pro":0.25,"labels":"PUR"},{"idx":9,"sentence":"We introduce Wikipedia Stance Detection BERT (WS-BERT) that infuses the knowledge into stance encoding.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":9,"sentence":"Extensive results on three benchmark datasets covering social media discussions and online debates indicate that our model significantly outperforms the state-of-the-art methods on target-specific stance detection, cross-target stance detection, and zero/few-shot stance detection.","offset":3,"pro":0.75,"labels":"CLN"},{"idx":10,"sentence":"By sharing parameters and providing task-independent shared features,","offset":0,"pro":0,"labels":"MTD"},{"idx":10,"sentence":"multi-task deep neural networks are considered one of the most interesting ways for parallel learning from different tasks and domains.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":10,"sentence":"However, fine-tuning on one task may compromise the performance of other tasks or restrict the generalization of the shared learned features. ","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":10,"sentence":"To address this issue, we propose to use task uncertainty to gauge the effect of the shared feature changes on other tasks and prevent the model from overfitting or over-generalizing. ","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":10,"sentence":"We conducted an experiment on 16 text classification tasks, ","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":10,"sentence":"and findings showed that the proposed method consistently improves the performance of the baseline, ","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":10,"sentence":"facilitates the knowledge transfer of learned features to unseen data, and provides explicit control over the generalization of the shared model.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":11,"sentence":"Many recent works in natural language processing have demonstrated ability to assess aspects of mental health from personal discourse.","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"At the same time, pre-trained contextual word embedding models have grown to dominate much of NLP but little is known empirically on how to best apply them for mental health assessment.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":11,"sentence":"Using degree of depression as a case study, we do an empirical analysis on which off-the-shelf language model, individual layers, and combinations of layers seem most promising when applied to human-level NLP tasks. ","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":11,"sentence":"Notably, we find RoBERTa most effective and, despite the standard in past work suggesting the second-to-last or concatenation of the last 4 layers, ","offset":3,"pro":0.5,"labels":"CLN"},{"idx":11,"sentence":"we find layer 19 (sixth-to last) is at least as good as layer 23 when using 1 layer. ","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":11,"sentence":"Further, when using multiple layers, distributing them across the second half (i.e. Layers 12+), rather than last 4, of the 24 layers yielded the most accurate results.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":12,"sentence":"Public opinion in social media is increasingly becoming a critical factor in pandemic control.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":" Understanding the emotions of a population towards vaccinations and COVID-19 may be valuable in convincing members to become vaccinated. ","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":12,"sentence":"We investigated the emotions of Japanese Twitter users towards Tweets related to COVID-19 vaccination. ","offset":2,"pro":0.2222222222222222,"labels":"PUR"},{"idx":12,"sentence":"Using the WRIME dataset, which provides emotion ratings for Japanese Tweets sourced from writers (Tweet posters) and readers, we fine-tuned a BERT model to predict levels of emotional intensity.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":12,"sentence":"This model achieved a training accuracy of $MSE$ = 0.356.","offset":4,"pro":0.4444444444444444,"labels":"RST"},{"idx":12,"sentence":"A separate dataset of 20,254 Japanese Tweets containing COVID-19 vaccine-related keywords was also collected, on which the fine-tuned BERT was used to perform emotion analysis.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":12,"sentence":"Afterwards, a correlation analysis between the extracted emotions and a set of vaccination measures in Japan was conducted.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":12,"sentence":"The results revealed that surprise and fear were the most intense emotions predicted by the model for writers and readers, respectively, on the vaccine-related Tweet dataset. ","offset":7,"pro":0.7777777777777778,"labels":"CLN"},{"idx":12,"sentence":"The correlation analysis also showed that vaccinations were weakly positively correlated with predicted levels of writer joy, writer/reader anticipation, and writer/reader trust.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":13,"sentence":"Domain adaptation methods often exploit domain-transferable input features, a.k.a. pivots. ","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"The task of Aspect and Opinion Term Extraction presents a special challenge for domain transfer: while opinion terms largely transfer across domains, aspects change drastically from one domain to another (e.g. from restaurants to laptops).","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":13,"sentence":"In this paper, we investigate and establish empirically a prior conjecture, which suggests that the linguistic relations connecting opinion terms to their aspects transfer well across domains and therefore can be leveraged for cross-domain aspect term extraction.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":13,"sentence":"We present several analyses supporting this conjecture, via experiments with four linguistic dependency formalisms to represent relation patterns. ","offset":3,"pro":0.5,"labels":"MTD"},{"idx":13,"sentence":"Subsequently, we present an aspect term extraction method that drives models to consider opinion{--}aspect relations via explicit multitask objectives. ","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":13,"sentence":"This method provides significant performance gains, even on top of a prior state-of-the-art linguistically-informed model, which are shown in analysis to stem from the relational pivoting signal.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":14,"sentence":"The main challenge in English-Malay cross-lingual emotion classification is that there are no Malay training emotion corpora. ","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"Given that machine translation could fall short in contextually complex tweets, we only limited machine translation to the word level. ","offset":1,"pro":0.125,"labels":"GAP"},{"idx":14,"sentence":"In this paper, we bridge the language gap between English and Malay through cross-lingual word embeddings constructed using singular value decomposition. ","offset":2,"pro":0.25,"labels":"PUR"},{"idx":14,"sentence":"We pre-trained our hierarchical attention model using English tweets and fine-tuned it using a set of gold standard Malay tweets.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":14,"sentence":"Our model uses significantly less computational resources compared to the language models.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":14,"sentence":"Experimental results show that the performance of our model is better than mBERT in zero-shot learning by 2.4{\\%} and Malay BERT by 0.8{\\%} when a limited number of Malay tweets is available. ","offset":5,"pro":0.625,"labels":"RST"},{"idx":14,"sentence":"In exchange for 6 {--} 7 times less in computational time, our model only lags behind mBERT and XLM-RoBERTa by a margin of 0.9 {--} 4.3 {\\%} in few-shot learning.","offset":6,"pro":0.75,"labels":"RST"},{"idx":14,"sentence":"Also, the word-level attention could be transferred to the Malay tweets accurately using the cross-lingual word embeddings.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":15,"sentence":"Models are increasing in size and complexity in the hunt for SOTA.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"But what if those 2{\\%}increase in performance does not make a difference in a production use case? ","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":15,"sentence":"Maybe benefits from a smaller, faster model outweigh those slight performance gains. ","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":15,"sentence":"Also, equally good performance across languages in multilingual tasks is more important than SOTA results on a single one. ","offset":3,"pro":0.3333333333333333,"labels":"GAP"},{"idx":15,"sentence":"We present the biggest, unified, multilingual collection of sentiment analysis datasets. ","offset":4,"pro":0.4444444444444444,"labels":"PUR"},{"idx":15,"sentence":"We use these to assess 11 models and 80 high-quality sentiment datasets (out of 342 raw datasets collected) in 27 languages and included results on the internally annotated datasets. ","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":15,"sentence":"We deeply evaluate multiple setups, including fine-tuning transformer-based models for measuring performance.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":15,"sentence":"We compare results in numerous dimensions addressing the imbalance in both languages coverage and dataset sizes. ","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":15,"sentence":"Finally, we present some best practices for working with such a massive collection of datasets and models for a multi-lingual perspective.","offset":8,"pro":0.8888888888888888,"labels":"MTD"},{"idx":16,"sentence":"Masked language models (MLMs) are pre-trained with a denoising objective that is in a mismatch with the objective of downstream fine-tuning.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"We propose pragmatic masking and surrogate fine-tuning as two complementing strategies that exploit social cues to drive pre-trained representations toward a broad set of concepts useful for a wide class of social meaning tasks. ","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":16,"sentence":"We test our models on 15 different Twitter datasets for social meaning detection. ","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":16,"sentence":"Our methods achieve 2.34{\\%} $F_1$ over a competitive baseline, while outperforming domain-specific language models pre-trained on large datasets. ","offset":3,"pro":0.5,"labels":"RST"},{"idx":16,"sentence":"Our methods also excel in few-shot learning: with only 5{\\%} of training data (severely few-shot), our methods enable an impressive 68.54{\\%} average $F_1$. ","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":16,"sentence":"The methods are also language agnostic, as we show in a zero-shot setting involving six datasets from three different languages.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":17,"sentence":"Inferring group membership of social media users is of high interest in many domains. ","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"Group membership is typically inferred via network interactions with other members, or by the usage of in-group language.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":17,"sentence":"However, network information is incomplete when users or groups move between platforms, and in-group keywords lose significance as public discussion about a group increases. ","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":17,"sentence":"Similarly, using keywords to filter content and users can fail to distinguish between the various groups that discuss a topic{---}perhaps confounding research on public opinion and narrative trends. ","offset":3,"pro":0.5,"labels":"GAP"},{"idx":17,"sentence":"We present a classifier intended to distinguish members of groups from users discussing a group based on contextual usage of keywords.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":17,"sentence":"We demonstrate the classifier on a sample of community pairs from Reddit and focus on results related to the COVID-19 pandemic.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":18,"sentence":"This paper presents the results of a replication experiment for automatic irony detection in Dutch social media text, investigating both a feature-based SVM classifier, as was done by Van Hee et al. (2017) and and a transformer-based approach.","offset":0,"pro":0,"labels":"PUR"},{"idx":18,"sentence":"In addition to building a baseline model, an important goal of this research is to explore the implementation of common-sense knowledge in the form of implicit sentiment, as we strongly believe that common-sense and connotative knowledge are essential to the identification of irony and implicit meaning in tweets.","offset":1,"pro":0.3333333333333333,"labels":"MTD"},{"idx":18,"sentence":"We show promising results and the presented approach can provide a solid baseline and serve as a staging ground to build on in future experiments for irony detection in Dutch.","offset":2,"pro":0.6666666666666666,"labels":"CLN"},{"idx":19,"sentence":"Research at the intersection of personality psychology, computer science, and linguistics has recently focused increasingly on modeling and predicting personality from language use.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"We report two major improvements in predicting personality traits from text data: (1) to our knowledge, the most comprehensive set of theory-based psycholinguistic features and (2) hybrid models that integrate a pre-trained Transformer Language Model BERT and Bidirectional Long Short-Term Memory (BLSTM) networks trained on within-text distributions ({`}text contours{'}) of psycholinguistic features.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":19,"sentence":"We experiment with BLSTM models (with and without Attention) and with two techniques for applying pre-trained language representations from the transformer model - {`}feature-based{'} and {`}fine-tuning{'}. ","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":19,"sentence":"We evaluate the performance of the models we built on two benchmark datasets that target the two dominant theoretical models of personality: the Big Five Essay dataset (Pennebaker and King, 1999) and the MBTI Kaggle dataset (Li et al., 2018).","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":19,"sentence":"Our results are encouraging as our models outperform existing work on the same datasets.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":19,"sentence":"More specifically, our models achieve improvement in classification accuracy by 2.9{\\%} on the Essay dataset and 8.28{\\%} on the Kaggle MBTI dataset. ","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":19,"sentence":"In addition, we perform ablation experiments to quantify the impact of different categories of psycholinguistic features in the respective personality prediction models.","offset":6,"pro":0.8571428571428571,"labels":"MTD"},{"idx":0,"sentence":"    Chinese Grammatical Error Detection(CGED) aims at detecting grammatical errors in Chinese texts. ","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"One of the main challenges for CGED is the lack of annotated data.","offset":1,"pro":0.08333333333333333,"labels":"GAP"},{"idx":0,"sentence":"To alleviate this problem, previous studies proposed various methods to automatically generate more training samples, which can be roughly categorized into rule-based methods and model-based methods. ","offset":2,"pro":0.16666666666666666,"labels":"BAC"},{"idx":0,"sentence":"The rule-based methods construct erroneous sentences by directly introducing noises into original sentences. ","offset":3,"pro":0.25,"labels":"BAC"},{"idx":0,"sentence":"However, the introduced noises are usually context-independent, which are quite different from those made by humans.","offset":4,"pro":0.3333333333333333,"labels":"GAP"},{"idx":0,"sentence":"The model-based methods utilize generative models to imitate human errors. ","offset":5,"pro":0.4166666666666667,"labels":"BAC"},{"idx":0,"sentence":"The generative model may bring too many changes to the original sentences and generate semantically ambiguous sentences, so it is difficult to detect grammatical errors in these generated sentences.","offset":6,"pro":0.5,"labels":"GAP"},{"idx":0,"sentence":" In addition, generated sentences may be error-free and thus become noisy data. ","offset":7,"pro":0.5833333333333334,"labels":"GAP"},{"idx":0,"sentence":"To handle these problems, we propose CNEG, a novel Conditional Non-Autoregressive Error Generation model for generating Chinese grammatical errors.","offset":8,"pro":0.6666666666666666,"labels":"PUR"},{"idx":0,"sentence":"Specifically, in order to generate a context-dependent error, we first mask a span in a correct text, then predict an erroneous span conditioned on both the masked text and the correct span. ","offset":9,"pro":0.75,"labels":"MTD"},{"idx":0,"sentence":"Furthermore, we filter out error-free spans by measuring their perplexities in the original sentences. ","offset":10,"pro":0.8333333333333334,"labels":"MTD"},{"idx":0,"sentence":"Experimental results show that our proposed method achieves better performance than all compared data augmentation methods on the CGED-2018 and CGED-2020 benchmarks.","offset":11,"pro":0.9166666666666666,"labels":"CLN"},{"idx":1,"sentence":"Recent research has made impressive progress in large-scale multimodal pre-training.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":" In the context of the rapid growth of model size, it is necessary to seek efficient and flexible methods other than finetuning. ","offset":1,"pro":0.2,"labels":"BAC"},{"idx":1,"sentence":"In this paper, we propose to use prompt vectors to align the modalities. ","offset":2,"pro":0.4,"labels":"PUR"},{"idx":1,"sentence":"Our method achieves comparable performance to several other multimodal fusion methods in low-resource settings.","offset":3,"pro":0.6,"labels":"CLN"},{"idx":1,"sentence":"We further show that our method is modular and parameter-efficient for processing tasks involving two or more data modalities.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":2,"sentence":"Machine translation typically adopts an encoder-to-decoder framework, in which the decoder generates the target sentence word-by-word in an auto-regressive manner. ","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"However, the auto-regressive decoder faces a deep-rooted one-pass issue whereby each generated word is considered as one element of the final output regardless of whether it is correct or not. ","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":2,"sentence":"These generated wrong words further constitute the target historical context to affect the generation of subsequent target words. ","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":2,"sentence":"This paper proposes a novel synchronous refinement method to revise potential errors in the generated words by considering part of the target future context. ","offset":3,"pro":0.5,"labels":"PUR"},{"idx":2,"sentence":"Particularly, the proposed approach allows the auto-regressive decoder to refine the previously generated target words and generate the next target word synchronously. ","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":2,"sentence":"The experimental results on three widely-used machine translation tasks demonstrated the effectiveness of the proposed approach.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":3,"sentence":"Previous works leverage context dependence information either from interaction history utterances or previous predicted queries ","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"but fail in taking advantage of both of them since of the mismatch between the natural language and logic-form SQL.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":3,"sentence":" In this work, we propose a History Information Enhanced text-to-SQL model (HIE-SQL) to exploit context dependence information from both history utterances and the last predicted SQL query.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":3,"sentence":" In view of the mismatch, we treat natural language and SQL as two modalities and propose a bimodal pre-trained model to bridge the gap between them. ","offset":3,"pro":0.5,"labels":"MTD"},{"idx":3,"sentence":"Besides, we design a schema-linking graph to enhance connections from utterances and the SQL query to database schema. ","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":3,"sentence":"We show our history information enhanced methods improve the performance of HIE-SQL by a significant margin, which achieves new state-of-the-art results on two context-dependent text-to-SQL benchmarks, the SparC and CoSQL datasets, at the writing time.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":4,"sentence":"Recently, Bert-based models have dominated the research of Chinese spelling correction (CSC). ","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"(1) they have poor performance on multi-typo texts. ","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":4,"sentence":"Such noisy context leads to the declining performance on multi-typo texts. ","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":4,"sentence":" We attempt to address these limitations in this paper. ","offset":3,"pro":0.5,"labels":"PUR"},{"idx":4,"sentence":"Then the correction model is forced to yield similar outputs based on the noisy and original contexts.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":4,"sentence":"Experiments are conducted on widely used benchmarks. ","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":5,"sentence":"The alignment between target and source words often implies the most informative source word for each target word, ","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":" but unfortunately the existing SiMT methods do not explicitly model the alignment to perform the control.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":5,"sentence":"In this paper, we propose Gaussian Multi-head Attention (GMA) to develop a new SiMT policy ","offset":2,"pro":0.4,"labels":"PUR"},{"idx":5,"sentence":"For SiMT policy, GMA models the aligned source position of each target word, ","offset":3,"pro":0.6,"labels":"MTD"},{"idx":5,"sentence":"To integrate the learning of alignment into the translation model, a Gaussian distribution centered on predicted aligned position is introduced as an alignment-related prior, which cooperates with translation-related soft attention to determine the final attention. ","offset":4,"pro":0.8,"labels":"MTD"},{"idx":6,"sentence":"They constitute a structure that contains additional helpful information about the inter-relatedness of the text instances based on the annotations. ","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":".We propose three batch composition strategies to incorporate such information and measure their performance over 14 heterogeneous pairwise sentence classification tasks. ","offset":1,"pro":0.3333333333333333,"labels":"MTD"},{"idx":6,"sentence":"Further, we see that even this baseline procedure can profit from having such structural information in a low-resource setting.","offset":2,"pro":0.6666666666666666,"labels":"RST"},{"idx":7,"sentence":"Pretrained language models can be queried for factual knowledge, with potential applications in knowledge base acquisition and tasks that require inference.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"and recent work has shown that monolingual English language models lack consistency when predicting factual knowledge, ","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":7,"sentence":" In this paper, we extend the analysis of consistency to a multilingual setting.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":7,"sentence":"and investigate (i) whether multilingual language models such as mBERT and XLM-R are more consistent than their monolingual counterparts;and","offset":3,"pro":0.5,"labels":"MTD"},{"idx":7,"sentence":"We find that mBERT is as inconsistent as English BERT in English paraphrases,","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":7,"sentence":"but that both mBERT and XLM-R exhibit a high degree of inconsistency in English and even more so for all the other 45 languages.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":8,"sentence":"With the increasing popularity of online chatting, stickers are becoming important in our online communication. ","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"To tackle these challenges, we propose a multitask learning method ","offset":1,"pro":0.2,"labels":"PUR"},{"idx":8,"sentence":"Ablation study further verifies the effectiveness of each auxiliary task.","offset":2,"pro":0.4,"labels":"CLN"},{"idx":8,"sentence":"Extensive experiments conducted on a recent challenging dataset show that our model can better combine the multimodal information and achieve significantly higher accuracy over strong baselines.","offset":3,"pro":0.6,"labels":"CLN"},{"idx":8,"sentence":"Our code is available at url{https://github.com/nonstopfor/Sticker-Selection.","offset":4,"pro":0.8,"labels":"CTN"},{"idx":9,"sentence":"Modern Chinese characters evolved from 3,000 years ago. ","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"Experts usually need to compare each ancient character to be examined with similar known ones in whole historical periods.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":9,"sentence":"However, it is inevitably limited by human memory and experience, which often cost a lot of time but associations are limited to a small scope. ","offset":2,"pro":0.4,"labels":"GAP"},{"idx":9,"sentence":"In addition, powered by the knowledge of radical systems in ZiNet, this paper introduces glyph similarity measurement between ancient Chinese characters, which could capture similar glyph pairs that are potentially related in origins or semantics.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":9,"sentence":"Finally, qualitative analysis and implicit future applications are presented.","offset":4,"pro":0.8,"labels":"PUR"},{"idx":10,"sentence":"Cross-lingual Entity Typing (CLET) aims at improving the quality of entity type prediction by transferring semantic knowledge learned from rich-resourced languages to low-resourced languages. ","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":" by utilizing multilingual transfer learning via the mixture-of-experts approach, ","offset":1,"pro":0.25,"labels":"MTD"},{"idx":10,"sentence":" Extensive experiments on multi-lingual datasets show that our method significantly outperforms multiple baselines and can robustly handle negative transfer.","offset":2,"pro":0.5,"labels":"CLN"},{"idx":10,"sentence":"We questioned the relationship between language similarity and the performance of CLET. ","offset":3,"pro":0.75,"labels":"MTD"},{"idx":11,"sentence":"In this paper, we propose to use it for data augmentation in NLP.","offset":0,"pro":0,"labels":"PUR"},{"idx":11,"sentence":"Our proposed data augmentation technique, called AMR-DA, converts a sample sentence to an AMR graph, modifies the graph according to various data augmentation policies,","offset":1,"pro":0.2,"labels":"MTD"},{"idx":11,"sentence":"and then generates augmentations from graphs. Our method combines both sentence-level techniques like back translation and token-level techniques like EDA (Easy Data Augmentation).","offset":2,"pro":0.4,"labels":"MTD"},{"idx":11,"sentence":"To evaluate the effectiveness of our method, we apply it to the tasks of semantic textual similarity (STS) and text classification. ","offset":3,"pro":0.6,"labels":"MTD"},{"idx":11,"sentence":"For text classification, AMR-DA outperforms EDA and AEDA and leads to more robust improvements.","offset":4,"pro":0.8,"labels":"RST"},{"idx":12,"sentence":"In this work, we present an extensive study on the use of pre-trained language models for the task of automatic Counter Narrative (CN) generation to fight online hate speech in English. ","offset":0,"pro":0,"labels":"PUR"},{"idx":12,"sentence":" Findings show that autoregressive models combined with stochastic decodings are the most promising. ","offset":1,"pro":0.25,"labels":"RST"},{"idx":12,"sentence":"We find out that a key element for successful out of target' experiments is not an overall similarity with the training data but the presence of a specific subset of training data,","offset":2,"pro":0.5,"labels":"RST"},{"idx":12,"sentence":"We finally introduce the idea of a pipeline based on the addition of an automatic post-editing step to refine generated CNs.","offset":3,"pro":0.75,"labels":"MTD"},{"idx":13,"sentence":" We aim to obtain strong robustness efficiently using fewer steps. ","offset":0,"pro":0,"labels":"PUR"},{"idx":13,"sentence":" Inspired by this, we propose friendly adversarial data augmentation (FADA) to generate friendly adversarial data.","offset":1,"pro":0.25,"labels":"MTD"},{"idx":13,"sentence":"On top of FADA, we propose geometry-aware adversarial training (GAT) to perform adversarial training on friendly adversarial data so that we can save a large number of search steps.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":13,"sentence":"Comprehensive experiments across two widely used datasets and three pre-trained language models demonstrate that GAT can obtain stronger robustness via fewer steps. ","offset":3,"pro":0.75,"labels":"RST"},{"idx":14,"sentence":"For a given task, we introduce a learnable confidence model to detect indicative guidance from context, and further propose a disentangled regularization to mitigate the over-reliance problem.","offset":0,"pro":0,"labels":"MTD"},{"idx":14,"sentence":"To address this issue, we propose Task-guided Disentangled Tuning (TDT) for PLMs,","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":14,"sentence":"which enhances the generalization of representations by disentangling task-relevant signals from the entangled representations.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":14,"sentence":"Experimental results on GLUE and CLUE benchmarks show that TDT gives consistently better results than fine-tuning with different PLMs,","offset":3,"pro":0.5,"labels":"RST"},{"idx":14,"sentence":"and extensive analysis demonstrates the effectiveness and robustness of our method.","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":14,"sentence":"Code is available at https://github.com/lemon0830/TDT.","offset":5,"pro":0.8333333333333334,"labels":"CTN"},{"idx":15,"sentence":"This technique requires a balanced mixture of two ingredients: positive (similar) and negative (dissimilar) samples. ","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"Prior works in the area typically uses a fixed-length negative sample queue,","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":15,"sentence":"The opaque impact of the number of negative samples on performance when employing contrastive learning aroused our in-depth exploration. ","offset":2,"pro":0.2857142857142857,"labels":"BAC"},{"idx":15,"sentence":"We add the prediction layer to the online branch to make the model asymmetric and together with EMA update mechanism of the target branch to prevent the model from collapsing.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":15,"sentence":"Our experiments find that the best results are obtained when the maximum traceable distance is at a certain range, ","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":15,"sentence":"We evaluate the proposed unsupervised MoCoSE on the semantic text similarity (STS) task ","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":15,"sentence":"Source code is available here.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":16,"sentence":"This paper evaluates popular scientific language models in handling (i) short-query texts and (ii) textual neighbors.","offset":0,"pro":0,"labels":"PUR"},{"idx":16,"sentence":"Our experiments showcase the inability to retrieve relevant documents for a short-query text even under the most relaxed conditions. ","offset":1,"pro":0.3333333333333333,"labels":"MTD"},{"idx":16,"sentence":" Further, an exhaustive categorization yields several classes of orthographically and semantically related, partially related and completely unrelated neighbors. ","offset":2,"pro":0.6666666666666666,"labels":"MTD"},{"idx":17,"sentence":"Nested entities are observed in many domains due to their compositionality,","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"A natural solution is to treat the task as a span classification problem.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":17,"sentence":"To fuse these heterogeneous factors, we propose a novel triaffine mechanism including triaffine attention and scoring.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":17,"sentence":".Triaffine scoring interacts with boundaries and span representations for classification.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":17,"sentence":"achieves the state-of-the-art F_1 scores on nested NER datasets GENIA and KBP2017, ","offset":4,"pro":0.8,"labels":"RST"},{"idx":18,"sentence":"Vision-Language Pre-training (VLP) has achieved impressive performance on various cross-modal downstream tasks. ","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"In this paper, we propose an end-to-end unified-modal pre-training framework, namely UNIMO-2, for joint learning on both aligned image-caption data and unaligned image-only and text-only corpus. ","offset":1,"pro":0.2,"labels":"PUR"},{"idx":18,"sentence":" In particular, we propose to conduct grounded learning on both images and texts via a sharing grounded space, which helps bridge unaligned images and texts,","offset":2,"pro":0.4,"labels":"MTD"},{"idx":18,"sentence":"and align the visual and textual semantic spaces on different types of corpora. ","offset":3,"pro":0.6,"labels":"MTD"},{"idx":18,"sentence":"Moreover, benefiting from effective joint modeling of different types of corpora, our model also achieves impressive performance on single-modal visual and textual tasks. ","offset":4,"pro":0.8,"labels":"RST"},{"idx":19,"sentence":"Chinese Spell Checking (CSC) aims to detect and correct Chinese spelling errors, which are mainly caused by the phonological or visual similarity.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"However, there exists a gap between the learned knowledge of PLMs and the goal of CSC task. ","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":19,"sentence":"To address this issue, we propose an Error-driven COntrastive Probability Optimization (ECOPO) framework for CSC task.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":19,"sentence":"ECOPO refines the knowledge representations of PLMs, ","offset":3,"pro":0.5,"labels":"MTD"},{"idx":19,"sentence":" Particularly, ECOPO is model-agnostic","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":19,"sentence":"and it can be combined with existing CSC methods to achieve better performance.","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":0,"sentence":"Data modification, either via additional training datasets, data augmentation, debiasing, and dataset filtering, has been proposed as an effective solution for generalizing to out-of-domain (OOD) inputs, in both natural language processing and computer vision literature.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"However, the effect of data modification on adversarial robustness remains unclear.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":0,"sentence":"In this work, we conduct a comprehensive study of common data modification strategies and evaluate not only their in-domain and OOD performance, but also their adversarial robustness (AR).","offset":2,"pro":0.25,"labels":"MTD"},{"idx":0,"sentence":"We also present results on a two-dimensional synthetic dataset to visualize the effect of each method on the training distribution.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":0,"sentence":"This work serves as an empirical study towards understanding the relationship between generalizing to unseen domains and defending against adversarial perturbations.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":0,"sentence":"Our findings suggest that more data (either via additional datasets or data augmentation) benefits both OOD accuracy and AR.","offset":5,"pro":0.625,"labels":"CLN"},{"idx":0,"sentence":"However, data filtering (previously shown to improve OOD accuracy on natural language inference) hurts OOD accuracy on other tasks such as question answering and image classification.","offset":6,"pro":0.75,"labels":"CLN"},{"idx":0,"sentence":"We provide insights from our experiments to inform future work in this direction.","offset":7,"pro":0.875,"labels":"IMP"},{"idx":1,"sentence":"The MultiWOZ 2.0 dataset has greatly boosted the research on dialogue state tracking (DST). ","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"However, substantial noise has been discovered in its state annotations.","offset":1,"pro":0.1,"labels":"GAP"},{"idx":1,"sentence":"Such noise brings about huge challenges for training DST models robustly.","offset":2,"pro":0.2,"labels":"GAP"},{"idx":1,"sentence":"Although several refined versions, including MultiWOZ 2.1-2.4, have been published recently, there are still lots of noisy labels, especially in the training set.","offset":3,"pro":0.3,"labels":"GAP"},{"idx":1,"sentence":"Besides, it is costly to rectify all the problematic annotations.","offset":4,"pro":0.4,"labels":"GAP"},{"idx":1,"sentence":"In this paper, instead of improving the annotation quality further, we propose a general framework, named ASSIST (lAbel noiSe-robuSt dIalogue State Tracking), to train DST models robustly from noisy labels.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":1,"sentence":"ASSIST first generates pseudo labels for each sample in the training set by using an auxiliary model trained on a small clean dataset, ","offset":6,"pro":0.6,"labels":"MTD"},{"idx":1,"sentence":"then puts the generated pseudo labels and vanilla noisy labels together to train the primary model. ","offset":7,"pro":0.7,"labels":"MTD"},{"idx":1,"sentence":"We show the validity of ASSIST theoretically.","offset":8,"pro":0.8,"labels":"MTD"},{"idx":1,"sentence":"Experimental results also demonstrate that ASSIST improves the joint goal accuracy of DST by up to 28.16% on MultiWOZ 2.0 and 8.41% on MultiWOZ 2.4, compared to using only the vanilla noisy labels.","offset":9,"pro":0.9,"labels":"RST"},{"idx":2,"sentence":"The state-of-the-art models for coreference resolution are based on independent mention pair-wise decisions. ","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"We propose a modelling approach that learns coreference at the document-level and takes global decisions. ","offset":1,"pro":0.2,"labels":"GAP"},{"idx":2,"sentence":"For this purpose, we model coreference links in a graph structure where the nodes are tokens in the text, and the edges represent the relationship between them.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":2,"sentence":"Our model predicts the graph in a non-autoregressive manner, then iteratively refines it based on previous predictions, allowing global dependencies between decisions. ","offset":3,"pro":0.6,"labels":"MTD"},{"idx":2,"sentence":"The experimental results show improvements over various baselines, reinforcing the hypothesis that document-level information improves conference resolution.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":3,"sentence":"Events are considered as the fundamental building blocks of the world.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"Mining event-centric opinions can benefit decision making, people communication, and social good. ","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":3,"sentence":"Unfortunately, there is little literature addressing event-centric opinion mining, although which significantly diverges from the well-studied entity-centric opinion mining in connotation, structure, and expression.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":3,"sentence":"In this paper, we propose and formulate the task of event-centric opinion mining based on event-argument structure and expression categorizing theory. ","offset":3,"pro":0.5,"labels":"PUR"},{"idx":3,"sentence":"We also benchmark this task by constructing a pioneer corpus and designing a two-step benchmark framework. ","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":3,"sentence":"Experiment results show that event-centric opinion mining is feasible and challenging, and the proposed task, dataset, and baselines are beneficial for future studies.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":4,"sentence":"Embedding-based methods have attracted increasing attention in recent entity alignment (EA) studies.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"Although great promise they can offer, there are still several limitations.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":4,"sentence":"The most notable is that they identify the aligned entities based on cosine similarity, ignoring the semantics underlying the embeddings themselves. ","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":4,"sentence":"Furthermore, these methods are shortsighted, heuristically selecting the closest entity as the target and allowing multiple entities to match the same candidate.","offset":3,"pro":0.42857142857142855,"labels":"GAP"},{"idx":4,"sentence":"To address these limitations, we model entity alignment as a sequential decision-making task, in which an agent sequentially decides whether two entities are matched or mismatched based on their representation vectors. ","offset":4,"pro":0.5714285714285714,"labels":"PUR"},{"idx":4,"sentence":"The proposed reinforcement learning (RL)-based entity alignment framework can be flexibly adapted to most embedding-based EA methods. ","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":4,"sentence":"The experimental results demonstrate that it consistently advances the performance of several state-of-the-art methods, with a maximum improvement of 31.1% on Hits@1.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":5,"sentence":"While multilingual training is now an essential ingredient in machine translation (MT) systems, recent work has demonstrated that it has different effects in different multilingual settings, such as many-to-one, one-to-many, and many-to-many learning. ","offset":0,"pro":0,"labels":"GAP"},{"idx":5,"sentence":"These training settings expose the encoder and the decoder in a machine translation model with different data distributions.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":5,"sentence":"In this paper, we examine how different varieties of multilingual training contribute to learning these two components of the MT model. ","offset":2,"pro":0.25,"labels":"PUR"},{"idx":5,"sentence":"Specifically, we compare bilingual models with encoders and/or decoders initialized by multilingual training. ","offset":3,"pro":0.375,"labels":"MTD"},{"idx":5,"sentence":"We show that multilingual training is beneficial to encoders in general, while it only benefits decoders for low-resource languages (LRLs).","offset":4,"pro":0.5,"labels":"CLN"},{"idx":5,"sentence":"We further find the important attention heads for each language pair and compare their correlations during inference. ","offset":5,"pro":0.625,"labels":"MTD"},{"idx":5,"sentence":"Our analysis sheds light on how multilingual translation models work and also enables us to propose methods to improve performance by training with highly related languages. ","offset":6,"pro":0.75,"labels":"IMP"},{"idx":5,"sentence":"Our many-to-one models for high-resource languages and one-to-many models for LRL outperform the best results reported by Aharoni et al. (2019).","offset":7,"pro":0.875,"labels":"RST"},{"idx":6,"sentence":"Chatbot models have achieved remarkable progress in recent years but tend to yield contradictory responses. ","offset":0,"pro":0,"labels":"GAP"},{"idx":6,"sentence":"In this paper, we exploit the advantage of contrastive learning technique to mitigate this issue.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":6,"sentence":"To endow the model with the ability of discriminating contradictory patterns, ","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":6,"sentence":"we minimize the similarity between the target response and contradiction related negative example.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":6,"sentence":"The negative example is generated with learnable latent noise, which receives contradiction related feedback from the pretrained critic. ","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":6,"sentence":"Experimental results show that our method helps to avoid contradictions in response generation while preserving response fluency, outperforming existing methods on both automatic and human evaluation.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":7,"sentence":"Current pre-trained language models (PLM) are typically trained with static data, ignoring that in real-world scenarios, streaming data of various sources may continuously grow.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"This requires PLMs to integrate the information from all the sources in a lifelong manner. ","offset":1,"pro":0.125,"labels":"BAC"},{"idx":7,"sentence":"Although this goal could be achieved by exhaustive pre-training on all the existing data, such a process is known to be computationally expensive.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":7,"sentence":"To this end, we propose ELLE, aiming at efficient lifelong pre-training for emerging data. ","offset":3,"pro":0.375,"labels":"PUR"},{"idx":7,"sentence":"Specifically, ELLE consists of (1) function preserved model expansion, which flexibly expands an existing PLM's width and depth to improve the efficiency of knowledge acquisition; and (2) pre-trained domain prompts, which disentangle the versatile knowledge learned during pre-training and stimulate the proper knowledge for downstream tasks. ","offset":4,"pro":0.5,"labels":"MTD"},{"idx":7,"sentence":"We experiment ELLE with streaming data from 5 domains on BERT and GPT. ","offset":5,"pro":0.625,"labels":"MTD"},{"idx":7,"sentence":"The results show the superiority of ELLE over various lifelong learning baselines in both pre-training efficiency and downstream performances.","offset":6,"pro":0.75,"labels":"CLN"},{"idx":7,"sentence":"The codes are publicly available at https://github.com/thunlp/ELLE.","offset":7,"pro":0.875,"labels":"CTN"},{"idx":8,"sentence":"While cultural backgrounds have been shown to affect linguistic expressions, existing natural language processing (NLP) research on culture modeling is overly coarse-grained and does not examine cultural differences among speakers of the same language. ","offset":0,"pro":0,"labels":"GAP"},{"idx":8,"sentence":"To address this problem and augment NLP models with cultural background features, we collect, annotate, manually validate, and benchmark EnCBP, a finer-grained news-based cultural background prediction dataset in English. ","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":8,"sentence":"Through language modeling (LM) evaluations and manual analyses,","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":8,"sentence":"we confirm that there are noticeable differences in linguistic expressions among five English-speaking countries and across four states in the US. ","offset":3,"pro":0.5,"labels":"CLN"},{"idx":8,"sentence":"Additionally, our evaluations on nine syntactic (CoNLL-2003), semantic (PAWS-Wiki, QNLI, STS-B, and RTE), and psycholinguistic tasks (SST-5, SST-2, Emotion, and Go-Emotions) show that, while introducing cultural background information does not benefit the Go-Emotions task due to text domain conflicts, it noticeably improves deep learning (DL) model performance on other tasks.","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":8,"sentence":"Our findings strongly support the importance of cultural background modeling to a wide variety of NLP tasks and demonstrate the applicability of EnCBP in culture-related research.","offset":5,"pro":0.8333333333333334,"labels":"IMP"},{"idx":9,"sentence":"Prompting language models (LMs) with training examples and task descriptions has been seen as critical to recent successes in few-shot learning. ","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"In this work, we show that finetuning LMs in the few-shot setting can considerably reduce the need for prompt engineering. ","offset":1,"pro":0.2,"labels":"PUR"},{"idx":9,"sentence":"In fact, one can use null prompts, prompts that contain neither task-specific templates nor training examples, and achieve competitive accuracy to manually-tuned prompts across a wide range of tasks. ","offset":2,"pro":0.4,"labels":"MTD"},{"idx":9,"sentence":"While finetuning LMs does introduce new parameters for each downstream task, we show that this memory overhead can be substantially reduced: finetuning only the bias terms can achieve comparable or better accuracy than standard finetuning while only updating 0.1% of the parameters.","offset":3,"pro":0.6,"labels":"CLN"},{"idx":9,"sentence":"All in all, we recommend finetuning LMs for few-shot learning as it is more accurate, robust to different prompts, and can be made nearly as efficient as using frozen LMs.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":10,"sentence":"We propose uFACT (Un-Faithful Alien Corpora Training), a training corpus construction method for data-to-text (d2t) generation models.","offset":0,"pro":0,"labels":"PUR"},{"idx":10,"sentence":"We show that d2t models trained on uFACT datasets generate utterances which represent the semantic content of the data sources more accurately compared to models trained on the target corpus alone. ","offset":1,"pro":0.14285714285714285,"labels":"CLN"},{"idx":10,"sentence":"Our approach is to augment the training set of a given target corpus with alien corpora which have different semantic representations.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":10,"sentence":"We show that while it is important to have faithful data from the target corpus, the faithfulness of additional corpora only plays a minor role. ","offset":3,"pro":0.42857142857142855,"labels":"CLN"},{"idx":10,"sentence":"Consequently, uFACT datasets can be constructed with large quantities of unfaithful data. ","offset":4,"pro":0.5714285714285714,"labels":"CLN"},{"idx":10,"sentence":"We show how uFACT can be leveraged to obtain state-of-the-art results on the WebNLG benchmark using METEOR as our performance metric.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":10,"sentence":"Furthermore, we investigate the sensitivity of the generation faithfulness to the training corpus structure using the PARENT metric, and provide a baseline for this metric on the WebNLG (Gardent et al., 2017) benchmark to facilitate comparisons with future work.","offset":6,"pro":0.8571428571428571,"labels":"MTD"},{"idx":11,"sentence":"We propose the task of culture-specific time expression grounding, i.e. mapping from expressions such as morning in English or Manh{~a in Portuguese to specific hours in the day. ","offset":0,"pro":0,"labels":"PUR"},{"idx":11,"sentence":"We propose 3 language-agnostic methods, one of which achieves promising results on gold standard annotations that we collected for a small number of languages.","offset":1,"pro":0.3333333333333333,"labels":"MTD"},{"idx":11,"sentence":"We then apply this method to 27 languages and analyze the similarities across languages in the grounding of time expressions.","offset":2,"pro":0.6666666666666666,"labels":"MTD"},{"idx":12,"sentence":"Online escort advertisement websites are widely used for advertising victims of human trafficking.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"Domain experts agree that advertising multiple people in the same ad is a strong indicator of trafficking. ","offset":1,"pro":0.125,"labels":"BAC"},{"idx":12,"sentence":"Thus, extracting person names from the text of these ads can provide valuable clues for further analysis. ","offset":2,"pro":0.25,"labels":"BAC"},{"idx":12,"sentence":"However, Named-Entity Recognition (NER) on escort ads is challenging because the text can be noisy, colloquial and often lacking proper grammar and punctuation. ","offset":3,"pro":0.375,"labels":"GAP"},{"idx":12,"sentence":"Most existing state-of-the-art NER models fail to demonstrate satisfactory performance in this task.","offset":4,"pro":0.5,"labels":"GAP"},{"idx":12,"sentence":"In this paper, we propose NEAT (Name Extraction Against Trafficking) for extracting person names.","offset":5,"pro":0.625,"labels":"PUR"},{"idx":12,"sentence":"It effectively combines classic rule-based and dictionary extractors with a contextualized language model to capture ambiguous names (e.g penny, hazel) and adapts to adversarial changes in the text by expanding its dictionary. ","offset":6,"pro":0.75,"labels":"MTD"},{"idx":12,"sentence":"NEAT shows 19% improvement on average in the F1 classification score for name extraction compared to previous state-of-the-art in two domain-specific datasets.","offset":7,"pro":0.875,"labels":"RST"},{"idx":13,"sentence":"Aligning parallel sentences in multilingual corpora is essential to curating data for downstream applications such as Machine Translation.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"In this work, we present OneAligner, an alignment model specially designed for sentence retrieval tasks. ","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":13,"sentence":"This model is able to train on only one language pair and transfers, in a cross-lingual fashion, to low-resource language pairs with negligible degradation in performance. ","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":13,"sentence":"When trained with all language pairs of a large-scale parallel multilingual corpus (OPUS-100), this model achieves the state-of-the-art result on the Tateoba dataset, outperforming an equally-sized previous model by 8.0 points in accuracy while using less than 0.6% of their parallel data. ","offset":3,"pro":0.42857142857142855,"labels":"RST"},{"idx":13,"sentence":"When finetuned on a single rich-resource language pair, be it English-centered or not, our model is able to match the performance of the ones finetuned on all language pairs under the same data budget with less than 2.0 points decrease in accuracy. ","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":13,"sentence":"Furthermore, with the same setup, scaling up the number of rich-resource language pairs monotonically improves the performance, reaching a minimum of 0.4 points discrepancy in accuracy, making it less mandatory to collect any low-resource parallel data. ","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":13,"sentence":"Finally, we conclude through empirical results and analyses that the performance of the sentence alignment task depends mostly on the monolingual and parallel data size, up to a certain size threshold, rather than on what language pairs are used for training or evaluation.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":14,"sentence":"Prior research has discussed and illustrated the need to consider linguistic norms at the community level when studying taboo (hateful/offensive/toxic etc.) language.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"However, a methodology for doing so, that is firmly founded on community language norms is still largely absent. ","offset":1,"pro":0.09090909090909091,"labels":"GAP"},{"idx":14,"sentence":"This can lead both to biases in taboo text classification and limitations in our understanding of the causes of bias. ","offset":2,"pro":0.18181818181818182,"labels":"GAP"},{"idx":14,"sentence":"We propose a method to study bias in taboo classification and annotation where a community perspective is front and center. ","offset":3,"pro":0.2727272727272727,"labels":"PUR"},{"idx":14,"sentence":"This is accomplished by using special classifiers tuned for each community's language. ","offset":4,"pro":0.36363636363636365,"labels":"MTD"},{"idx":14,"sentence":"In essence, these classifiers represent community level language norms. ","offset":5,"pro":0.45454545454545453,"labels":"MTD"},{"idx":14,"sentence":"We use these to study bias and find, for example, biases are largest against African Americans (7/10 datasets and all 3 classifiers examined).","offset":6,"pro":0.5454545454545454,"labels":"MTD"},{"idx":14,"sentence":"In contrast to previous papers we also study other communities and find, for example, strong biases against South Asians.","offset":7,"pro":0.6363636363636364,"labels":"MTD"},{"idx":14,"sentence":"In a small scale user study we illustrate our key idea which is that common utterances, i.e., those with high alignment scores with a community (community classifier confidence scores) are unlikely to be regarded taboo.","offset":8,"pro":0.7272727272727273,"labels":"MTD"},{"idx":14,"sentence":"Annotators who are community members contradict taboo classification decisions and annotations in a majority of instances.","offset":9,"pro":0.8181818181818182,"labels":"CLN"},{"idx":14,"sentence":"This paper is a significant step toward reducing false positive taboo decisions that over time harm minority communities.","offset":10,"pro":0.9090909090909091,"labels":"CTN"},{"idx":15,"sentence":"End-to-end sign language generation models do not accurately represent the prosody in sign language.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"A lack of temporal and spatial variations leads to poor-quality generated presentations that confuse human interpreters. ","offset":1,"pro":0.1,"labels":"BAC"},{"idx":15,"sentence":"In this paper, we aim to improve the prosody in generated sign languages by modeling intensification in a data-driven manner. ","offset":2,"pro":0.2,"labels":"PUR"},{"idx":15,"sentence":"We present different strategies grounded in linguistics of sign language that inform how intensity modifiers can be represented in gloss annotations. ","offset":3,"pro":0.3,"labels":"MTD"},{"idx":15,"sentence":"To employ our strategies, ","offset":4,"pro":0.4,"labels":"PUR"},{"idx":15,"sentence":"we first annotate a subset of the benchmark PHOENIX-14T, a German Sign Language dataset, with different levels of intensification. ","offset":5,"pro":0.5,"labels":"MTD"},{"idx":15,"sentence":"We then use a supervised intensity tagger to extend the annotated dataset and obtain labels for the remaining portion of it. ","offset":6,"pro":0.6,"labels":"MTD"},{"idx":15,"sentence":"This enhanced dataset is then used to train state-of-the-art transformer models for sign language generation.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":15,"sentence":"Human evaluation also indicates a higher preference of the videos generated using our model.","offset":8,"pro":0.8,"labels":"CLN"},{"idx":15,"sentence":"find that our efforts in intensification modeling yield better results when evaluated with automatic metrics","offset":9,"pro":0.9,"labels":"CLN"},{"idx":16,"sentence":"To guide the generation of large pretrained language models (LM), ","offset":0,"pro":0,"labels":"PUR"},{"idx":16,"sentence":"In this work, we propose a novel lightweight framework for controllable GPT2 generation, which utilizes a set of small attribute-specific vectors, called prefixes (Li and Liang, 2021), to steer natural language generation.","offset":1,"pro":0.25,"labels":"PUR"},{"idx":16,"sentence":"Different from Li and Liang (2021), where each prefix is trained independently, we take the relationship among prefixes into consideration and train multiple prefixes simultaneously. ","offset":2,"pro":0.5,"labels":"MTD"},{"idx":16,"sentence":"e propose a novel supervised method and also an unsupervised method to train the prefixes for single-aspect control while the combination of these two methods can achieve multi-aspect contro","offset":3,"pro":0.75,"labels":"MTD"},{"idx":17,"sentence":"Recent work by S{ogaard (2020) showed that, treebank size aside, overlap between training and test graphs (termed leakage) explains more of the observed variation in dependency parsing performance than other explanations.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"In this work we revisit this claim, testing it on more models and languages. ","offset":1,"pro":0.3333333333333333,"labels":"PUR"},{"idx":17,"sentence":"We then propose a more fine-grained measure of such leakage which, unlike the original measure, not only explains but also correlates with observed performance variation. ","offset":2,"pro":0.6666666666666666,"labels":"MTD"},{"idx":18,"sentence":"When a software bug is reported, developers engage in a discussion to collaboratively resolve it.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"While the solution is likely formulated within the discussion, it is often buried in a large amount of text, making it difficult to comprehend and delaying its implementation. ","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":18,"sentence":"We build a corpus for this task using a novel technique for obtaining noisy supervision from repository changes linked to bug reports, with which we establish benchmarks. ","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":18,"sentence":"With automated and human evaluation, we find this task to form an ideal testbed for complex reasoning in long, bimodal dialogue context.","offset":3,"pro":0.5,"labels":"CLN"},{"idx":18,"sentence":"o expedite bug resolution, we propose generating a concise natural language description of the solution by synthesizing relevant content within the discussion, which encompasses both natural language and source cod","offset":4,"pro":0.6666666666666666,"labels":"PUR"},{"idx":18,"sentence":"also design two systems for generating a description during an ongoing discussion by classifying when sufficient context for performing the task emerges in real-time.","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":19,"sentence":"We proposes a novel algorithm, ANTHRO, that inductively extracts over 600K human-written text perturbations in the wild and leverages them for realistic adversarial attack. ","offset":0,"pro":0,"labels":"PUR"},{"idx":19,"sentence":"We find that adversarial texts generated by ANTHRO achieve the best trade-off between (1) attack success rate, (2) semantic preservation of the original text, and (3) stealthiness-i.e. indistinguishable from human writings hence harder to be flagged as suspicious. ","offset":1,"pro":0.25,"labels":"RST"},{"idx":19,"sentence":"Moreover, it outperformed the TextBugger baseline with an increase of 50% and 40% in terms of semantic preservation and stealthiness when evaluated by both layperson and professional human workers. ","offset":2,"pro":0.5,"labels":"RST"},{"idx":19,"sentence":"pecifically, our attacks accomplished around 83% and 91% attack success rates on BERT and RoBERTa, respective","offset":3,"pro":0.75,"labels":"RST"},{"idx":0,"sentence":"Multimodal pre-training with text, layout, and image has achieved SOTA performance for visually rich document understanding tasks recently, which demonstrates the great potential for joint learning across different modalities. ","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"However, the existed research work has focused only on the English domain while neglecting the importance of multilingual generalization.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":0,"sentence":"In this paper, we introduce a human-annotated multilingual form understanding benchmark dataset named XFUND, which includes form understanding samples in 7 languages (Chinese, Japanese, Spanish, French, Italian, German, Portuguese). ","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":0,"sentence":"Meanwhile, we present LayoutXLM, a multimodal pre-trained model for multilingual document understanding, ","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":0,"sentence":"which aims to bridge the language barriers for visually rich document understanding. ","offset":4,"pro":0.5714285714285714,"labels":"PUR"},{"idx":0,"sentence":"Experimental results show that the LayoutXLM model has significantly outperformed the existing SOTA cross-lingual pre-trained models on the XFUND dataset. ","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":0,"sentence":"The XFUND dataset and the pre-trained LayoutXLM model have been publicly available at https://aka.ms/layoutxlm.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":1,"sentence":"Grammatical Error Correction (GEC) aims to automatically detect and correct grammatical errors. ","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"In this aspect, dominant models are trained by one-iteration learning while performing multiple iterations of corrections during inference. ","offset":1,"pro":0.09090909090909091,"labels":"BAC"},{"idx":1,"sentence":"Previous studies mainly focus on the data augmentation approach to combat the exposure bias, which suffers from two drawbacks.","offset":2,"pro":0.18181818181818182,"labels":"GAP"},{"idx":1,"sentence":"First, they simply mix additionally-constructed training instances and original ones to train models, which fails to help models be explicitly aware of the procedure of gradual corrections.","offset":3,"pro":0.2727272727272727,"labels":"GAP"},{"idx":1,"sentence":"Second, they ignore the interdependence between different types of corrections.","offset":4,"pro":0.36363636363636365,"labels":"GAP"},{"idx":1,"sentence":"In this paper, we propose a Type-Driven Multi-Turn Corrections approach for GEC. ","offset":5,"pro":0.45454545454545453,"labels":"PUR"},{"idx":1,"sentence":"Using this approach, from each training instance, we additionally construct multiple training instances, each of which involves the correction of a specific type of errors. ","offset":6,"pro":0.5454545454545454,"labels":"MTD"},{"idx":1,"sentence":"Then, we use these additionally-constructed training instances and the original one to train the model in turn.","offset":7,"pro":0.6363636363636364,"labels":"MTD"},{"idx":1,"sentence":"Experimental results and in-depth analysis show that our approach significantly benefits the model training.","offset":8,"pro":0.7272727272727273,"labels":"CLN"},{"idx":1,"sentence":"Particularly, our enhanced model achieves state-of-the-art single-model performance on English GEC benchmarks. ","offset":9,"pro":0.8181818181818182,"labels":"CLN"},{"idx":1,"sentence":"We release our code at Github.","offset":10,"pro":0.9090909090909091,"labels":"CTN"},{"idx":2,"sentence":"Commonsense reasoning (CSR) requires models to be equipped with general world knowledge.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"While CSR is a language-agnostic process, most comprehensive knowledge sources are restricted to a small number of languages, especially English. ","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":2,"sentence":"Thus, it remains unclear how to effectively conduct multilingual commonsense reasoning (XCSR) for various languages.","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":2,"sentence":"In this work, we propose to use English as a pivot language, utilizing English knowledge sources for our our commonsense reasoning framework via a translate-retrieve-translate (TRT) strategy.","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":2,"sentence":"For multilingual commonsense questions and answer candidates, we collect related knowledge via translation and retrieval from the knowledge in the source language.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":2,"sentence":"The retrieved knowledge is then translated into the target language and integrated into a pre-trained multilingual language model via visible knowledge attention. ","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":2,"sentence":"Then we utilize a diverse of four English knowledge sources to provide more comprehensive coverage of knowledge in different formats.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":2,"sentence":"Extensive results on the XCSR benchmark demonstrate that TRT with external knowledge can significantly improve multilingual commonsense reasoning in both zero-shot and translate-train settings, ","offset":7,"pro":0.7777777777777778,"labels":"CLN"},{"idx":2,"sentence":"consistently outperforming the state-of-the-art by more than 3% on the multilingual commonsense reasoning benchmark X-CSQA and X-CODAH.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":3,"sentence":"Prior studies use one attention mechanism to improve contextual semantic representation learning for implicit discourse relation recognition (IDRR). ","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"However, diverse relation senses may benefit from different attention mechanisms.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":3,"sentence":"We also argue that some linguistic relation in between two words can be further exploited for IDRR.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":3,"sentence":"This paper proposes a Multi-Attentive Neural Fusion (MANF) model to encode and fuse both semantic connection and linguistic evidence for IDRR.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":3,"sentence":"In MANF, we design a Dual Attention Network (DAN) to learn and fuse two kinds of attentive representation for arguments as its semantic connection. ","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":3,"sentence":"We also propose an Offset Matrix Network (OMN) to encode the linguistic relations of word-pairs as linguistic evidence.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":3,"sentence":"Our MANF model achieves the state-of-the-art results on the PDTB 3.0 corpus.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":4,"sentence":"The increasing volume of commercially available conversational agents (CAs) on the market has resulted in users being burdened with learning and adopting multiple agents to accomplish their tasks. ","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"Though prior work has explored supporting a multitude of domains within the design of a single agent, the interaction experience suffers due to the large action space of desired capabilities. ","offset":1,"pro":0.125,"labels":"GAP"},{"idx":4,"sentence":"To address these problems, we introduce a new task BBAI: Black-Box Agent Integration, focusing on combining the capabilities of multiple black-box CAs at scale.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":4,"sentence":"We explore two techniques: question agent pairing and question response pairing aimed at resolving this task.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":4,"sentence":"Leveraging these techniques, we design One For All (OFA), a scalable system that provides a unified interface to interact with multiple CAs. ","offset":4,"pro":0.5,"labels":"MTD"},{"idx":4,"sentence":"Additionally, we introduce MARS: Multi-Agent Response Selection, a new encoder model for question response pairing that jointly encodes user question and agent response pairs.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":4,"sentence":"We demonstrate that OFA is able to automatically and accurately integrate an ensemble of commercially available CAs spanning disparate domains.","offset":6,"pro":0.75,"labels":"CLN"},{"idx":4,"sentence":"Specifically, using the MARS encoder we achieve the highest accuracy on our BBAI task, outperforming strong baselines.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":5,"sentence":"We present two simple modifications for word-level perturbation: Word Replacement considering Length (WR-L) and Compositional Word Replacement (CWR).","offset":0,"pro":0,"labels":"PUR"},{"idx":5,"sentence":"In conventional word replacement, a word in an input is replaced with a word sampled from the entire vocabulary, regardless of the length and context of the target word.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":5,"sentence":"WR-L considers the length of a target word by sampling words from the Poisson distribution.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":5,"sentence":"CWR considers the compositional candidates by restricting the source of sampling to related words that appear in subword regularization.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":5,"sentence":"Experimental results showed that the combination of WR-L and CWR improved the performance of text classification and machine translation.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":6,"sentence":"In recent years, large-scale pre-trained language models (PLMs) have made extraordinary progress in most NLP tasks. ","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"But, in the unsupervised POS tagging task, works utilizing PLMs are few and fail to achieve state-of-the-art (SOTA) performance. ","offset":1,"pro":0.125,"labels":"GAP"},{"idx":6,"sentence":"The recent SOTA performance is yielded by a Guassian HMM variant proposed by He et al. (2018). ","offset":2,"pro":0.25,"labels":"BAC"},{"idx":6,"sentence":"However, as a generative model, HMM makes very strong independence assumptions, making it very challenging to incorporate contexualized word representations from PLMs.","offset":3,"pro":0.375,"labels":"GAP"},{"idx":6,"sentence":"In this work, we for the first time propose a neural conditional random field autoencoder (CRF-AE) model for unsupervised POS tagging.","offset":4,"pro":0.5,"labels":"PUR"},{"idx":6,"sentence":"The discriminative encoder of CRF-AE can straightforwardly incorporate ELMo word representations.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":6,"sentence":"Moreover, inspired by feature-rich HMM, we reintroduce hand-crafted features into the decoder of CRF-AE.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":6,"sentence":"Finally, experiments clearly show that our model outperforms previous state-of-the-art models by a large margin on Penn Treebank and multilingual Universal Dependencies treebank v2.0.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":7,"sentence":"The finetuning of pretrained transformer-based language generation models are typically conducted in an end-to-end manner, where the model learns to attend to relevant parts of the input by itself. ","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"However, there does not exist a mechanism to directly control the model's focus. ","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":7,"sentence":"This work aims to develop a control mechanism by which a user can select spans of context as highlights for the model to focus on, and generate relevant output. ","offset":2,"pro":0.2222222222222222,"labels":"PUR"},{"idx":7,"sentence":"To achieve this goal,","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":7,"sentence":"we augment a pretrained model with trainable focus vectors that are directly applied to the model's embeddings, while the model itself is kept fixed. ","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":7,"sentence":"These vectors, trained on automatic annotations derived from attribution methods, act as indicators for context importance. ","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":7,"sentence":"We test our approach on two core generation tasks: dialogue response generation and abstractive summarization. ","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":7,"sentence":"We also collect evaluation data where the highlight-generation pairs are annotated by humans. ","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":7,"sentence":"Our experiments show that the trained focus vectors are effective in steering the model to generate outputs that are relevant to user-selected highlights.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":8,"sentence":"Opinion summarization focuses on generating summaries that reflect popular subjective information expressed in multiple online reviews.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"While generated summaries offer general and concise information about a particular hotel or product, the information may be insufficient to help the user compare multiple different choices.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":8,"sentence":"Thus, the user may still struggle with the question Which one should I pick? ","offset":2,"pro":0.25,"labels":"GAP"},{"idx":8,"sentence":"In this paper, we propose the comparative opinion summarization task, ","offset":3,"pro":0.375,"labels":"PUR"},{"idx":8,"sentence":"which aims at generating two contrastive summaries and one common summary from two different candidate sets of reviews.","offset":4,"pro":0.5,"labels":"PUR"},{"idx":8,"sentence":"We develop a comparative summarization framework CoCoSum, which consists of two base summarization models that jointly generate contrastive and common summaries.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":8,"sentence":"Experimental results on a newly created benchmark CoCoTrip show that CoCoSum can produce higher-quality contrastive and common summaries than state-of-the-art opinion summarization models.","offset":6,"pro":0.75,"labels":"CLN"},{"idx":8,"sentence":"The dataset and code are available at https://github.com/megagonlabs/cocosum","offset":7,"pro":0.875,"labels":"CTN"},{"idx":9,"sentence":"The recent success of distributed word representations has led to an increased interest in analyzing the properties of their spatial distribution. ","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"Several studies have suggested that contextualized word embedding models do not isotropically project tokens into vector space.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":9,"sentence":"However, current methods designed to measure isotropy, such as average random cosine similarity and the partition score, have not been thoroughly analyzed and are not appropriate for measuring isotropy. ","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":9,"sentence":"We propose IsoScore: a novel tool that quantifies the degree to which a point cloud uniformly utilizes the ambient vector space. ","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":9,"sentence":"Using rigorously designed tests, we demonstrate that IsoScore is the only tool available in the literature that accurately measures how uniformly distributed variance is across dimensions in vector space.","offset":4,"pro":0.5714285714285714,"labels":"CLN"},{"idx":9,"sentence":"Additionally, we use IsoScore to challenge a number of recent conclusions in the NLP literature that have been derived using brittle metrics of isotropy. ","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":9,"sentence":"We caution future studies from using existing tools to measure isotropy in contextualized embedding space as resulting conclusions will be misleading or altogether inaccurate.","offset":6,"pro":0.8571428571428571,"labels":"IMP"},{"idx":10,"sentence":"This means that, even when considered accurate and fluent, MT output can still sound less natural than high quality human translations or text originally written in the target language. ","offset":0,"pro":0,"labels":"GAP"},{"idx":10,"sentence":"Machine translation output notably exhibits lower lexical diversity, and employs constructs that mirror those in the source sentence. ","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":10,"sentence":"In this work we propose a method for training MT systems to achieve a more natural style, i.e. mirroring the style of text originally written in the target language.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":10,"sentence":"Our method tags parallel training data according to the naturalness of the target side by contrasting language models trained on natural and translated data.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":10,"sentence":"Tagging data allows us to put greater emphasis on target sentences originally written in the target language.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":10,"sentence":"Automatic metrics show that the resulting models achieve lexical richness on par with human translations, mimicking a style much closer to sentences originally written in the target language. ","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":10,"sentence":"Furthermore, we find that their output is preferred by human experts when compared to the baseline translations.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":11,"sentence":"We present a generalized paradigm for adaptation of propositional analysis (predicate-argument pairs) to new tasks and domains. ","offset":0,"pro":0,"labels":"PUR"},{"idx":11,"sentence":"We leverage an analogy between stances (belief-driven sentiment) and concerns (topical issues with moral dimensions/endorsements) to produce an explanatory representation. ","offset":1,"pro":0.125,"labels":"MTD"},{"idx":11,"sentence":"A key contribution is the combination of semi-automatic resource building for extraction of domain-dependent concern types (with 2-4 hours of human labor per domain) and an entirely automatic procedure for extraction of domain-independent moral dimensions and endorsement values. ","offset":2,"pro":0.25,"labels":"CTN"},{"idx":11,"sentence":"Prudent (automatic) selection of terms from propositional structures for lexical expansion (via semantic similarity) produces new moral dimension lexicons at three levels of granularity beyond a strong baseline lexicon. ","offset":3,"pro":0.375,"labels":"CLN"},{"idx":11,"sentence":"We develop a ground truth (GT) based on expert annotators and compare our concern detection output to GT, to yield 231% improvement in recall over baseline, with only a 10% loss in precision. ","offset":4,"pro":0.5,"labels":"RST"},{"idx":11,"sentence":"F1 yields 66% improvement over baseline and 97.8% of human performance. ","offset":5,"pro":0.625,"labels":"RST"},{"idx":11,"sentence":"Our lexically based approach yields large savings over approaches that employ costly human labor and model building. ","offset":6,"pro":0.75,"labels":"CLN"},{"idx":11,"sentence":"We provide to the community a newly expanded moral dimension/value lexicon, annotation guidelines, and GT.","offset":7,"pro":0.875,"labels":"CTN"},{"idx":12,"sentence":"Our approach, contextual universal embeddings (CUE), trains LMs on one type of contextual data and adapts to novel context types.","offset":0,"pro":0,"labels":"MTD"},{"idx":12,"sentence":"The model consists of a pretrained neural sentence LM, a BERT-based contextual encoder, and a masked transfomer decoder that estimates LM probabilities using sentence-internal and contextual evidence.","offset":1,"pro":0.1,"labels":"MTD"},{"idx":12,"sentence":"When contextually annotated data is unavailable, our model learns to combine contextual and sentence-internal information using noisy oracle unigram embeddings as a proxy. ","offset":2,"pro":0.2,"labels":"MTD"},{"idx":12,"sentence":"Real context data can be introduced later and used to adapt a small number of parameters that map contextual data into the decoder's embedding space. ","offset":3,"pro":0.3,"labels":"MTD"},{"idx":12,"sentence":"We validate the CUE framework on a NYTimes text corpus with multiple metadata types,","offset":4,"pro":0.4,"labels":"MTD"},{"idx":12,"sentence":"for which the LM perplexity can be lowered from 36.6 to 27.4 by conditioning on context.","offset":5,"pro":0.5,"labels":"RST"},{"idx":12,"sentence":"Bootstrapping a contextual LM with only a subset of the metadata during training retains 85% of the achievable gain. ","offset":6,"pro":0.6,"labels":"RST"},{"idx":12,"sentence":"Training the model initially with proxy context retains 67% of the perplexity gain after adapting to real context. ","offset":7,"pro":0.7,"labels":"RST"},{"idx":12,"sentence":"Furthermore, we can swap one type of pretrained sentence LM for another without retraining the context encoders, by only adapting the decoder model. ","offset":8,"pro":0.8,"labels":"MTD"},{"idx":12,"sentence":"Overall, we obtain a modular framework that allows incremental, scalable training of context-enhanced LMs.","offset":9,"pro":0.9,"labels":"CLN"},{"idx":13,"sentence":"Our cross-lingual framework includes an offline unsupervised construction of a translated UMLS dictionary and a per-document pipeline which identifies UMLS candidate mentions and uses a fine-tuned pretrained transformer language model to filter candidates according to context. ","offset":0,"pro":0,"labels":"MTD"},{"idx":13,"sentence":"We demonstrate results of our approach on both Hebrew and English.","offset":1,"pro":0.3333333333333333,"labels":"MTD"},{"idx":13,"sentence":"We achieve new state-of-the-art (SOTA) results on the Hebrew Camoni corpus, +8.9 F1 on average across three communities in the dataset. ","offset":2,"pro":0.6666666666666666,"labels":"RST"},{"idx":14,"sentence":"Pruning aims to reduce the number of parameters while maintaining performance close to the original network.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"This work proposes a novel self-distillation based pruning strategy, whereby the representational similarity between the pruned and unpruned versions of the same network is maximized. ","offset":1,"pro":0.2,"labels":"PUR"},{"idx":14,"sentence":"We show that the proposed cross-correlation objective for self-distilled pruning implicitly encourages sparse solutions, naturally complementing magnitude-based pruning criteria. ","offset":2,"pro":0.4,"labels":"CLN"},{"idx":14,"sentence":"Self-distilled pruned models also outperform smaller Transformers with an equal number of parameters and are competitive against (6 times) larger distilled networks. ","offset":3,"pro":0.6,"labels":"CLN"},{"idx":14,"sentence":"providing further insights into why self-distilled pruning improves generalization.","offset":4,"pro":0.8,"labels":"IMP"},{"idx":15,"sentence":"Continual relation extraction (CRE) aims to continuously train a model on data with new relations while avoiding forgetting old ones.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"Some previous work has proved that storing a few typical samples of old relations and replaying them when learning new relations can effectively avoid forgetting. ","offset":1,"pro":0.2,"labels":"BAC"},{"idx":15,"sentence":"To solve these challenges, a consistent representation learning method is proposed, which maintains the stability of the relation embedding by adopting contrastive learning and knowledge distillation when replaying memory. ","offset":2,"pro":0.4,"labels":"PUR"},{"idx":15,"sentence":"Then, contrastive replay is conducted of the samples in memory and makes the model retain the knowledge of historical relations through memory knowledge distillation to prevent the catastrophic forgetting of the old task. ","offset":3,"pro":0.6,"labels":"MTD"},{"idx":15,"sentence":"Extensive experiments on FewRel and TACRED datasets show that our method significantly outperforms state-of-the-art baselines and yield strong robustness on the imbalanced dataset.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":16,"sentence":"Open-ended text generation tasks, such as dialogue generation and story completion, require models to generate a coherent continuation given limited preceding context.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"The open-ended nature of these tasks brings new challenges to the neural auto-regressive text generators nowadays. ","offset":1,"pro":0.2,"labels":"BAC"},{"idx":16,"sentence":"To bridge this gap, we propose a novel two-stage method which explicitly arranges the ensuing events in open-ended text generation. ","offset":2,"pro":0.4,"labels":"PUR"},{"idx":16,"sentence":"Experiments on two open-ended text generation tasks demonstrate that our proposed method effectively improves the quality of the generated text, especially in coherence and diversity.","offset":3,"pro":0.6,"labels":"CLN"},{"idx":16,"sentence":"We will release the codes to the community for further exploration.","offset":4,"pro":0.8,"labels":"CTN"},{"idx":17,"sentence":"We investigate Referring Image Segmentation (RIS), which outputs a segmentation map corresponding to the natural language description.","offset":0,"pro":0,"labels":"PUR"},{"idx":17,"sentence":"Addressing RIS efficiently requires considering the interactions happening across visual and linguistic modalities and the interactions within each modality. ","offset":1,"pro":0.14285714285714285,"labels":"MTD"},{"idx":17,"sentence":"We address this limitation by performing all three interactions simultaneously through a Synchronous Multi-Modal Fusion Module (SFM).","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":17,"sentence":"Moreover, to produce refined segmentation masks,","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":17,"sentence":"we propose a novel Hierarchical Cross-Modal Aggregation Module (HCAM), where linguistic features facilitate the exchange of contextual information across the visual hierarchy.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":17,"sentence":"We present thorough ablation studies and validate our approach's performance on four benchmark datasets","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":17,"sentence":"showing considerable performance gains over the existing state-of-the-art (SOTA) methods.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":18,"sentence":"Task weighting, which assigns weights on the including tasks during training, significantly matters the performance of Multi-task Learning (MTL); thus, recently, there has been an explosive interest in it.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"However, existing task weighting methods assign weights only based on the training loss, while ignoring the gap between the training loss and generalization loss.","offset":1,"pro":0.25,"labels":"GAP"},{"idx":18,"sentence":"It degenerates MTL's performance. ","offset":2,"pro":0.5,"labels":"GAP"},{"idx":18,"sentence":"Extensive experiments are conducted to validate the superiority of our proposed method in multi-task text classification.","offset":3,"pro":0.75,"labels":"CLN"},{"idx":19,"sentence":"Weighted decoding methods composed of the pretrained language model (LM) and the controller have achieved promising results for controllable text generation.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"However, these models often suffer from a control strength/fluency trade-off problem as higher control strength is more likely to generate incoherent and repetitive text.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":19,"sentence":"In this paper, we illustrate this trade-off is arisen by the controller imposing the target attribute on the LM at improper positions.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":19,"sentence":"And we propose a novel framework based on existing weighted decoding methods called CAT-PAW, which introduces a lightweight regulator to adjust bias signals from the controller at different decoding positions.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":19,"sentence":"Experiments on positive sentiment control, topic control, and language detoxification show the effectiveness of our CAT-PAW upon 4 SOTA models.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":0,"sentence":"Recent works have shown promising results of prompt tuning in stimulating pre-trained language models (PLMs) for natural language processing (NLP) tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"However, to the best of our knowledge, existing works focus on prompt-tuning generative PLMs that are pre-trained to generate target tokens, such as BERT.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":0,"sentence":"It is still unknown whether and how discriminative PLMs, e.g., ELECTRA, can be effectively prompt-tuned.","offset":2,"pro":0.4,"labels":"GAP"},{"idx":0,"sentence":"Comprehensive experiments on text classification and question answering show that, compared with vanilla fine-tuning, DPT achieves significantly higher performance, and also prevents the unstable problem in tuning large PLMs in both full-set and low-resource settings.","offset":3,"pro":0.6,"labels":"RST"},{"idx":0,"sentence":"In this work, we present DPT, the first prompt tuning framework for discriminative PLMs, which reformulates NLP tasks into a discriminative language modeling problem.","offset":4,"pro":0.8,"labels":"PUR"},{"idx":1,"sentence":"Recall and ranking are two critical steps in personalized news recommendation.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"Most existing news recommender systems conduct personalized news recall and ranking separately with different models.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":1,"sentence":"However, maintaining multiple models leads to high computational cost and poses great challenges to meeting the online latency requirement of news recommender systems.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":1,"sentence":"In order to handle this problem, in this paper we propose UniRec, a unified method for recall and ranking in news recommendation.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":1,"sentence":"In our method, we first infer user embedding for ranking from the historical news click behaviors of a user using a user encoder model.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":1,"sentence":"Then we derive the user embedding for recall from the obtained user embedding for ranking by using it as the attention query to select a set of basis user embeddings which encode different general user interests and synthesize them into a user embedding for recall.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":1,"sentence":"The extensive experiments on benchmark dataset demonstrate that our method can improve both efficiency and effectiveness for recall and ranking in news recommendation.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":2,"sentence":"Procedural text contains rich anaphoric phenomena,","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"yet has not received much attention in NLP.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":2,"sentence":"To fill this gap, we investigate the textual properties of two types of procedural text, recipes and chemical patents, and generalize an anaphora annotation framework developed for the chemical domain for modeling anaphoric phenomena in recipes.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":2,"sentence":"We apply this framework to annotate the RecipeRef corpus with both bridging and coreference relations.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":2,"sentence":"Through comparison to chemical patents, we show the complexity of anaphora resolution in recipes.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":2,"sentence":"We demonstrate empirically that transfer learning from the chemical domain improves resolution of anaphora in recipes,","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":2,"sentence":"suggesting transferability of general procedural knowledge.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":3,"sentence":"Logical reasoning is of vital importance to natural language understanding.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"Previous studies either employ graph-based models to incorporate prior knowledge about logical relations, or introduce symbolic logic into neural models through data augmentation.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":3,"sentence":"These methods, however, heavily depend on annotated training data, and thus suffer from over-fitting and poor generalization problems due to the dataset sparsity.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":3,"sentence":"To address these two problems, in this paper, we propose MERIt, a MEta-path guided contrastive learning method for logical ReasonIng of text, to perform self-supervised pre-training on abundant unlabeled text data.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":3,"sentence":"Two novel strategies serve as indispensable components of our method.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":3,"sentence":"In particular, a strategy based on meta-path is devised to discover the logical structure in natural texts, followed by a counterfactual data augmentation strategy to eliminate the information shortcut induced by pre-training.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":3,"sentence":"The experimental results on two challenging logical reasoning benchmarks, i.e., ReClor and LogiQA, demonstrate that our method outperforms the SOTA baselines with significant improvements.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":4,"sentence":"As more and more pre-trained language models adopt on-cloud deployment, the privacy issues grow quickly, mainly for the exposure of plain-text user data (e.g., search history, medical record, bank account).","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"Privacy-preserving inference of transformer models is on the demand of cloud service users.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":4,"sentence":"To protect privacy, it is an attractive choice to compute only with ciphertext in homomorphic encryption (HE)","offset":2,"pro":0.2857142857142857,"labels":"BAC"},{"idx":4,"sentence":"However, enabling pre-trained models inference on ciphertext data is difficult due to the complex computations in transformer blocks, which are not supported by current HE tools yet.","offset":3,"pro":0.42857142857142855,"labels":"GAP"},{"idx":4,"sentence":"In this work, we introduce THE-X, an approximation approach for transformers, which enables privacy-preserving inference of pre-trained models developed by popular frameworks.","offset":4,"pro":0.5714285714285714,"labels":"PUR"},{"idx":4,"sentence":"THE-X proposes a workflow to deal with complex computation in transformer networks, including all the non-polynomial functions like GELU, softmax, and LayerNorm.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":4,"sentence":"Experiments reveal our proposed THE-X can enable transformer inference on encrypted data for different downstream tasks, all with negligible performance drop but enjoying the theory-guaranteed privacy-preserving advantage.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":5,"sentence":"Many populous countries including India are burdened with a considerable backlog of legal cases.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"Development of automated systems that could process legal documents and augment legal practitioners can mitigate this.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":5,"sentence":"However, there is a dearth of high-quality corpora that is needed to develop such data-driven systems.","offset":2,"pro":0.2,"labels":"GAP"},{"idx":5,"sentence":"The problem gets even more pronounced in the case of low resource languages such as Hindi.","offset":3,"pro":0.3,"labels":"GAP"},{"idx":5,"sentence":"In this resource paper, we introduce the Hindi Legal Documents Corpus (HLDC), a corpus of more than 900K legal documents in Hindi.","offset":4,"pro":0.4,"labels":"PUR"},{"idx":5,"sentence":"Documents are cleaned and structured to enable the development of downstream applications.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":5,"sentence":"Further, as a use-case for the corpus, we introduce the task of bail prediction.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":5,"sentence":"We experiment with a battery of models and propose a Multi-Task Learning (MTL) based model for the same.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":5,"sentence":"MTL models use summarization as an auxiliary task along with bail prediction as the main task.","offset":8,"pro":0.8,"labels":"MTD"},{"idx":5,"sentence":"Experiments with different models are indicative of the need for further research in this area.","offset":9,"pro":0.9,"labels":"CLN"},{"idx":6,"sentence":"Instead, we head back to the original Transformer model and hope to answer the following question: Is the capacity of current models strong enough for document-level translation?","offset":0,"pro":0,"labels":"PUR"},{"idx":6,"sentence":"We evaluate this model and several recent approaches on nine document-level datasets and two sentence-level datasets across six languages.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":6,"sentence":"Experiments show that document-level Transformer models outperforms sentence-level ones and many previous methods in a comprehensive set of metrics, including BLEU, four lexical indices, three newly proposed assistant linguistic indicators, and human evaluation.","offset":2,"pro":0.4,"labels":"RST"},{"idx":6,"sentence":"This paper does not aim at introducing a novel model for document-level neural machine translation.","offset":3,"pro":0.6,"labels":"PUR"},{"idx":6,"sentence":"Interestingly, we observe that the original Transformer with appropriate training techniques can achieve strong results for document translation, even with a length of 2000 words.","offset":4,"pro":0.8,"labels":"MTD"},{"idx":7,"sentence":"Conventional approaches to medical intent detection require fixed pre-defined intent categories.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"However, due to the incessant emergence of new medical intents in the real world, such requirement is not practical.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":7,"sentence":"Considering that it is computationally expensive to store and re-train the whole data every time new data and intents come in,","offset":2,"pro":0.25,"labels":"BAC"},{"idx":7,"sentence":"we propose to incrementally learn emerged intents while avoiding catastrophically forgetting old intents.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":7,"sentence":"We first formulate incremental learning for medical intent detection.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":7,"sentence":"Then, we employ a memory-based method to handle incremental learning.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":7,"sentence":"We further propose to enhance the method with contrast replay networks, which use multilevel distillation and contrast objective to address training data imbalance and medical rare words respectively.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":7,"sentence":"Experiments show that the proposed method outperforms the state-of-the-art model by 5.7% and 9.1% of accuracy on two benchmarks respectively.","offset":7,"pro":0.875,"labels":"RST"},{"idx":8,"sentence":"In this paper, we propose LaPraDoR, a pretrained dual-tower dense retriever that does not require any supervised data for training.","offset":0,"pro":0,"labels":"PUR"},{"idx":8,"sentence":"Specifically, we first present Iterative Contrastive Learning (ICoL) that iteratively trains the query and document encoders with a cache mechanism.","offset":1,"pro":0.14285714285714285,"labels":"MTD"},{"idx":8,"sentence":"We then propose Lexicon-Enhanced Dense Retrieval (LEDR) as a simple yet effective way to enhance dense retrieval with lexical matching.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":8,"sentence":"ICoL not only enlarges the number of negative instances but also keeps representations of cached examples in the same hidden space.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":8,"sentence":"We evaluate LaPraDoR on the recently proposed BEIR benchmark, including 18 datasets of 9 zero-shot text retrieval tasks.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":8,"sentence":"Experimental results show that LaPraDoR achieves state-of-the-art performance compared with supervised dense retrieval models, and further analysis reveals the effectiveness of our training strategy and objectives.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":8,"sentence":"Compared to re-ranking, our lexicon-enhanced approach can be run in milliseconds (22.5x faster) while achieving superior performance.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":9,"sentence":"In recent years, pre-trained language models (PLMs) have been shown to capture factual knowledge from massive texts, which encourages the proposal of PLM-based knowledge graph completion (KGC) models.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"However, these models are still quite behind the SOTA KGC models in terms of performance.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":9,"sentence":"To alleviate these problems, we highlight a more accurate evaluation setting under the open-world assumption (OWA), which manual checks the correctness of knowledge that is not in KGs.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":9,"sentence":"The basic idea is to convert each triple and its support information into natural prompt sentences, which is further fed into PLMs for classification.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":9,"sentence":"In this work, we find two main reasons for the weak performance: (1) Inaccurate evaluation setting.","offset":4,"pro":0.5,"labels":"GAP"},{"idx":9,"sentence":"The evaluation setting under the closed-world assumption (CWA) may underestimate the PLM-based KGC models since they introduce more external knowledge;","offset":5,"pro":0.625,"labels":"GAP"},{"idx":9,"sentence":"(2) Inappropriate utilization of PLMs. Most PLM-based KGC models simply splice the labels of entities and relations as inputs, leading to incoherent sentences that do not take full advantage of the implicit knowledge in PLMs.","offset":6,"pro":0.75,"labels":"GAP"},{"idx":9,"sentence":"Moreover, motivated by prompt tuning, we propose a novel PLM-based KGC model named PKGC.","offset":7,"pro":0.875,"labels":"MTD"},{"idx":10,"sentence":"This work revisits the consistency regularization in self-training and presents explicit and implicit consistency regularization enhanced language model (EICO).","offset":0,"pro":0,"labels":"PUR"},{"idx":10,"sentence":"While the prompt-based fine-tuning methods had advanced few-shot natural language understanding tasks, self-training methods are also being explored.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":10,"sentence":"By employing both explicit and implicit consistency regularization, EICO advances the performance of prompt-based few-shot text classification.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":10,"sentence":"For implicit consistency regularization, we generate pseudo-label from the weakly-augmented view and predict pseudo-label from the strongly-augmented view.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":10,"sentence":"For explicit consistency regularization, we minimize the difference between the prediction of the augmentation view and the prediction of the original view.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":10,"sentence":"We conducted extensive experiments on six text classification datasets","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":10,"sentence":"and found that with sixteen labeled examples, EICO achieves competitive performance compared to existing self-training few-shot learning methods.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":11,"sentence":"Existing studies have demonstrated that adversarial examples can be directly attributed to the presence of non-robust features, which are highly predictive, but can be easily manipulated by adversaries to fool NLP models.","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"In this study, we explore the feasibility of capturing task-specific robust features, while eliminating the non-robust ones by using the information bottleneck theory.","offset":1,"pro":0.3333333333333333,"labels":"PUR"},{"idx":11,"sentence":"Through extensive experiments, we show that the models trained with our information bottleneck-based method are able to achieve a significant improvement in robust accuracy, exceeding performances of all the previously reported defense methods while suffering almost no performance drop in clean accuracy on SST-2, AGNEWS and IMDB datasets.","offset":2,"pro":0.6666666666666666,"labels":"RST"},{"idx":12,"sentence":"Aspect-based sentiment analysis (ABSA) predicts sentiment polarity towards a specific aspect in the given sentence.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"While pre-trained language models such as BERT have achieved great success, incorporating dynamic semantic changes into ABSA remains challenging.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":12,"sentence":"To this end, in this paper, we propose to address this problem by Dynamic Re-weighting BERT (DR-BERT), a novel method designed to learn dynamic aspect-oriented semantics for ABSA.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":12,"sentence":"Specifically, we first take the Stack-BERT layers as a primary encoder to grasp the overall semantic of the sentence and then fine-tune it by incorporating a lightweight Dynamic Re-weighting Adapter (DRA).","offset":3,"pro":0.5,"labels":"MTD"},{"idx":12,"sentence":"Note that the DRA can pay close attention to a small region of the sentences at each step and re-weigh the vitally important words for better aspect-aware sentiment understanding.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":12,"sentence":"Finally, experimental results on three benchmark datasets demonstrate the effectiveness and the rationality of our proposed model and provide good interpretable insights for future semantic modeling.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":13,"sentence":"The task of joint dialog sentiment classification (DSC) and act recognition (DAR) aims to simultaneously predict the sentiment label and act label for each utterance in a dialog.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"which models the explicit dependencies via integrating prediction-level interactions other than semantics-level interactions, more consistent with human intuition.","offset":1,"pro":0.25,"labels":"MTD"},{"idx":13,"sentence":"To implement our framework, we propose a novel model dubbed DARER, which first generates the context-, speaker- and temporal-sensitive utterance representations via modeling SATG, then conducts recurrent dual-task relational reasoning on DRTG, in which process the estimated label distributions act as key clues in prediction-level interactions.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":13,"sentence":"while requiring much less computation resource and costing less training time.","offset":3,"pro":0.75,"labels":"CTN"},{"idx":14,"sentence":"Text semantic matching is a fundamental task that has been widely used in various scenarios, such as community question answering, information retrieval, and recommendation.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"Most state-of-the-art matching models, e.g., BERT, directly perform text comparison by processing each word uniformly.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":14,"sentence":"However, a query sentence generally comprises content that calls for different levels of matching granularity.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":14,"sentence":"Specifically, keywords represent factual information such as action, entity, and event that should be strictly matched, while intents convey abstract concepts and ideas that can be paraphrased into various expressions.","offset":3,"pro":0.375,"labels":"BAC"},{"idx":14,"sentence":"In this work, we propose a simple yet effective training strategy for text semantic matching","offset":4,"pro":0.5,"labels":"PUR"},{"idx":14,"sentence":"in a divide-and-conquer manner by disentangling keywords from intents.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":14,"sentence":"Our approach can be easily combined with pre-trained language models (PLM) without influencing their inference efficiency,","offset":6,"pro":0.75,"labels":"CLN"},{"idx":14,"sentence":"achieving stable performance improvements against a wide range of PLMs on three benchmarks.","offset":7,"pro":0.875,"labels":"RST"},{"idx":15,"sentence":"Off-the-shelf models are widely used by computational social science researchers to measure properties of text, such as sentiment.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"We introduce two lightweight techniques for this scenario,","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":15,"sentence":"and demonstrate that they reliably increase out-of-domain accuracy on four multi-domain text classification datasets when used with linear and contextual embedding models.","offset":2,"pro":0.3333333333333333,"labels":"RST"},{"idx":15,"sentence":"We conclude with recommendations for model producers and consumers, and release models and replication code to accompany this paper.","offset":3,"pro":0.5,"labels":"CTN"},{"idx":15,"sentence":"Here, we treat domain adaptation as a modular process that involves separate model producers and model consumers,","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":15,"sentence":"and show how they can independently cooperate to facilitate more accurate measurements of text.","offset":5,"pro":0.8333333333333334,"labels":"PUR"},{"idx":16,"sentence":"Word-level adversarial attacks have shown success in NLP models, drastically decreasing the performance of transformer-based models in recent years.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"As a countermeasure, adversarial defense has been explored, but relatively few efforts have been made to detect adversarial examples.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":16,"sentence":"However, detecting adversarial examples may be crucial for automated tasks (e.g. review sentiment analysis) that wish to amass information about a certain population and additionally be a step towards a robust defense system.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":16,"sentence":"To this end, we release a dataset for four popular attack methods on four datasets and four models to encourage further research in this field.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":16,"sentence":"Along with it, we propose a competitive baseline based on density estimation that has the highest auc on 29 out of 30 dataset-attack-model combinations.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":16,"sentence":"The source code is released (https://github.com/bangawayoo/adversarial-examples-in-text-classification).","offset":5,"pro":0.8333333333333334,"labels":"CTN"},{"idx":17,"sentence":"Modern NLP classifiers are known to return uncalibrated estimations of class posteriors.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"Existing methods for posterior calibration rescale the predicted probabilities but often have an adverse impact on final classification accuracy, thus leading to poorer generalization.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":17,"sentence":"We propose an end-to-end trained calibrator, Platt-Binning, that directly optimizes the objective while minimizing the difference between the predicted and empirical posterior probabilities.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":17,"sentence":"hus not only reducing the calibration error but also improving task performance. ","offset":3,"pro":0.5,"labels":"RST"},{"idx":17,"sentence":"Our method leverages the sample efficiency of Platt scaling and the verification guarantees of histogram binning, ","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":17,"sentence":"Empirical evaluation of benchmark NLP classification tasks echoes the efficacy of our proposal.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":18,"sentence":"We introduce a novel setup for low-resource task-oriented semantic parsing which incorporates several constraints that may arise in real-world scenarios: (1) lack of similar datasets/models from a related domain, (2) inability to sample useful logical forms directly from a grammar, and (3) privacy requirements for unlabeled natural utterances.","offset":0,"pro":0,"labels":"PUR"},{"idx":18,"sentence":"Our goal is to improve a low-resource semantic parser using utterances collected through user interactions.","offset":1,"pro":0.25,"labels":"PUR"},{"idx":18,"sentence":"In this highly challenging but realistic setting, we investigate data augmentation approaches involving generating a set of structured canonical utterances corresponding to logical forms, before simulating corresponding natural language and filtering the resulting pairs.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":18,"sentence":"We find that such approaches are effective despite our restrictive setup: in a low-resource setting on the complex SMCalFlow calendaring dataset (Andreas et al. 2020), we observe 33% relative improvement over a non-data-augmented baseline in top-1 match.","offset":3,"pro":0.75,"labels":"RST"},{"idx":19,"sentence":"Entity linking (EL) is the task of linking entity mentions in a document to referent entities in a knowledge base (KB).","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"Many previous studies focus on Wikipedia-derived KBs.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":19,"sentence":"There is little work on EL over Wikidata, even though it is the most extensive crowdsourced KB.","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":19,"sentence":"The scale of Wikidata can open up many new real-world applications, but its massive number of entities also makes EL challenging.","offset":3,"pro":0.3333333333333333,"labels":"BAC"},{"idx":19,"sentence":"Wikidata entities and their textual fields are first indexed into a text search engine (e.g., Elasticsearch).","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":19,"sentence":"During inference, given a mention and its context, we use a sequence-to-sequence (seq2seq) model to generate the profile of the target entity, which consists of its title and description.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":19,"sentence":"We use the profile to query the indexed search engine to retrieve candidate entities.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":19,"sentence":"Combined with a simple cross-attention reranker, our complete EL framework achieves state-of-the-art results on three Wikidata-based datasets and strong performance on TACKBP-2010.","offset":7,"pro":0.7777777777777778,"labels":"CLN"},{"idx":19,"sentence":"Our approach complements the traditional approach of using a Wikipedia anchor-text dictionary, enabling us to further design a highly effective hybrid method for candidate retrieval.","offset":8,"pro":0.8888888888888888,"labels":"RST"},{"idx":0,"sentence":"Recent research analyzing the sensitivity of natural language understanding models to word-order perturbations has shown that neural models are surprisingly insensitive to the order of words","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"In this paper, we investigate this phenomenon by developing order-altering perturbations on the order of words, subwords, and characters to analyze their effect on neural models' performance on language understanding tasks","offset":1,"pro":0.25,"labels":"PUR"},{"idx":0,"sentence":"We experiment with measuring the impact of perturbations to the local neighborhood of characters and global position of characters in the perturbed texts and observe that perturbation functions found in prior literature only affect the global ordering while the local ordering remains relatively unperturbed.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":0,"sentence":"We empirically show that neural models, invariant of their inductive biases, pretraining scheme, or the choice of tokenization, mostly rely on the local structure of text to build understanding and make limited use of the global structure.","offset":3,"pro":0.75,"labels":"RST"},{"idx":1,"sentence":"Despite recent success, large neural models often generate factually incorrect text.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":" Compounding this is the lack of a standard automatic evaluation for factuality-it cannot be meaningfully improved if it cannot be measured.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":1,"sentence":"Grounded generation promises a path to solving both of these problems: models draw on a reliable external document (grounding) for factual information, simplifying the challenge of factuality","offset":2,"pro":0.2,"labels":"BAC"},{"idx":1,"sentence":"We study this problem for content transfer, in which generations extend a prompt, using information from factual grounding.","offset":3,"pro":0.3,"labels":"PUR"},{"idx":1,"sentence":"Measuring factuality is also simplified-to factual consistency, testing whether the generation agrees with the grounding, rather than all facts","offset":4,"pro":0.4,"labels":"BAC"},{"idx":1,"sentence":"Yet, without a standard automatic metric for factual consistency, factually grounded generation remains an open problem.","offset":5,"pro":0.5,"labels":"GAP"},{"idx":1,"sentence":"Particularly, this domain allows us to introduce the notion of factual ablation for automatically measuring factual consistency: this captures the intuition that the model should be less likely to produce an output given a less relevant grounding document.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":1,"sentence":"In practice, we measure this by presenting a model with two grounding documents, and the model should prefer to use the more factually relevant one.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":1,"sentence":"We contribute two evaluation sets to measure this.","offset":8,"pro":0.8,"labels":"MTD"},{"idx":1,"sentence":"Applying our new evaluation, we propose multiple novel methods improving over strong baselines","offset":9,"pro":0.9,"labels":"RST"},{"idx":2,"sentence":"State-of-the-art neural models typically encode document-query pairs using cross-attention for re-ranking.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"To this end, models generally utilize an encoder-only (like BERT) paradigm or an encoder-decoder (like T5) approach.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":2,"sentence":"These paradigms, however, are not without flaws, i.e., running the model on all query-document pairs at inference-time incurs a significant computational cost.","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":2,"sentence":"This paper proposes a new training and inference paradigm for re-ranking.","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":2,"sentence":"We propose to finetune a pretrained encoder-decoder model using in the form of document to query generation.","offset":4,"pro":0.4444444444444444,"labels":"PUR"},{"idx":2,"sentence":"Subsequently, we show that this encoder-decoder architecture can be decomposed into a decoder-only language model during inference","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":2,"sentence":"This results in significant inference time speedups since the decoder-only architecture only needs to learn to interpret static encoder embeddings during inference.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":2,"sentence":"Our experiments show that this new paradigm achieves results that are comparable to the more expensive cross-attention ranking approaches while being up to 6.8X faster.","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":2,"sentence":"We believe this work paves the way for more efficient neural rankers that leverage large pretrained models.","offset":8,"pro":0.8888888888888888,"labels":"CTN"},{"idx":3,"sentence":"Question answering-based summarization evaluation metrics must automatically determine whether the QA model's prediction is correct or not, a task known as answer verification","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"In this work, we benchmark the lexical answer verification methods which have been used","offset":1,"pro":0.2,"labels":"PUR"},{"idx":3,"sentence":"by current QA-based metrics as well as two more sophisticated text comparison methods, BERTScore and LERC","offset":2,"pro":0.4,"labels":"MTD"},{"idx":3,"sentence":"We find that LERC out-performs the other methods in some settings while remaining statistically indistinguishable from lexical overlap in others.","offset":3,"pro":0.6,"labels":"RST"},{"idx":3,"sentence":"However, our experiments reveal that improved verification performance does not necessarily translate to overall QA-based metric quality: In some scenarios, using a worse verification method - or using none at all - has comparable performance to using the best verification method, a result that we attribute to properties of the datasets.","offset":4,"pro":0.8,"labels":"RST"},{"idx":4,"sentence":"However, the majority of existing methods with vanilla encoder-decoder structures fail to sufficiently explore all of them","offset":0,"pro":0,"labels":"GAP"},{"idx":4,"sentence":"Based on this concern, we propose a novel method called Prior knowledge and memory Enriched Transformer (PET) for SLT, which incorporates the auxiliary information into vanilla transformer.","offset":1,"pro":0.1111111111111111,"labels":"PUR"},{"idx":4,"sentence":"This paper attacks","offset":2,"pro":0.2222222222222222,"labels":"PUR"},{"idx":4,"sentence":"the challenging problem of sign language translation (SLT), which involves not only visual and textual understanding but also additional prior knowledge learning (i.e. performing style, syntax)","offset":3,"pro":0.3333333333333333,"labels":"BAC"},{"idx":4,"sentence":"Concretely, we develop gated interactive multi-head attention which associates the multimodal representation and global signing style with adaptive gated functions.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":4,"sentence":"One Part-of-Speech (POS) sequence generator relies on the associated information to predict the global syntactic structure, which is thereafter leveraged to guide the sentence generation.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":4,"sentence":"Besides, considering that the visual-textual context information, and additional auxiliary knowledge of a word may appear in more than one video, we design a multi-stream memory structure to obtain higher-quality translations, which stores the detailed correspondence between a word and its various relevant information, leading to a more comprehensive understanding for each word.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":4,"sentence":"We conduct extensive empirical studies on RWTH-PHOENIX-Weather-2014 dataset with both signer-dependent and signer-independent conditions","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":4,"sentence":"The quantitative and qualitative experimental results comprehensively reveal the effectiveness of PET.","offset":8,"pro":0.8888888888888888,"labels":"RST"},{"idx":5,"sentence":"In this paper, we set out to quantify the syntactic capacity of BERT in the evaluation regime of non-context free patterns, as occurring in Dutch.","offset":0,"pro":0,"labels":"PUR"},{"idx":5,"sentence":"We devise a test suite based on a mildly context-sensitive formalism, from which we derive grammars that capture the linguistic phenomena of control verb nesting and verb raising.","offset":1,"pro":0.25,"labels":"MTD"},{"idx":5,"sentence":"The grammars, paired with a small lexicon, provide us with a large collection of naturalistic utterances, annotated with verb-subject pairings, that serve as the evaluation test bed for an attention-based span selection probe.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":5,"sentence":"Our results, backed by extensive analysis, suggest that the models investigated fail in the implicit acquisition of the dependencies examined.","offset":3,"pro":0.75,"labels":"RST"},{"idx":6,"sentence":"Character-based neural machine translation models have become the reference models for cognate prediction, a historical linguistics task.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"So far, all linguistic interpretations about latent information captured by such models have been based on external analysis (accuracy, raw results, errors)","offset":1,"pro":0.25,"labels":"BAC"},{"idx":6,"sentence":"In this paper, we investigate what probing can tell us about both models and previous interpretations","offset":2,"pro":0.5,"labels":"PUR"},{"idx":6,"sentence":"and learn that though our models store linguistic and diachronic information, they do not achieve it in previously assumed ways.","offset":3,"pro":0.75,"labels":"RST"},{"idx":7,"sentence":"Automatic Readability Assessment (ARA), the task of assigning a reading level to a text, is traditionally treated as a classification problem in NLP research.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"In this paper, we propose the first neural, pairwise ranking approach to ARA and compare it with existing classification, regression, and (non-neural) ranking methods.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":7,"sentence":"We establish the performance of our approach by conducting experiments with three English, one French and one Spanish datasets","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":7,"sentence":"We demonstrate that our approach performs well in monolingual single/cross corpus testing scenarios and achieves a zero-shot cross-lingual ranking accuracy of over 80% for both French and Spanish when trained on English data","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":7,"sentence":"Additionally, we also release a new parallel bilingual readability dataset, that could be useful for future research.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":7,"sentence":"o our knowledge, this paper proposes the first neural pairwise ranking model for ARA,","offset":5,"pro":0.7142857142857143,"labels":"PUR"},{"idx":7,"sentence":"and shows the first results of cross-lingual, zero-shot evaluation of ARA with neural models.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":8,"sentence":"However, beam search has been shown to amplify demographic biases exhibited by a model","offset":0,"pro":0,"labels":"GAP"},{"idx":8,"sentence":"We aim to address this, focusing on gender bias resulting from systematic errors in grammatical gender translation.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":8,"sentence":"Almost all prior work on this problem adjusts the training data or the model itself","offset":2,"pro":0.2857142857142857,"labels":"BAC"},{"idx":8,"sentence":"By contrast, our approach changes only the inference procedure","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":8,"sentence":"We constrain beam search to improve gender diversity in n-best lists, and rerank n-best lists using gender features obtained from the source sentence","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":8,"sentence":"Combining these strongly improves WinoMT gender translation accuracy for three language pairs without additional bilingual data or retraining.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":8,"sentence":"We also demonstrate our approach's utility for consistently gendering named entities, and its flexibility to handle new gendered language beyond the binary.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":9,"sentence":"Few-shot dialogue state tracking (DST) is a realistic solution to this problem","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"In this paper, we hypothesize that dialogue summaries are essentially unstructured dialogue states","offset":1,"pro":0.1111111111111111,"labels":"MTD"},{"idx":9,"sentence":"hence, we propose to reformulate dialogue state tracking as a dialogue summarization problem","offset":2,"pro":0.2222222222222222,"labels":"PUR"},{"idx":9,"sentence":"To elaborate","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":9,"sentence":"we train a text-to-text language model with synthetic template-based dialogue summaries, generated by a set of rules from the dialogue states","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":9,"sentence":"Then, the dialogue states can be recovered by inversely applying the summary generation rules.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":9,"sentence":"We empirically show that our method DS2 outperforms previous works on few-shot DST in MultiWoZ 2.0 and 2.1, in both cross-domain and multi-domain settings","offset":6,"pro":0.6666666666666666,"labels":"RST"},{"idx":9,"sentence":"Our method also exhibits vast speedup during both training and inference as it can generate all states at once","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":9,"sentence":"Finally, based on our analysis, we discover that the naturalness of the summary templates plays a key role for successful training.","offset":8,"pro":0.8888888888888888,"labels":"RST"},{"idx":10,"sentence":"Recognizing the language of ambiguous texts has become a main challenge in language identification (LID","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"When using multilingual applications, users have their own language preferences, which can be regarded as external knowledge for LID.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":10,"sentence":"Nevertheless, current studies do not consider the inter-personal variations due to the lack of user annotated training data","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":10,"sentence":"To fill this gap, we introduce preference-aware LID and propose a novel unsupervised learning strategy","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":10,"sentence":"Concretely, we construct pseudo training set for each user by extracting training samples from a standard LID corpus according to his/her historical language distribution.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":10,"sentence":"Besides, we contribute the first user labeled LID test set called U-LID. Experimental results reveal that our model can incarnate user traits and significantly outperforms existing LID systems on handling ambiguous texts.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":10,"sentence":"Our code and benchmark have been released","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":11,"sentence":"The environmental costs of research are progressively important to the NLP community and their associated challenges are increasingly debated.","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"In this work, we analyse the carbon cost (measured as CO2-equivalent) associated with journeys made by researchers attending in-person NLP conferences","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":11,"sentence":"We obtain the necessary data by text-mining all publications from the ACL anthology available at the time of the study (n=60,572) and extracting information about an author's affiliation, including their address.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":11,"sentence":"This allows us to estimate the corresponding carbon cost and compare it to previously known values for training large models","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":11,"sentence":"Further, we look at the benefits of in-person conferences by demonstrating that they can increase participation diversity by encouraging attendance from the region surrounding the host country","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":11,"sentence":"We show how the trade-off between carbon cost and diversity of an event depends on its location and type.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":11,"sentence":"Our aim is to foster further discussion on the best way to address the joint issue of emissions and diversity in the future","offset":6,"pro":0.8571428571428571,"labels":"PUR"},{"idx":12,"sentence":"Research Replication Prediction (RRP) is the task of predicting whether a published research result can be replicated or not.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"Building an interpretable neural text classifier for RRP promotes the understanding of why a research paper is predicted as replicable or non-replicable and therefore makes its real-world application more reliable and trustworthy.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":12,"sentence":"However, the prior works on model interpretation mainly focused on improving the model interpretability at the word/phrase level, which are insufficient especially for long research papers in RRP.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":12,"sentence":"Furthermore, the existing methods cannot utilize a large size of unlabeled dataset to further improve the model interpretability.","offset":3,"pro":0.42857142857142855,"labels":"GAP"},{"idx":12,"sentence":"To address these limitations, we aim to build an interpretable neural model which can provide sentence-level explanations and apply weakly supervised approach to further leverage the large corpus of unlabeled datasets to boost the interpretability in addition to improving prediction performance as existing works have done","offset":4,"pro":0.5714285714285714,"labels":"PUR"},{"idx":12,"sentence":"In this work, we propose the Variational Contextual Consistency Sentence Masking (VCCSM) method to automatically extract key sentences based on the context in the classifier, using both labeled and unlabeled datasets.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":12,"sentence":"Results of our experiments on RRP along with European Convention of Human Rights (ECHR) datasets demonstrate that VCCSM is able to improve the model interpretability for the long document classification tasks using the area over the perturbation curve and post-hoc accuracy as evaluation metrics.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":13,"sentence":"In this paper, we introduce a new task called synesthesia detection, which aims to extract the sensory word of a sentence, and to predict the original and synesthetic sensory modalities of the corresponding sensory word.","offset":0,"pro":0,"labels":"PUR"},{"idx":13,"sentence":"Synesthesia refers to the description of perceptions in one sensory modality through concepts from other modalities","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":13,"sentence":" It involves not only a linguistic phenomenon, but also a cognitive phenomenon structuring human thought and action, which makes it become a bridge between figurative linguistic phenomenon and abstract cognition, and thus be helpful to understand the deep semantics.","offset":2,"pro":0.2222222222222222,"labels":"BAC"},{"idx":13,"sentence":"Based on this dataset, we propose a family of strong and representative baseline models.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":13,"sentence":"To address this","offset":4,"pro":0.4444444444444444,"labels":"PUR"},{"idx":13,"sentence":"we construct a large-scale human-annotated Chinese synesthesia dataset, which contains 7,217 annotated sentences accompanied by 187 sensory words.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":13,"sentence":"Upon these baselines, we further propose a radical-based neural network model to identify the boundary of the sensory word, and to jointly detect the original and synesthetic sensory modalities for the word.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":13,"sentence":"Through extensive experiments, we observe that the importance of the proposed task and dataset can be verified by the statistics and progressive performances.","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":13,"sentence":"In addition, our proposed model achieves state-of-the-art results on the synesthesia dataset","offset":8,"pro":0.8888888888888888,"labels":"CTN"},{"idx":14,"sentence":"We introduce the task of implicit offensive text detection in dialogues, where a statement may have either an offensive or non-offensive interpretation, depending on the listener and context.","offset":0,"pro":0,"labels":"PUR"},{"idx":14,"sentence":"Experiments using the data show that state-of-the-art methods of offense detection perform poorly when asked to detect implicitly offensive statements, achieving only {sim 11% accuracy","offset":1,"pro":0.125,"labels":"RST"},{"idx":14,"sentence":"In contrast to existing offensive text detection datasets, SLIGHT features human-annotated chains of reasoning which describe the mental process by which an offensive interpretation can be reached from each ambiguous statement.","offset":2,"pro":0.25,"labels":"RST"},{"idx":14,"sentence":"We explore the potential for a multi-hop reasoning approach by utilizing existing entailment models to score the probability of these chains","offset":3,"pro":0.375,"labels":"MTD"},{"idx":14,"sentence":"We argue that reasoning is crucial for understanding this broader class of offensive utterances","offset":4,"pro":0.5,"labels":"MTD"},{"idx":14,"sentence":"and release SLIGHT, a dataset to support research on this task","offset":5,"pro":0.625,"labels":"MTD"},{"idx":14,"sentence":"and show that even naive reasoning models can yield improved performance in most situations","offset":6,"pro":0.75,"labels":"RST"},{"idx":14,"sentence":"Analysis of the chains provides insight into the human interpretation process and emphasizes the importance of incorporating additional commonsense knowledge","offset":7,"pro":0.875,"labels":"CTN"},{"idx":15,"sentence":"Dialogue safety problems severely limit the real-world deployment of neural conversational models and have attracted great research interests recently.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"However, dialogue safety problems remain under-defined and the corresponding dataset is scarce.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":15,"sentence":"We propose a taxonomy for dialogue safety specifically designed to capture unsafe behaviors in human-bot dialogue settings, with focuses on context-sensitive unsafety, which is under-explored in prior works.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":15,"sentence":"To spur research in this direction, we compile DiaSafety, a dataset with rich context-sensitive unsafe examples.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":15,"sentence":"Experiments show that existing safety guarding tools fail severely on our dataset.","offset":4,"pro":0.5,"labels":"RST"},{"idx":15,"sentence":"As a remedy, we train a dialogue safety classifier to provide a strong baseline for context-sensitive dialogue unsafety detection.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":15,"sentence":"With our classifier, we perform safety evaluations on popular conversational models","offset":6,"pro":0.75,"labels":"MTD"},{"idx":15,"sentence":" show that existing dialogue systems still exhibit concerning context-sensitive safety problems.","offset":7,"pro":0.875,"labels":"RST"},{"idx":16,"sentence":"Chinese Word Segmentation (CWS) intends to divide a raw sentence into words through sequence labeling","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"Thinking in reverse, CWS can also be viewed as a process of grouping a sequence of characters into a sequence of words","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":16,"sentence":"In such a way, CWS is reformed as a separation inference task in every adjacent character pair.","offset":2,"pro":0.2222222222222222,"labels":"BAC"},{"idx":16,"sentence":"Since every character is either connected or not connected to the others, the tagging schema is simplified as two tags Connection (C) or NoConnection (NC)","offset":3,"pro":0.3333333333333333,"labels":"BAC"},{"idx":16,"sentence":"Therefore, bigram is specially tailored for C-NC to model the separation state of every two consecutive characters","offset":4,"pro":0.4444444444444444,"labels":"BAC"},{"idx":16,"sentence":"Our Separation Inference (SpIn) framework is evaluated on five public datasets","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":16,"sentence":"is demonstrated to work for machine learning and deep learning models","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":16,"sentence":"and outperforms state-of-the-art performance for CWS in all experiments.","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":16,"sentence":"Performance boosts on Japanese Word Segmentation (JWS) and Korean Word Segmentation (KWS) further prove the framework is universal and effective for East Asian Languages.","offset":8,"pro":0.8888888888888888,"labels":"CTN"},{"idx":17,"sentence":"Word Segmentation is a fundamental step for understanding Chinese language","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"Previous neural approaches for unsupervised Chinese Word Segmentation (CWS) only exploits shallow semantic information, which can miss important context.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":17,"sentence":"Large scale Pre-trained language models (PLM) have achieved great success in many areas because of its ability to capture the deep contextual semantic relation.","offset":2,"pro":0.4,"labels":"BAC"},{"idx":17,"sentence":"In this paper, we propose to take advantage of the deep semantic information embedded in PLM (e.g., BERT) with a self-training manner, which iteratively probes and transforms the semantic information in PLM into explicit word segmentation ability","offset":3,"pro":0.6,"labels":"PUR"},{"idx":17,"sentence":"Extensive experiment results show that our proposed approach achieves state-of-the-art F1 score on two CWS benchmark datasets.","offset":4,"pro":0.8,"labels":"RST"},{"idx":18,"sentence":"The ability to recognize analogies is fundamental to human cognition.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"Existing benchmarks to test word analogy do not reveal the underneath process of analogical reasoning of neural models","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":18,"sentence":"Holding the belief that models capable of reasoning should be right for the right reasons, we propose a first-of-its-kind Explainable Knowledge-intensive Analogical Reasoning benchmark (E-KAR","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":18,"sentence":"Our benchmark consists of 1,655 (in Chinese) and 1,251 (in English) problems sourced from the Civil Service Exams, which require intensive background knowledge to solve","offset":3,"pro":0.5,"labels":"MTD"},{"idx":18,"sentence":"More importantly, we design a free-text explanation scheme to explain whether an analogy should be drawn, and manually annotate them for each and every question and candidate answer","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":18,"sentence":"Empirical results suggest that this benchmark is very challenging for some state-of-the-art models for both explanation generation and analogical question answering tasks, which invites further research in this area.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":19,"sentence":"Relation linking (RL) is a vital module in knowledge-based question answering (KBQA) systems","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":" It aims to link the relations expressed in natural language (NL) to the corresponding ones in knowledge graph (KG)","offset":1,"pro":0.125,"labels":"BAC"},{"idx":19,"sentence":"Existing methods mainly rely on the textual similarities between NL and KG to build relation links","offset":2,"pro":0.25,"labels":"BAC"},{"idx":19,"sentence":"Due to the ambiguity of NL and the incompleteness of KG, many relations in NL are implicitly expressed, and may not link to a single relation in KG, which challenges the current methods","offset":3,"pro":0.375,"labels":"GAP"},{"idx":19,"sentence":"In this paper, we propose an implicit RL method called ImRL, which links relation phrases in NL to relation paths in KG","offset":4,"pro":0.5,"labels":"PUR"},{"idx":19,"sentence":"To find proper relation paths, we propose a novel path ranking model that aligns not only textual information in the word embedding space but also structural information in the KG embedding space between relation phrases in NL and relation paths in KG","offset":5,"pro":0.625,"labels":"MTD"},{"idx":19,"sentence":"Besides, we leverage a gated mechanism with attention to inject prior knowledge from external paraphrase dictionaries to address the relation phrases with vague meaning","offset":6,"pro":0.75,"labels":"MTD"},{"idx":19,"sentence":"Our experiments on two benchmark and a newly-created datasets show that ImRL significantly outperforms several state-of-the-art methods, especially for implicit RL","offset":7,"pro":0.875,"labels":"RST"},{"idx":0,"sentence":" This paper describes a novel framework to estimate the data quality of a collection of product descriptions to identify required relevant information for accurate product listing classification for tax-code assignment. ","offset":0,"pro":0,"labels":"PUR"},{"idx":0,"sentence":"Our Data Quality Estimation (DQE) framework consists of a Question Answering (QA) based attribute value extraction model to identify missing attributes and a classification model to identify bad quality records. ","offset":1,"pro":0.25,"labels":"MTD"},{"idx":0,"sentence":"We show that our framework can accurately predict the quality of product descriptions. ","offset":2,"pro":0.5,"labels":"RST"},{"idx":0,"sentence":"In addition to identifying low-quality product listings, our framework can also generate a detailed report at a category level showing missing product information resulting in a better customer experience.","offset":3,"pro":0.75,"labels":"CTN"},{"idx":1,"sentence":"In the real world, annotated data typically contains noise caused by a variety of factors such as task difficulty, annotator experience, and annotator bias.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"Label quality is critical for label validation tasks;","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":1,"sentence":"however, correcting for noise by collecting more data is often costly.","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":1,"sentence":" In this paper, we propose a contrastive meta-learning framework (CML) to address the challenges introduced by noisy annotated data, specifically in the context of natural language processing. ","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":1,"sentence":"CML combines contrastive and meta learning to improve the quality of text feature representations. ","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":1,"sentence":"Meta-learning is also used to generate confidence scores to assess label quality. ","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":1,"sentence":"We demonstrate that a model built on CML-filtered data outperforms a model built on clean data.","offset":6,"pro":0.6666666666666666,"labels":"RST"},{"idx":1,"sentence":"Furthermore, we perform experiments on deidentified commercial voice assistant datasets ","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":1,"sentence":"and demonstrate that our model outperforms several SOTA approaches.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":2,"sentence":"Ensuring relevance quality in product search is a critical task as it impacts the customer's ability to find intended products in the short-term as well as the general perception and trust of the e-commerce system in the long term.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":" In this work we leverage a high-precision cross-encoder BERT model for semantic similarity between customer query and products and survey its effectiveness for three ranking applications where offline-generated scores could be used: (1) as an offline metric for estimating relevance quality impact, (2) as a re-ranking feature covering head/torso queries, and (3) as a training objective for optimization.","offset":1,"pro":0.25,"labels":"PUR"},{"idx":2,"sentence":"We present results on effectiveness of this strategy for the large e-commerce setting, ","offset":2,"pro":0.5,"labels":"MTD"},{"idx":2,"sentence":"which has general applicability for choice of other high-precision models and tasks in ranking.","offset":3,"pro":0.75,"labels":"RST"},{"idx":3,"sentence":"We model products' reviews to generate comparative responses consisting of positive and negative experiences regarding the product.","offset":0,"pro":0,"labels":"PUR"},{"idx":3,"sentence":"Specifically, we generate a single-sentence, comparative response from a given positive and a negative opinion. ","offset":1,"pro":0.25,"labels":"MTD"},{"idx":3,"sentence":"We contribute the first dataset for this task of Comparative Snippet Generation ","offset":2,"pro":0.5,"labels":"CTN"},{"idx":3,"sentence":"from contrasting opinions regarding a product, and an analysis of performance of a pre-trained BERT model to generate such snippets.","offset":3,"pro":0.75,"labels":"MTD"},{"idx":4,"sentence":"Automatic monitoring systems for inappropriate user-generated messages have been found to be effective in reducing human operation costs in Consumer to Consumer (C2C) marketplace services, in which customers send messages directly to other customers.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"We propose a lightweight neural network","offset":1,"pro":0.3333333333333333,"labels":"PUR"},{"idx":4,"sentence":"that takes a conversation as input, which we deployed to a production service.","offset":2,"pro":0.6666666666666666,"labels":"MTD"},{"idx":5,"sentence":"In E-commerce search, spelling correction plays an important role to find desired products for customers in processing user-typed search queries.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"However, resolving phonetic errors is a critical but much overlooked area. ","offset":1,"pro":0.2,"labels":"GAP"},{"idx":5,"sentence":"In this work, we propose a generalized spelling correction system integrating phonetics to address phonetic errors in E-commerce search without additional latency cost.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":5,"sentence":"Using India (IN) E-commerce market for illustration, ","offset":3,"pro":0.6,"labels":"MTD"},{"idx":5,"sentence":"This phonetic spelling correction system has been deployed to production, currently serving hundreds of millions of customers.","offset":4,"pro":0.8,"labels":"CTN"},{"idx":6,"sentence":"In recent years, large pretrained models have been used in dialogue systems to improve successful task completion rates.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"However, lack of reasoning capabilities of dialogue platforms make it difficult to provide relevant and fluent responses, unless the designers of a conversational experience spend a considerable amount of time implementing these capabilities in external rule based modules. ","offset":1,"pro":0.25,"labels":"GAP"},{"idx":6,"sentence":".Our method includes a synthetic data generation mechanism which helps the model learn logical relations, such as comparison between list of numerical values, inverse relations (and negation), inclusion and exclusion for categorical attributes, and application of a combination of attributes over both numerical and categorical values, and spoken form for numerical values, without need for additional training data. ","offset":2,"pro":0.5,"labels":"MTD"},{"idx":6,"sentence":"We observe that transformer based models such as UnifiedQA-T5 can be fine-tuned to perform logical reasoning (such as numerical and categorical attributes' comparison) over attributes seen at training time (e.g., accuracy of 90%+ for comparison of smaller than kmax=5 values over heldout test dataset).","offset":3,"pro":0.75,"labels":"CLN"},{"idx":7,"sentence":"For this, classical WIE methods leverage the Document Object Model (DOM) tree of a website. ","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"To address this challenge we propose to reformulate WIE as a context-aware Webpage Object Detection task. ","offset":1,"pro":0.3333333333333333,"labels":"PUR"},{"idx":7,"sentence":" To study the approach we collect a new large-scale datase of e-commerce websites for which we manually annotate every web element with four labels: product price, product title, product image and others.","offset":2,"pro":0.6666666666666666,"labels":"MTD"},{"idx":8,"sentence":"Online marketplaces use attribute-value pairs, such as brand, size, size type, color, etc. to help define important and relevant facts about a listing.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"These help buyers to curate their search results using attribute filtering and overall create a richer experience.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":8,"sentence":"Although their critical importance for listings' discoverability, getting sellers to input tens of different attribute-value pairs per listing is costly and often results in missing information. ","offset":2,"pro":0.4,"labels":"GAP"},{"idx":8,"sentence":" In this paper we demonstrate using a Text-to-Text hierarchical multi-label ranking model framework to predict the most relevant attributes per listing, along with their expected values, using historic user behavioral data.","offset":3,"pro":0.6,"labels":"PUR"},{"idx":8,"sentence":"Specifically for eBay's case we show that using this model can improve the relevancy of the attribute extraction process by 33.2% compared to the current highly-optimized production system. ","offset":4,"pro":0.8,"labels":"RST"},{"idx":9,"sentence":"In this paper, we build a benchmark with annotations for both evidence selection and answer generation covering 6 information sources.","offset":0,"pro":0,"labels":"PUR"},{"idx":9,"sentence":"Handling all sources within one single model can produce comparable confidence scores across sources and combining multiple sources for training always helps, even for sources with totally different structures.","offset":1,"pro":0.14285714285714285,"labels":"RST"},{"idx":9,"sentence":"We show that all sources are important and contribute to answering questions.","offset":2,"pro":0.2857142857142857,"labels":"RST"},{"idx":9,"sentence":"We further propose a novel data augmentation method to iteratively create training samples for answer generation,","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":9,"sentence":"which achieves close-to-human performance with only a few thousandannotations.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":9,"sentence":"Finally, we perform an in-depth error analysis of model predictions","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":9,"sentence":"and highlight the challenges for future research.","offset":6,"pro":0.8571428571428571,"labels":"IMP"},{"idx":10,"sentence":"Product question answering (PQA) aims to automatically address customer questions to improve their online shopping experience.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"Apart from the above two sources, a lot of product information is represented in a semi-structured way, e.g., key-value pairs, lists, tables, json and xml files, etc.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":10,"sentence":"These semi-structured data can be a valuable answer source since they are better organized than free text, while being easier to construct than structured knowledge bases. ","offset":2,"pro":0.25,"labels":"BAC"},{"idx":10,"sentence":"To fill in this blank, here we study how to effectively incorporate semi-structured answer sources for PQA and focus on presenting answers in a natural, fluent sentence.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":10,"sentence":"To this end, we present semiPQA: a dataset to benchmark PQA over semi-structured data.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":10,"sentence":"It contains 11,243 written questions about json-formatted data covering 320 unique attribute types.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":10,"sentence":"Each data point is paired with manually-annotated text that describes its contents, so that we can train a neural answer presenter to present the data in a natural way. ","offset":6,"pro":0.75,"labels":"MTD"},{"idx":10,"sentence":" In general, state-of-the-art neural models can perform remarkably well when dealing with seen attribute types. ","offset":7,"pro":0.875,"labels":"CLN"},{"idx":11,"sentence":"Responding to online customer reviews has become an essential part of successfully managing and growing a business both in e-commerce and the hospitality and tourism sectors. ","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"However, they also tend to learn a strong, undesirable bias towards generating overly generic, one-size-fits-all outputs to a wide range of inputs. ","offset":1,"pro":0.2,"labels":"GAP"},{"idx":11,"sentence":"In this work we examine the task of generating more specific responses for online reviews in the hospitality domain","offset":2,"pro":0.4,"labels":"PUR"},{"idx":11,"sentence":"by identifying generic responses in the training data, filtering them and fine-tuning the generation model.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":11,"sentence":"We experiment with a range of data-driven filtering methods ","offset":4,"pro":0.8,"labels":"MTD"},{"idx":12,"sentence":"Although most studies have treated attribute value extraction (AVE) as named entity recognition, these approaches are not practical in real-world e-commerce platforms because they perform poorly, and require canonicalization of extracted values.","offset":0,"pro":0,"labels":"GAP"},{"idx":12,"sentence":"Furthermore, since values needed for actual services is static in many attributes, extraction of new values is not always necessary. ","offset":1,"pro":0.2,"labels":"GAP"},{"idx":12,"sentence":"A major problem in solving AVE as XMC is that the distribution between positive and negative labels for products is heavily imbalanced. ","offset":2,"pro":0.4,"labels":"MTD"},{"idx":12,"sentence":"We exploit attribute taxonomy designed for e-commerce platforms to determine which labels are negative for products.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":12,"sentence":"Experimental results using a dataset collected from a Japanese e-commerce platform demonstrate that the label masking improves micro and macro F_1 scores by 3.38 and 23.20 points, respectively.","offset":4,"pro":0.8,"labels":"RST"},{"idx":13,"sentence":"Query classification is a fundamental task in an e-commerce search engine, which assigns one or multiple predefined product categories in response to each search query.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"Taking click-through logs as training data in deep learning methods is a common and effective approach for query classification. ","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":13,"sentence":"The lack of reliable user feedback information results in worse performance of long-tail queries compared with frequent queries. ","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":13,"sentence":"The long-tail queries are guided by the contrastive loss to obtain category-aligned representations in the auxiliary module, where the variant frequent queries serve as anchors in the representation space. ","offset":3,"pro":0.5,"labels":"MTD"},{"idx":13,"sentence":"o solve the above problem, we propose a novel method that leverages an auxiliary module to enhance the representations of long-tail queries ","offset":4,"pro":0.6666666666666666,"labels":"PUR"},{"idx":13,"sentence":"The results and further analysis demonstrate the effectiveness of our proposed method.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":14,"sentence":"We use Multilingual BERT (mBERT; Devlin et al., 2019) as a starting point and follow the knowledge distillation approach of (Sahn et al., 2019) to train a smaller multilingual BERT model that is adapted to the domain at hand. ","offset":0,"pro":0,"labels":"MTD"},{"idx":14,"sentence":"Whereas much previous work with BERT has fine-tuned the encoder weights during task training, we show that the model improvements from distillation on in-domain data persist even when the encoder weights are frozen during task training, allowing a single encoder to support classifiers for multiple tasks and languages.","offset":1,"pro":0.3333333333333333,"labels":"CLN"},{"idx":14,"sentence":"We demonstrate that knowledge distillation can be used not only to reduce model size, but to simultaneously adapt a contextual language model to a specific domain.","offset":2,"pro":0.6666666666666666,"labels":"PUR"},{"idx":15,"sentence":"Extracting attribute-value information from unstructured product descriptions continue to be of a vital importance in e-commerce applications.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"One of the most important product attributes is the brand which highly influences costumers' purchasing behaviour. ","offset":1,"pro":0.125,"labels":"BAC"},{"idx":15,"sentence":"Under the open world assumption, several approaches have adopted deep learning models to extract attribute-values using sequence tagging paradigm. ","offset":2,"pro":0.25,"labels":"BAC"},{"idx":15,"sentence":"In this paper, we introduce OpenBrand, a novel approach for discovering brand names.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":15,"sentence":"OpenBrand is a BiLSTM-CRF-Attention model with embeddings at different granularities.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":15,"sentence":"Such embeddings are learned using CNN and LSTM architectures to provide more accurate representations.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":15,"sentence":"We further propose a new dataset for brand value extraction, with a very challenging task on zero-shot extraction. ","offset":6,"pro":0.75,"labels":"MTD"},{"idx":15,"sentence":" and shown that it outperforms state-of-the-art models in brand name discovery.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":16,"sentence":"Noisy labels in large E-commerce product data (i.e., product items are placed into incorrect categories) is a critical issue for product categorization task because they are unavoidable, non-trivial to remove and degrade prediction performance significantly.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"Training a product title classification model which is robust to noisy labels in the data is very important to make product classification applications more practical.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":16,"sentence":"In this paper, we study the impact of instance-dependent noise to performance of product title classification","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":16,"sentence":"by comparing our data denoising algorithm and different noise-resistance training algorithms which were designed to prevent a classifier model from over-fitting to noise. ","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":16,"sentence":"Along with recent methods of stimulating instance-dependent noise, we propose a novel noise stimulation algorithm based on product title similarity.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":16,"sentence":"Our experiments cover multiple datasets, various noise methods and different training solutions.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":16,"sentence":"Results uncover the limit of classification task when noise rate is not negligible and data distribution is highly skewed.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":17,"sentence":"The automated analysis of Terms and Conditions has gained attention in recent years, mainly due to its relevance to consumer protection.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"Well-structured data sets are the base for every analysis. ","offset":1,"pro":0.25,"labels":"BAC"},{"idx":17,"sentence":"In this paper, we present an approach to extract the content and hierarchy of Terms and Conditions from German and English online shops.","offset":2,"pro":0.5,"labels":"PUR"},{"idx":17,"sentence":"Our evaluation shows, that the approach outperforms the current state of the art. ","offset":3,"pro":0.75,"labels":"CLN"},{"idx":18,"sentence":" However, item-to-item recommendations today do not allow users to explore changes along selected dimensions: given a query item, can a model suggest something similar but in a different color?","offset":0,"pro":0,"labels":"GAP"},{"idx":18,"sentence":"We consider item recommendations of the comparative nature (e.g. something darker) and show how CLIP-based models can support this use case in a zero-shot manner. ","offset":1,"pro":0.3333333333333333,"labels":"PUR"},{"idx":18,"sentence":" we introduce GradREC and its industry potential, and offer a first rounded assessment of its strength and weaknesses.","offset":2,"pro":0.6666666666666666,"labels":"CTN"},{"idx":19,"sentence":"Often, the sheer size of these contracts allows the drafting party to hide unfavourable terms from the other party.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"based on a data-set of more than 6,000 clauses from more than 170 contracts, which we collected from German and English online shops and annotated based on a taxonomy of clause topics, that we developed together with legal experts.","offset":1,"pro":0.3333333333333333,"labels":"MTD"},{"idx":19,"sentence":"We will show that, in our comparison of seven approaches, from simple keyword matching to transformer language models, BERT performed best with an F1-score of up to 0.91, ","offset":2,"pro":0.6666666666666666,"labels":"RST"},{"idx":20,"sentence":"Many e-commerce websites provide Product-related Question Answering (PQA) platform where potential customers can ask questions related to a product, ","offset":0,"pro":0,"labels":"BAC"},{"idx":20,"sentence":"Recently, there has been a growing interest in providing automated responses to product questions.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":20,"sentence":"In this paper, we investigate the suitability of the generative approach for PQA. ","offset":2,"pro":0.25,"labels":"PUR"},{"idx":20,"sentence":"On closer examination, we find several drawbacks in this approach:","offset":3,"pro":0.375,"labels":"RST"},{"idx":20,"sentence":"(1) input reviews are not always utilized significantly for answer generation, ","offset":4,"pro":0.5,"labels":"RST"},{"idx":20,"sentence":" (3) many of the generated answers contain phrases like I do not know which are taken from the reference answer in training data, and these answers do not convey any information to the customer.","offset":5,"pro":0.625,"labels":"RST"},{"idx":20,"sentence":"Although these approaches achieve a high ROUGE score, it does not reflect upon these shortcomings of the generated answers. ","offset":6,"pro":0.75,"labels":"RST"},{"idx":20,"sentence":"and future research will focus on addressing these shortcomings in PQA.","offset":7,"pro":0.875,"labels":"IMP"},{"idx":21,"sentence":"Item categorization (IC) is a core natural language processing (NLP) task in e-commerce. ","offset":0,"pro":0,"labels":"BAC"},{"idx":21,"sentence":"To improve IC performance further, other product metadata, e.g., product images, have been used. ","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":21,"sentence":"In this paper, we proposed a new way of using product images to improve text-only IC model: ","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":21,"sentence":"Our experiments on the three genres in the public Amazon product dataset show that the proposed method generates improved prediction accuracy and macro-F1 values than simply using the original BERT.","offset":3,"pro":0.5,"labels":"RST"},{"idx":21,"sentence":"Moreover, the proposed method is able to keep using existing text-only IC inference implementation ","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":21,"sentence":"nd shows a resource advantage than the deployment of a dual-input MIC system.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":22,"sentence":"Recently, semantic search has been successfully applied to E-commerce product search ","offset":0,"pro":0,"labels":"BAC"},{"idx":22,"sentence":"Yet, whether generalization can conveniently emerge has not been thoroughly studied in the domain thus far.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":22,"sentence":"In this paper, we examine several general-domain and domain-specific pre-trained Roberta variants","offset":2,"pro":0.4,"labels":"PUR"},{"idx":22,"sentence":"and discover that general-domain fine-tuning does not really help generalization which aligns with the discovery of prior art, ","offset":3,"pro":0.6,"labels":"RST"},{"idx":22,"sentence":"et proper domain-specific fine-tuning with clickstream data can lead to better model generalization, ","offset":4,"pro":0.8,"labels":"RST"},{"idx":23,"sentence":"For any e-commerce service, persuasive, faithful, and informative product descriptions can attract shoppers and improve sales.","offset":0,"pro":0,"labels":"BAC"},{"idx":23,"sentence":"While not all sellers are capable of providing such interesting descriptions, a language generation system can be a source of such descriptions at scale,","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":23,"sentence":"and potentially assist sellers to improve their product descriptions.","offset":2,"pro":0.3333333333333333,"labels":"BAC"},{"idx":23,"sentence":"limited attributes such as titles (Chen et al., 2019; Chan et al., 2020), and focused on only one product type (Wang et al., 2017; Munigala et al., 2018; Hong et al., 2021).","offset":3,"pro":0.5,"labels":"BAC"},{"idx":23,"sentence":"In this paper, we jointly train image features and 10 text attributes across 23 diverse product types, with two different target text types with different writing styles: bullet points and paragraph descriptions. ","offset":4,"pro":0.6666666666666666,"labels":"PUR"},{"idx":23,"sentence":"but are less faithful and informative, especially out of domain.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":24,"sentence":"Automatic Speech Recognition(ASR) has been dominated by deep learning-based end-to-end speech recognition models.","offset":0,"pro":0,"labels":"BAC"},{"idx":24,"sentence":"These approaches require large amounts of labeled data in the form of audio-text pairs.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":24,"sentence":"Moreover, these models are more susceptible to domain shift as compared to traditional models.","offset":2,"pro":0.25,"labels":"BAC"},{"idx":24,"sentence":"It is common practice to train generic ASR models and then adapt them to target domains using comparatively smaller data sets. ","offset":3,"pro":0.375,"labels":"BAC"},{"idx":24,"sentence":"In this work, we propose a simple baseline technique for domain adaptation in end-to-end speech recognition models.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":24,"sentence":"We convert the text-only corpus to audio data using single speaker Text to Speech (TTS) engine. ","offset":5,"pro":0.625,"labels":"MTD"},{"idx":24,"sentence":"We show that single speaker synthetic TTS data coupled with final dense layer only fine-tuning provides reasonable improvements in word error rates. ","offset":6,"pro":0.75,"labels":"RST"},{"idx":24,"sentence":"to show the effectiveness of our low-cost baseline approach on CTC and attention-based models.","offset":7,"pro":0.875,"labels":"RST"},{"idx":25,"sentence":"In a large online marketplace, lot offerings play an important role, allowing buyers and sellers to set price levels to optimally balance supply and demand needs. ","offset":0,"pro":0,"labels":"BAC"},{"idx":25,"sentence":"The ability to identify lot offerings plays a key role in many fundamental tasks, from matching offerings to catalog products, through ranking search results, to providing effective pricing guidance.","offset":1,"pro":0.25,"labels":"BAC"},{"idx":25,"sentence":"In this work, we seek to determine the lot status (and lot size) of each offering, in order to facilitate an improved buyer experience, while reducing the friction for sellers posting new offerings. ","offset":2,"pro":0.5,"labels":"PUR"},{"idx":25,"sentence":"by adapting state-of-the-art natural language techniques to the lot identification problem.","offset":3,"pro":0.75,"labels":"MTD"},{"idx":26,"sentence":"Dependency parsing is a method for doing surface-level syntactic analysis on natural language texts. ","offset":0,"pro":0,"labels":"BAC"},{"idx":26,"sentence":"This paper focuses on a novel approach that uses word-to-word dependency tagging using BERT models to improve the malt parser performance.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":26,"sentence":"We used Tamil, a morphologically rich and free word language. ","offset":2,"pro":0.4,"labels":"MTD"},{"idx":26,"sentence":" and the dependency relations are recognized using Machine Learning Algorithms. ","offset":3,"pro":0.6,"labels":"MTD"},{"idx":26,"sentence":"he results obtained are used in the malt parser ","offset":4,"pro":0.8,"labels":"MTD"},{"idx":27,"sentence":"Increased use of online social media sites has given rise to tremendous amounts of user generated data. ","offset":0,"pro":0,"labels":"BAC"},{"idx":27,"sentence":"Social media sites such as Twitter limit the number of characters used to express a thought in a tweet, leading to increased use of creative, humorous and confusing language in order to convey the message.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":27,"sentence":"Due to this, automatic humor detection has become a difficult task, especially for low-resource languages such as the Dravidian languages. ","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":27,"sentence":"In this paper, we have attempted to solve this issue","offset":3,"pro":0.5,"labels":"PUR"},{"idx":27,"sentence":"by collecting and annotating Telugu tweets and performing automatic humor detection on the collected data. ","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":27,"sentence":"We concluded that XLM-RoBERTa was the best-performing model ","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":28,"sentence":"Accuracy of English-language Question Answering (QA) systems has improved significantly in recent years with the advent of Transformer-based models (e.g., BERT). ","offset":0,"pro":0,"labels":"BAC"},{"idx":28,"sentence":"However, QA datasets on such a scale are not available for most of the other languages.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":28,"sentence":"Multi-lingual BERT-based models (mBERT) are often used to transfer knowledge from high-resource languages to low-resource languages.","offset":2,"pro":0.3333333333333333,"labels":"BAC"},{"idx":28,"sentence":"Since these models are pre-trained with huge text corpora containing multiple languages, they typically learn language-agnostic embeddings for tokens from different languages. ","offset":3,"pro":0.5,"labels":"BAC"},{"idx":28,"sentence":"In this work, we augment the QA samples of the target language using translation and transliteration into other languages and use the augmented data to fine-tune an mBERT-based QA model, which is already pre-trained in English. ","offset":4,"pro":0.6666666666666666,"labels":"PUR"},{"idx":28,"sentence":"We further show that introducing a contrastive loss between the translated question-context feature pairs during the fine-tuning process, prevents such degradation with cross-lingual family translations and leads to marginal improvement. ","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":29,"sentence":"Task-Oriented Dialogue (TOD) systems allow users to accomplish tasks by giving directions to the system using natural language utterances.","offset":0,"pro":0,"labels":"BAC"},{"idx":29,"sentence":"With the widespread adoption of conversational agents and chat platforms, TOD has become mainstream in NLP research today.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":29,"sentence":"However, developing TOD systems require massive amounts of data,","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":29,"sentence":"Towards this objective, we introduce TamilATIS - a TOD dataset for Tamil which contains 4874 utterances.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":29,"sentence":"We train state-of-the-art NLU models and report their performances.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":29,"sentence":"The joint BERT model with XLM-Roberta as utterance encoder achieved the highest score with an intent accuracy of 96.26% and slot F1 of 94.01%.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":0,"sentence":"Attention mechanism has become the dominant module in natural language processing model","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"t is computationally intensive and depends on massive power-hungry multiplications","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":0,"sentence":"In this paper, we rethink variants of attention mechanism from the energy consumption aspect","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":0,"sentence":"fter reaching the conclusion that the energy costs of several energy-friendly operations are far less than their multiplication counterparts, we build a novel attention model by replacing multiplications with either selective operations or additions","offset":3,"pro":0.5,"labels":"PUR"},{"idx":0,"sentence":"mpirical results on three machine translation tasks demonstrate that the proposed model, against the vanilla one, achieves competitable accuracy while saving 99% and 66% energy during alignment calculation and the whole attention procedur","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":0,"sentence":"Our code will be released upon the acceptance","offset":5,"pro":0.8333333333333334,"labels":"CTN"},{"idx":1,"sentence":"Sampling is a promising bottom-up method for exposing what generative models have learned about language","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"but it remains unclear how to generate representative samples from popular masked language models (MLMs) like BERT","offset":1,"pro":0.125,"labels":"GAP"},{"idx":1,"sentence":"he MLM objective yields a dependency network with no guarantee of consistent conditional distributions, posing a problem for naive approaches","offset":2,"pro":0.25,"labels":"GAP"},{"idx":1,"sentence":"rawing from theories of iterated learning in cognitive science","offset":3,"pro":0.375,"labels":"MTD"},{"idx":1,"sentence":"e explore the use of serial reproduction chains to sample from BERT's prior","offset":4,"pro":0.5,"labels":"PUR"},{"idx":1,"sentence":"In particular, we observe that a unique and consistent estimator of the ground-truth joint distribution is given by a Generative Stochastic Network (GSN) sampler, which randomly selects which token to mask and reconstruct on each step","offset":5,"pro":0.625,"labels":"MTD"},{"idx":1,"sentence":"We show that the lexical and syntactic statistics of sentences from GSN chains closely match the ground-truth corpus distribution and perform better than other methods in a large corpus of naturalness judgments","offset":6,"pro":0.75,"labels":"CLN"},{"idx":1,"sentence":"Our findings establish a firmer theoretical foundation for bottom-up probing and highlight richer deviations from human priors","offset":7,"pro":0.875,"labels":"CTN"},{"idx":2,"sentence":"Modern Natural Language Processing (NLP) models are known to be sensitive to input perturbations and their performance can decrease when applied to real-world, noisy data","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"However, it is still unclear why models are less robust to some perturbations than others","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":2,"sentence":"n this work, we test the hypothesis that the extent to which a model is affected by an unseen textual perturbation (robustness) can be explained by the learnability of the perturbation (defined as how well the model learns to identify the perturbation with a small amount of evidence","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":2,"sentence":"e further give a causal justification for the learnability metric","offset":3,"pro":0.5,"labels":"MTD"},{"idx":2,"sentence":"e conduct extensive experiments with four prominent NLP models - TextRNN, BERT, RoBERTa and XLNet - over eight types of textual perturbations on three datasets","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":2,"sentence":"We show that a model which is better at identifying a perturbation (higher learnability) becomes worse at ignoring such a perturbation at test time (lower robustness), providing empirical support for our hypothesi","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":3,"sentence":"ense retrieval (DR) methods conduct text retrieval by first encoding texts in the embedding space and then matching them by nearest neighbor search. ","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"This requires strong locality properties from the representation space, e.g., close allocations of each small group of relevant texts, which are hard to generalize to domains without sufficient training data","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":3,"sentence":"In this paper, we aim to improve the generalization ability of DR models from source training domains with rich supervision signals to target domains without any relevance label, in the zero-shot settin","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":3,"sentence":"o achieve that, we propose Momentum adversarial Domain Invariant Representation learning (MoDIR), which introduces a momentum method to train a domain classifier that distinguishes source versus target domains, and then adversarially updates the DR encoder to learn domain invariant representations","offset":3,"pro":0.5,"labels":"MTD"},{"idx":3,"sentence":"Our experiments show that MoDIR robustly outperforms its baselines on 10+ ranking datasets collected in the BEIR benchmark in the zero-shot setup, with more than 10% relative gains on datasets with enough sensitivity for DR models' evaluatio","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":3,"sentence":"ource code is available at https://github.com/ji-xin/modi","offset":5,"pro":0.8333333333333334,"labels":"CTN"},{"idx":4,"sentence":"Approaches based only on dialogue synthesis are insufficient, as dialogues generated from state-machine based models are poor approximations of real-life conversatio","offset":0,"pro":0,"labels":"GAP"},{"idx":4,"sentence":" Furthermore, previously proposed dialogue state representations are ambiguous and lack the precision necessary for building an effective age","offset":1,"pro":0.1,"labels":"GAP"},{"idx":4,"sentence":"This paper proposes a new dialogue representation and a sample-efficient methodology that can predict precise dialogue states in WOZ conversations","offset":2,"pro":0.2,"labels":"PUR"},{"idx":4,"sentence":"We extended the ThingTalk representation to capture all information an agent needs to respond properly.","offset":3,"pro":0.3,"labels":"MTD"},{"idx":4,"sentence":"ur training strategy is sample-efficien","offset":4,"pro":0.4,"labels":"MTD"},{"idx":4,"sentence":"we combine (1) few-shot data sparsely sampling the full dialogue space and (2) synthesized data covering a subset space of dialogues generated by a succinct state-based dialogue model","offset":5,"pro":0.5,"labels":"MTD"},{"idx":4,"sentence":"he completeness of the extended ThingTalk language is demonstrated with a fully operational agent, which is also used in training data synthes","offset":6,"pro":0.6,"labels":"MTD"},{"idx":4,"sentence":"e demonstrate the effectiveness of our methodology on MultiWOZ 3.0, a reannotation of the MultiWOZ 2.1 dataset in ThingTalk","offset":7,"pro":0.7,"labels":"RST"},{"idx":4,"sentence":"hingTalk can represent 98% of the test turns, while the simulator can emulate 85% of the validation se","offset":8,"pro":0.8,"labels":"RST"},{"idx":4,"sentence":"e train a contextual semantic parser using our strategy, and obtain 79% turn-by-turn exact match accuracy on the reannotated test set.","offset":9,"pro":0.9,"labels":"RST"},{"idx":5,"sentence":"Controllable paraphrase generation (CPG) incorporates various external conditions to obtain desirable paraphras","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"However, existing works only highlight a special condition under two indispensable aspects of CPG (i.e., lexically and syntactically CPG) individually, lacking a unified circumstance to explore and analyze their effectiveness","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":5,"sentence":"In this paper, we propose a general controllable paraphrase generation framework (GCPG), which represents both lexical and syntactical conditions as text sequences and uniformly processes them in an encoder-decoder paradigm","offset":2,"pro":0.2222222222222222,"labels":"PUR"},{"idx":5,"sentence":"Under GCPG, we reconstruct commonly adopted lexical condition (i.e., Keywords) and syntactical conditions (i.e., Part-Of-Speech sequence, Constituent Tree, Masked Template and Sentential Exemplar) and study the combination of the two types","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":5,"sentence":"In particular, for Sentential Exemplar condition, we propose a novel exemplar construction method - Syntax-Similarity based Exemplar (S","offset":4,"pro":0.4444444444444444,"labels":"PUR"},{"idx":5,"sentence":"SSE retrieves a syntactically similar but lexically different sentence as the exemplar for each target sentence, avoiding exemplar-side words copying problem.","offset":5,"pro":0.5555555555555556,"labels":"RST"},{"idx":5,"sentence":"xtensive experiments demonstrate that GCPG with SSE achieves state-of-the-art performance on two popular benchmarks.","offset":6,"pro":0.6666666666666666,"labels":"RST"},{"idx":5,"sentence":"n addition, the combination of lexical and syntactical conditions shows the significant controllable ability of paraphrase generati","offset":7,"pro":0.7777777777777778,"labels":"CLN"},{"idx":5,"sentence":"these empirical results could provide novel insight to user-oriented paraphrasing","offset":8,"pro":0.8888888888888888,"labels":"IMP"},{"idx":6,"sentence":"ask-oriented personal assistants enable people to interact with a host of devices and services using natural languag","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"One of the challenges of making neural dialogue systems available to more users is the lack of training data for all but a few languages","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":6,"sentence":"ero-shot methods try to solve this issue by acquiring task knowledge in a high-resource language such as English with the aim of transferring it to the low-resource languag","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":6,"sentence":"To this end","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":6,"sentence":"we introduce CrossAligner, the principal method of a variety of effective approaches for zero-shot cross-lingual transfer based on learning alignment from unlabelled parallel data","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":6,"sentence":"We present a quantitative analysis of individual methods as well as their weighted combinations, several of which exceed state-of-the-art (SOTA) scores as evaluated across nine languages, fifteen test sets and three benchmark multilingual datasets","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":6,"sentence":" detailed qualitative error analysis of the best methods shows that our fine-tuned language models can zero-shot transfer the task knowledge better than anticipated","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":7,"sentence":"e explore how a multi-modal transformer trained for generation of longer image descriptions learns syntactic and semantic representations about entities and relations grounded in objects at the level of masked self-attention (text generation) and cross-modal attention (information fusion)","offset":0,"pro":0,"labels":"PUR"},{"idx":7,"sentence":"We observe that cross-attention learns the visual grounding of noun phrases into objects and high-level semantic information about spatial relations, while text-to-text attention captures low-level syntactic knowledge between word","offset":1,"pro":0.25,"labels":"MTD"},{"idx":7,"sentence":"his concludes that language models in a multi-modal task learn different semantic information about objects and relations cross-modally and uni-modally (text-on","offset":2,"pro":0.5,"labels":"CLN"},{"idx":7,"sentence":"Our code is available here: https://github.com/GU-CLASP/attention-as-grounding","offset":3,"pro":0.75,"labels":"CTN"},{"idx":8,"sentence":"Cross-lingual transfer between a high-resource language and its dialects or closely related language varieties should be facilitated by their similarity","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"However, current approaches that operate in the embedding space do not take surface similarity into account. ","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":8,"sentence":"his work presents a simple yet effective strategy to improve cross-lingual transfer between closely related varieties. ","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":8,"sentence":"We propose to augment the data of the high-resource source language with character-level noise to make the model more robust towards spelling variation","offset":3,"pro":0.5,"labels":"PUR"},{"idx":8,"sentence":"Our strategy shows consistent improvements over several languages and task","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":8,"sentence":"Our work provides evidence for the usefulness of simple surface-level noise in improving transfer between language varietie","offset":5,"pro":0.8333333333333334,"labels":"CTN"},{"idx":9,"sentence":"yntactic structure has long been argued to be potentially useful for enforcing accurate word alignment and improving generalization performance of machine translation.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"Unfortunately, existing wisdom demonstrates its significance by considering only the syntactic structure of source tokens, neglecting the rich structural information from target tokens and the structural similarity between the source and target sentences","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":9,"sentence":"n this work, we propose to incorporate the syntactic structure of both source and target tokens into the encoder-decoder framework, tightly correlating the internal logic of word alignment and machine translation for multi-task learnin","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":9,"sentence":" Particularly, we won't leverage any annotated syntactic graph of the target side during training, so we introduce Dynamic Graph Convolution Networks (DGCN) on observed target tokens to sequentially and simultaneously generate the target tokens and the corresponding syntactic graphs, and further guide the word alignme","offset":3,"pro":0.5,"labels":"MTD"},{"idx":9,"sentence":" this basis, Hierarchical Graph Random Walks (HGRW) are performed on the syntactic graphs of both source and target sides, for incorporating structured constraints on machine translation outputs.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":9,"sentence":"Experiments on four publicly available language pairs verify that our method is highly effective in capturing syntactic structure in different languages, consistently outperforming baselines in alignment accuracy and demonstrating promising results in translation quality","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":10,"sentence":"Automatic email to-do item generation is the task of generating to-do items from a given email to help people overview emails and schedule daily work.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"ifferent from prior research on email summarization, to-do item generation focuses on generating action mentions to provide more structured summaries of email tex","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":10,"sentence":"Prior work either requires large amount of annotation for key sentences with potential actions or fails to pay attention to nuanced actions from these unstructured emails, and thus often lead to unfaithful summaries.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":10,"sentence":"o fill these gap","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":10,"sentence":"we propose a simple and effective learning to highlight and summarize framework (LHS) to learn to identify the most salient text and actions, and incorporate these structured representations to generate more faithful to-do item","offset":4,"pro":0.5714285714285714,"labels":"PUR"},{"idx":10,"sentence":"Experiments show that our LHS model outperforms the baselines and achieves the state-of-the-art performance in terms of both quantitative evaluation and human judgeme","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":10,"sentence":"We also discussed specific challenges that current models faced with email to-do summarization","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":11,"sentence":"n this paper, we explore the capacity of a language model-based method for grammatical error detection in detail.","offset":0,"pro":0,"labels":"PUR"},{"idx":11,"sentence":"We first show that 5 to 10% of training data are enough for a BERT-based error detection method to achieve performance equivalent to what a non-language model-based method can achieve with the full training data","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":11,"sentence":"recall improves much faster with respect to training data size in the BERT-based method than in the non-language model method","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":11,"sentence":"This suggests that (i) the BERT-based method should have a good knowledge of the grammar required to recognize certain types of error and that (ii) it can transform the knowledge into error detection rules by fine-tuning with few training samples, which explains its high generalization ability in grammatical error detection","offset":3,"pro":0.5,"labels":"RST"},{"idx":11,"sentence":"e further show with pseudo error data that it actually exhibits such nice properties in learning rules for recognizing various types of erro","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":11,"sentence":"Finally, based on these findings, we discuss a cost-effective method for detecting grammatical errors with feedback comments explaining relevant grammatical rules to learne","offset":5,"pro":0.8333333333333334,"labels":"CTN"},{"idx":12,"sentence":"e explore the notion of uncertainty in the context of modern abstractive summarization model","offset":0,"pro":0,"labels":"PUR"},{"idx":12,"sentence":"using the tools of Bayesian Deep Learnin","offset":1,"pro":0.1,"labels":"MTD"},{"idx":12,"sentence":"ur approach approximates Bayesian inference by first extending state-of-the-art summarization models with Monte Carlo dropou","offset":2,"pro":0.2,"labels":"MTD"},{"idx":12,"sentence":"hen using them to perform multiple stochastic forward passes","offset":3,"pro":0.3,"labels":"MTD"},{"idx":12,"sentence":"Based on Bayesian inference we are able to effectively quantify uncertainty at prediction tim","offset":4,"pro":0.4,"labels":"RST"},{"idx":12,"sentence":"Having a reliable uncertainty measure,","offset":5,"pro":0.5,"labels":"MTD"},{"idx":12,"sentence":"we can improve the experience of the end user by filtering out generated summaries of high uncertainty","offset":6,"pro":0.6,"labels":"PUR"},{"idx":12,"sentence":"Furthermore, uncertainty estimation could be used as a criterion for selecting samples for annotation, and can be paired nicely with active learning and human-in-the-loop approache","offset":7,"pro":0.7,"labels":"MTD"},{"idx":12,"sentence":"nally, Bayesian inference enables us to find a Bayesian summary which performs better than a deterministic one and is more robust to uncertaint","offset":8,"pro":0.8,"labels":"CLN"},{"idx":12,"sentence":"In practice, we show that our Variational Bayesian equivalents of BART and PEGASUS can outperform their deterministic counterparts on multiple benchmark dataset","offset":9,"pro":0.9,"labels":"CLN"},{"idx":13,"sentence":"As large and powerful neural language models are developed, researchers have been increasingly interested in developing diagnostic tools to probe them","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"There are many papers with conclusions of the form observation X is found in model Y, using their own datasets with varying size","offset":1,"pro":0.125,"labels":"BAC"},{"idx":13,"sentence":"arger probing datasets bring more reliability, but are also expensive to collec","offset":2,"pro":0.25,"labels":"GAP"},{"idx":13,"sentence":"There is yet to be a quantitative method for estimating reasonable probing dataset sizes","offset":3,"pro":0.375,"labels":"GAP"},{"idx":13,"sentence":"We tackle this omission in the context of comparing two probing configurations","offset":4,"pro":0.5,"labels":"PUR"},{"idx":13,"sentence":"We present a novel method to estimate the required number of data samples in such experiment","offset":5,"pro":0.625,"labels":"PUR"},{"idx":13,"sentence":", we verify that our estimations have sufficient statistical pow","offset":6,"pro":0.75,"labels":"CLN"},{"idx":13,"sentence":"ur framework helps to systematically construct probing datasets to diagnose neural NLP model","offset":7,"pro":0.875,"labels":"CTN"},{"idx":14,"sentence":"Recent Quality Estimation (QE) models based on multilingual pre-trained representations have achieved very competitive results in predicting the overall quality of translated sentence","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"However, detecting specifically which translated words are incorrect is a more challenging task, especially when dealing with limited amounts of training dat","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":14,"sentence":"We hypothesize that, not unlike humans, successful QE models rely on translation errors to predict overall sentence quality","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":14,"sentence":"By exploring a set of feature attribution methods that assign relevance scores to the inputs to explain model prediction","offset":3,"pro":0.5,"labels":"MTD"},{"idx":14,"sentence":"e study the behaviour of state-of-the-art sentence-level QE models and show that explanations (i.e. rationales) extracted from these models can indeed be used to detect translation error","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":14,"sentence":"We therefore (i) introduce a novel semi-supervised method for word-level QE; and (ii) propose to use the QE task as a new benchmark for evaluating the plausibility of feature attribution, i.e. how interpretable model explanations are to humans","offset":5,"pro":0.8333333333333334,"labels":"CTN"},{"idx":15,"sentence":"Recent work in task-independent graph semantic parsing has shifted from grammar-based symbolic approaches to neural models, showing strong performance on different types of meaning representations","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"owever, it is still unclear that what are the limitations of these neural parsers, and whether these limitations can be compensated by incorporating symbolic knowledge into model inference","offset":1,"pro":0.125,"labels":"GAP"},{"idx":15,"sentence":"n this paper, we address these questions by taking English Resource Grammar (ERG) parsing as a case stud","offset":2,"pro":0.25,"labels":"PUR"},{"idx":15,"sentence":"pecifically, we first develop a state-of-the-art, T5-based neural ERG parser, and conduct detail analyses of parser performance within fine-grained linguistic categorie","offset":3,"pro":0.375,"labels":"MTD"},{"idx":15,"sentence":"he neural parser attains superior performance on in-distribution test set, but degrades significantly on long-tail situations, while the symbolic parser performs more robustly","offset":4,"pro":0.5,"labels":"RST"},{"idx":15,"sentence":"o address this","offset":5,"pro":0.625,"labels":"PUR"},{"idx":15,"sentence":"we further propose a simple yet principled collaborative framework for neural-symbolic semantic parsing, by designing a decision criterion for beam search that incorporates the prior knowledge from a symbolic parser and accounts for model uncertainty","offset":6,"pro":0.75,"labels":"MTD"},{"idx":15,"sentence":" Experimental results show that the proposed framework yields comprehensive improvement over neural baseline across long-tail categories, yielding the best known Smatch score (97.01) on the well-studied DeepBank benchmark","offset":7,"pro":0.875,"labels":"CLN"},{"idx":16,"sentence":"Traditional sequence labeling frameworks treat the entity types as class IDs and rely on extensive data and high-quality annotations to learn semantics which are typically expensive in practice","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"n this paper, we aim to build an entity recognition model requiring only a few shots of annotated document image","offset":1,"pro":0.125,"labels":"PUR"},{"idx":16,"sentence":"o overcome the data limitation, we propose to leverage the label surface names to better inform the model of the target entity type semantics and also embed the labels into the spatial embedding space to capture the spatial correspondence between regions and labels.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":16,"sentence":"ecifically, we go beyond sequence labeling and develop a novel label-aware seq2seq framework, LASER","offset":3,"pro":0.375,"labels":"MTD"},{"idx":16,"sentence":"proposed model follows a new labeling scheme that generates the label surface names word-by-word explicitly after generating the entities","offset":4,"pro":0.5,"labels":"MTD"},{"idx":16,"sentence":"ring training, LASER refines the label semantics by updating the label surface name representations and also strengthens the label-region correlatio","offset":5,"pro":0.625,"labels":"RST"},{"idx":16,"sentence":"n this way, LASER recognizes the entities from document images through both semantic and layout correspondenc","offset":6,"pro":0.75,"labels":"RST"},{"idx":16,"sentence":"Extensive experiments on two benchmark datasets demonstrate the superiority of LASER under the few-shot setting","offset":7,"pro":0.875,"labels":"CLN"},{"idx":17,"sentence":"Despite the remarkable success deep models have achieved in Textual Matching (TM) task","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"t still remains unclear whether they truly understand language or measure the semantic similarity of texts by exploiting statistical bias in datasets","offset":1,"pro":0.08333333333333333,"labels":"GAP"},{"idx":17,"sentence":"In this work, we provide a new perspective to study this issue - via the length divergence bias","offset":2,"pro":0.16666666666666666,"labels":"PUR"},{"idx":17,"sentence":"e find the length divergence heuristic widely exists in prevalent TM datasets, providing direct cues for predictio","offset":3,"pro":0.25,"labels":"RST"},{"idx":17,"sentence":"o determine whether TM models have adopted such heuristic","offset":4,"pro":0.3333333333333333,"labels":"PUR"},{"idx":17,"sentence":"e introduce an adversarial evaluation scheme which invalidates the heuristic","offset":5,"pro":0.4166666666666667,"labels":"MTD"},{"idx":17,"sentence":"n this adversarial setting, all TM models perform worse, indicating they have indeed adopted this heuristi","offset":6,"pro":0.5,"labels":"RST"},{"idx":17,"sentence":"hrough a well-designed probing experimen","offset":7,"pro":0.5833333333333334,"labels":"MTD"},{"idx":17,"sentence":"e empirically validate that the bias of TM models can be attributed in part to extracting the text length information during training","offset":8,"pro":0.6666666666666666,"labels":"RST"},{"idx":17,"sentence":" alleviate the length divergence bias,","offset":9,"pro":0.75,"labels":"PUR"},{"idx":17,"sentence":"e propose an adversarial training metho","offset":10,"pro":0.8333333333333334,"labels":"MTD"},{"idx":17,"sentence":"he results demonstrate we successfully improve the robustness and generalization ability of models at the same tim","offset":11,"pro":0.9166666666666666,"labels":"CLN"},{"idx":18,"sentence":"Existing model-based metrics for system response evaluation are trained on human annotated data, which is cumbersome to collect","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"n this work, we propose to use information that can be automatically extracted from the next user utterance, such as its sentiment or whether the user explicitly ends the conversation, as a proxy to measure the quality of the previous system response","offset":1,"pro":0.2,"labels":"PUR"},{"idx":18,"sentence":"This allows us to train on a massive set of dialogs with weak supervision, without requiring manual system turn quality annotations.","offset":2,"pro":0.4,"labels":"RST"},{"idx":18,"sentence":"Experiments show that our model is comparable to models trained on human annotated dat","offset":3,"pro":0.6,"labels":"CLN"},{"idx":18,"sentence":"urthermore, our model generalizes across both spoken and written open-domain dialog corpora collected from real and paid user","offset":4,"pro":0.8,"labels":"CLN"},{"idx":19,"sentence":"raining retrieval models to fetch contexts for Question Answering (QA) over large corpora requires labeling relevant passages in those corpora.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"ince obtaining exhaustive manual annotations of all relevant passages is not feasible, prior work uses text overlap heuristics to find passages that are likely to contain the answer, but this is not feasible when the task requires deeper reasoning and answers are not extractable spans (e.g.: multi-hop, discrete reasonin","offset":1,"pro":0.2,"labels":"GAP"},{"idx":19,"sentence":"We address this issue","offset":2,"pro":0.4,"labels":"PUR"},{"idx":19,"sentence":"y identifying relevant passages based on whether they are useful for a trained QA model to arrive at the correct answers, and develop a search process guided by the QA model's loss","offset":3,"pro":0.6,"labels":"MTD"},{"idx":19,"sentence":"ur experiments show that this approach enables identifying relevant context for unseen data greater than 90% of the time on the IIRC dataset and generalizes better to the end QA task than those trained on just the gold retrieval data on IIRC and QASC datasets","offset":4,"pro":0.8,"labels":"RST"},{"idx":20,"sentence":"act checking is a challenging task that requires corresponding evidences to verify the property of a claim based on reasoning.","offset":0,"pro":0,"labels":"BAC"},{"idx":20,"sentence":"evious studies generally","offset":1,"pro":0.25,"labels":"BAC"},{"idx":20,"sentence":"owards the above issues, we propose a novel heterogeneous-graph reasoning and fine-grained aggregation model","offset":2,"pro":0.5,"labels":"PUR"},{"idx":20,"sentence":"Extensive experiments on the benchmark dataset demonstrate that our proposed model achieves much better performance than state-of-the-art method","offset":3,"pro":0.75,"labels":"CLN"},{"idx":21,"sentence":"any people read online reviews to learn about real-world entities of their intere","offset":0,"pro":0,"labels":"BAC"},{"idx":21,"sentence":"owever, majority of reviews only describes general experiences and opinions of the customers, and may not reveal facts that are specific to the entity being reviewe","offset":1,"pro":0.125,"labels":"GAP"},{"idx":21,"sentence":" this work, we focus on a novel task of mining from a review corpus sentences that are unique for each en","offset":2,"pro":0.25,"labels":"PUR"},{"idx":21,"sentence":"alient facts are extremely scarce due to their very natur","offset":3,"pro":0.375,"labels":"GAP"},{"idx":21,"sentence":"To alleviate this scarcity problem, ","offset":4,"pro":0.5,"labels":"PUR"},{"idx":21,"sentence":"e develop an unsupervised method, ZL-Distiller, which leverages contextual language representations of the reviews and their distributional patterns to identify salient sentences about entitie","offset":5,"pro":0.625,"labels":"PUR"},{"idx":21,"sentence":"ur experiments on multiple domains (hotels, products, and restaurants) show that ZL-Distiller achieves state-of-the-art performance and further boosts the performance of other supervised/unsupervised algorithms for the task","offset":6,"pro":0.75,"labels":"CLN"},{"idx":21,"sentence":"urthermore, we show that salient sentences mined by ZL-Distiller provide unique and detailed information about entities, which benefit downstream NLP applications including question answering and summarizatio","offset":7,"pro":0.875,"labels":"CLN"},{"idx":22,"sentence":"Automatic fake news detection models are ostensibly based on logic, where the truth of a claim made in a headline can be determined by supporting or refuting evidence found in a resulting web query","offset":0,"pro":0,"labels":"BAC"},{"idx":22,"sentence":"These models are believed to be reasoning in some way; however, it has been shown that these same results, or better, can be achieved without considering the claim at all - only the evidenc","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":22,"sentence":"s implies that other signals are contained within the examined evidence, and could be based on manipulable factors such as emotion, sentiment, or part-of-speech (POS) frequencies, which are vulnerable to adversarial input","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":22,"sentence":"e neutralize some of these signals through multiple forms of both neural and non-neural pre-processing and style transfe","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":22,"sentence":"nd that this flattening of extraneous indicators can induce the models to actually require both claims and evidence to perform wel","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":22,"sentence":"e conclude with the construction of a model using emotion vectors built off a lexicon and passed through an emotional attention mechanism to appropriately weight certain emotions","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":22,"sentence":"We provide quantifiable results that prove our hypothesis that manipulable features are being used for fact-checkin","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":23,"sentence":"he influence of fake news in the perception of reality has become a mainstream topic in the last years due to the fast propagation of misleading information","offset":0,"pro":0,"labels":"BAC"},{"idx":23,"sentence":"n order to help in the fight against misinformatio","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":23,"sentence":"automated solutions to fact-checking are being actively developed within the research communi","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":23,"sentence":"this context, the task of Automated Claim Verification is defined as assessing the truthfulness of a claim by finding evidence about its veraci","offset":3,"pro":0.5,"labels":"MTD"},{"idx":23,"sentence":"n this work we empirically demonstrate that enriching a BERT model with explicit semantic information such as Semantic Role Labelling helps to improve results in claim verification as proposed by the FEVER benchma","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":23,"sentence":"urthermore, we perform a number of explainability tests that suggest that the semantically-enriched model is better at handling complex cases, such as those including passive forms or multiple propositio","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":24,"sentence":"Work on social media rumour verification utilises signals from posts, their propagation and users involve","offset":0,"pro":0,"labels":"BAC"},{"idx":24,"sentence":"Other lines of work target identifying and fact-checking claims based on information from Wikipedia, or trustworthy news articles without considering social media conte","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":24,"sentence":"However works combining the information from social media with external evidence from the wider web are lackin","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":24,"sentence":"o facilitate research in this directio","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":24,"sentence":"e release a novel dataset, PHEMEPlus, an extension of the PHEME benchmark, which contains social media conversations as well as relevant external evidence for each rumo","offset":4,"pro":0.5714285714285714,"labels":"PUR"},{"idx":24,"sentence":"We demonstrate the effectiveness of incorporating such evidence in improving rumour verification mode","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":24,"sentence":"dditionally, as part of the evidence collection, we evaluate various ways of query formulation to identify the most effective method","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":25,"sentence":"e ability to reason about tabular or semi-structured knowledge is a fundamental problem for today's Natural Language Processing (NLP) systems","offset":0,"pro":0,"labels":"BAC"},{"idx":25,"sentence":"hile significant progress has been achieved in the direction of tabular reasoning, these advances are limited to English due to the absence of multilingual benchmark datasets for semi-structured data","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":25,"sentence":"In this paper, we use machine translation methods to construct a multilingual tabular NLI dataset, namely XINFOTABS, which expands the English tabular NLI dataset of INFOTABS to ten diverse language","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":25,"sentence":"We also present several baselines for multilingual tabular reasoning, e.g., machine translation-based methods and cross-lingu","offset":3,"pro":0.5,"labels":"MTD"},{"idx":25,"sentence":"e discover that the XINFOTABS evaluation suite is both practical and challenging","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":25,"sentence":"As a result, this dataset will contribute to increased linguistic inclusion in tabular reasoning research and applications","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":26,"sentence":"Computational fact-checking aims at supporting the verification process of textual claims by exploiting trustworthy sources","offset":0,"pro":0,"labels":"BAC"},{"idx":26,"sentence":"owever, there are large classes of complex claims that cannot be automatically verified, for instance those related to temporal reasoning","offset":1,"pro":0.2,"labels":"GAP"},{"idx":26,"sentence":"o this aim, in this work, we focus on the verification of economic claims against time series sources","offset":2,"pro":0.4,"labels":"PUR"},{"idx":26,"sentence":"arting from given textual claims in natural language, we propose a neural machine translation approach to produce respective queries expressed in a recently proposed temporal fragment of the Datalog language.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":26,"sentence":"he adopted deep neural approach shows promising preliminary results for the translation of 10 categories of claims extracted from real use cases","offset":4,"pro":0.8,"labels":"CLN"},{"idx":27,"sentence":"efect Triage is a time-sensitive and critical process in a large-scale agile software development lifecycle for e-comm","offset":0,"pro":0,"labels":"BAC"},{"idx":27,"sentence":"nefficiencies arising from human and process dependencies in this domain have motivated research in automated approaches using machine learning to accurately assign defects to qualified team","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":27,"sentence":"his work proposes a novel framework for automated defect triage (DEFTr","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":27,"sentence":"sing fine-tuned state-of-the-art pre-trained BERT on labels fused text embeddings to improve contextual representations from human-generated product defect","offset":3,"pro":0.5,"labels":"MTD"},{"idx":27,"sentence":"For our multi-label text classification defect triage task, we also introduce a Walmart proprietary dataset of product defect","offset":4,"pro":0.6666666666666666,"labels":"PUR"},{"idx":27,"sentence":" using weak supervision and adversarial learning, in a few-shot setting","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":28,"sentence":"s the multi-modal e-commerce is thriving, high-quality advertising product copywriting has gain more attentions, which plays a crucial role in the e-commerce recommender, advertising and even search platforms","offset":0,"pro":0,"labels":"BAC"},{"idx":28,"sentence":"The advertising product copywriting is able to enhance the user experience by highlighting the product's characteristics with textual descriptions and thus to improve the likelihood of user click and purchase","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":28,"sentence":"Automatically generating product copywriting has attracted noticeable interests from both academic and industrial communities, where existing solutions merely make use of a product's title and attribute information to generate its corresponding description","offset":2,"pro":0.3333333333333333,"labels":"BAC"},{"idx":28,"sentence":"However, in addition to the product title and attributes, we observe that there are various auxiliary descriptions created by the shoppers or marketers in the e-commerce platforms (namely human knowledge), which contains valuable information for product copywriting generation, yet always accompanying lots of noises","offset":3,"pro":0.5,"labels":"GAP"},{"idx":28,"sentence":" this work, we propose a novel solution to automatically generating product copywriting that involves all the title, attributes and denoised auxiliary knowledge","offset":4,"pro":0.6666666666666666,"labels":"PUR"},{"idx":28,"sentence":"o be specific, we design an end-to-end generation framework equipped with two variational autoencoders that works interactively to select informative human knowledge and generate diverse copywritin","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":29,"sentence":"n a leading e-commerce business, we receive hundreds of millions of customer feedback from different text communication channels such as product reviews.","offset":0,"pro":0,"labels":"BAC"},{"idx":29,"sentence":"he feedback can contain rich information regarding customers' dissatisfaction in the quality of goods and services","offset":1,"pro":0.1,"labels":"BAC"},{"idx":29,"sentence":" harness such information to better serve customers","offset":2,"pro":0.2,"labels":"PUR"},{"idx":29,"sentence":"n this paper, we created a machine learning approach to automatically identify product issues and uncover root causes from the customer feedback sentence","offset":3,"pro":0.3,"labels":"MTD"},{"idx":29,"sentence":"e identify issues at two levels: coarse grained (L-Coarse) and fine grained (L-Granular). ","offset":4,"pro":0.4,"labels":"MTD"},{"idx":29,"sentence":"We formulate this multi-level product issue identification problem as a seq2seq language generation problem","offset":5,"pro":0.5,"labels":"MTD"},{"idx":29,"sentence":"Specifically, we utilize transformer-based seq2seq models due to their versatility and strong transfer-learning capability","offset":6,"pro":0.6,"labels":"MTD"},{"idx":29,"sentence":"We demonstrate that our approach is label efficient and outperforms the traditional approach such as multi-class multi-label classification formulatio","offset":7,"pro":0.7,"labels":"CLN"},{"idx":29,"sentence":"sed on human evaluation, our fine-tuned model achieves 82.1% and 95.4% human-level performance for L-Coarse and L-Granular issue identification, respective","offset":8,"pro":0.8,"labels":"RST"},{"idx":29,"sentence":"Furthermore, our experiments illustrate that the model can generalize to identify unseen L-Granular issue","offset":9,"pro":0.9,"labels":"CLN"},{"idx":0,"sentence":"With the rise of social media and internet, thereis a necessity to provide an inclusive space andprevent the abusive topics against any gender,race or community","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"This paper describes thesystem submitted to the ACL-2022 shared taskon fine-grained abuse detection in Tamil","offset":1,"pro":0.25,"labels":"PUR"},{"idx":0,"sentence":"In ourapproach we transliterated code-mixed datasetas an augmentation technique to increase thesize of the data","offset":2,"pro":0.5,"labels":"MTD"},{"idx":0,"sentence":"Using this method we wereable to rank 3rd on the task with a 0.290 macroaverage F1 score and a 0.590 weighted F1score","offset":3,"pro":0.75,"labels":"RST"},{"idx":1,"sentence":"This working notes summarises the participation of the UMUTeam on the TamilNLP (ACL 2022) shared task concerning emotion analysis in Tamil","offset":0,"pro":0,"labels":"PUR"},{"idx":1,"sentence":"We participated in the two multi-classification challenges proposed with a neural network that combines linguistic features with different feature sets based on contextual and non-contextual sentence embeddings","offset":1,"pro":0.2,"labels":"MTD"},{"idx":1,"sentence":"Our proposal achieved the 1st result for the second subtask, with an f1-score of 15.1% discerning among 30 different emotions","offset":2,"pro":0.4,"labels":"RST"},{"idx":1,"sentence":"However, our results for the first subtask were not recorded in the official leader board","offset":3,"pro":0.6,"labels":"RST"},{"idx":1,"sentence":"Accordingly, we report our results for this subtask with the validation split, reaching a macro f1-score of 32.360%","offset":4,"pro":0.8,"labels":"RST"},{"idx":2,"sentence":"Social media has become a dangerous place as bullies take advantage of the anonymity the Internet provides to target and intimidate vulnerable individuals and groups","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"In the past few years, the research community has focused on developing automatic classification tools for detecting hate-speech, its variants, and other types of abusive behaviour","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":2,"sentence":"However, these methods are still at an early stage in low-resource languages","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":2,"sentence":"With the aim of reducing this barrier, the TamilNLP shared task has proposed a multi-classification challenge for Tamil written in Tamil script and code-mixed to detect abusive comments and hope-speech","offset":3,"pro":0.5,"labels":"PUR"},{"idx":2,"sentence":"Our participation consists of a knowledge integration strategy that combines sentence embeddings from BERT, RoBERTa, FastText and a subset of language-independent linguistic features","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":2,"sentence":"We achieved our best result in code-mixed, reaching 3rd position with a macro-average f1-score of 35%","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":3,"sentence":"Social media platforms often act as breeding grounds for various forms of trolling or malicious content targeting users or communities","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"One way of trolling users is by creating memes, which in most cases unites an image with a short piece of text embedded on top of it","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":3,"sentence":"The situation is more complex for multilingual(e.g., Tamil) memes due to the lack of benchmark datasets and models","offset":2,"pro":0.2857142857142857,"labels":"BAC"},{"idx":3,"sentence":"We explore several models to detect Troll memes in Tamil based on the shared task, Troll Meme Classification in DravidianLangTech2022 at ACL-2022","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":3,"sentence":"We observe while the text-based model MURIL performs better for Non-troll meme classification, the image-based model VGG16 performs better for Troll-meme classification","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":3,"sentence":"Further fusing these two modalities help us achieve stable outcomes in both classes","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":3,"sentence":"Our fusion model achieved a 0.561 weighted average F1 score and ranked second in this task","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":4,"sentence":"Using technology for analysis of human emotion is a relatively nascent research area","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"There are several types of data where emotion recognition can be employed, such as - text, images, audio and video","offset":1,"pro":0.09090909090909091,"labels":"BAC"},{"idx":4,"sentence":"In this paper, the focus is on emotion recognition in text data","offset":2,"pro":0.18181818181818182,"labels":"PUR"},{"idx":4,"sentence":"Emotion recognition in text can be performed from both written comments and from conversations","offset":3,"pro":0.2727272727272727,"labels":"MTD"},{"idx":4,"sentence":"In this paper, the dataset used for emotion recognition is a list of comments","offset":4,"pro":0.36363636363636365,"labels":"MTD"},{"idx":4,"sentence":"While extensive research is being performed in this area, the language of the text plays a very important role","offset":5,"pro":0.45454545454545453,"labels":"MTD"},{"idx":4,"sentence":"In this work, the focus is on the Dravidian language of Tamil","offset":6,"pro":0.5454545454545454,"labels":"MTD"},{"idx":4,"sentence":"The language and its script demands an extensive pre-processing","offset":7,"pro":0.6363636363636364,"labels":"MTD"},{"idx":4,"sentence":"The paper contributes to this by adapting various pre-processing methods to the Dravidian Language of Tamil","offset":8,"pro":0.7272727272727273,"labels":"CTN"},{"idx":4,"sentence":"A CNN method has been adopted for the task at hand","offset":9,"pro":0.8181818181818182,"labels":"MTD"},{"idx":4,"sentence":"The proposed method has achieved a comparable result","offset":10,"pro":0.9090909090909091,"labels":"RST"},{"idx":5,"sentence":"Abusive language content such as hate speech, profanity, and cyberbullying etc., which is common in online platforms is creating lot of problems to the users as well as policy makers.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"Hence, detection of such abusive language in user-generated online content has become increasingly important over the past few years","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":5,"sentence":"Online platforms strive hard to moderate the abusive content to reduce societal harm, comply with laws, and create a more inclusive environment for their users","offset":2,"pro":0.2222222222222222,"labels":"BAC"},{"idx":5,"sentence":"In spite of various methods to automatically detect abusive languages in online platforms, the problem still persists","offset":3,"pro":0.3333333333333333,"labels":"GAP"},{"idx":5,"sentence":"To address the automatic detection of abusive languages in online platforms, this paper describes the models submitted by our team - MUCIC to the shared task on Abusive Comment Detection in Tamil-ACL 2022","offset":4,"pro":0.4444444444444444,"labels":"PUR"},{"idx":5,"sentence":"This shared task addresses the abusive comment detection in native Tamil script texts and code-mixed Tamil texts","offset":5,"pro":0.5555555555555556,"labels":"PUR"},{"idx":5,"sentence":"To address this challenge, two models: i) n-gram-Multilayer Perceptron (n-gram-MLP) model utilizing MLP classifier fed with char-n gram features and ii) 1D Convolutional Long Short-Term Memory (1D Conv-LSTM) model, were submitted","offset":6,"pro":0.6666666666666666,"labels":"PUR"},{"idx":5,"sentence":"The n-gram-MLP model fared well among these two models with weighted F1-scores of 0.560 and 0.430 for code-mixed Tamil and native Tamil script texts, respectively","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":5,"sentence":"This work may be reproduced using the code available in https://github.com/anushamdgowda/abusive-detection","offset":8,"pro":0.8888888888888888,"labels":"CTN"},{"idx":6,"sentence":"This paper describes the approach of team CEN-Tamil used for abusive comment detection in Tamil","offset":0,"pro":0,"labels":"PUR"},{"idx":6,"sentence":"This task aims to identify whether a given comment contains abusive comments","offset":1,"pro":0.2,"labels":"PUR"},{"idx":6,"sentence":"We used TF-IDF with char-wb analyzers with Random Kitchen Sink (RKS) algorithm to create feature vectors and the Support Vector Machine (SVM) classifier with polynomial kernel for classification","offset":2,"pro":0.4,"labels":"MTD"},{"idx":6,"sentence":"We used this method for both Tamil and Tamil-English datasets and secured first place with an f1-score of 0.32 and seventh place with an f1-score of 0.25, respectively","offset":3,"pro":0.6,"labels":"RST"},{"idx":6,"sentence":"The code for our approach is shared in the GitHub repository.","offset":4,"pro":0.8,"labels":"CTN"},{"idx":7,"sentence":"Toxic span identification in Tamil is a shared task that focuses on identifying harmful content, contributing to offensiveness","offset":0,"pro":0,"labels":"GAP"},{"idx":7,"sentence":"In this work, we have built a model that can efficiently identify the span of text contributing to offensive content","offset":1,"pro":0.25,"labels":"PUR"},{"idx":7,"sentence":"We have used various transformer-based models to develop the system,","offset":2,"pro":0.5,"labels":"MTD"},{"idx":7,"sentence":"out of which the fine-tuned MuRIL model was able to achieve the best overall character F1-score of 0.4489","offset":3,"pro":0.75,"labels":"RST"},{"idx":8,"sentence":"The spread of fake news, propaganda, misinformation, disinformation, and harmful content online raised concerns among social mediaplatforms, government agencies, policymakers, and society as a whole","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"This is because such harmful or abusive content leads to several consequences to people such as physical, emotional, relational, and financial","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":8,"sentence":"Among different harmful content trolling-based online content is one of them, where the idea is to post a message that is provocative, offensive, or menacing with an intent to mislead the audience","offset":2,"pro":0.3333333333333333,"labels":"BAC"},{"idx":8,"sentence":"The content can be textual, visual, a combination of both, or a meme. In this study, we provide a comparative analysis of troll-based memes classification using the textual, visual, and multimodal content","offset":3,"pro":0.5,"labels":"BAC"},{"idx":8,"sentence":"We report several interesting findings in terms of code-mixed text, multimodal setting, and combining an additional dataset,","offset":4,"pro":0.6666666666666666,"labels":"PUR"},{"idx":8,"sentence":"which shows improvements over the majority baseline","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":9,"sentence":"This paper describes the systems built by our team for the Emotion Analysis in Tamil shared task at the Second Workshop on Speech and Language Technologies for Dravidian Languages at ACL 2022","offset":0,"pro":0,"labels":"PUR"},{"idx":9,"sentence":"There were two multi-class classification sub-tasks as a part of this shared task","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":9,"sentence":"The dataset for sub-task A contained 11 types of emotions while sub-task B was more fine-grained with 31 emotions","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":9,"sentence":" For sub-task A, the XLM-RoBERTa model achieved an accuracy of 0.46 and the DeBERTa model achieved an accuracy of 0.45","offset":3,"pro":0.42857142857142855,"labels":"RST"},{"idx":9,"sentence":"We had the best classification performance out of 11 teams for sub-task A","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":9,"sentence":"For sub-task B, the XLM-RoBERTa model's accuracy was 0.33 and the DeBERTa model had an accuracy of 0.26","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":9,"sentence":"We ranked 2nd out of 7 teams for sub-task B","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":10,"sentence":"This paper presents transformer-based models for the Abusive Comment Detection shared task at the Second Workshop on Speech and Language Technologies for Dravidian Languages at ACL 2022","offset":0,"pro":0,"labels":"PUR"},{"idx":10,"sentence":"Our team participated in both the multi-class classification sub-tasks as a part of this shared task. The dataset for sub-task A was in Tamil text; while B was code-mixed Tamil-English sentence","offset":1,"pro":0.125,"labels":"MTD"},{"idx":10,"sentence":"Both the datasets contained 8 classes of abusive comments","offset":2,"pro":0.25,"labels":"MTD"},{"idx":10,"sentence":"We trained an XLM-RoBERTa and DeBERTA base model on the training splits for each sub-task","offset":3,"pro":0.375,"labels":"MTD"},{"idx":10,"sentence":"For sub-task A, the XLM-RoBERTa model achieved an accuracy of 0.66 and the DeBERTa model achieved an accuracy of 0.62","offset":4,"pro":0.5,"labels":"RST"},{"idx":10,"sentence":"For sub-task B, both the models achieved a classification accuracy of 0.72","offset":5,"pro":0.625,"labels":"RST"},{"idx":10,"sentence":"however, the DeBERTa model performed better in other classification metrics","offset":6,"pro":0.75,"labels":"RST"},{"idx":10,"sentence":"Our team ranked 2nd in the code-mixed classification sub-task and 8th in Tamil-text sub-task","offset":7,"pro":0.875,"labels":"RST"},{"idx":11,"sentence":"Identifying abusive content or hate speech in social media text has raised the research community's interest in recent times","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"The major driving force behind this is the widespread use of social media websites","offset":1,"pro":0.125,"labels":"BAC"},{"idx":11,"sentence":"Further, it also leads to identifying abusive content in low-resource regional languages, which is an important research problem in computational linguistics","offset":2,"pro":0.25,"labels":"BAC"},{"idx":11,"sentence":"As part of ACL-2022, organizers of DravidianLangTech@ACL 2022 have released a shared task on abusive category identification in Tamil and Tamil-English code-mixed text to encourage further research on offensive content identification in low-resource Indic languages","offset":3,"pro":0.375,"labels":"PUR"},{"idx":11,"sentence":"This paper presents the working notes for the model submitted by IIITDWD at DravidianLangTech@ACL 2022","offset":4,"pro":0.5,"labels":"PUR"},{"idx":11,"sentence":"Our team competed in Sub-Task B and finished in 9th place among the participating teams","offset":5,"pro":0.625,"labels":"MTD"},{"idx":11,"sentence":"In our proposed approach, we used a pre-trained transformer model such as Indic-bert for feature extraction, and on top of that, SVM classifier is used for stance detection","offset":6,"pro":0.75,"labels":"MTD"},{"idx":11,"sentence":"Further, our model achieved 62 % accuracy on code-mixed Tamil-English sentence","offset":7,"pro":0.875,"labels":"RST"},{"idx":12,"sentence":"As the world around us continues to become increasingly digital, it has been acknowledged that there is a growing need for emotion analysis of social media content","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"The task of identifying the emotion in a given text has many practical applications ranging from screening public health to business and management","offset":1,"pro":0.25,"labels":"BAC"},{"idx":12,"sentence":"n this paper, we propose a language-agnostic model that focuses on emotion analysis in Tamil sentence","offset":2,"pro":0.5,"labels":"PUR"},{"idx":12,"sentence":"Our experiments yielded an F1-score of 0.010","offset":3,"pro":0.75,"labels":"RST"},{"idx":13,"sentence":"Abusive language has lately been prevalent in comments on various social media platforms","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"The increasing hostility observed on the internet calls for the creation of a system that can identify and flag such acerbic content, to prevent conflict and mental distress","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":13,"sentence":"This task becomes more challenging when low-resource languages like Tamil, as well as the often-observed Tamil-English code-mixed text, are involved","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":13,"sentence":"The approach used in this paper for the classification model includes different methods of feature extraction and the use of traditional classifiers","offset":3,"pro":0.5,"labels":"MTD"},{"idx":13,"sentence":"We propose a novel method of combining language-agnostic sentence embeddings with the TF-IDF vector representation that uses a curated corpus of words as vocabulary, to create a custom embedding, which is then passed to an SVM classifier","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":13,"sentence":"Our experimentation yielded an accuracy of 52% and an F1-score of 0.54","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":14,"sentence":"In connection to the task, five language pairs were provided to test the accuracy of submitted model","offset":0,"pro":0,"labels":"MTD"},{"idx":14,"sentence":"A baseline transformer model with Neural Machine Translation(NMT) technique is used which has been taken directly from the OpenNMT framework","offset":1,"pro":0.25,"labels":"MTD"},{"idx":14,"sentence":"On this baseline model, tokenization is applied using the IndicNLP library","offset":2,"pro":0.5,"labels":"MTD"},{"idx":14,"sentence":"Finally, the evaluation is performed using the BLEU scoring mechanism","offset":3,"pro":0.75,"labels":"MTD"},{"idx":15,"sentence":"Emotion analysis is the process of identifying and analyzing the underlying emotions expressed in textual data","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"Identifying emotions from a textual conversation is a challenging task due to the absence of gestures, vocal intonation, and facial expressions","offset":1,"pro":0.125,"labels":"GAP"},{"idx":15,"sentence":"Once the chatbots and messengers detect and report the emotions of the user, a comfortable conversation can be carried out with no misunderstandings","offset":2,"pro":0.25,"labels":"GAP"},{"idx":15,"sentence":"Our task is to categorize text into a predefined notion of emotion","offset":3,"pro":0.375,"labels":"PUR"},{"idx":15,"sentence":"In this thesis, it is required to classify text into several emotional labels depending on the task","offset":4,"pro":0.5,"labels":"MTD"},{"idx":15,"sentence":"We have adopted the transformer model approach to identify the emotions present in the text sequence","offset":5,"pro":0.625,"labels":"MTD"},{"idx":15,"sentence":"Our task is to identify whether a given comment contains emotion, and the emotion it stands for","offset":6,"pro":0.75,"labels":"PUR"},{"idx":15,"sentence":"The datasets were provided to us by the LT-EDI organizers (CITATION) for two tasks, in the Tamil language","offset":7,"pro":0.875,"labels":"MTD"},{"idx":16,"sentence":"The ACL shared task of DravidianLangTech-2022 for Troll Meme classification is a binary classification task that involves identifying Tamil memes as troll or not-troll","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"Classification of memes is a challenging task since memes express humour and sarcasm in an implicit way","offset":1,"pro":0.2,"labels":"GAP"},{"idx":16,"sentence":"Team SSN{_MLRG1 tested and compared results obtained by using three models namely BERT, ALBERT and XLNET","offset":2,"pro":0.4,"labels":"MTD"},{"idx":16,"sentence":"The XLNet model outperformed the other two models in terms of various performance metrics","offset":3,"pro":0.6,"labels":"RST"},{"idx":16,"sentence":"The proposed XLNet model obtained the 3rd rank in the shared task with a weighted F1-score of 0.558","offset":4,"pro":0.8,"labels":"RST"},{"idx":17,"sentence":"Social Media platforms have grown their reach worldwide","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"As an effect of this growth, many vernacular social media platforms have also emerged, focusing more on the diverse languages in the specific regions","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":17,"sentence":"Tamil has also emerged as a popular language for use on social media platforms due to the increasing penetration of vernacular media like Sharechat and Moj, which focus more on local Indian languages than English and encourage their users to converse in Indic languages","offset":2,"pro":0.3333333333333333,"labels":"BAC"},{"idx":17,"sentence":"Abusive language remains a significant challenge in the social media framework and more so when we consider languages like Tamil, which are low-resource languages and have poor performance on multilingual models and lack language-specific models","offset":3,"pro":0.5,"labels":"GAP"},{"idx":17,"sentence":"Based on this shared task, Abusive Comment detection in Tamil@DravidianLangTech-ACL 2022, we present an exploration of different techniques used to tackle and increase the accuracy of our models using data augmentation in NLP","offset":4,"pro":0.6666666666666666,"labels":"PUR"},{"idx":17,"sentence":"We also show the results of these techniques","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":18,"sentence":"Emotion Analysis (EA) is the process of automatically analyzing and categorizing the input text into one of the predefined sets of emotions","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"In recent years, people have turned to social media to express their emotions, opinions or feelings about news, movies, products, services, and so on","offset":1,"pro":0.09090909090909091,"labels":"BAC"},{"idx":18,"sentence":"These users' emotions may help the public, governments, business organizations, film producers, and others in devising strategies, making decisions, and so on","offset":2,"pro":0.18181818181818182,"labels":"BAC"},{"idx":18,"sentence":"The increasing number of social media users and the increasing amount of user generated text containing emotions on social media demands automated tools for the analysis of such data as handling this data manually is labor intensive and error prone","offset":3,"pro":0.2727272727272727,"labels":"BAC"},{"idx":18,"sentence":"Further, the characteristics of social media data makes the EA challenging","offset":4,"pro":0.36363636363636365,"labels":"GAP"},{"idx":18,"sentence":"Most of the EA research works have focused on English language leaving several Indian languages including Tamil unexplored for this task","offset":5,"pro":0.45454545454545453,"labels":"GAP"},{"idx":18,"sentence":"To address the challenges of EA in Tamil texts, in this paper, we - team MUCS, describe the model submitted to the shared task on Emotion Analysis in Tamil at DravidianLangTech@ACL 2022","offset":6,"pro":0.5454545454545454,"labels":"PUR"},{"idx":18,"sentence":"Out of the two subtasks in this shared task, our team submitted the model only for Task a","offset":7,"pro":0.6363636363636364,"labels":"MTD"},{"idx":18,"sentence":"The proposed model comprises of an Ensemble of Logistic Regression (LR) classifiers with three penalties, namely: L1, L2, and Elasticnet","offset":8,"pro":0.7272727272727273,"labels":"MTD"},{"idx":18,"sentence":"This Ensemble model trained with Term Frequency - Inverse Document Frequency (TF-IDF) of character bigrams and trigrams secured 4th rank in Task a with a macro averaged F1-score of 0.04","offset":9,"pro":0.8181818181818182,"labels":"RST"},{"idx":18,"sentence":"The code to reproduce the proposed models is available in github1","offset":10,"pro":0.9090909090909091,"labels":"CTN"},{"idx":19,"sentence":"Trolling refers to any user behaviour on the internet to intentionally provoke or instigate conflict predominantly in social media","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"This paper aims to classify troll meme captions in Tamil-English code-mixed form","offset":1,"pro":0.2,"labels":"PUR"},{"idx":19,"sentence":"Embeddings are obtained for raw code-mixed text and the translated and transliterated version of the text and their relative performances are compared","offset":2,"pro":0.4,"labels":"MTD"},{"idx":19,"sentence":"Furthermore, this paper compares the performances of 11 different classification algorithms using Accuracy and F1- Score","offset":3,"pro":0.6,"labels":"MTD"},{"idx":19,"sentence":"We conclude that we were able to achieve a weighted F1 score of 0.74 through MuRIL pretrained model","offset":4,"pro":0.8,"labels":"RST"},{"idx":20,"sentence":"Social media platforms along with many other public forums on the Internet have shown a significant rise in the cases of abusive behavior such as Misogynism, Misandry, Homophobia, and Cyberbullying","offset":0,"pro":0,"labels":"BAC"},{"idx":20,"sentence":"To tackle these concerns","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":20,"sentence":"technologies are being developed and applied, as it is a tedious and time-consuming task to identify, report and block these offenders","offset":2,"pro":0.2857142857142857,"labels":"BAC"},{"idx":20,"sentence":"Our task was to automate the process of identifying abusive comments and classify them into appropriate categories","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":20,"sentence":"The datasets provided by the DravidianLangTech@ACL2022 organizers were a code-mixed form of Tamil sentence","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":20,"sentence":"We trained the datasets using pre-trained transformer models such as BERT,m-BERT, and XLNET","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":20,"sentence":"and achieved a weighted average of F1 scores of 0.96 for Tamil-English code mixed text and 0.59 for Tamil sentence","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":21,"sentence":"In this paper, we present our system for the task of Emotion analysis in Tamil","offset":0,"pro":0,"labels":"PUR"},{"idx":21,"sentence":"Over 3.96 million people use these platforms to send messages formed using texts, images, videos, audio or combinations of these to express their thoughts and feelings","offset":1,"pro":0.125,"labels":"MTD"},{"idx":21,"sentence":"Text communication on social media platforms is quite overwhelming due to its enormous quantity and simplicity","offset":2,"pro":0.25,"labels":"MTD"},{"idx":21,"sentence":"The data must be processed to understand the general feeling felt by the author","offset":3,"pro":0.375,"labels":"MTD"},{"idx":21,"sentence":"We present a lexicon-based approach for the extraction emotion in Tamil texts","offset":4,"pro":0.5,"labels":"MTD"},{"idx":21,"sentence":"We use dictionaries of words labelled with their respective emotions","offset":5,"pro":0.625,"labels":"MTD"},{"idx":21,"sentence":"The process of assigning an emotional label to each text, and then capture the main emotion expressed in it","offset":6,"pro":0.75,"labels":"MTD"},{"idx":21,"sentence":"Finally, the F1-score in the official test set is 0.0300 and our method ranks 5th","offset":7,"pro":0.875,"labels":"RST"},{"idx":22,"sentence":"With the substantial rise of internet usage, social media has become a powerful communication medium to convey information, opinions, and feelings on various issues. Recently, memes have become a popular way of sharing information on social media","offset":0,"pro":0,"labels":"BAC"},{"idx":22,"sentence":"Usually, memes are visuals with text incorporated into them and quickly disseminate hatred and offensive content","offset":1,"pro":0.1,"labels":"BAC"},{"idx":22,"sentence":"Detecting or classifying memes is challenging due to their region-specific interpretation and multimodal nature","offset":2,"pro":0.2,"labels":"GAP"},{"idx":22,"sentence":"This work presents a meme classification technique in Tamil developed by the CUET NLP team under the shared task (DravidianLangTech-ACL2022)","offset":3,"pro":0.3,"labels":"PUR"},{"idx":22,"sentence":"Several computational models have been investigated to perform the classification task","offset":4,"pro":0.4,"labels":"MTD"},{"idx":22,"sentence":"This work also explored visual and textual features using VGG16, ResNet50, VGG19, CNN and CNN+LSTM models","offset":5,"pro":0.5,"labels":"MTD"},{"idx":22,"sentence":"Multimodal features are extracted by combining image (VGG16) and text (CNN, LSTM+CNN) characteristics","offset":6,"pro":0.6,"labels":"MTD"},{"idx":22,"sentence":"Results demonstrate that the textual strategy with CNN+LSTM achieved the highest weighted f_1-score (0.52) and recall (0.57)","offset":7,"pro":0.7,"labels":"RST"},{"idx":22,"sentence":"Moreover, the CNN-Text+VGG16 outperformed the other models concerning the multimodal memes detection by achieving the highest f_1-score of 0.49","offset":8,"pro":0.8,"labels":"RST"},{"idx":22,"sentence":"but the LSTM+CNN model allowed the team to achieve 4^{th place in the shared task","offset":9,"pro":0.9,"labels":"RST"},{"idx":23,"sentence":"This paper presents a summary of the findings that we obtained based on the shared task on machine translation of Dravidian languages","offset":0,"pro":0,"labels":"PUR"},{"idx":23,"sentence":"As a part of this shared task, we carried out neural machine translations for the following five language pairs: Kannada to Tamil, Kannada to Telugu, Kannada to Malayalam, Kannada to Sanskrit, and Kannada to Tulu","offset":1,"pro":0.2,"labels":"MTD"},{"idx":23,"sentence":"The datasets for each of the five language pairs were used to train various translation models, including Seq2Seq models such as LSTM, bidirectional LSTM, Conv Seq2Seq, and training state-of-the-art as transformers from scratch, and fine-tuning already pre-trained models","offset":2,"pro":0.4,"labels":"MTD"},{"idx":23,"sentence":"For some models involving monolingual corpora, we implemented backtranslation as well","offset":3,"pro":0.6,"labels":"MTD"},{"idx":23,"sentence":"These models' accuracy was later tested with a part of the same dataset using BLEU score as an evaluation metric","offset":4,"pro":0.8,"labels":"MTD"},{"idx":24,"sentence":"Code-switching refers to the textual or spoken data containing multiple languages","offset":0,"pro":0,"labels":"BAC"},{"idx":24,"sentence":"Application of natural language processing (NLP) tasks like sentiment analysis is a harder problem on code-switched languages due to the irregularities in the sentence structuring and ordering","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":24,"sentence":"This paper shows the experiment results of building a Kernel based Extreme Learning Machines(ELM) for sentiment analysis for code-switched Dravidian languages with English","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":24,"sentence":"Our results show that ELM performs better than traditional machine learning classifiers on various metrics as well as trains faster than deep learning models","offset":3,"pro":0.5,"labels":"MTD"},{"idx":24,"sentence":"We also show that Polynomial kernels perform better than others in the ELM architecture","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":24,"sentence":"We were able to achieve a median AUC of 0.79 with a polynomial kernel","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":25,"sentence":"With the proliferation of internet usage, a massive growth of consumer-generated content on social media has been witnessed in recent years that provide people's opinions on diverse issues","offset":0,"pro":0,"labels":"BAC"},{"idx":25,"sentence":"Through social media, users can convey their emotions and thoughts in distinctive forms such as text, image, audio, video, and emoji, which leads to the advancement of the multimodality of the content users on social networking sites","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":25,"sentence":"This paper presents a technique for classifying multimodal sentiment using the text modality into five categories: highly positive, positive, neutral, negative, and highly negative categories","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":25,"sentence":"A shared task was organized to develop models that can identify the sentiments expressed by the videos of movie reviewers in both Malayalam and Tamil languages","offset":3,"pro":0.5,"labels":"MTD"},{"idx":25,"sentence":"This work applied several machine learning techniques (LR, DT, MNB, SVM) and deep learning (BiLSTM, CNN+BiLSTM) to accomplish the task","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":25,"sentence":"Results demonstrate that the proposed model with the decision tree (DT) outperformed the other methods and won the competition by acquiring the highest macro f_1-score of 0.24","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":26,"sentence":"Recently, emotion analysis has gained increased attention by NLP researchers due to its various applications in opinion mining, e-commerce, comprehensive search, healthcare, personalized recommendations and online education","offset":0,"pro":0,"labels":"BAC"},{"idx":26,"sentence":"Developing an intelligent emotion analysis model is challenging in resource-constrained languages like Tamil","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":26,"sentence":"Therefore a shared task is organized to identify the underlying emotion of a given comment expressed in the Tamil language","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":26,"sentence":"The paper presents our approach to classifying the textual emotion in Tamil into 11 classes: ambiguous, anger, anticipation, disgust, fear, joy, love, neutral, sadness, surprise and trust","offset":3,"pro":0.5,"labels":"PUR"},{"idx":26,"sentence":"We investigated various machine learning (LR, DT, MNB, SVM), deep learning (CNN, LSTM, BiLSTM) and transformer-based models (Multilingual-BERT, XLM-R)","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":26,"sentence":"Results reveal that the XLM-R model outdoes all other models by acquiring the highest macro f_1-score (0.33)","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":27,"sentence":"Online Social Network has let people to connect and interact with each other","offset":0,"pro":0,"labels":"BAC"},{"idx":27,"sentence":"It does, however, also provide a platform for online abusers to propagate abusive content","offset":1,"pro":0.1,"labels":"GAP"},{"idx":27,"sentence":"The vast majority of abusive remarks are written in a multilingual style, which allows them to easily slip past internet inspection","offset":2,"pro":0.2,"labels":"GAP"},{"idx":27,"sentence":"This paper presents a system developed for the Shared Task on Abusive Comment Detection (Misogyny, Misandry, Homophobia, Transphobic, Xenophobia, CounterSpeech, Hope Speech) in Tamil DravidianLangTech@ACL 2022 to detect the abusive category of each comment","offset":3,"pro":0.3,"labels":"PUR"},{"idx":27,"sentence":"We approach the task with three methodologies - Machine Learning, Deep Learning and Transformer-based modeling, for two sets of data - Tamil and Tamil+English language dataset","offset":4,"pro":0.4,"labels":"MTD"},{"idx":27,"sentence":"The dataset used in our system can be accessed from the competition on CodaLab","offset":5,"pro":0.5,"labels":"MTD"},{"idx":27,"sentence":"For Machine Learning, eight algorithms were implemented, among which Random Forest gave the best result with Tamil+English dataset, with a weighted average F1-score of 0.78","offset":6,"pro":0.6,"labels":"RST"},{"idx":27,"sentence":"For Deep Learning, Bi-Directional LSTM gave best result with pre-trained word embeddings","offset":7,"pro":0.7,"labels":"RST"},{"idx":27,"sentence":"In Transformer-based modeling, we used IndicBERT and mBERT with fine-tuning","offset":8,"pro":0.8,"labels":"MTD"},{"idx":27,"sentence":"among which mBERT gave the best result for Tamil dataset with a weighted average F1-score of 0.7","offset":9,"pro":0.9,"labels":"RST"},{"idx":28,"sentence":"n social media, there are instances where people present their opinions in strong language, resorting to abusive/toxic comments","offset":0,"pro":0,"labels":"BAC"},{"idx":28,"sentence":"There are instances of communal hatred, hate-speech, toxicity and bullying","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":28,"sentence":"And, in this age of social media, it's very important to find means to keep check on these toxic comments, as to preserve the mental peace of people in social media","offset":2,"pro":0.2857142857142857,"labels":"BAC"},{"idx":28,"sentence":"While there are tools, models to detect andpotentially filter these kind of content, developing these kinds of models for the low resource language space is an issue of research","offset":3,"pro":0.42857142857142855,"labels":"GAP"},{"idx":28,"sentence":"In this paper, the task of abusive comment identification in Tamil language, is seen upon as a multi-class classification problem","offset":4,"pro":0.5714285714285714,"labels":"PUR"},{"idx":28,"sentence":"There are different pre-processing as well as modelling approaches discussed in this paper","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":28,"sentence":"The different approaches are compared on the basis of weighted average accuracy","offset":6,"pro":0.8571428571428571,"labels":"MTD"},{"idx":29,"sentence":"With the widespread usage of social media and effortless internet access, millions of posts and comments are generated every minute","offset":0,"pro":0,"labels":"BAC"},{"idx":29,"sentence":"Unfortunately, with this substantial rise, the usage of abusive language has increased significantly in these mediums","offset":1,"pro":0.1,"labels":"GAP"},{"idx":29,"sentence":"This proliferation leads to many hazards such as cyber-bullying, vulgarity, online harassment and abuse","offset":2,"pro":0.2,"labels":"GAP"},{"idx":29,"sentence":"Therefore, it becomes a crucial issue to detect and mitigate the usage of abusive language","offset":3,"pro":0.3,"labels":"BAC"},{"idx":29,"sentence":"This work presents our system developed as part of the shared task to detect the abusive language in Tamil","offset":4,"pro":0.4,"labels":"PUR"},{"idx":29,"sentence":"We employed three machine learning (LR, DT, SVM), two deep learning (CNN+BiLSTM, CNN+BiLSTM with FastText) and a transformer-based model (Indic-BERT)","offset":5,"pro":0.5,"labels":"MTD"},{"idx":29,"sentence":"The experimental results show that Logistic regression (LR) and CNN+BiLSTM models outperformed the others","offset":6,"pro":0.6,"labels":"CLN"},{"idx":29,"sentence":"Both Logistic Regression (LR) and CNN+BiLSTM with FastText achieved the weighted F_1-score of 0.39","offset":7,"pro":0.7,"labels":"RST"},{"idx":29,"sentence":"However, LR obtained a higher recall value (0.44) than CNN+BiLSTM (0.36)","offset":8,"pro":0.8,"labels":"RST"},{"idx":29,"sentence":"This leads us to stand the 2^{nd rank in the shared task competition","offset":9,"pro":0.9,"labels":"RST"},{"idx":0,"sentence":"This paper aims to perform an emotion analysis of social media comments in Tamil.","offset":0,"pro":0,"labels":"PUR"},{"idx":0,"sentence":"Emotion analysis is the process of identifying the emotional context of the text.","offset":1,"pro":0.14285714285714285,"labels":"MTD"},{"idx":0,"sentence":"In this paper, we present the findings obtained by Team Optimize{_Prime in the ACL 2022 shared task Emotion Analysis in Tamil.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":0,"sentence":"The task aimed to classify social media comments into categories of emotion like Joy, Anger, Trust, Disgust, etc.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":0,"sentence":"The task was further divided into two subtasks, one with 11 broad categories of emotions and the other with 31 specific categories of emotion.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":0,"sentence":"We implemented three different approaches to tackle this problem: transformer-based models, Recurrent Neural Networks (RNNs), and Ensemble models.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":0,"sentence":"XLM-RoBERTa performed the best on the first task with a macro-averaged f1 score of 0.27, while MuRIL provided the best results on the second task with a macro-averaged f1 score of 0.13.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":1,"sentence":"This paper tries to address the problem of abusive comment detection in low-resource indic languages.","offset":0,"pro":0,"labels":"PUR"},{"idx":1,"sentence":"Abusive comments are statements that are offensive to a person or a group of people.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":1,"sentence":"These comments are targeted toward individuals belonging to specific ethnicities, genders, caste, race, sexuality, etc.","offset":2,"pro":0.2222222222222222,"labels":"BAC"},{"idx":1,"sentence":"Abusive Comment Detection is a significant problem, especially with the recent rise in social media users.","offset":3,"pro":0.3333333333333333,"labels":"BAC"},{"idx":1,"sentence":"This paper presents the approach used by our team - Optimize{_Prime, in the ACL 2022 shared task Abusive Comment Detection in Tamil.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":1,"sentence":"This task detects and classifies YouTube comments in Tamil and Tamil-English Codemixed format into multiple categories.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":1,"sentence":"We have used three methods to optimize our results: Ensemble models, Recurrent Neural Networks, and Transformers.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":1,"sentence":"In the Tamil data, MuRIL and XLM-RoBERTA were our best performing models with a macro-averaged f1 score of 0.43.","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":1,"sentence":"Furthermore, for the Code-mixed data, MuRIL and M-BERT provided sublime results, with a macro-averaged f1 score of 0.45.","offset":8,"pro":0.8888888888888888,"labels":"RST"},{"idx":2,"sentence":"This paper investigates the effectiveness of sentence-level transformers for zero-shot offensive span identification on a code-mixed Tamil dataset.","offset":0,"pro":0,"labels":"PUR"},{"idx":2,"sentence":"More specifically, we evaluate rationale extraction methods of Local Interpretable Model Agnostic Explanations (LIME) (CITATION) and Integrated Gradients (IG) (CITATION) for adapting transformer based offensive language classification models for zero-shot offensive span identification.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":2,"sentence":"To this end, we find that LIME and IG show baseline F_{1 of 26.35% and 44.83%, respectively.","offset":2,"pro":0.4,"labels":"RST"},{"idx":2,"sentence":"Besides, we study the effect of data set size and training process on the overall accuracy of span identification.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":2,"sentence":"As a result, we find both LIME and IG to show significant improvement with Masked Data Augmentation and Multilabel Training, with F_{1 of 50.23% and 47.38% respectively.","offset":4,"pro":0.8,"labels":"RST"},{"idx":3,"sentence":"This paper presents our sys-tem submission to the subtask 1, focusing onusing supervised approaches for extracting Of-fensive spans from code-mixed Tamil-Englishcomments.","offset":0,"pro":0,"labels":"PUR"},{"idx":3,"sentence":"To identify offensive spans, wedeveloped the Bidirectional Long Short-TermMemory (BiLSTM) model with Glove Em-bedding.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":3,"sentence":"To this end, the developed systemachieved an overall F1 of 0.1728.","offset":2,"pro":0.4,"labels":"RST"},{"idx":3,"sentence":"Addition-ally, for comments with less than 30 characters,the developed system shows an F1 of 0.3890,","offset":3,"pro":0.6,"labels":"RST"},{"idx":3,"sentence":"competitive with other submissions.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":4,"sentence":"This paper presents the findings of the shared task on Multimodal Sentiment Analysis and Troll meme classification in Dravidian languages held at ACL 2022.","offset":0,"pro":0,"labels":"PUR"},{"idx":4,"sentence":"Multimodal sentiment analysis deals with the identification of sentiment from video. ","offset":1,"pro":0.5,"labels":"BAC"},{"idx":5,"sentence":"Offensive content moderation is vital in social media platforms to support healthy online discussions. ","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"However, their prevalence in code-mixed Dravidian languages is limited to classifying whole comments without identifying part of it contributing to offensiveness","offset":1,"pro":0.25,"labels":"GAP"},{"idx":5,"sentence":"Such limitation is primarily due to the lack of annotated data for offensive spans.","offset":2,"pro":0.5,"labels":"GAP"},{"idx":5,"sentence":"Accordingly, in this shared task, we provide Tamil-English code-mixed social comments with offensive spans.","offset":3,"pro":0.75,"labels":"PUR"},{"idx":6,"sentence":"A description of the datasets used, approach taken for analysis of submissions and the results have been illustrated in this paper.","offset":0,"pro":0,"labels":"PUR"},{"idx":9,"sentence":"The social media is one of the significantdigital platforms that create a huge im-pact in peoples of all levels.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"Dialogue summarization helps users capture salient information from various types of dialogues has received much attention recently.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"However, current works mainly focus on English dialogue summarization, leaving other languages less well explored.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":10,"sentence":"Therefore, we present a multi-lingual dialogue summarization dataset, namely MSAMSum, which covers dialogue-summary pairs in six languages. ","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":10,"sentence":"Specifically, we derive MSAMSum from the standard SAMSum using sophisticated translation techniques and further employ two methods to ensure the integral translation quality and summary factual consistency.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":10,"sentence":"Given the proposed MSAMum, we systematically set up five multi-lingual settings for this task, including a novel mix-lingual dialogue summarization setting.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":10,"sentence":"To illustrate the utility of our dataset, we benchmark various experiments with pre-trained models under different settings and report results in both supervised and zero-shot manners.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":10,"sentence":"We also discuss some future works towards this task to motivate future researches.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":11,"sentence":"With the advances in deep learning, tremendous progress has been made with chit-chat dialogue systems and task-oriented dialogue systems.","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"However, these two systems are often tackled separately in current methods.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":11,"sentence":"To achieve more natural interaction with humans, dialogue systems need to be capable of both chatting and accomplishing tasks.","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":11,"sentence":"To this end, we propose a unified dialogue system (UniDS) with the two aforementioned skills.","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":11,"sentence":"In particular, we design a unified dialogue data schema, compatible for both chit-chat and task-oriented dialogues.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":11,"sentence":"Besides, we propose a two-stage training method to train UniDS based on the unified dialogue data schema.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":11,"sentence":"UniDS does not need to adding extra parameters to existing chit-chat dialogue systems.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":11,"sentence":"Experimental results demonstrate that the proposed UniDS works comparably well as the state-of-the-art chit-chat dialogue systems and task-oriented dialogue systems.","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":11,"sentence":"More importantly, UniDS achieves better robustness than pure dialogue systems and satisfactory switch ability between two types of dialogues.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":12,"sentence":"Recent work building open-domain chatbots has demonstrated that increasing model size improves performance (Adiwardana et al., 2020; Roller et al., 2020).","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"On the other hand, latency and connectivity considerations dictate the move of digital assistants on the device (Verge, 2021).","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":12,"sentence":"Giving a digital assistant like Siri, Alexa, or Google Assistant the ability to discuss just about anything leads to the need for reducing the chatbot model size such that it fits on the user's device.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":12,"sentence":"Additionally, we propose a generic framework that accounts for variety in question types, tracks reference throughout multi-turn conversations, and removes inconsistent and potentially toxic responses.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":12,"sentence":"We demonstrate that low parameter models can simultaneously retain their general knowledge conversational abilities while improving in a specific domain.","offset":4,"pro":0.5714285714285714,"labels":"CLN"},{"idx":12,"sentence":"We evaluate our framework on 1 internal and 4 public benchmark datasets using both automatic (Perplexity) and human (SSA - Sensibleness and Specificity Average) evaluation metrics and establish comparable performance while reducing model parameters by 90%.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":12,"sentence":"Our framework seamlessly transitions between chatting and performing transactional tasks, which will ultimately make interactions with digital assistants more human-like.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":13,"sentence":"Question answering (QA) with disambiguation questions is essential for practical QA systems because user questions often do not contain information enough to find their answers. ","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"There are two major problems in building a clarifying question answering system: data preparation of possible ambiguous questions and the generation of clarifying questions.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":13,"sentence":"In this paper, we tackle these problems by sentence generation methods using sentence structures.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":13,"sentence":"Ambiguous questions are generated by eliminating a part of a sentence considering the sentence structure.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":13,"sentence":"Clarifying the question generation method based on case frame dictionary and sentence structure is also proposed.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":13,"sentence":"Our experimental results verify that our pseudo ambiguous question generation successfully adds ambiguity to questions.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":13,"sentence":"Moreover, the proposed clarifying question generation recovers the performance drop by asking the user for missing information.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":14,"sentence":"A long-term ambition of information seeking QA systems is to reason over multi-modal contexts and generate natural answers to user queries.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"Today, memory intensive pre-trained language models are adapted to downstream tasks such as QA by fine-tuning the model on QA data in a specific modality like unstructured text or structured tables.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":14,"sentence":"To avoid training such memory-hungry models while utilizing a uniform architecture for each modality, parameter-efficient adapters add and train small task-specific bottle-neck layers between transformer layers.","offset":2,"pro":0.25,"labels":"BAC"},{"idx":14,"sentence":"In this work, we study parameter-efficient abstractive QA in encoder-decoder models over structured tabular data and unstructured textual data","offset":3,"pro":0.375,"labels":"PUR"},{"idx":14,"sentence":"using only 1.5% additional parameters for each modality.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":14,"sentence":"We also ablate over adapter layers in both encoder and decoder modules to study the efficiency-performance trade-off","offset":5,"pro":0.625,"labels":"MTD"},{"idx":14,"sentence":"and demonstrate that reducing additional trainable parameters down to 0.7%-1.0% leads to comparable results.","offset":6,"pro":0.75,"labels":"RST"},{"idx":14,"sentence":"Our models out-perform current state-of-the-art models on tabular QA datasets such as Tablesum and FeTaQA, and achieve comparable performance on a textual QA dataset such as NarrativeQA using significantly less trainable parameters than fine-tuning.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":15,"sentence":"This task is referred as dialogue disentanglement. ","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"In this paper, we propose a hierarchical model, named Dialogue BERT (DIALBERT), which integrates the local and global semantics in the context range by using BERT to encode each message-pair and using BiLSTM to aggregate the chronological context information into the output of BERT.","offset":1,"pro":0.25,"labels":"PUR"},{"idx":15,"sentence":"In order to integrate the conversation structure information into the model, two types of loss of conversation-structure loss and tree-structure loss are designed.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":15,"sentence":"In this way, our model can implicitly learn and leverage the conversation structures without being restricted to the lack of explicit access to such structures during the inference stage.","offset":3,"pro":0.75,"labels":"MTD"},{"idx":16,"sentence":"We focus on the task of selecting the next clarification question, given conversation context.","offset":0,"pro":0,"labels":"MTD"},{"idx":16,"sentence":"Our method leverages passage retrieval from background content to fine-tune two deep-learning models for ranking candidate clarification questions.","offset":1,"pro":0.125,"labels":"MTD"},{"idx":16,"sentence":"We evaluated our method on two different use-cases.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":16,"sentence":"The first is an open domain conversational search in a large web collection.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":16,"sentence":"We show that our method performs well on both use-cases.","offset":4,"pro":0.5,"labels":"RST"},{"idx":16,"sentence":"We deal with the scenario of conversational search, where user queries are under-specified or ambiguous.","offset":5,"pro":0.625,"labels":"BAC"},{"idx":16,"sentence":"This calls for a mixed-initiative setup.","offset":6,"pro":0.75,"labels":"GAP"},{"idx":16,"sentence":"User-asks (queries) and system-answers, as well as system-asks (clarification questions) and user response, in order to clarify her information needs.","offset":7,"pro":0.875,"labels":"GAP"},{"idx":17,"sentence":"Coreference resolution such as for anaphora has been an essential challenge that is commonly found in conversational machine reading comprehension (CMRC). ","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"Existing approaches based on pre-trained language models (PLMs) mainly rely on an end-to-end method, which still has limitations in clarifying referential dependency.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":17,"sentence":"In this study, a novel graph-based approach is proposed to integrate the coreference of given text into graph structures (called coreference graphs), which can pinpoint a pronoun's referential entity.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":17,"sentence":"We propose two graph-combined methods, evidence-enhanced and the fusion model, for CMRC to integrate coreference graphs from different levels of the PLM architecture.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":17,"sentence":"Evidence-enhanced refers to textual level methods that include an evidence generator (for generating new text to elaborate a pronoun) and enhanced question (for rewriting a pronoun in a question) as PLM input.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":17,"sentence":"The fusion model is a structural level method that combines the PLM with a graph neural network.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":17,"sentence":"We evaluated these approaches on a CoQA pronoun-containing dataset and the whole CoQA dataset.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":17,"sentence":"The result showed that our methods can outperform baseline PLM methods with BERT and RoBERTa.","offset":7,"pro":0.875,"labels":"RST"},{"idx":18,"sentence":"We work on a recommendation dialogue system to help a user understand the appealing points of some target (e.g., a movie).","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"In such dialogues, the recommendation system needs to utilize structured external knowledge to make informative and detailed recommendations.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":18,"sentence":"However, there is no dialogue dataset with structured external knowledge designed to make detailed recommendations for the target.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":18,"sentence":"Therefore, we construct a dialogue dataset, Japanese Movie Recommendation Dialogue (JMRD), in which the recommender recommends one movie in a long dialogue (23 turns on average)","offset":3,"pro":0.375,"labels":"PUR"},{"idx":18,"sentence":"The external knowledge used in this dataset is hierarchically structured, including title, casts, reviews, and plots.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":18,"sentence":"Every recommender's utterance is associated with the external knowledge related to the utterance.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":18,"sentence":"We then create a movie recommendation dialogue system that considers the structure of the external knowledge and the history of the knowledge used.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":18,"sentence":"Experimental results show that the proposed model is superior in knowledge selection to the baseline models.","offset":7,"pro":0.875,"labels":"RST"},{"idx":19,"sentence":"To diversify and enrich generated dialogue responses, knowledge-grounded dialogue has been investigated in recent years.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"The existing methods tackle the knowledge grounding challenge by retrieving the relevant sentences over a large corpus and augmenting the dialogues with explicit extra information.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":19,"sentence":"This paper proposes KnowExpert, an end-to-end framework to bypass the explicit retrieval process and inject knowledge into the pre-trained language models with lightweight adapters and adapt to the knowledge-grounded dialogue task.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":19,"sentence":"The experimental results show that KnowExpert performs comparably with some retrieval-based baselines while being time-efficient in inference, demonstrating the effectiveness of our proposed method.","offset":3,"pro":0.6,"labels":"RST"},{"idx":19,"sentence":"To the best of our knowledge, this is the first attempt to tackle this challenge without retrieval in this task under an open-domain chit-chat scenario.","offset":4,"pro":0.8,"labels":"CTN"},{"idx":20,"sentence":"Goal-oriented dialogues generation grounded in multiple documents(MultiDoc2Dial) is a challenging and realistic task.","offset":0,"pro":0,"labels":"BAC"},{"idx":20,"sentence":"Unlike previous works which treat document-grounded dialogue modeling as a machine reading comprehension task from single document, MultiDoc2Dial task faces challenges of both seeking information from multiple documents and generating conversation response simultaneously.","offset":1,"pro":0.25,"labels":"GAP"},{"idx":20,"sentence":"This paper summarizes our entries to agent response generation subtask in MultiDoc2Dial dataset.","offset":2,"pro":0.5,"labels":"PUR"},{"idx":20,"sentence":"We propose a three-stage solution, Grounding-guided goal-oriented dialogues generation(G4), which predicts groundings from retrieved passages to guide the generation of the final response. ","offset":3,"pro":0.75,"labels":"MTD"},{"idx":21,"sentence":"We propose a pipeline system, comprising (1) document retrieval, (2) passage retrieval, and (3) response generation.","offset":0,"pro":0,"labels":"PUR"},{"idx":21,"sentence":"We thus significantly boost the baseline system's performance (over +10 points for both F1 and SacreBLEU). ","offset":1,"pro":0.3333333333333333,"labels":"RST"},{"idx":21,"sentence":"Our code is released at this link.","offset":2,"pro":0.6666666666666666,"labels":"CTN"},{"idx":22,"sentence":"MultiDoc2Dial presents an important challenge on modeling dialogues grounded with multiple documents.","offset":0,"pro":0,"labels":"BAC"},{"idx":22,"sentence":"This paper proposes a pipeline system of retrieve, re-rank, and generate, where each component is individually optimized.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":22,"sentence":"This enables the passage re-ranker and response generator to fully exploit training with ground-truth data.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":22,"sentence":"Furthermore, we use a deep cross-encoder trained with localized hard negative passages from the retriever.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":22,"sentence":"For the response generator, we use grounding span prediction as an auxiliary task to be jointly trained with the main task of response generation.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":22,"sentence":"We also adopt a passage dropout and regularization technique to improve response generation performance.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":22,"sentence":"Experimental results indicate that the system clearly surpasses the competitive baseline and our team CPII-NLP ranked 1st among the public submissions on ALL four leaderboards based on the sum of F1, SacreBLEU, METEOR and RougeL scores.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":23,"sentence":"Question Answering (QA) is a Natural Language Processing (NLP) task that can measure language and semantics understanding ability, it requires a system not only to retrieve relevant documents from a large number of articles but also to answer corresponding questions according to documents.","offset":0,"pro":0,"labels":"BAC"},{"idx":23,"sentence":"However, various language styles and sources of human questions and evidence documents form the different embedding semantic spaces, which may bring some errors to the downstream QA task.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":23,"sentence":"To alleviate these problems, we propose a framework for enhancing downstream evidence retrieval by generating evidence, aiming at improving the performance of response generation.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":23,"sentence":"Specifically, we take the pre-training language model as a knowledge base, storing documents' information and knowledge into model parameters.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":23,"sentence":"Extensive experiments carried out on the multi-documents dataset show that the proposed method can improve the final performance, which demonstrates the effectiveness of the proposed framework.","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":23,"sentence":"With the Child-Tuning approach being designed, the knowledge storage and evidence generation avoid catastrophic forgetting for response generation.","offset":5,"pro":0.8333333333333334,"labels":"CTN"},{"idx":24,"sentence":"The proposed task is split into grounding span prediction and agent response generation.","offset":0,"pro":0,"labels":"MTD"},{"idx":24,"sentence":"The baseline for the task is the retrieval augmented generation model, which consists of a dense passage retrieval model for the retrieval part and the BART model for the generation part.","offset":1,"pro":0.3333333333333333,"labels":"MTD"},{"idx":24,"sentence":"To overcome this challenge, we adopt model pretraining, fine-tuning, and multi-task learning to enhance our model's coverage of pretrained knowledge. ","offset":2,"pro":0.6666666666666666,"labels":"MTD"},{"idx":25,"sentence":"Information-seeking dialogue systems, including knowledge identification and response generation, aim to respond to users with fluent, coherent, and informative answers based on users' needs.","offset":0,"pro":0,"labels":"BAC"},{"idx":25,"sentence":"This paper discusses our proposed approach, Docalog, for the DialDoc-22 (MultiDoc2Dial) shared task.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":25,"sentence":"In the test phase of MultiDoc2Dial 2022, Docalog achieved f1-scores of 36.07% and 28.44% and SacreBLEU scores of 23.70% and 20.52%, respectively on the MDD-SEEN and MDD-UNSEEN folds.","offset":2,"pro":0.4,"labels":"RST"},{"idx":25,"sentence":"Docalog identifies the most relevant knowledge in the associated document, in a multi-document setting.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":25,"sentence":"Docalog, is a three-stage pipeline consisting of (1) a document retriever model (DR. TEIT), (2) an answer span prediction model, and (3) an ultimate span picker deciding on the most likely answer span, out of all predicted spans.","offset":4,"pro":0.8,"labels":"MTD"},{"idx":28,"sentence":"Grounded text generation systems often generate text that contains factual inconsistencies, hindering their real-world applicability.","offset":0,"pro":0,"labels":"BAC"},{"idx":28,"sentence":"Automatic factual consistency evaluation may help alleviate this limitation by accelerating evaluation cycles, filtering inconsistent outputs and augmenting training data.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":28,"sentence":"While attracting increasing attention, such evaluation metrics are usually developed and evaluated in silo for a single task or dataset, slowing their adoption.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":28,"sentence":"Moreover, previous meta-evaluation protocols focused on system-level correlations with human annotations, which leave the example-level accuracy of such metrics unclear.","offset":3,"pro":0.42857142857142855,"labels":"GAP"},{"idx":28,"sentence":"Across diverse state-of-the-art metrics and 11 datasets we find that large-scale NLI and question generation-and-answering-based approaches achieve strong and complementary results.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":28,"sentence":"We recommend those methods as a starting point for model and metric developers, and hope TRUE will foster progress towards even better methods.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":28,"sentence":"Our standardization enables an example-level meta-evaluation protocol that is more actionable and interpretable than previously reported correlations, yielding clearer quality measures.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":29,"sentence":"Comments are widely used by users in collaborative documents every day.","offset":0,"pro":0,"labels":"BAC"},{"idx":29,"sentence":"The documents' comments enable collaborative editing and review dynamics, transforming each document into a context-sensitive communication channel.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":29,"sentence":"Understanding the role of comments in communication dynamics within documents is the first step towards automating their management.","offset":2,"pro":0.25,"labels":"BAC"},{"idx":29,"sentence":"In this paper we propose the first ever taxonomy for different types of in-document comments based on analysis of a large scale dataset of public documents from the web.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":29,"sentence":"We envision that the next generation of intelligent collaborative document experiences allow interactive creation and consumption of content, there We also introduce the components necessary for developing novel tools that automate the handling of comments through natural language interaction with the documents.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":29,"sentence":"We identify the commands that users would use to respond to various types of comments.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":29,"sentence":"We train machine learning algorithms to recognize the different types of comments and assess their feasibility.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":29,"sentence":"We conclude by discussing some of the implications for the design of automatic document management tools.","offset":7,"pro":0.875,"labels":"MTD"},{"idx":0,"sentence":"This paper proposes a novel task on commonsense-enhanced task-based dialogue grounded in documents and describes the Task2Dial dataset, a novel dataset of document-grounded task-based dialogues, where an Information Giver (IG) provides instructions (by consulting a document) to an Information Follower (IF), so that the latter can successfully complete the task.","offset":0,"pro":0,"labels":"PUR"},{"idx":0,"sentence":"In this unique setting, the IF can ask clarification questions which may not be grounded in the underlying document and require commonsense knowledge to be answered.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":0,"sentence":"The Task2Dial dataset poses new challenges: (1) its human reference texts show more lexical richness and variation than other document-grounded dialogue datasets; (2) generating from this set requires paraphrasing as instructional responses might have been modified from the underlying document; (3) requires commonsense knowledge, since questions might not necessarily be grounded in the document; (4) generating requires planning based on context, as task steps need to be provided in order.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":0,"sentence":"The Task2Dial dataset contains dialogues with an average 18.15 number of turns and 19.79 tokens per turn, as compared to 12.94 and 12 respectively in existing datasets.","offset":3,"pro":0.6,"labels":"RST"},{"idx":0,"sentence":"As such, learning from this dataset promises more natural, varied and less template-like system utterances.","offset":4,"pro":0.8,"labels":"CTN"},{"idx":1,"sentence":"Cross-lingual Transfer Learning typically involves training a model on a high-resource sourcelanguage and applying it to a low-resource tar-get language","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"In this work we introduce a lexi-cal database calledValency Patterns Leipzig(ValPal)which provides the argument patterninformation about various verb-forms in mul-tiple languages including low-resource langua-ges","offset":1,"pro":0.25,"labels":"PUR"},{"idx":1,"sentence":"We also provide a framework to integratethe ValPal database knowledge into the state-of-the-art LSTM based model for cross-lingualsemantic role labelling.","offset":2,"pro":0.5,"labels":"PUR"},{"idx":1,"sentence":"Experimental resultsshow that integrating such knowledge resultedin am improvement in performance of the mo-del on all the target languages on which it isevaluated.","offset":3,"pro":0.75,"labels":"RST"},{"idx":2,"sentence":"Postpositions, which are characterized as multiple form-function associations and thus polysemous, pose a challenge to automatic identification of their usage","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"Several studies have used contextualized word-embedding models to reveal the functions of Korean postpositions","offset":1,"pro":0.2,"labels":"BAC"},{"idx":2,"sentence":"Despite the superior classification performance of previous studies, the particular reason how these models resolve the polysemy of Korean postpositions is not enough clear","offset":2,"pro":0.4,"labels":"GAP"},{"idx":2,"sentence":"To add more interpretation, for this reason, we devised a classification model by employing two transformer-architecture models-BERT and GPT-2-and introduces a computational simulation that interactively demonstrates how these transformer-architecture models simulate human interpretation of word-level polysemy involving Korean adverbial postpositions -ey, -eyse, and -(u)lo","offset":3,"pro":0.6,"labels":"PUR"},{"idx":2,"sentence":"Results reveal that (i) the BERT model performs better than the GPT-2 model to classify the intended function of postpositions, (ii) there is an inverse relationship between the classification accuracy and the number of functions that each postposition manifests, (iii) model performance is affected by the corpus size of each function, (iv) the models' performance gradually improves as the epoch proceeds, and (vi) the models are affected by the scarcity of input and/or semantic closeness between the items","offset":4,"pro":0.8,"labels":"RST"},{"idx":3,"sentence":"Dense retrieval aims at searching for the most relevant documents to the given query by encoding texts in the embedding space, requiring a large amount of query-document pairs to train","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"Since manually constructing such training data is challenging, recent work has proposed to generate synthetic queries from documents and use them to train a dense retriever","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":3,"sentence":"However, compared to the manually composed queries, synthetic queries do not generally ask for implicit information, therefore leading to a degraded retrieval performance","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":3,"sentence":"In this work, we propose Query Generation with External Knowledge (QGEK), a novel method for generating queries with external information related to the corresponding document","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":3,"sentence":"Specifically, we convert a query into a triplet-based template form to accommodate external information and transmit it to a pre-trained language model (PLM)","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":3,"sentence":"We validate QGEK on both in-domain and out-domain dense retrieval settings","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":3,"sentence":"The dense retriever with the queries requiring implicit information is found to make good performance improvement","offset":6,"pro":0.6666666666666666,"labels":"RST"},{"idx":3,"sentence":"Also, such queries are similar to manually composed queries","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":3,"sentence":"confirmed by both human evaluation and unique {& non-unique words distribution.","offset":8,"pro":0.8888888888888888,"labels":"CTN"},{"idx":4,"sentence":"The possibility to extract moral attitude rapidly from natural language is an appealing perspective that would enable a deeper understanding of social interaction dynamics and the individual cognitive and behavioral dimension","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"In this work we focus on detecting moral content from natural language and we test our methods on a corpus of tweets previously labeled as containing moral values or violations, according to Moral Foundation Theory","offset":1,"pro":0.1111111111111111,"labels":"PUR"},{"idx":4,"sentence":"We develop and compare two different approaches: (i) a frame-based symbolic value detector based on knowledge graphs and (ii) a zero-shot machine learning model fine-tuned on a task of Natural Language Inference (NLI) and a task of emotion detection","offset":2,"pro":0.2222222222222222,"labels":"MTD"},{"idx":4,"sentence":"The final outcome from our work consists in two approaches meant to perform without the need for prior training process on a moral value detection task.","offset":3,"pro":0.3333333333333333,"labels":"RST"},{"idx":4,"sentence":"Moral values as commonsense norms shape our everyday individual and community behavior","offset":4,"pro":0.4444444444444444,"labels":"BAC"},{"idx":4,"sentence":"The possibility to extract moral attitude rapidly from natural language is an appealing perspective that would enable a deeper understanding of social interaction dynamics and the individual cognitive and behavioral dimension","offset":5,"pro":0.5555555555555556,"labels":"BAC"},{"idx":4,"sentence":"In this work we focus on detecting moral content from natural language and we test our methods on a corpus of tweets previously labeled as containing moral values or violations, according to Moral Foundation Theory","offset":6,"pro":0.6666666666666666,"labels":"PUR"},{"idx":4,"sentence":"We develop and compare two different approaches: (i) a frame-based symbolic value detector based on knowledge graphs and (ii) a zero-shot machine learning model fine-tuned on a task of Natural Language Inference (NLI) and a task of emotion detection.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":4,"sentence":"The final outcome from our work consists in two approaches meant to perform without the need for prior training process on a moral value detection task.","offset":8,"pro":0.8888888888888888,"labels":"RST"},{"idx":5,"sentence":"The possibility to extract moral attitude rapidly from natural language is an appealing perspective that would enable a deeper understanding of social interaction dynamics and the individual cognitive and behavioral dimension","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"In this work we focus on detecting moral content from natural language and we test our methods on a corpus of tweets previously labeled as containing moral values or violations, according to Moral Foundation Theory","offset":1,"pro":0.1111111111111111,"labels":"PUR"},{"idx":5,"sentence":"We develop and compare two different approaches: (i) a frame-based symbolic value detector based on knowledge graphs and (ii) a zero-shot machine learning model fine-tuned on a task of Natural Language Inference (NLI) and a task of emotion detection","offset":2,"pro":0.2222222222222222,"labels":"MTD"},{"idx":5,"sentence":"The final outcome from our work consists in two approaches meant to perform without the need for prior training process on a moral value detection task.","offset":3,"pro":0.3333333333333333,"labels":"RST"},{"idx":5,"sentence":"Moral values as commonsense norms shape our everyday individual and community behavior","offset":4,"pro":0.4444444444444444,"labels":"BAC"},{"idx":5,"sentence":"The possibility to extract moral attitude rapidly from natural language is an appealing perspective that would enable a deeper understanding of social interaction dynamics and the individual cognitive and behavioral dimension","offset":5,"pro":0.5555555555555556,"labels":"BAC"},{"idx":5,"sentence":"In this work we focus on detecting moral content from natural language and we test our methods on a corpus of tweets previously labeled as containing moral values or violations, according to Moral Foundation Theory","offset":6,"pro":0.6666666666666666,"labels":"PUR"},{"idx":5,"sentence":"We develop and compare two different approaches: (i) a frame-based symbolic value detector based on knowledge graphs and (ii) a zero-shot machine learning model fine-tuned on a task of Natural Language Inference (NLI) and a task of emotion detection.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":5,"sentence":"The final outcome from our work consists in two approaches meant to perform without the need for prior training process on a moral value detection task.","offset":8,"pro":0.8888888888888888,"labels":"RST"},{"idx":6,"sentence":"While entity retrieval models continue to advance their capabilities","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"our understanding of their wide-ranging applications is limited, especially in domain-specific settings","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":6,"sentence":"We highlighted this issue by using recent general-domain entity-linking models, LUKE and GENRE","offset":2,"pro":0.2222222222222222,"labels":"MTD"},{"idx":6,"sentence":"to inject external knowledge into a question-answering (QA) model for a financial QA task with a hybrid tabular-textual dataset","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":6,"sentence":"We found that both models improved the baseline model by 1.57% overall and 8.86% on textual data.","offset":4,"pro":0.4444444444444444,"labels":"RST"},{"idx":6,"sentence":"Nonetheless, the challenge remains as they still struggle to handle tabular inputs","offset":5,"pro":0.5555555555555556,"labels":"GAP"},{"idx":6,"sentence":"We subsequently conducted a comprehensive attention-weight analysis, revealing how LUKE utilizes external knowledge supplied by GENRE.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":6,"sentence":"The analysis also elaborates how the injection of symbolic knowledge can be helpful and what needs further improvement","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":6,"sentence":"paving the way for future research on this challenging QA task and advancing our understanding of how a language model incorporates external knowledge.","offset":8,"pro":0.8888888888888888,"labels":"CTN"},{"idx":7,"sentence":"Natural language inference on tabular data is a challenging task","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"Existing approaches lack the world and common sense knowledge required to perform at a human level","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":7,"sentence":"While massive amounts of KG data exist, approaches to integrate them with deep learning models to enhance tabular reasoning are uncommon","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":7,"sentence":"In this paper, we investigate a new approach using BiLSTMs to incorporate knowledge effectively into language models","offset":3,"pro":0.5,"labels":"PUR"},{"idx":7,"sentence":"Through extensive analysis","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":7,"sentence":"we show that our proposed architecture, Trans-KBLSTM improves the benchmark performance on InfoTabS, a tabular NLI dataset.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":8,"sentence":"We study few-shot debugging of transformer based natural language understanding models","offset":0,"pro":0,"labels":"PUR"},{"idx":8,"sentence":"using recently popularized test suites to not just diagnose but correct a problem","offset":1,"pro":0.14285714285714285,"labels":"MTD"},{"idx":8,"sentence":"Given a few debugging examples of a certain phenomenon, and a held-out test set of the same phenomenon","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":8,"sentence":"we aim to maximize accuracy on the phenomenon at a minimal cost of accuracy on the original test set","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":8,"sentence":"We examine several methods that are faster than full epoch retraining","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":8,"sentence":"We introduce a new fast method, which samples a few in-danger examples from the original training set.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":8,"sentence":"Compared to fast methods using parameter distance constraints or Kullback-Leibler divergence, we achieve superior original accuracy for comparable debugging accuracy.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":9,"sentence":"In the real world, many relational facts require consentence","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"for instance, a politician holds a given elected position only for a particular timespan","offset":1,"pro":0.125,"labels":"BAC"},{"idx":9,"sentence":"This context (the timespan) is typically ignored in knowledge graph link prediction tasks, or is leveraged by models designed specifically to make use of it (i.e. n-ary link prediction models)","offset":2,"pro":0.25,"labels":"GAP"},{"idx":9,"sentence":"Here, we show that the task of n-ary link prediction is easily performed using language models","offset":3,"pro":0.375,"labels":"PUR"},{"idx":9,"sentence":"applied with a basic method for constructing cloze-style query sentences","offset":4,"pro":0.5,"labels":"MTD"},{"idx":9,"sentence":"We introduce a pre-training methodology based around an auxiliary entity-linked corpus that outperforms other popular pre-trained models like BERT, even with a smaller model","offset":5,"pro":0.625,"labels":"MTD"},{"idx":9,"sentence":"This methodology also enables n-ary link prediction without access to any n-ary training set, which can be invaluable in circumstances where expensive and time-consuming curation of n-ary knowledge graphs is not feasible","offset":6,"pro":0.75,"labels":"MTD"},{"idx":9,"sentence":"We achieve state-of-the-art performance on the primary n-ary link prediction dataset WD50K and on WikiPeople facts that include literals - typically ignored by knowledge graph embedding methods.","offset":7,"pro":0.875,"labels":"RST"},{"idx":10,"sentence":"GPT-3 has attracted lots of attention due to its superior performance across a wide range of NLP tasks, especially with its in-context learning abilities","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"Despite its success, we found that the empirical results of GPT-3 depend heavily on the choice of in-context examples","offset":1,"pro":0.125,"labels":"BAC"},{"idx":10,"sentence":"In this work, we investigate whether there are more effective strategies for judiciously selecting in-context examples (relative to random sampling) that better leverage GPT-3's in-context learning capabilities","offset":2,"pro":0.25,"labels":"PUR"},{"idx":10,"sentence":"Inspired by the recent success of leveraging a retrieval module to augment neural networks, we propose to retrieve examples that are semantically-similar to a test query sample to formulate its corresponding prompt","offset":3,"pro":0.375,"labels":"PUR"},{"idx":10,"sentence":"Intuitively, the examples selected with such a strategy may serve as more informative inputs to unleash GPT-3's power of text generation","offset":4,"pro":0.5,"labels":"MTD"},{"idx":10,"sentence":"We evaluate the proposed approach on several natural language understanding and generation benchmarks, where the retrieval-based prompt selection approach consistently outperforms the random selection baseline","offset":5,"pro":0.625,"labels":"MTD"},{"idx":10,"sentence":"Moreover, it is observed that the sentence encoders fine-tuned on task-related datasets yield even more helpful retrieval results","offset":6,"pro":0.75,"labels":"RST"},{"idx":10,"sentence":"Notably, significant gains are observed on tasks such as table-to-text generation (44.3% on the ToTTo dataset) and open-domain question answering (45.5% on the NQ dataset)","offset":7,"pro":0.875,"labels":"CTN"},{"idx":11,"sentence":"Knowledge graphs are often used to store common sense information that is useful for various tasks","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"However, the extraction of contextually-relevant knowledge is an unsolved problem, and current approaches are relatively simple","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":11,"sentence":"Here we introduce a triple selection method based on a ranking model","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":11,"sentence":"find that it improves question answering accuracy over existing methods.","offset":3,"pro":0.5,"labels":"CLN"},{"idx":11,"sentence":"We additionally investigate methods to ensure that extracted triples form a connected graph","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":11,"sentence":"Graph connectivity is important for model interpretability, as paths are frequently used as explanations for the reasoning that connects question and answer.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":12,"sentence":"Story comprehension that involves complex causal and temporal relations is a critical task in NLP","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"but previous studies have focused predominantly on English, leaving open the question of how the findings generalize to other languages, such as Indonesian","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":12,"sentence":"In this paper, we follow the Story Cloze Test framework of Mostafazadeh et al. (2016) in evaluating story understanding in Indonesian","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":12,"sentence":"by constructing a four-sentence story with one correct ending and one incorrect ending","offset":3,"pro":0.5,"labels":"MTD"},{"idx":12,"sentence":"To investigate commonsense knowledge acquisition in language models, we experimented with: (1) a classification task to predict the correct ending; and (2) a generation task to complete the story with a single sentence","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":12,"sentence":"We investigate these tasks in two settings: (i) monolingual training and ii) zero-shot cross-lingual transfer between Indonesian and English.","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":13,"sentence":"Neural language models have attracted a lot of attention in the past few years","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"More and more researchers are getting intrigued by how language models encode commonsense, specifically what kind of commonsense they understand, and why they do.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":13,"sentence":"These psycholinguistic tests are designed to draw conclusions based on predictive responses in context, making them very well suited to test word-prediction models such as BERT in natural settings.","offset":2,"pro":0.2,"labels":"MTD"},{"idx":13,"sentence":"They can provide the appropriate prompts and tasks to answer questions about linguistic mechanisms underlying predictive responses.","offset":3,"pro":0.3,"labels":"MTD"},{"idx":13,"sentence":"This paper analyzed neural language models' understanding of commonsense pragmatics (i.e., implied meanings) through human behavioral and neurophysiological data","offset":4,"pro":0.4,"labels":"MTD"},{"idx":13,"sentence":"This paper adopted psycholinguistic datasets to probe language models' commonsense reasoning.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":13,"sentence":"Findings suggest that GPT-3's performance was mostly at chance in the psycholinguistic tasks","offset":6,"pro":0.6,"labels":"RST"},{"idx":13,"sentence":"We also showed that DistillBERT had some understanding of the (implied) intent that's shared among most people","offset":7,"pro":0.7,"labels":"RST"},{"idx":13,"sentence":"Such intent is implicitly reflected in the usage of conversational implicatures and presuppositions","offset":8,"pro":0.8,"labels":"RST"},{"idx":13,"sentence":"Whether or not fine-tuning improved its performance to human-level depends on the type of commonsense reasoning.","offset":9,"pro":0.9,"labels":"RST"},{"idx":14,"sentence":"Large-scale visual-linguistic pre-training aims to capture the generic representations from multimodal features, which are essential for downstream vision-language tasks","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"Existing methods mostly focus on learning the semantic connections between visual objects and linguistic content, which tend to be recognitionlevel information and may not be sufficient for commonsensical reasoning tasks like VCR","offset":1,"pro":0.09090909090909091,"labels":"GAP"},{"idx":14,"sentence":"In this paper, we propose a novel commonsensical vision-language pre-training framework to bridge the gap.","offset":2,"pro":0.18181818181818182,"labels":"PUR"},{"idx":14,"sentence":"We first augment the conventional image-caption pre-training datasets with commonsense inferences from a visuallinguistic GPT-2.","offset":3,"pro":0.2727272727272727,"labels":"MTD"},{"idx":14,"sentence":"o pre-train models on image, caption and commonsense inferences together,","offset":4,"pro":0.36363636363636365,"labels":"PUR"},{"idx":14,"sentence":"we propose two new tasks: masked commonsense modeling (MCM) and commonsense type prediction (CTP)","offset":5,"pro":0.45454545454545453,"labels":"MTD"},{"idx":14,"sentence":"To reduce the shortcut effect between captions and commonsense inferences","offset":6,"pro":0.5454545454545454,"labels":"PUR"},{"idx":14,"sentence":"we further introduce the domain-wise adaptive masking that dynamically adjusts the masking ratio","offset":7,"pro":0.6363636363636364,"labels":"MTD"},{"idx":14,"sentence":"Experimental results on downstream tasks, VCR and VQA, show the improvement of our pre-training strategy over previous methods.","offset":8,"pro":0.7272727272727273,"labels":"RST"},{"idx":14,"sentence":"Human evaluation also validates the relevance, informativeness, and diversity of the generated commonsense inferences.","offset":9,"pro":0.8181818181818182,"labels":"CTN"},{"idx":14,"sentence":"Overall, we demonstrate the potential of incorporating commonsense knowledge into the conventional recognition-level visual-linguistic pre-training.","offset":10,"pro":0.9090909090909091,"labels":"CLN"},{"idx":15,"sentence":"Starting from the COMET methodology by Bosselut et al. (2019), generating commonsense knowledge directly from pre-trained language models has recently received significant attention","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"Surprisingly, up to now no materialized resource of commonsense knowledge generated this way is publicly available.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":15,"sentence":"We posit that the availability of these resources is important for the advancement of the field, as it enables an off-the-shelf-use of the resulting knowledge, as well as further analyses on its strengths and weaknesses.","offset":2,"pro":0.25,"labels":"RST"},{"idx":15,"sentence":"This paper fills this gap","offset":3,"pro":0.375,"labels":"PUR"},{"idx":15,"sentence":"and uses the materialized resources","offset":4,"pro":0.5,"labels":"MTD"},{"idx":15,"sentence":"to perform a detailed analysis of the potential of this approach in terms of precision and recall","offset":5,"pro":0.625,"labels":"PUR"},{"idx":15,"sentence":"Furthermore, we identify common problem cases","offset":6,"pro":0.75,"labels":"MTD"},{"idx":15,"sentence":"and outline use cases enabled by materialized resources","offset":7,"pro":0.875,"labels":"RST"},{"idx":16,"sentence":"Previous studies have shown the efficacy of knowledge augmentation methods in pretrained language models","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"However, these methods behave differently across domains and downstream tasks","offset":1,"pro":0.2,"labels":"BAC"},{"idx":16,"sentence":"In this work, we investigate the augmentation of pretrained language models with knowledge graph data in the cause-effect relation classification and commonsense causal reasoning tasks.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":16,"sentence":"After automatically verbalizing triples in ATOMIC2020, a wide coverage commonsense reasoning knowledge graph, we continually pretrain BERT and evaluate the resulting model on cause-effect pair classification and answering commonsense causal reasoning questions","offset":3,"pro":0.6,"labels":"MTD"},{"idx":16,"sentence":"Our results show that a continually pretrained language model augmented with commonsense reasoning knowledge outperforms our baselines on two commonsense causal reasoning benchmarks, COPA and BCOPA-CE, and a Temporal and Causal Reasoning (TCR) dataset, without additional improvement in model architecture or using quality-enhanced data for fine-tuning.","offset":4,"pro":0.8,"labels":"RST"},{"idx":17,"sentence":"Predicting the effects of unexpected situations is an important reasoning task, e.g., would cloudy skies help or hinder plant growth?","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"Given a context, the goal of such situational reasoning is to elicit the consequences of a new situation (st) that arises in that context.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":17,"sentence":"We propose CURIE, a method to iteratively build a graph of relevant consequences explicitly in a structured situational graph (st graph) using natural language queries over a finetuned language model","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":17,"sentence":"Across multiple domains, CURIE generates st graphs that humans find relevant and meaningful in eliciting the consequences of a new situation (75% of the graphs were judged correct by humans).","offset":3,"pro":0.5,"labels":"MTD"},{"idx":17,"sentence":"We present a case study of a situation reasoning end task (WIQA-QA), where simply augmenting their input with st graphs improves accuracy by 3 points.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":17,"sentence":"We show that these improvements mainly come from a hard subset of the data, that requires background knowledge and multi-hop reasoning.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":18,"sentence":"We present the findings of the shared task at the CONSTRAINT 2022 Workshop: Hero, Villain, and Victim: Dissecting harmful memes for Semantic role labeling of entities.","offset":0,"pro":0,"labels":"PUR"},{"idx":18,"sentence":"In more nuanced terms, the shared task focuses on determining the victimizing, glorifying, and vilifying intentions embedded in meme entities to explicate their connotations","offset":1,"pro":0.1111111111111111,"labels":"MTD"},{"idx":18,"sentence":"To this end","offset":2,"pro":0.2222222222222222,"labels":"PUR"},{"idx":18,"sentence":"we curate HVVMemes, a novel meme dataset of about 7000 memes spanning the domains of COVID-19 and US Politics, each containing entities and their associated roles: hero, villain, victim, or none.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":18,"sentence":"The task aims to delve deeper into the domain of meme comprehension","offset":4,"pro":0.4444444444444444,"labels":"BAC"},{"idx":18,"sentence":"by deciphering the connotations behind the entities present in a meme","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":18,"sentence":"The shared task attracted 105 participants, but eventually only 6 submissions were made.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":18,"sentence":"Most of the successful submissions relied on fine-tuning pre-trained language and multimodal models along with ensembles.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":18,"sentence":"The best submission achieved an F1-score of 58.67.","offset":8,"pro":0.8888888888888888,"labels":"CTN"},{"idx":19,"sentence":"The memes serve as an important tool in online communication, whereas some hateful memes endanger cyberspace by attacking certain people or subjects","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"Recent studies address hateful memes detection while further understanding of relationships of entities in memes remains unexplored","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":19,"sentence":"This paper presents our work at the Constraint@ACL2022 Shared Task: Hero, Villain and Victim: Dissecting harmful memes for semantic role labelling of entities.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":19,"sentence":"In particular, we propose our approach utilizing transformer-based multimodal models","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":19,"sentence":"through a VCR method with data augmentation, continual pretraining, loss re-weighting, and ensemble learning","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":19,"sentence":"We describe the models used, the ways of preprocessing and experiments implementation.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":19,"sentence":"s a result, our best model achieves the Macro F1-score of 54.707 on the test set of this shared task","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":20,"sentence":"Because of the growing popularity of memes, the amount of offensive information published on the internet is expanding at an alarming rate","offset":0,"pro":0,"labels":"BAC"},{"idx":20,"sentence":"It generated a larger need to address this issue and analyze the memes for content moderation.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":20,"sentence":"Framing is used to show the entities engaged as heroes, villains, victims, or others so that readers may better anticipate and understand their attitudes and behaviors as characters.","offset":2,"pro":0.2857142857142857,"labels":"BAC"},{"idx":20,"sentence":"Positive phrases are used to characterize heroes, whereas negative terms depict victims and villains, and terms that tend to be neutral are mapped to others.","offset":3,"pro":0.42857142857142855,"labels":"BAC"},{"idx":20,"sentence":"n this paper, we propose two approaches to role label the entities of the meme as hero, villain, victim, or other through Named-Entity Recognition(NER), Sentiment Analysis, etc","offset":4,"pro":0.5714285714285714,"labels":"PUR"},{"idx":20,"sentence":"With an F1-score of 23.855","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":20,"sentence":"our team secured eighth position in the Shared Task @ Constraint 2022.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":21,"sentence":"This paper describes our system for the Constraint 2022 challenge at ACL 2022, whose goal is to detect which entities are glorified, vilified or victimised, within a meme ","offset":0,"pro":0,"labels":"PUR"},{"idx":21,"sentence":"The task should be done considering the perspective of the meme's author","offset":1,"pro":0.14285714285714285,"labels":"MTD"},{"idx":21,"sentence":"n our work, the challenge is treated as a multi-class classification task.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":21,"sentence":"For a given pair of a meme and an entity, we need to classify whether the entity is being referenced as Hero, a Villain, a Victim or Other. Our solution combines (ensembling) different models based on Unimodal (Text only) model and Multimodal model (Text + Images)","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":21,"sentence":"We conduct several experiments and benchmarks different competitive pre-trained transformers and vision models in this work.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":21,"sentence":"Our solution, based on an ensembling method, is ranked first on the leaderboard and obtains a macro F1-score of 0.58 on test set.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":21,"sentence":"The code for the experiments and results are available at https://bitbucket.org/logicallydevs/constraint{_2022/src/master/","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":22,"sentence":"This paper describes the system we developed for the shared task Hero, Villain and Victim: Dissecting harmful memes for Semantic role labelling of entities' organised in the framework of the Second Workshop on Combating Online Hostile Posts in Regional Languages during Emergency Situation (Constraint 2022)","offset":0,"pro":0,"labels":"PUR"},{"idx":22,"sentence":"We present an ensemble approach combining transformer-based models and linguistic information, such as the presence of irony and implicit sentiment associated to the target named entities","offset":1,"pro":0.3333333333333333,"labels":"MTD"},{"idx":22,"sentence":"The ensemble system obtains promising classification scores, resulting in a third place finish in the competition.","offset":2,"pro":0.6666666666666666,"labels":"RST"},{"idx":23,"sentence":"Harmful or abusive online content has been increasing over time and it has been raising concerns among social media platforms, government agencies, and policymakers.","offset":0,"pro":0,"labels":"BAC"},{"idx":23,"sentence":"Such harmful or abusive content has a significant negative impact on society such as cyberbullying led to suicides, COVID-19 related rumors led to hundreds of deaths","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":23,"sentence":"The content that is posted and shared online can be textual, visual, a combination of both, or a meme.","offset":2,"pro":0.3333333333333333,"labels":"BAC"},{"idx":23,"sentence":"In this paper, we provide our study on detecting the roles of entities in harmful memes, which is part of the CONSTRAINT-2022 shared task","offset":3,"pro":0.5,"labels":"PUR"},{"idx":23,"sentence":"We report the results on the participated system","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":23,"sentence":"We further provide a comparative analysis on different experimental settings (i.e., unimodal, multimodal, attention, and augmentation).","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":24,"sentence":"We propose our solution to the multimodal semantic role labeling task from the CONSTRAINT'22 workshop","offset":0,"pro":0,"labels":"PUR"},{"idx":24,"sentence":"The task aims at classifying entities in memes into classes such as hero and villain.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":24,"sentence":"We use several pre-trained multi-modal models to jointly encode the text and image of the memes, and implement three systems to classify the role of the entities","offset":2,"pro":0.4,"labels":"MTD"},{"idx":24,"sentence":"We propose dynamic sampling strategies to tackle the issue of class imbalance","offset":3,"pro":0.6,"labels":"MTD"},{"idx":24,"sentence":"Finally, we perform qualitative analysis on the representations of the entities.","offset":4,"pro":0.8,"labels":"MTD"},{"idx":25,"sentence":"During the COVID-19 pandemic, the spread of misinformation on online social media has grown exponentially","offset":0,"pro":0,"labels":"BAC"},{"idx":25,"sentence":"Unverified bogus claims on these platforms regularly mislead people, leading them to believe in half-baked truths","offset":1,"pro":0.09090909090909091,"labels":"BAC"},{"idx":25,"sentence":"The current vogue is to employ manual fact-checkers to verify claims to combat this avalanche of misinformation","offset":2,"pro":0.18181818181818182,"labels":"BAC"},{"idx":25,"sentence":" However, establishing such claims' veracity is becoming increasingly challenging, partly due to the plethora of information available, which is difficult to process manually","offset":3,"pro":0.2727272727272727,"labels":"BAC"},{"idx":25,"sentence":"Thus, it becomes imperative to verify claims automatically without human interventions","offset":4,"pro":0.36363636363636365,"labels":"BAC"},{"idx":25,"sentence":"To cope up with this issue, we propose an automated claim verification solution encompassing two steps - document retrieval and veracity prediction","offset":5,"pro":0.45454545454545453,"labels":"PUR"},{"idx":25,"sentence":"or the retrieval module, we employ a hybrid search-based system with BM25 as a base retriever and experiment with recent state-of-the-art transformer-based models for re-ranking","offset":6,"pro":0.5454545454545454,"labels":"MTD"},{"idx":25,"sentence":"Furthermore, we use a BART-based textual entailment architecture to authenticate the retrieved documents in the later step","offset":7,"pro":0.6363636363636364,"labels":"MTD"},{"idx":25,"sentence":"We report experimental findings, demonstrating that our retrieval module outperforms the best baseline system by 10.32 NDCG@100 points","offset":8,"pro":0.7272727272727273,"labels":"RST"},{"idx":25,"sentence":"We escort a demonstration to assess the efficacy and impact of our suggested solution","offset":9,"pro":0.8181818181818182,"labels":"RST"},{"idx":25,"sentence":"As a byproduct of this study, we present an open-source, easily deployable, and user-friendly Python API that the community can adopt","offset":10,"pro":0.9090909090909091,"labels":"RST"},{"idx":26,"sentence":"Recently, detection and categorization of undesired (e. g., aggressive, abusive, offensive, hate) content from online platforms has grabbed the attention of researchers because of its detrimental impact on society.","offset":0,"pro":0,"labels":"BAC"},{"idx":26,"sentence":"Several attempts have been made to mitigate the usage and propagation of such content","offset":1,"pro":0.1,"labels":"BAC"},{"idx":26,"sentence":"However, most past studies were conducted primarily for English, where low-resource languages like Bengali remained out of the focus","offset":2,"pro":0.2,"labels":"GAP"},{"idx":26,"sentence":"Therefore, to facilitate research in this arena, this paper introduces a novel multilabel Bengali dataset (named M-BAD) containing 15650 texts to detect aggressive texts and their targets","offset":3,"pro":0.3,"labels":"PUR"},{"idx":26,"sentence":"Each text of M-BAD went through rigorous two-level annotations.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":26,"sentence":"At the primary level, each text is labelled as either aggressive or non-aggressive.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":26,"sentence":"In the secondary level, the aggressive texts have been further annotated into five fine-grained target classes: religion, politics, verbal, gender and race","offset":6,"pro":0.6,"labels":"MTD"},{"idx":26,"sentence":"Baseline experiments are carried out with different machine learning (ML), deep learning (DL) and transformer models, where Bangla-BERT acquired the highest weighted f_1-score in both detection (0.92) and target identification (0.83) tasks","offset":7,"pro":0.7,"labels":"RST"},{"idx":26,"sentence":"Error analysis of the models exhibits the difficulty to identify context-dependent aggression","offset":8,"pro":0.8,"labels":"RST"},{"idx":26,"sentence":"and this work argues that further research is required to address these issues","offset":9,"pro":0.9,"labels":"IMP"},{"idx":27,"sentence":"This study investigates how fake news use the thumbnail image for a news article","offset":0,"pro":0,"labels":"PUR"},{"idx":27,"sentence":"We aim at capturing the degree of semantic incongruity between news text and image by using the pretrained CLIP representation","offset":1,"pro":0.125,"labels":"PUR"},{"idx":27,"sentence":"Motivated by the stylistic distinctiveness in fake news text, we examine whether fake news tends to use an irrelevant image to the news content","offset":2,"pro":0.25,"labels":"MTD"},{"idx":27,"sentence":"Results show that fake news tends to have a high degree of semantic incongruity than general news","offset":3,"pro":0.375,"labels":"RST"},{"idx":27,"sentence":"We further attempt to detect such image-text incongruity by training classification models on a newly generated datase","offset":4,"pro":0.5,"labels":"MTD"},{"idx":27,"sentence":"A manual evaluation suggests our method can find news articles of which the thumbnail image is semantically irrelevant to news text with an accuracy of 0.8","offset":5,"pro":0.625,"labels":"RST"},{"idx":27,"sentence":"We also release a new dataset of image and news text pairs with the incongruity labe","offset":6,"pro":0.75,"labels":"RST"},{"idx":27,"sentence":"facilitating future studies on the direction","offset":7,"pro":0.875,"labels":"IMP"},{"idx":28,"sentence":"The COVID-19 pandemic has created threats to global health control.","offset":0,"pro":0,"labels":"BAC"},{"idx":28,"sentence":"Misinformation circulated on social media and news outlets has undermined public trust towards Government and health agencies","offset":1,"pro":0.125,"labels":"BAC"},{"idx":28,"sentence":"This problem is further exacerbated in developing countries or low-resource regions, where the news is not equipped with abundant English fact-checking information","offset":2,"pro":0.25,"labels":"BAC"},{"idx":28,"sentence":"In this paper, we make the first attempt to detect COVID-19 misinformation (in English, Spanish, and Haitian French) populated in the Caribbean regions, using the fact-checked claims in the US (in English)","offset":3,"pro":0.375,"labels":"PUR"},{"idx":28,"sentence":"We started by collecting a dataset of Caribbean real {& fake claims","offset":4,"pro":0.5,"labels":"MTD"},{"idx":28,"sentence":"Then we trained several classification and language models on COVID-19 in the high-resource language regions and transferred the knowledge to the Caribbean claim dataset","offset":5,"pro":0.625,"labels":"MTD"},{"idx":28,"sentence":"The experimental results of this paper reveal the limitations of current fake claim detection in low-resource regions","offset":6,"pro":0.75,"labels":"RST"},{"idx":28,"sentence":" encourage further research on multi-lingual detection.","offset":7,"pro":0.875,"labels":"IMP"},{"idx":29,"sentence":"The speech corpus includes audio data, annotations, software tools for data-processing, two databases and a web application","offset":0,"pro":0,"labels":"MTD"},{"idx":29,"sentence":"We have published part of the audio data and annotations","offset":1,"pro":0.2,"labels":"MTD"},{"idx":29,"sentence":"The software tool for parsing annotation files and feeding a relational database is developed and published under a free license","offset":2,"pro":0.4,"labels":"MTD"},{"idx":29,"sentence":"A web application is developed and available","offset":3,"pro":0.6,"labels":"CTN"},{"idx":29,"sentence":"At this moment, about 300 words and 200 phrases can be displayed using this web application.","offset":4,"pro":0.8,"labels":"CTN"},{"idx":0,"sentence":"Focus on language-specific properties with insights from formal minimalist syntax can improve universal dependency (UD) parsing","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"uch improvements are especially sensitive for low-resource African languages, like Wolof, which have fewer UD treebanks in number and amount of annotations, and fewer contributing annotators.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":0,"sentence":"For two different UD parser pipelines, one parser model was trained on the original Wolof treebank, and one was trained on an edited treebank.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":0,"sentence":"ccuracy for universal dependency relations improved as much as 2.90%, while accuracy for universal dependency labels increased as much as 3.38%. ","offset":3,"pro":0.5,"labels":"RST"},{"idx":0,"sentence":"For each parser pipeline, the accuracy of the edited treebank was higher than the original for both the dependency relations and dependency labels.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":0,"sentence":"n annotation scheme that better fits a language's distinct syntax results in better parsing accuracy","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":1,"sentence":"Language technologies, particularly speech technologies, are becoming more pervasive for access to digital platforms and resources. ","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"Additionally, research shows speech recognition to be more accurate for men than for women and more accurate for individuals younger than 30 years of age than those older.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":1,"sentence":"t is also important to note that in varying contexts within the Global South, ","offset":2,"pro":0.2857142857142857,"labels":"BAC"},{"idx":1,"sentence":"n the Global South where languages are low resource, these same issues should be taken into consideration in data collection efforts to not replicate these mistakes.","offset":3,"pro":0.42857142857142855,"labels":"GAP"},{"idx":1,"sentence":"his paper documents i) the designing and execution of a Linguists Engagement for purposes of building an inclusive Kiswahili Speech Recognition dataset,","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":1,"sentence":"epresentative of the diversity among speakers of the language ii) the unexpected yet key learning in terms of socio-linguistcs which demonstrate the importance of multi-disciplinarity in teams developing datasets and NLP technologies","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":1,"sentence":"the creation of a test dataset intended to be used for evaluating the performance of Speech Recognition models on demographic groups that are likely to be underrepresented","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":2,"sentence":"Language revitalisation should not be understood as a direct outcome of language documentation, which is mainly focused on the creation of language repositories.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"Natural language processing (NLP) offers the potential to complement and exploit these repositories through the development of language technologies that may contribute to improving the vitality status of endangered languages.","offset":1,"pro":0.25,"labels":"BAC"},{"idx":2,"sentence":"In this paper, we discuss the current state of the interaction between language documentation and computational linguistics, present a diagnosis of how the outputs of recent documentation projects for endangered languages are underutilised for the NLP community, and discuss how the situation could change from both the documentary linguistics and NLP perspectives.","offset":2,"pro":0.5,"labels":"PUR"},{"idx":2,"sentence":"All this is introduced as a bridging paradigm dubbed as Computational Language Documentation and Development (CLD{mbox{^2). CLD{mbox{^2 calls for (1) the inclusion of NLP-friendly annotated data as a deliverable of future language documentation projects; and (2) the exploitation of language documentation databases by the NLP community to promote the computerization of endangered languages, as one way to contribute to their revitalization","offset":3,"pro":0.75,"labels":"CTN"},{"idx":3,"sentence":"Data augmentation strategies are increasingly important in NLP pipelines for low-resourced and endangered languages, and in neural morphological inflection, augmentation by so called data hallucination is a popular technique","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"his paper presents a detailed analysis of inflection models trained with and without data hallucination for the low-resourced Canadian Indigenous language Gitksan","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":3,"sentence":"Our analysis reveals evidence for a concatenative inductive bias in augmented models-in contrast to models trained without hallucination, they strongly prefer affixing inflection patterns over suppletive ones","offset":2,"pro":0.2857142857142857,"labels":"RST"},{"idx":3,"sentence":"We find that preference for affixation in general improves inflection performance in wug test like settings, where the model is asked to inflect lexemes missing from the training set.","offset":3,"pro":0.42857142857142855,"labels":"RST"},{"idx":3,"sentence":"However, data hallucination dramatically reduces prediction accuracy for reduplicative forms due to a misanalysis of reduplication as affixation.","offset":4,"pro":0.5714285714285714,"labels":"IMP"},{"idx":3,"sentence":"While the overall impact of data hallucination for unseen lexemes remains positive, our findings call for greater qualitative analysis and more varied evaluation conditions in testing automatic inflection systems.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":3,"sentence":"Our results indicate that further innovations in data augmentation for computational morphology are desirable.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":4,"sentence":"One bottleneck is the time-intensive nature of annotation. An even narrower bottleneck occurs for recordings with access constraints, such as language that must be vetted or filtered by authorised community members before annotation can begin.","offset":0,"pro":0,"labels":"GAP"},{"idx":4,"sentence":"Many archival recordings of speech from endangered languages remain unannotated and inaccessible to community members and language learning programs","offset":1,"pro":0.2,"labels":"GAP"},{"idx":4,"sentence":"We propose a privacy-preserving workflow to widen both bottlenecks for recordings where speech in the endangered language is intermixed with a more widely-used language such as English for meta-linguistic commentary and questions (e.g.What is the word for tree'?).","offset":2,"pro":0.4,"labels":"PUR"},{"idx":4,"sentence":"We integrate voice activity detection (VAD), spoken language identification (SLI), and automatic speech recognition (ASR) to transcribe the metalinguistic content, which an authorised person can quickly scan to triage recordings that can be annotated by people with lower levels of access","offset":3,"pro":0.6,"labels":"MTD"},{"idx":4,"sentence":"We report work-in-progress processing 136 hours archival audio containing a mix of English and Muruwari","offset":4,"pro":0.8,"labels":"MTD"},{"idx":5,"sentence":"The engine and interface have been designed to prioritize the developer experience of potential contributors without requiring a high level of programming knowledge","offset":0,"pro":0,"labels":"MTD"},{"idx":5,"sentence":"i2P_i' already provides mappings for 30 (mostly Indigenous) languages, and the package is accompanied by a web-based interactive development environment, a RESTful API, and extensive documentation to encourage the addition of more mappings in the future","offset":1,"pro":0.3333333333333333,"labels":"MTD"},{"idx":5,"sentence":"We also present three downstream applications of G_i2P_i' and show results of a preliminary","offset":2,"pro":0.6666666666666666,"labels":"RST"},{"idx":6,"sentence":"Accelerating the process of data collection, annotation, and analysis is an urgent need for linguistic fieldwork and documentation of endangered languages (Bird, 2009).","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"Our experiments describe how we maximize the quality for","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":6,"sentence":"Native speaker language consultants were trained to annotate a minimally selected raw data set (Su{'arez et al.,2019).","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":6,"sentence":"the Nepal Bhasa syntactic complement structure chunking model","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":6,"sentence":"The embedded clauses, matrix verbs, and embedded verbs are annotated. We apply both statistical training algorithms and transfer learning in our training, including Naive Bayes, MaxEnt, and fine-tuning the pre-trained mBERT model (Devlin et al., 2018","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":6,"sentence":"We show that with limited annotated data, the model is already sufficient for the task","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":6,"sentence":"The modeling resources we used are largely available for many other endangered languages","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":7,"sentence":"We describe recent extensions to the open source Learning And Reading","offset":0,"pro":0,"labels":"PUR"},{"idx":7,"sentence":"Assistant (LARA) supporting image-based and phonetically annotated texts","offset":1,"pro":0.3333333333333333,"labels":"PUR"},{"idx":7,"sentence":"We motivate the utility of these extensions both in general and specifically in relation to endangered and archaic languages,","offset":2,"pro":0.6666666666666666,"labels":"MTD"},{"idx":8,"sentence":"In this paper we present an approach to efficiently recover texts from corrupted documents of endangered languages.","offset":0,"pro":0,"labels":"PUR"},{"idx":8,"sentence":"Textual resources for such languages are scarce, and sometimes the few available resources are corrupted PDF documents","offset":1,"pro":0.2,"labels":"MTD"},{"idx":8,"sentence":"Endangered languages are not supported by standard tools and present even the additional difficulties of not possessing any corpus over which to train language models to assist with the recovery","offset":2,"pro":0.4,"labels":"MTD"},{"idx":8,"sentence":"The approach presented is able to fully recover born digital PDF documents with minimal effort","offset":3,"pro":0.6,"labels":"MTD"},{"idx":8,"sentence":"helping the preservation effort of endangered languages, by extending the range","offset":4,"pro":0.8,"labels":"IMP"},{"idx":9,"sentence":"Transcribing speech for primarily oral, local languages is often a joint effort involving speakers and outsiders. It is","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"We explore the task of learning through transcription' through the design of a system for collaborative speech annotation","offset":1,"pro":0.25,"labels":"PUR"},{"idx":9,"sentence":"We have developed a prototype to support local and remote learner-speaker interactions in remote Aboriginal communities in northern Australia","offset":2,"pro":0.5,"labels":"MTD"},{"idx":9,"sentence":"We show that situated systems design for inclusive non-expert practice is a promising new direction for working with speakers of local languages","offset":3,"pro":0.75,"labels":"CLN"},{"idx":10,"sentence":"This paper discusses the development of a Part-of-Speech tagger for te reo M{=aori which is the Indigenous language of Aotearoa, also known as New Zealand, see Morrison","offset":0,"pro":0,"labels":"PUR"},{"idx":10,"sentence":"Henceforth, Part-of-Speech will be referred to as POS throughout this paper and te reo M{=aori will be referred to as M{=aori, while Universal Dependencies will be referred to as UD.","offset":1,"pro":0.09090909090909091,"labels":"MTD"},{"idx":10,"sentence":"Prior to the development of this tagger, there was no POS tagger for M{=aori from Aotearoa. POS taggers tag words according to their syntactic or grammatical category","offset":2,"pro":0.18181818181818182,"labels":"MTD"},{"idx":10,"sentence":"However, many traditional syntactic categories, and by consequence POS labels, do not work for M{=aori. By this we mean that, for some of the traditional categories, The definition of, or guidelines for, an existing category is not suitable for M{=aori.","offset":3,"pro":0.2727272727272727,"labels":"GAP"},{"idx":10,"sentence":"They do not have an existing category for certain word classes of M{=aori. They do not reflect a M{=aori worldview of the M{=aori language.","offset":4,"pro":0.36363636363636365,"labels":"GAP"},{"idx":10,"sentence":"We wanted a tagset that is usable with industry-wide tools, but we also needed a tagset that would meet the needs of M{=aori","offset":5,"pro":0.45454545454545453,"labels":"PUR"},{"idx":10,"sentence":"Therefore, we based our tagset and guidelines on the UD tagset and tagging conventions,","offset":6,"pro":0.5454545454545454,"labels":"MTD"},{"idx":10,"sentence":"however the categorization of words has been significantly altered to be appropriate for M{=aori","offset":7,"pro":0.6363636363636364,"labels":"MTD"},{"idx":10,"sentence":"This is because at the time of development of our POS tagger, the UD conventions had still not been used to tag a Polyneisan language such as M{=aori, nor did it provide any guidelines about how to tag them.","offset":8,"pro":0.7272727272727273,"labels":"MTD"},{"idx":10,"sentence":"that end, we worked with highly-proficient, specially-selected M{=aori speakers and linguists who are specialists in M{=aori.","offset":9,"pro":0.8181818181818182,"labels":"MTD"},{"idx":10,"sentence":"This has ensured that our POS labels and guidelines conventions faithfully reflect a M{=aori speaker's conceptualization of their language","offset":10,"pro":0.9090909090909091,"labels":"RST"},{"idx":11,"sentence":"language of the Innu, an Indigenous people that now lives for the most part in a dozen communities across Quebec and Labrador","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"Although it is alive, Innu-Aimun sees important preservation and revitalization challenges and issues","offset":1,"pro":0.125,"labels":"GAP"},{"idx":11,"sentence":"The state of its technology is still nascent, with very few existing applications","offset":2,"pro":0.25,"labels":"GAP"},{"idx":11,"sentence":"This paper proposes a first survey of the available linguistic resources and existing technology for Innu-Aimun","offset":3,"pro":0.375,"labels":"PUR"},{"idx":11,"sentence":"Considering the existing linguistic and textual resources, we argue that developing language technology is feasible and propose first steps towards NLP applications like machine translation.","offset":4,"pro":0.5,"labels":"PUR"},{"idx":11,"sentence":"The goal of developing such technologies is first and foremost to help efforts in improving language transmission and cultural safety and preservation for Innu-Aimun speakers, as those are considered urgent and vital issues","offset":5,"pro":0.625,"labels":"IMP"},{"idx":11,"sentence":"Finally, we discuss the importance of close collaboration and consultation with the Innu community in order to ensure that language technologies","offset":6,"pro":0.75,"labels":"MTD"},{"idx":11,"sentence":"are developed respectfully and in accordance with that goal","offset":7,"pro":0.875,"labels":"MTD"},{"idx":12,"sentence":"This paper describes how emerging linguistic resources and technologies can be used to build a language learning platform for Irish, an endangered language.","offset":0,"pro":0,"labels":"PUR"},{"idx":12,"sentence":"This platform, An Sc{'eala{'i, harvests learner corpora - a vital resource both to study the stages of learners' language acquisition and to guide future platform development","offset":1,"pro":0.2,"labels":"BAC"},{"idx":12,"sentence":"technical description of the platform is provided, including details of how different speech technologies and linguistic resources are fused to provide a holistic learner experience","offset":2,"pro":0.4,"labels":"MTD"},{"idx":12,"sentence":"The active continuous participation","offset":3,"pro":0.6,"labels":"MTD"},{"idx":12,"sentence":"the community, and platform evaluations by learners and teachers, are","offset":4,"pro":0.8,"labels":"MTD"},{"idx":13,"sentence":"For decades, researchers in natural language processing and computational linguistics have been developing models and algorithms that aim to serve the needs of language documentation projects","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"However, these models have seen little use in language documentation despite their great potential for making documentary linguistic artefacts better and easier to produce","offset":1,"pro":0.2,"labels":"GAP"},{"idx":13,"sentence":"In this work, we argue that a major reason for this NLP gap is the lack of a strong foundation of application software","offset":2,"pro":0.4,"labels":"PUR"},{"idx":13,"sentence":"which can on the one hand serve the complex needs of language documentation and on the other hand provide effortless integration with NLP models.","offset":3,"pro":0.6,"labels":"IMP"},{"idx":13,"sentence":"We further present and describe a work-in-progress system we have developed to serve this need, Glam","offset":4,"pro":0.8,"labels":"MTD"},{"idx":14,"sentence":"Machine translation for low-resource languages, such as Guarani, is a challenging task due to the lack of data","offset":0,"pro":0,"labels":"GAP"},{"idx":14,"sentence":"One way of tackling it is using pretrained word embeddings for model initialization.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":14,"sentence":"In this work we try to check if currently available data is enough to train rich embeddings for enhancing MT for Guarani and Spanish","offset":2,"pro":0.4,"labels":"PUR"},{"idx":14,"sentence":"by building a set of word embedding collections and training MT systems using them.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":14,"sentence":"We found that the trained vectors are strong enough to slightly improve the performance of some of the translation models and also to speed up the training convergence","offset":4,"pro":0.8,"labels":"CLN"},{"idx":15,"sentence":"In this paper, we present a game with a purpose (GWAP) (Von Ahn 2006","offset":0,"pro":0,"labels":"PUR"},{"idx":15,"sentence":"aim of the game is to promote language learning and noticing' (Skehan, 2013)","offset":1,"pro":0.2,"labels":"BAC"},{"idx":15,"sentence":"This research incorporates game development, language pedagogy and ICALL language materials development","offset":2,"pro":0.4,"labels":"MTD"},{"idx":15,"sentence":"This paper will focus on the language materials development as this is a bottleneck in the teaching and learning of minority and endangered languages","offset":3,"pro":0.6,"labels":"IMP"},{"idx":15,"sentence":"The game has been designed for Irish, but the framework could be used for other languages. Irish is a minority language which means that L2 learners have limited opportunities for exposure to the language, and additionally, there are also limited (digital) learning resources available","offset":4,"pro":0.8,"labels":"BAC"},{"idx":16,"sentence":"Many endangered Uralic languages have multilingual machine readable dictionaries saved in an XML format","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"However, the dictionaries cover translations very inconsistently between language pairs, for instance, the Livonian dictionary has some translations to Finnish, Latvian and Estonian, and the","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":16,"sentence":"Komi-Zyrian dictionary has some translations to Finnish, English and ","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":16,"sentence":"We utilize graph-based approaches to augment such dictionaries","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":16,"sentence":"predicting new translations to existing and new languages based on different","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":16,"sentence":"dictionaries for endangered languages and Wiktionaries. Our","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":16,"sentence":"Based on the evaluation, the method predicted good or acceptable translations 77% of the time. Furthermore","offset":6,"pro":0.6666666666666666,"labels":"RST"},{"idx":16,"sentence":"neural prediction model to predict the quality of the automatically predicted translations","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":16,"sentence":"The resulting extensions to the dictionaries are made available on the online dictionary platform used by the speakers of these languages","offset":8,"pro":0.8888888888888888,"labels":"CTN"},{"idx":17,"sentence":"Grammar checkers (GEC) are needed for digital language survival","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"Very low resource languages like Lule S{'ami with less than 3,000 speakers need to hurry to build these tools, but do not have the big corpus data that are required for the construction of machine learning tools","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":17,"sentence":"We present a rule-based tool and a workflow where the work done for a related language can speed up the process","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":17,"sentence":"We use an existing grammar to infer rules for the new language, and we do not need a large gold corpus of annotated grammar errors, but a smaller corpus of regression tests is built while developing the tool","offset":3,"pro":0.5,"labels":"MTD"},{"idx":17,"sentence":"We present a test case for Lule S{'ami reusing resources from North S{'ami, show how we achieve a categorisation of the most frequent errors, and present a preliminary evaluation of the system.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":17,"sentence":"hope this serves as an inspiration for small languages that need advanced tools in a limited amount of time, but do not have big data","offset":5,"pro":0.8333333333333334,"labels":"IMP"},{"idx":18,"sentence":"There are many challenges in morphological fieldwork annotation, it heavily relies on segmentation and feature labeling (which have both practical and theoretical drawbacks), it's time-intensive, and the annotator needs to be linguistically trained and may still annotate things inconsistently","offset":0,"pro":0,"labels":"GAP"},{"idx":18,"sentence":"We propose a workflow that relies on unsupervised and active learning grounded in Word-and-Paradigm morphology (WP","offset":1,"pro":0.2,"labels":"PUR"},{"idx":18,"sentence":"Machine learning has the potential to greatly accelerate the annotation process and allow a human annotator to focus on problematic cases","offset":2,"pro":0.4,"labels":"BAC"},{"idx":18,"sentence":"while the WP approach makes for an annotation system that is word-based and relational, removing the need to make decisions about feature labeling and segmentation early in the process and allowing speakers of ","offset":3,"pro":0.6,"labels":"MTD"},{"idx":18,"sentence":"We present a proof-of-concept for the first step of the workflow, in a realistic fieldwork setting, annotators can process hundreds of forms per hour","offset":4,"pro":0.8,"labels":"CTN"},{"idx":19,"sentence":"This is a report on results obtained in the development of speech recognition tools intended to support linguistic documentation efforts","offset":0,"pro":0,"labels":"PUR"},{"idx":19,"sentence":"The test case is an extensive fieldwork corpus of Japhug, an endangered language of the Trans-Himalayan (Sino-Tibetan) family. The","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":19,"sentence":"note difficulties in implementation, in terms of learning stability. But this approach brings significant improvements nonetheless","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":19,"sentence":"Subjective evaluation of the tool by the author of the training data confirms the usefulness of this approach","offset":3,"pro":0.5,"labels":"CLN"},{"idx":19,"sentence":"The quality of phonemic transcription is improved over earlier experiments","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":19,"sentence":"and most significantly, the new approach allows for reaching the stage of automatic word recognition","offset":5,"pro":0.8333333333333334,"labels":"IMP"},{"idx":20,"sentence":"The project XXXX is developing a platform to enable researchers of living languages to easily create and make available state-of-the-art spoken and textual annotated resources","offset":0,"pro":0,"labels":"BAC"},{"idx":20,"sentence":"As a case study we use Greek and Pomak, the latter being an endangered oral Slavic language of the Balkans (including Thrace/Greece).","offset":1,"pro":0.2,"labels":"MTD"},{"idx":20,"sentence":"The linguistic documentation of Pomak is an ongoing work by an interdisciplinary team in close cooperation with the Pomak community of Greece","offset":2,"pro":0.4,"labels":"MTD"},{"idx":20,"sentence":"We describe our experience in the development of a Latin-based orthography and morphologically annotated text corpora of Pomak with state-of-the-art NLP technology.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":20,"sentence":"These resources will be made openly available on the XXXX site and the gold annotated corpora of Pomak will be made available","offset":4,"pro":0.8,"labels":"CTN"},{"idx":21,"sentence":"This study investigates applications of automatic speech recognition (ASR) techniques to Hupa, a critically endangered Native American language from the Dene (Athabaskan) language family.","offset":0,"pro":0,"labels":"PUR"},{"idx":21,"sentence":"sing around 9h12m of spoken data produced by one elder who is a first-language Hupa speaker, we experimented with different evaluation schemes and training setting","offset":1,"pro":0.2,"labels":"MTD"},{"idx":21,"sentence":" On average a fully connected deep neural network reached a word error rate of 35.26%","offset":2,"pro":0.4,"labels":"RST"},{"idx":21,"sentence":"Our overall results illustrate the utility of ASR for making Hupa language documentation more accessible and usable. I","offset":3,"pro":0.6,"labels":"RST"},{"idx":21,"sentence":"his shows promise for speech corpora of indigenous languages that commonly include transcriptions produced by second-language speakers or linguists who have advanced knowledge in the language of interest.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":22,"sentence":"Distributional semantic models capture word-level meaning that is useful in many natural language processing tasks and have even been shown to capture cognitive aspects of word meaning. ","offset":0,"pro":0,"labels":"BAC"},{"idx":22,"sentence":"he majority of these models are purely text based, even though the human sensory experience is much richer. I","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":22,"sentence":" this paper we create visually grounded word embeddings by combining English text and images and compare them to popular text-based methods, to see if visual information allows our model to better capture cognitive aspects of word meaning","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":22,"sentence":"ur analysis shows that visually grounded embedding similarities are more predictive of the human reaction times in a large priming experiment than the purely text-based embeddings","offset":3,"pro":0.42857142857142855,"labels":"RST"},{"idx":22,"sentence":"e visually grounded embeddings also correlate well with human word similarity ratings.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":22,"sentence":"portantly, in both experiments we show that the grounded embeddings account for a unique portion of explained variance, even when we include text-based embeddings trained on huge corpora. ","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":22,"sentence":"is shows that visual grounding allows our model to capture information that cannot be extracted using text as the only source of information.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":23,"sentence":"We propose a new neural model for word embeddings, which uses Unitary Matrices as the primary device for encoding lexical information.","offset":0,"pro":0,"labels":"PUR"},{"idx":23,"sentence":"t uses simple matrix multiplication to derive matrices for large units, yielding a sentence processing model that is strictly compositional, does not lose information over time steps, and is transparent, in the sense that word embeddings can be analysed regardless of consentence","offset":1,"pro":0.2,"labels":"MTD"},{"idx":23,"sentence":"his model does not employ activation functions, and so the network is fully accessible to analysis by the methods of linear algebra at each point in its operation on an input sequence. ","offset":2,"pro":0.4,"labels":"MTD"},{"idx":23,"sentence":"e test it in two NLP agreement tasks and obtain rule like perfect accuracy, with greater stability than current state-of-the-art system","offset":3,"pro":0.6,"labels":"MTD"},{"idx":23,"sentence":"Our proposed model goes some way towards offering a class of computationally powerful deep learning systems that can be fully understood and compared to human cognitive processes for natural language learning and representation.","offset":4,"pro":0.8,"labels":"IMP"},{"idx":24,"sentence":"Noun-noun compounds (NNCs) occur frequently in the English language. Accurate NNC interpretation, i.e. determining the implicit relationship between the constituents of a NNC, is crucial for the advancement of many natural language processing tasks","offset":0,"pro":0,"labels":"BAC"},{"idx":24,"sentence":"Until now, computational NNC interpretation has been limited to approaches involving linguistic representations only.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":24,"sentence":"owever, much research suggests that grounding linguistic representations in vision or other modalities can increase performance on this and other tasks","offset":2,"pro":0.3333333333333333,"labels":"BAC"},{"idx":24,"sentence":"Our work is a novel comparison of linguistic and visuo-linguistic representations for the task of NNC interpretation.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":24,"sentence":"e frame NNC interpretation as a relation classification task, evaluating on a large, relationally-annotated NNC dataset.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":24,"sentence":"We combine distributional word vectors with image vectors to investigate how visual information can help improve NNC interpretation systems. ","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":25,"sentence":"In this work, we use a transformer-based pre-trained multimodal model, CLIP, to shed light on the mechanisms employed by human speakers when referring to visual entities","offset":0,"pro":0,"labels":"PUR"},{"idx":25,"sentence":"In particular, we use CLIP to quantify the degree of descriptiveness (how well an utterance describes an image in isolation) and discriminativeness (to what extent an utterance is effective in picking out a single image among similar images) of human referring utterances within multimodal dialogues. ","offset":1,"pro":0.25,"labels":"MTD"},{"idx":25,"sentence":"Through analysis, we propose that this trend could be due to participants relying on the previous mentions in the dialogue history, as well as being able to distill the most discriminative information from the visual contex","offset":2,"pro":0.5,"labels":"CLN"},{"idx":25,"sentence":"In general, our study opens up the possibility of using this and similar models to quantify patterns in human data and shed light on the underlying cognitive mechanisms.","offset":3,"pro":0.75,"labels":"CTN"},{"idx":26,"sentence":"Codenames is a popular board game, in which knowledge and cooperation between players play an important rol","offset":0,"pro":0,"labels":"BAC"},{"idx":26,"sentence":"he task of a player playing as a spymaster is to find words (clues) that a teammate finds related to as many of some given words as possible, but not to other specified words. ","offset":1,"pro":0.2,"labels":"GAP"},{"idx":26,"sentence":"his is a hard challenge even with today's advanced language technology method","offset":2,"pro":0.4,"labels":"GAP"},{"idx":26,"sentence":"n our study, we create spymaster agents using four types of relatedness measures that require only a raw text corpus to produce. ","offset":3,"pro":0.6,"labels":"PUR"},{"idx":26,"sentence":"o generate clues in Codenames, we combine relatedness measures with four different scoring functions, for two languages, English and Hungarian. F","offset":4,"pro":0.8,"labels":"MTD"},{"idx":27,"sentence":"We investigate how to use pretrained static word embeddings to deliver improved estimates of bilexical co-occurrence probabilities: ","offset":0,"pro":0,"labels":"PUR"},{"idx":27,"sentence":"Such probabilities play important roles in psycholinguistics, corpus linguistics, and usage-based cognitive modeling of language more generally","offset":1,"pro":0.25,"labels":"IMP"},{"idx":27,"sentence":"e propose a log-bilinear model taking pretrained vector representations of the two words as input, enabling generalization based on the distributional information contained in both vectors.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":27,"sentence":"probabilities of nominal direct objects given their head verbs, given limited training data in Arabic, English, Korean, and Spanish.","offset":3,"pro":0.75,"labels":"CLN"},{"idx":28,"sentence":"Child language learners develop with remarkable uniformity, both in their learning trajectories and ultimate outcomes, despite major differences in their learning environments.","offset":0,"pro":0,"labels":"BAC"},{"idx":28,"sentence":"n this paper, we explore the role that the frequencies and distributions of irregular lexical items in the input plays in driving learning trajectories. ","offset":1,"pro":0.3333333333333333,"labels":"PUR"},{"idx":28,"sentence":"t also interacts with input distributions to drive cross-linguistic variation in learning trajectories.","offset":2,"pro":0.6666666666666666,"labels":"CLN"},{"idx":29,"sentence":"Scalar implicature (SI) arises when a speaker uses an expression (e.g., some) that is semantically compatible with a logically stronger alternative on the same scale (e.g., all), leading the listener to infer that they did not intend to convey the stronger meaning.","offset":0,"pro":0,"labels":"BAC"},{"idx":29,"sentence":"Prior work has demonstrated that SI rates are highly variable across scales, raising the question of what factors determine the SI strength for a particular scale.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":29,"sentence":"ere, we test the hypothesis that SI rates depend on the listener's confidence in the underlying scale, which we operationalize as uncertainty over the distribution of possible alternatives conditioned on the consentence","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":29,"sentence":"We use a T5 model fine-tuned on a text infilling task to estimate this distribution. ","offset":3,"pro":0.5,"labels":"MTD"},{"idx":29,"sentence":"urthermore, we do not find a significant effect of the surprisal of the strong scalemate.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":29,"sentence":"ur results suggest that pragmatic inferences depend on listeners' context-driven uncertainty over alternatives.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":0,"sentence":"Focus on language-specific properties with insights from formal minimalist syntax can improve universal dependency (UD) parsing. ","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"Such improvements are especially sensitive for low-resource African languages, like Wolof, which have fewer UD treebanks in number and amount of annotations, and fewer contributing annotators. ","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":0,"sentence":"For two different UD parser pipelines, one parser model was trained on the original Wolof treebank,","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":0,"sentence":"and one was trained on an edited treebank.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":0,"sentence":"For each parser pipeline, the accuracy of the edited treebank was higher than the original for both the dependency relations and dependency labels. ","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":0,"sentence":"Accuracy for universal dependency relations improved as much as 2.90%, while accuracy for universal dependency labels increased as much as 3.38%. ","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":0,"sentence":"An annotation scheme that better fits a language's distinct syntax results in better parsing accuracy.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":1,"sentence":"Language technologies, particularly speech technologies, are becoming more pervasive for access to digital platforms and resources.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"This brings to the forefront concerns of their inclusivity, first in terms of language diversity.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":1,"sentence":"Additionally, research shows speech recognition to be more accurate for men than for women and more accurate for individuals younger than 30 years of age than those older. ","offset":2,"pro":0.4,"labels":"GAP"},{"idx":1,"sentence":"It is also important to note that in varying contexts within the Global South, this work presents additional nuance and potential for bias based on accents, related dialects and variants of a language. ","offset":3,"pro":0.6,"labels":"PUR"},{"idx":1,"sentence":"ii) the unexpected yet key learning in terms of socio-linguistcs which demonstrate the importance of multi-disciplinarity in teams developing datasets and NLP technologies ","offset":4,"pro":0.8,"labels":"MTD"},{"idx":2,"sentence":"Language revitalisation should not be understood as a direct outcome of language documentation, which is mainly focused on the creation of language repositories. ","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"In this paper, we discuss the current state of the interaction between language documentation and computational linguistics, present a diagnosis of how the outputs of recent documentation projects for endangered languages are underutilised for the NLP community, and discuss how the situation could change from both the documentary linguistics and NLP perspectives.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":2,"sentence":"All this is introduced as a bridging paradigm dubbed as Computational Language Documentation and Development (CLD{mbox{^2). CLD{mbox{^2 calls for (1) the inclusion of NLP-friendly annotated data","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":2,"sentence":"as a deliverable of future language documentation projects;","offset":3,"pro":0.5,"labels":"CTN"},{"idx":2,"sentence":"and (2) the exploitation of language documentation databases by the NLP community","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":2,"sentence":"to promote the computerization of endangered languages, as one way to contribute to their revitalization.","offset":5,"pro":0.8333333333333334,"labels":"CTN"},{"idx":3,"sentence":"Data augmentation strategies are increasingly important in NLP pipelines for low-resourced and endangered languages, and in neural morphological inflection, augmentation by so called data hallucination is a popular technique","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"This paper presents a detailed analysis of inflection models trained with and without data hallucination for the low-resourced Canadian Indigenous language Gitksan.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":3,"sentence":"they strongly prefer affixing inflection patterns over suppletive ones. ","offset":2,"pro":0.4,"labels":"RST"},{"idx":3,"sentence":"Our results indicate that further innovations in data augmentation for computational morphology are desirable.","offset":3,"pro":0.6,"labels":"IMP"},{"idx":3,"sentence":"However, data hallucination dramatically reduces prediction accuracy for reduplicative forms due to a misanalysis of reduplication as affixation.","offset":4,"pro":0.8,"labels":"RST"},{"idx":4,"sentence":"Many archival recordings of speech from endangered languages remain unannotated and inaccessible to community members and language learning programs.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"One bottleneck is the time-intensive nature of annotation. ","offset":1,"pro":0.25,"labels":"BAC"},{"idx":4,"sentence":"We propose a privacy-preserving workflow to widen both bottlenecks for recordings where speech in the endangered language is intermixed with a more widely-used language such as English for meta-linguistic commentary and questions (e.g.What is the word for tree'?). ","offset":2,"pro":0.5,"labels":"PUR"},{"idx":4,"sentence":"We report work-in-progress processing 136 hours archival audio containing a mix of English and Muruwari. ","offset":3,"pro":0.75,"labels":"MTD"},{"idx":5,"sentence":"This paper describes the motivation and implementation details for a rule-based, index-preserving grapheme-to-phoneme engine G_i2P_i' implemented in pure Python and released under the open source MIT license.","offset":0,"pro":0,"labels":"PUR"},{"idx":5,"sentence":"The engine and interface have been designed to prioritize the developer experience of potential contributors without requiring a high level of programming knowledge. ","offset":1,"pro":0.25,"labels":"MTD"},{"idx":5,"sentence":"and the package is accompanied by a web-based interactive development environment, a RESTful API, and extensive documentation","offset":2,"pro":0.5,"labels":"MTD"},{"idx":5,"sentence":"to encourage the addition of more mappings in the future. ","offset":3,"pro":0.75,"labels":"CLN"},{"idx":6,"sentence":"Accelerating the process of data collection, annotation, and analysis is an urgent need for linguistic fieldwork and documentation of endangered languages (Bird, 2009).","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"Our experiments describe how we maximize the quality for the Nepal Bhasa syntactic complement structure chunking model. ","offset":1,"pro":0.2,"labels":"PUR"},{"idx":6,"sentence":"The embedded clauses, matrix verbs, and embedded verbs are annotated. ","offset":2,"pro":0.4,"labels":"MTD"},{"idx":6,"sentence":"We show that with limited annotated data, the model is already sufficient for the task. ","offset":3,"pro":0.6,"labels":"RST"},{"idx":6,"sentence":"The practice is easy to duplicate for training a shallow parser for other endangered languages in general.","offset":4,"pro":0.8,"labels":"CTN"},{"idx":7,"sentence":"We motivate the utility of these extensions both in general and specifically in relation to endangered and archaic languages, and illustrate with examples from the revived Australian language Barngarla, Icelandic Sign Language, Irish Gaelic, Old Norse manuscripts and Egyptian hieroglyphics.","offset":0,"pro":0,"labels":"MTD"},{"idx":8,"sentence":"Endangered languages are not supported by standard tools ","offset":0,"pro":0,"labels":"GAP"},{"idx":8,"sentence":"Textual resources for such languages are scarce, and sometimes the few available resources are corrupted PDF documents.","offset":1,"pro":0.25,"labels":"BAC"},{"idx":8,"sentence":"The approach presented is able to fully recover born digital PDF documents with minimal effort,","offset":2,"pro":0.5,"labels":"CLN"},{"idx":8,"sentence":"by extending the range of documents usable for corpus building.","offset":3,"pro":0.75,"labels":"MTD"},{"idx":9,"sentence":"Transcribing speech for primarily oral, local languages is often a joint effort involving speakers and outsiders. ","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"We explore the task of learning through transcription' ","offset":1,"pro":0.3333333333333333,"labels":"PUR"},{"idx":9,"sentence":" We have developed a prototype to support local and remote learner-speaker interactions in remote Aboriginal communities in northern Australia. ","offset":2,"pro":0.6666666666666666,"labels":"MTD"},{"idx":10,"sentence":"This paper discusses the development of a Part-of-Speech tagger for te reo M{=aori which is the Indigenous language of Aotearoa, also known as New Zealand, see Morrison. ","offset":0,"pro":0,"labels":"PUR"},{"idx":10,"sentence":" Prior to the development of this tagger, there was no POS tagger for M{=aori from Aotearoa. ","offset":1,"pro":0.125,"labels":"MTD"},{"idx":10,"sentence":"However, many traditional syntactic categories, and by consequence POS labels, do not work for M{=aori. ","offset":2,"pro":0.25,"labels":"GAP"},{"idx":10,"sentence":"They do not reflect a M{=aori worldview of the M{=aori language. ","offset":3,"pro":0.375,"labels":"GAP"},{"idx":10,"sentence":"but we also needed a tagset that would meet the needs of M{=aori. ","offset":4,"pro":0.5,"labels":"PUR"},{"idx":10,"sentence":"however the categorization of words has been significantly altered to be appropriate for M{=aori.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":10,"sentence":"To that end, we worked with highly-proficient, specially-selected M{=aori speakers and linguists who are specialists in M{=aori.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":10,"sentence":"This has ensured that our POS labels and guidelines conventions faithfully reflect a M{=aori speaker's conceptualization of their language.","offset":7,"pro":0.875,"labels":"RST"},{"idx":11,"sentence":"Innu-Aimun is an Algonquian language spoken in Eastern Canada.","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"It is the language of the Innu, an Indigenous people that now lives for the most part in a dozen communities across Quebec and Labrador. ","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":11,"sentence":"The state of its technology is still nascent, with very few existing applications.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":11,"sentence":"Considering the existing linguistic and textual resources, ","offset":3,"pro":0.5,"labels":"MTD"},{"idx":11,"sentence":"The goal of developing such technologies is first and foremost to help efforts in improving language transmission and cultural safety and preservation for Innu-Aimun speakers, as those are considered urgent and vital issues. ","offset":4,"pro":0.6666666666666666,"labels":"IMP"},{"idx":11,"sentence":"in order to ensure that language technologies are developed respectfully and in accordance with that goal.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":12,"sentence":"A technical description of the platform is provided, including details of how different speech technologies and linguistic resources are fused to provide a holistic learner experience.","offset":0,"pro":0,"labels":"MTD"},{"idx":12,"sentence":"The active continuous participation of the community, and platform evaluations by learners and teachers, are discussed.","offset":1,"pro":0.5,"labels":"MTD"},{"idx":13,"sentence":"For decades, researchers in natural language processing and computational linguistics have been developing models and algorithms that aim to serve the needs of language documentation projects. ","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"In this work, we argue that a major reason for this NLP gap is the lack of a strong foundation of application software which can on the one hand serve the complex needs of language documentation and on the other hand provide effortless integration with NLP models. ","offset":1,"pro":0.5,"labels":"PUR"},{"idx":14,"sentence":"Machine translation for low-resource languages, such as Guarani, is a challenging task due to the lack of data. ","offset":0,"pro":0,"labels":"GAP"},{"idx":14,"sentence":" In this work we try to check if currently available data is enough to train rich embeddings for enhancing MT for Guarani and Spanish,","offset":1,"pro":0.3333333333333333,"labels":"PUR"},{"idx":14,"sentence":"by building a set of word embedding collections and training MT systems using them. ","offset":2,"pro":0.6666666666666666,"labels":"MTD"},{"idx":15,"sentence":"In this paper, we present a game with a purpose (GWAP) (Von Ahn 2006). ","offset":0,"pro":0,"labels":"PUR"},{"idx":15,"sentence":" Irish is a minority language which means that L2 learners have limited opportunities for exposure to the language, ","offset":1,"pro":0.3333333333333333,"labels":"GAP"},{"idx":15,"sentence":" This research incorporates game development, language pedagogy and ICALL language materials development. ","offset":2,"pro":0.6666666666666666,"labels":"PUR"},{"idx":16,"sentence":"Many endangered Uralic languages have multilingual machine readable dictionaries saved in an XML format.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"However, the dictionaries cover translations very inconsistently between language pairs, for instance, the Livonian dictionary has some translations to Finnish, Latvian and Estonian, and the Komi-Zyrian dictionary has some translations to Finnish, English and Russian. ","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":16,"sentence":"by predicting new translations to existing and new languages based on different dictionaries for endangered languages and Wiktionaries. ","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":16,"sentence":"We evaluate our approach by human judges fluent in the three endangered languages in question.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":16,"sentence":"Based on the evaluation, the method predicted good or acceptable translations 77% of the time. ","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":16,"sentence":"The resulting extensions to the dictionaries are made available on the online dictionary platform used by the speakers of these languages","offset":5,"pro":0.8333333333333334,"labels":"CTN"},{"idx":17,"sentence":"but do not have the big corpus data that are required for the construction of machine learning tools. ","offset":0,"pro":0,"labels":"GAP"},{"idx":17,"sentence":"We use an existing grammar to infer rules for the new language, ","offset":1,"pro":0.3333333333333333,"labels":"MTD"},{"idx":17,"sentence":"We present a test case for Lule S{'ami reusing resources from North S{'ami, show how we achieve a categorisation of the most frequent errors, and present a preliminary evaluation of the system. ","offset":2,"pro":0.6666666666666666,"labels":"MTD"},{"idx":18,"sentence":"There are many challenges in morphological fieldwork annotation,","offset":0,"pro":0,"labels":"GAP"},{"idx":18,"sentence":"it's time-intensive, ","offset":1,"pro":0.2,"labels":"GAP"},{"idx":18,"sentence":"We propose a workflow that relies on unsupervised and active learning grounded in Word-and-Paradigm morphology (WP). ","offset":2,"pro":0.4,"labels":"PUR"},{"idx":18,"sentence":" while the WP approach makes for an annotation system that is word-based and relational, removing the need to make decisions about feature labeling and segmentation early in the process and allowing speakers of the language of interest to participate more actively, since linguistic training is not necessary. ","offset":3,"pro":0.6,"labels":"MTD"},{"idx":18,"sentence":" annotators can process hundreds of forms per hour.","offset":4,"pro":0.8,"labels":"RST"},{"idx":19,"sentence":"This is a report on results obtained in the development of speech recognition tools intended to support linguistic documentation efforts.","offset":0,"pro":0,"labels":"PUR"},{"idx":19,"sentence":"The test case is an extensive fieldwork corpus of Japhug, an endangered language of the Trans-Himalayan (Sino-Tibetan) family. ","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":19,"sentence":"The method used is a deep learning approach based on the language-specific tuning of a generic pre-trained representation model, XLS-R, using a Transformer architecture. ","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":19,"sentence":"The quality of phonemic transcription is improved over earlier experiments;","offset":3,"pro":0.42857142857142855,"labels":"RST"},{"idx":19,"sentence":"and most significantly, the new approach allows for reaching the stage of automatic word recognition.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":19,"sentence":"But this approach brings significant improvements nonetheless.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":19,"sentence":"Subjective evaluation of the tool by the author of the training data confirms the usefulness of this approach.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":0,"sentence":" The project XXXX is developing a platform to enable researchers of living languages to easily create and make available state-of-the-art spoken and textual annotated resources.","offset":0,"pro":0,"labels":"PUR"},{"idx":0,"sentence":"As a case study we use Greek and Pomak, the latter being an endangered oral Slavic language of the Balkans (including Thrace/Greece).","offset":1,"pro":0.2,"labels":"MTD"},{"idx":0,"sentence":"The linguistic documentation of Pomak is an ongoing work by an interdisciplinary team in close cooperation with the Pomak community of Greece.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":0,"sentence":"We describe our experience in the development of a Latin-based orthography and morphologically annotated text corpora of Pomak with state-of-the-art NLP technology.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":0,"sentence":"These resources will be made openly available on the XXXX site and the gold annotated corpora of Pomak will be made available on the Universal Dependencies treebank repository.","offset":4,"pro":0.8,"labels":"CTN"},{"idx":1,"sentence":"This study investigates applications of automatic speech recognition (ASR) techniques to Hupa, a critically endangered Native American language from the Dene (Athabaskan) language family.","offset":0,"pro":0,"labels":"PUR"},{"idx":1,"sentence":"Using around 9h12m of spoken data produced by one elder who is a first-language Hupa speaker, we experimented with different evaluation schemes and training settings.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":1,"sentence":"On average a fully connected deep neural network reached a word error rate of 35.26%.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":1,"sentence":"Our overall results illustrate the utility of ASR for making Hupa language documentation more accessible and usable.","offset":3,"pro":0.5,"labels":"RST"},{"idx":1,"sentence":"In addition, we found that when training acoustic models, using recordings with transcripts that were not carefully verified did not necessarily have a negative effect on model performance.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":1,"sentence":"This shows promise for speech corpora of indigenous languages that commonly include transcriptions produced by second-language speakers or linguists who have advanced knowledge in the language of interest.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":2,"sentence":" In this paper we create visually grounded word embeddings by combining English text and images and compare them to popular text-based methods, to see if visual information allows our model to better capture cognitive aspects of word meaning.","offset":0,"pro":0,"labels":"PUR"},{"idx":2,"sentence":"Our analysis shows that visually grounded embedding similarities are more predictive of the human reaction times in a large priming experiment than the purely text-based embeddings.","offset":1,"pro":0.2,"labels":"RST"},{"idx":2,"sentence":" The visually grounded embeddings also correlate well with human word similarity ratings.","offset":2,"pro":0.4,"labels":"RST"},{"idx":2,"sentence":"Importantly, in both experiments we show that the grounded embeddings account for a unique portion of explained variance, even when we include text-based embeddings trained on huge corpora.","offset":3,"pro":0.6,"labels":"RST"},{"idx":2,"sentence":"This shows that visual grounding allows our model to capture information that cannot be extracted using text as the only source of information.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":3,"sentence":"We propose a new neural model for word embeddings, which uses Unitary Matrices as the primary device for encoding lexical information.","offset":0,"pro":0,"labels":"PUR"},{"idx":3,"sentence":" We test it in two NLP agreement tasks and obtain rule like perfect accuracy, with greater stability than current state-of-the-art systems. ","offset":1,"pro":0.25,"labels":"RST"},{"idx":3,"sentence":"Our proposed model goes some way towards offering a class of computationally powerful deep learning systems that can be fully understood and compared to human cognitive processes for natural language learning and representation.","offset":2,"pro":0.5,"labels":"CLN"},{"idx":3,"sentence":"It uses simple matrix multiplication to derive matrices for large units, yielding a sentence processing model that is strictly compositional, does not lose information over time steps, and is transparent, in the sense that word embeddings can be analysed regardless of context.","offset":3,"pro":0.75,"labels":"MTD"},{"idx":4,"sentence":"Accurate NNC interpretation, i.e. determining the implicit relationship between the constituents of a NNC, is crucial for the advancement of many natural language processing tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"Until now, computational NNC interpretation has been limited to approaches involving linguistic representations only.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":4,"sentence":"However, much research suggests that grounding linguistic representations in vision or other modalities can increase performance on this and other tasks.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":4,"sentence":"Our work is a novel comparison of linguistic and visuo-linguistic representations for the task of NNC interpretation.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":4,"sentence":"We frame NNC interpretation as a relation classification task, evaluating on a large, relationally-annotated NNC dataset.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":4,"sentence":"We combine distributional word vectors with image vectors to investigate how visual information can help improve NNC interpretation systems.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":4,"sentence":"We find that adding visual vectors increases classification performance on our dataset in many cases.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":5,"sentence":"Overall, our results show that utterances become less descriptive over time while their discriminativeness remains unchanged.","offset":0,"pro":0,"labels":"RST"},{"idx":5,"sentence":"Through analysis, we propose that this trend could be due to participants relying on the previous mentions in the dialogue history, as well as being able to distill the most discriminative information from the visual context.","offset":1,"pro":0.3333333333333333,"labels":"CLN"},{"idx":5,"sentence":"In general, our study opens up the possibility of using this and similar models to quantify patterns in human data and shed light on the underlying cognitive mechanisms.","offset":2,"pro":0.6666666666666666,"labels":"CTN"},{"idx":6,"sentence":"Codenames is a popular board game, in which knowledge and cooperation between players play an important role.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"In our study, we create spymaster agents using four types of relatedness measures that require only a raw text corpus to produce.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":6,"sentence":"These include newly introduced ones based on co-occurrences, which outperform FastText cosine similarity on gold standard relatedness data.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":6,"sentence":"To generate clues in Codenames, we combine relatedness measures with four different scoring functions, for two languages, English and Hungarian.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":6,"sentence":"For testing, we collect decisions of human guesser players in an online game,","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":6,"sentence":"and our configurations outperform previous agents among methods using raw corpora only.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":6,"sentence":"The task of a player playing as a spymaster is to find words (clues) that a teammate finds related to as many of some given words as possible, but not to other specified words.","offset":6,"pro":0.8571428571428571,"labels":"GAP"},{"idx":7,"sentence":"Such probabilities play important roles in psycholinguistics, corpus linguistics, and usage-based cognitive modeling of language more generally.","offset":0,"pro":0,"labels":"PUR"},{"idx":7,"sentence":"We propose a log-bilinear model taking pretrained vector representations of the two words as input, enabling generalization based on the distributional information contained in both vectors.","offset":1,"pro":0.3333333333333333,"labels":"MTD"},{"idx":7,"sentence":"We show that this model outperforms baselines in estimating probabilities of adjectives given nouns that they attributively modify, and probabilities of nominal direct objects given their head verbs, given limited training data in Arabic, English, Korean, and Spanish.","offset":2,"pro":0.6666666666666666,"labels":"RST"},{"idx":8,"sentence":"Child language learners develop with remarkable uniformity, both in their learning trajectories and ultimate outcomes, despite major differences in their learning environments.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"We conclude that while the Tolerance Principle, a type-based model of productivity learning, accounts for inter-learner uniformity, it also interacts with input distributions to drive cross-linguistic variation in learning trajectories.","offset":1,"pro":0.5,"labels":"CLN"},{"idx":9,"sentence":"Scalar implicature (SI) arises when a speaker uses an expression (e.g., some) that is semantically compatible with a logically stronger alternative on the same scale (e.g., all), leading the listener to infer that they did not intend to convey the stronger meaning.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"Prior work has demonstrated that SI rates are highly variable across scales, raising the question of what factors determine the SI strength for a particular scale.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":9,"sentence":"Here, we test the hypothesis that SI rates depend on the listener's confidence in the underlying scale, which we operationalize as uncertainty over the distribution of possible alternatives conditioned on the context.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":9,"sentence":"We use a T5 model fine-tuned on a text infilling task to estimate this distribution.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":9,"sentence":"We find that scale uncertainty predicts human SI rates, measured as entropy over the sampled alternatives and over latent classes among alternatives in sentence embedding space.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":9,"sentence":"Our results suggest that pragmatic inferences depend on listeners' context-driven uncertainty over alternatives.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":10,"sentence":"Attention describes cognitive processes that are important to many human phenomena including reading.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"The term is also used to describe the way in which transformer neural networks perform natural language processing.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":10,"sentence":"this paper presents an analysis of the correlations between transformer attention and overt human attention during reading tasks.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":10,"sentence":"showed that the dwell times of human eye movements were strongly correlated with the attention patterns occurring in the early layers of pre-trained transformers such as BERT.","offset":3,"pro":0.5,"labels":"RST"},{"idx":10,"sentence":"Additionally, the strength of a correlation was not related to the number of parameters within a transformer.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":10,"sentence":"This suggests that something about the transformers' architecture determined how closely the two measures were correlated.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":11,"sentence":"Aspect is a linguistic concept that describes how an action, event, or state of a verb phrase is situated in time.","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"In this paper, we explore whether different transformer models are capable of identifying aspectual features.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":11,"sentence":"We focus on two specific aspectual features: telicity and duration.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":11,"sentence":"Telicity marks whether the verb's action or state has an endpoint or not (telic/atelic), and duration denotes whether a verb expresses an action (dynamic) or a state (stative).","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":11,"sentence":"These features are integral to the interpretation of natural language, but also hard to annotate and identify with NLP methods.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":11,"sentence":"We perform experiments in English and French,","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":11,"sentence":"and our results show that transformer models adequately capture information on telicity and duration in their vectors, even in their non-finetuned forms, but are somewhat biased with regard to verb tense and word order.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":12,"sentence":"Eye tracking data during reading is a useful source of information to understand the cognitive processes that take place during language comprehension processes.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"Different languages account for different cognitive triggers, however there seems to be some uniform indicatorsacross languages.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":12,"sentence":"In this paper, we describe our submission to the CMCL 2022 shared task on predicting human reading patterns for multi-lingual dataset.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":12,"sentence":"Our model uses text representations from transformers and some hand engineered features with a regression layer on top to predict statistical measures of mean and standard deviation for 2 main eye-tracking features.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":12,"sentence":"We train an end-to-end model to extract meaningful information from different languages and test our model on two separate datasets.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":12,"sentence":"We compare different transformer models andshow ablation studies affecting model performance.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":12,"sentence":"Our final submission ranked 4th place for SubTask-1 and 1st place for SubTask-2 forthe shared task.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":13,"sentence":"In this paper, we present a unified model that works for both multilingual and crosslingual prediction of reading times of words in various languages.","offset":0,"pro":0,"labels":"PUR"},{"idx":13,"sentence":"To the best of our knowledge, this is the first study to favorably exploit this phonological property of language for the two tasks.","offset":1,"pro":0.2,"labels":"CTN"},{"idx":13,"sentence":"The secret behind the success of this model is in the preprocessing step where all words are transformed to their universal language representation via the International Phonetic Alphabet (IPA).","offset":2,"pro":0.4,"labels":"MTD"},{"idx":13,"sentence":"A finetuned Random Forest model obtained best performance for both tasks with 3.8031 and 3.9065 MAE scores for mean first fixation duration (FFDAvg) and mean total reading time (TRTAvg) respectively.","offset":3,"pro":0.6,"labels":"RST"},{"idx":13,"sentence":"Various feature types were extracted covering basic frequencies, n-grams, information theoretic, and psycholinguistically-motivated predictors for model training.","offset":4,"pro":0.8,"labels":"MTD"},{"idx":14,"sentence":"Eye movement data are used in psycholinguistic studies to infer information regarding cognitive processes during reading.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"In this paper, we describe our proposed method for the Shared Task of Cognitive Modeling and Computational Linguistics (CMCL) 2022 - Subtask 1, which involves data from multiple datasets on 6 languages.","offset":1,"pro":0.25,"labels":"PUR"},{"idx":14,"sentence":"We compared different regression models using features of the target word and its previous word, and target word surprisal as regression features.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":14,"sentence":"Our final system, using a gradient boosting regressor, achieved the lowest mean absolute error (MAE), resulting in the best system of the competition.","offset":3,"pro":0.75,"labels":"RST"},{"idx":15,"sentence":"We present the second shared task on eye-tracking data prediction of the Cognitive Modeling and Computational Linguistics Workshop (CMCL).","offset":0,"pro":0,"labels":"PUR"},{"idx":16,"sentence":"Eye-Tracking data is a very useful source of information to study cognition and especially language comprehension in humans.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"In this paper, we describe our systems for the CMCL 2022 shared task on predicting eye-tracking information.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":16,"sentence":"We describe our experiments withpretrained models like BERT and XLM and the different ways in which we used those representations to predict four eye-tracking features.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":16,"sentence":"Along with analysing the effect of using two different kinds of pretrained multilingual language models and different ways of pooling the token-level representations, we also explore how contextual information affects the performance of the systems.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":16,"sentence":"Finally, we also explore if factors like augmenting linguistic information affect the predictions.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":16,"sentence":"Our submissions achieved an average MAE of 5.72 and ranked 5th in the shared task.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":16,"sentence":"The average MAE showed further reduction to 5.25 in post task evaluation.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":17,"sentence":"To this end, we train adapters' inserted into the layers of frozen transformer-based pretrained language models.","offset":0,"pro":0,"labels":"MTD"},{"idx":17,"sentence":"We find that multilingual models equipped with adapters perform well in predicting eye-tracking features.","offset":1,"pro":0.3333333333333333,"labels":"RST"},{"idx":17,"sentence":"Our results suggest that utilizing language- and task-specific adapters is beneficial and translating test sets into similar languages that exist in the training set could help with zero-shot transferability in the prediction of human reading behavior.","offset":2,"pro":0.6666666666666666,"labels":"CLN"},{"idx":18,"sentence":"The healthcare domain suffers from the spread of poor quality articles on the Internet.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"While manual efforts exist to check the quality of online healthcare articles, they are not sufficient to assess all those in circulation.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":18,"sentence":"Such quality assessment can be automated as a text classification task, however, explanations for the labels are necessary for the users to trust the model predictions.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":18,"sentence":"While current explainable systems tackle explanation generation as summarization, we propose a new approach based on question answering (QA) that allows us to generate explanations for multiple criteria using a single model.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":18,"sentence":"We also introduce a human evaluation protocol more appropriate than automatic metrics for the evaluation of explanation generation models.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":18,"sentence":"We show that this QA-based approach is competitive with the current state-of-the-art, and complements summarization-based models for explainable quality assessment.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":19,"sentence":"Motivated by the fact that many relations cross the sentence boundary, there has been increasing interest in document-level relation extraction (DocRE).","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"DocRE requires integrating information within and across sentences, capturing complex interactions between mentions of entities.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":19,"sentence":"Most existing methods are pipeline-based, requiring entities as input.","offset":2,"pro":0.2,"labels":"GAP"},{"idx":19,"sentence":"However, jointly learning to extract entities and relations can improve performance and be more efficient due to shared parameters and training steps.","offset":3,"pro":0.3,"labels":"GAP"},{"idx":19,"sentence":"In this paper, we develop a sequence-to-sequence approach, seq2rel, that can learn the subtasks of DocRE (entity extraction, coreference resolution and relation extraction) end-to-end, replacing a pipeline of task-specific components.","offset":4,"pro":0.4,"labels":"PUR"},{"idx":19,"sentence":"Using a simple strategy we call entity hinting, we compare our approach to existing pipeline-based methods on several popular biomedical datasets, in some cases exceeding their performance.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":19,"sentence":"We also report the first end-to-end results on these datasets for future comparison.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":19,"sentence":"Finally, we demonstrate that, under our model, an end-to-end approach outperforms a pipeline-based approach.","offset":7,"pro":0.7,"labels":"RST"},{"idx":19,"sentence":"Our code, data and trained models are available at https://github.com/johngiorgi/seq2rel.","offset":8,"pro":0.8,"labels":"CTN"},{"idx":19,"sentence":"An online demo is available at https://share.streamlit.io/johngiorgi/seq2rel/main/demo.py.","offset":9,"pro":0.9,"labels":"CTN"},{"idx":0,"sentence":"Probing factual knowledge in Pre-trained Language Models (PLMs) using prompts has indirectly implied that language models (LMs) can be treated as knowledge bases","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"To this end, this phenomenon has been effective, especially when these LMs are fine-tuned towards not just data, but also to the style or linguistic pattern of the prompts themselves","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":0,"sentence":"We observe that satisfying a particular linguistic pattern in prompts is an unsustainable, time-consuming constraint in the probing task, especially because they are often manually designed and the range of possible prompt template patterns can vary depending on the prompting task","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":0,"sentence":"To alleviate this constraint, we propose using a position-attention mechanism to capture positional information of each word in a prompt relative to the mask to be filled, hence avoiding the need to re-construct prompts when the prompts' linguistic pattern changes","offset":3,"pro":0.5,"labels":"PUR"},{"idx":0,"sentence":"Using our approach, we demonstrate the ability of eliciting answers (in a case study on health outcome generation) to not only common prompt templates like Cloze and Prefix but also rare ones too, such as Postfix and Mixed patterns whose masks are respectively at the start and in multiple random places of the prompt.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":0,"sentence":"More so, using various biomedical PLMs, our approach consistently outperforms a baseline in which the default PLMs representation is used to predict masked tokens","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":1,"sentence":"Automatic speech recognition (ASR) systems usually incorporate postprocessing mechanisms to remove disfluencies, facilitating the generation of clear, fluent transcripts that are conducive to many downstream NLP tasks","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"However, verbal disfluencies have proved to be predictive of dementia status, although little is known about how various types of verbal disfluencies, nor automatically detected disfluencies, affect predictive performance","offset":1,"pro":0.2,"labels":"GAP"},{"idx":1,"sentence":"We experiment with an off-the-shelf disfluency annotator to tag disfluencies in speech transcripts for a well-known cognitive health assessment task","offset":2,"pro":0.4,"labels":"MTD"},{"idx":1,"sentence":"We evaluate the performance of this model on detecting repetitions and corrections or retracing, and measure the influence of gold annotated versus automatically detected verbal disfluencies on dementia detection through a series of experiments","offset":3,"pro":0.6,"labels":"MTD"},{"idx":1,"sentence":"We find that removing both gold and automatically-detected disfluencies negatively impacts dementia detection performance, degrading classification accuracy by 5.6% and 3% respectively","offset":4,"pro":0.8,"labels":"RST"},{"idx":2,"sentence":"We study the zero-shot setting for the aspect-based scientific document summarization task","offset":0,"pro":0,"labels":"PUR"},{"idx":2,"sentence":"Summarizing scientific documents with respect to an aspect can remarkably improve document assistance systems and readers experience.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":2,"sentence":"However, existing large-scale datasets contain a limited variety of aspects, causing summarization models to over-fit to a small set of aspects and a specific domain","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":2,"sentence":"We establish baseline results in zero-shot performance (over unseen aspects and the presence of domain shift), paraphrasing, leave-one-out, and limited supervised samples experimental setups","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":2,"sentence":"We propose a self-supervised pre-training approach to enhance the zero-shot performance","offset":4,"pro":0.5714285714285714,"labels":"PUR"},{"idx":2,"sentence":" We leverage the PubMed structured abstracts to create a biomedical aspect-based summarization dataset","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":2,"sentence":"Experimental results on the PubMed and FacetSum aspect-based datasets show promising performance when the model is pre-trained using unlabelled in-domain data.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":3,"sentence":"We study the effect of seven data augmentation (DA) methods in factoid question answering, focusing on the biomedical domain, where obtaining training instances is particularly difficult","offset":0,"pro":0,"labels":"PUR"},{"idx":3,"sentence":"We experiment with data from the BIOASQ challenge, which we augment with training instances obtained from an artificial biomedical machine reading comprehension dataset, or via back-translation, information retrieval, word substitution based on WORD2VEC embeddings, or masked language modeling, question generation, or extending the given passage with additional consentence","offset":1,"pro":0.2,"labels":"MTD"},{"idx":3,"sentence":"We show that DA can lead to very significant performance gains, even when using large pre-trained Transformers, contributing to a broader discussion of if/when DA benefits large pre-trained models","offset":2,"pro":0.4,"labels":"RST"},{"idx":3,"sentence":"One of the simplest DA methods, WORD2VEC-based word substitution, performed best and is recommended","offset":3,"pro":0.6,"labels":"RST"},{"idx":3,"sentence":"We release our artificial training instances and code","offset":4,"pro":0.8,"labels":"CTN"},{"idx":4,"sentence":" The task typically consists of a series of sub-tasks such as Named Entity Recognition and Relation Extraction","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"Sourcing entity and relation type specific training data is a major bottleneck in domains with limited resources such as biomedicine","offset":1,"pro":0.125,"labels":"BAC"},{"idx":4,"sentence":" In this work we present a slot filling approach to the task of biomedical IE, effectively replacing the need for entity and relation-specific training data, allowing us to deal with zero-shot settings","offset":2,"pro":0.25,"labels":"PUR"},{"idx":4,"sentence":"We follow the recently proposed paradigm of coupling a Tranformer-based bi-encoder, Dense Passage Retrieval, with a Transformer-based reading comprehension model to extract relations from biomedical text.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":4,"sentence":"We assemble a biomedical slot filling dataset for both retrieval and reading comprehension and conduct a series of experiments demonstrating that our approach outperforms a number of simpler baselines","offset":4,"pro":0.5,"labels":"MTD"},{"idx":4,"sentence":"We also evaluate our approach end-to-end for standard as well as zero-shot settings","offset":5,"pro":0.625,"labels":"MTD"},{"idx":4,"sentence":"Our work provides a fresh perspective on how to solve biomedical IE tasks, in the absence of relevant training data","offset":6,"pro":0.75,"labels":"RST"},{"idx":4,"sentence":"Our code, models and datasets are available at https://github.com/tba","offset":7,"pro":0.875,"labels":"CTN"},{"idx":5,"sentence":"Term clustering is important in biomedical knowledge graph construction. Using similarities between terms embedding is helpful for term clustering","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"State-of-the-art term embeddings leverage pretrained language models to encode terms, and use synonyms and relation knowledge from knowledge graphs to guide contrastive learning","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":5,"sentence":"These embeddings provide close embeddings for terms belonging to the same concept","offset":2,"pro":0.2857142857142857,"labels":"BAC"},{"idx":5,"sentence":"However, from our probing experiments, these embeddings are not sensitive to minor textual differences which leads to failure for biomedical term clustering","offset":3,"pro":0.42857142857142855,"labels":"GAP"},{"idx":5,"sentence":"To alleviate this problem","offset":4,"pro":0.5714285714285714,"labels":"PUR"},{"idx":5,"sentence":"we adjust the sampling strategy in pretraining term embeddings by providing dynamic hard positive and negative samples during contrastive learning to learn fine-grained representations which result in better biomedical term clustering","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":5,"sentence":"We name our proposed method as CODER++, and it has been applied in clustering biomedical concepts in the newly released Biomedical Knowledge Graph named BIOS","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":6,"sentence":"Pretrained language models have served as important backbones for natural language processing","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"Recently, in-domain pretraining has been shown to benefit various domain-specific downstream tasks","offset":1,"pro":0.1,"labels":"BAC"},{"idx":6,"sentence":" In the biomedical domain, natural language generation (NLG) tasks are of critical importance, while understudied.","offset":2,"pro":0.2,"labels":"BAC"},{"idx":6,"sentence":"Approaching natural language understanding (NLU) tasks as NLG achieves satisfying performance in the general domain through constrained language generation or language prompting","offset":3,"pro":0.3,"labels":"BAC"},{"idx":6,"sentence":"We emphasize the lack of in-domain generative language models and the unsystematic generative downstream benchmarks in the biomedical domain, hindering the development of the research community.","offset":4,"pro":0.4,"labels":"GAP"},{"idx":6,"sentence":"In this work, we introduce the generative language model BioBART that adapts BART to the biomedical domain","offset":5,"pro":0.5,"labels":"PUR"},{"idx":6,"sentence":"We collate various biomedical language generation tasks including dialogue, summarization, entity linking, and named entity recognition.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":6,"sentence":"BioBART pretrained on PubMed abstracts has enhanced performance compared to BART and set strong baselines on several tasks","offset":7,"pro":0.7,"labels":"MTD"},{"idx":6,"sentence":"Furthermore, we conduct ablation studies on the pretraining tasks for BioBART","offset":8,"pro":0.8,"labels":"MTD"},{"idx":6,"sentence":"find that sentence permutation has negative effects on downstream tasks.","offset":9,"pro":0.9,"labels":"RST"},{"idx":7,"sentence":"Medical dialogue systems have the potential to assist doctors in expanding access to medical care, improving the quality of patient experiences, and lowering medical expenses","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"The computational methods are still in their early stages and are not ready for widespread application despite their great potential","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":7,"sentence":"Existing transformer-based language models have shown promising results but lack domain-specific knowledge. ","offset":2,"pro":0.2857142857142857,"labels":"BAC"},{"idx":7,"sentence":"However, to diagnose like doctors, an automatic medical diagnosis necessitates more stringent requirements for the rationality of the dialogue in the context of relevant knowledge","offset":3,"pro":0.42857142857142855,"labels":"GAP"},{"idx":7,"sentence":"In this study, we propose a new method that addresses the challenges of medical dialogue generation by incorporating medical knowledge into transformer-based language models.","offset":4,"pro":0.5714285714285714,"labels":"PUR"},{"idx":7,"sentence":"We present a method that leverages an external medical knowledge graph and injects triples as domain knowledge into the utterances","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":7,"sentence":"Automatic and human evaluation on a publicly available dataset demonstrates that incorporating medical knowledge outperforms several state-of-the-art baseline methods","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":8,"sentence":"Automatic generating the clinically accurate radiology report from X-ray images is important but challenging","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"The identification of multi-grained abnormal regions in image and corresponding abnormalities is difficult for data-driven neural models","offset":1,"pro":0.2,"labels":"BAC"},{"idx":8,"sentence":"In this work, we introduce a Memory-aligned Knowledge Graph (MaKG) of clinical abnormalities to better learn the visual patterns of abnormalities and their relationships by integrating it into a deep model architecture for the report generation","offset":2,"pro":0.4,"labels":"PUR"},{"idx":8,"sentence":"We carry out extensive experiments","offset":3,"pro":0.6,"labels":"MTD"},{"idx":8,"sentence":" show that the proposed MaKG deep model can improve the clinical accuracy of the generated reports","offset":4,"pro":0.8,"labels":"RST"},{"idx":9,"sentence":"Data augmentation is important in addressing data sparsity and low resources in NLP.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"Unlike data augmentation for other tasks such as sentence-level and sentence-pair ones, data augmentation for named entity recognition (NER) requires preserving the semantic of entities","offset":1,"pro":0.2,"labels":"BAC"},{"idx":9,"sentence":"To that end, in this paper we propose a simple semantic-based data augmentation method for biomedical NER","offset":2,"pro":0.4,"labels":"PUR"},{"idx":9,"sentence":"Our method leverages semantic information from pre-trained language models for both entity-level and sentence-level","offset":3,"pro":0.6,"labels":"MTD"},{"idx":9,"sentence":"Experimental results on two datasets: i2b2-2010 (English) and VietBioNER (Vietnamese) showed that the proposed method could improve NER performance.","offset":4,"pro":0.8,"labels":"RST"},{"idx":10,"sentence":"Named entity recognition (NER) is one of the elemental technologies, which has been used for knowledge extraction from biomedical sentence","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"As one of the NER improvement approaches, multi-task learning that learns a model from multiple training data has been used","offset":1,"pro":0.125,"labels":"BAC"},{"idx":10,"sentence":"Among multi-task learning, an auxiliary learning method, which uses an auxiliary task for improving its target task, has shown higher NER performance than conventional multi-task learning for improving all the tasks simultaneously by using only one auxiliary task in the auxiliary learning","offset":2,"pro":0.25,"labels":"BAC"},{"idx":10,"sentence":"We propose Multiple Utilization of NER Corpora Helpful for Auxiliary BLESsing (MUNCH ABLES).","offset":3,"pro":0.375,"labels":"PUR"},{"idx":10,"sentence":"MUNCHABLES utilizes multiple training datasets as auxiliary training data by the following methods","offset":4,"pro":0.5,"labels":"MTD"},{"idx":10,"sentence":"the first one is to finetune the NER model of the target task by sequentially performing auxiliary learning for each auxiliary training dataset, and the other is to use all training datasets in one auxiliary learning","offset":5,"pro":0.625,"labels":"MTD"},{"idx":10,"sentence":"We evaluate MUNCHABLES on eight biomedical-related domain NER tasks, where seven training datasets are used as auxiliary training data","offset":6,"pro":0.75,"labels":"MTD"},{"idx":10,"sentence":"The experiment results show that MUNCHABLES achieves higher accuracy than conventional multi-task learning methods on average while showing state-of-the-art accuracy.","offset":7,"pro":0.875,"labels":"RST"},{"idx":11,"sentence":"Self-supervised pre-training methods have brought remarkable breakthroughs in the understanding of text, image, and speech","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"Recent developments in genomics has also adopted these pre-training methods for genome understanding","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":11,"sentence":"However, they focus only on understanding haploid sequences, which hinders their applicability towards understanding genetic variations, also known as single nucleotide polymorphisms (SNPs), which is crucial for genome-wide association study","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":11,"sentence":"In this paper, we introduce SNP2Vec, a scalable self-supervised pre-training approach for understanding SNP","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":11,"sentence":"We apply SNP2Vec to perform long-sequence genomics modeling","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":11,"sentence":"and we evaluate the effectiveness of our approach on predicting Alzheimer's disease risk in a Chinese cohort","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":11,"sentence":"Our approach significantly outperforms existing polygenic risk score methods and all other baselines, including the model that is trained entirely with haploid sequences","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":12,"sentence":"Biomedical Named Entity Recognition (BMNER) is one of the most important tasks in the field of biomedical text mining","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"Most work so far on this task has not focused on identification of discontinuous and overlapping entities, even though they are present in significant fractions in real-life biomedical datasets","offset":1,"pro":0.2,"labels":"GAP"},{"idx":12,"sentence":"In this paper, we introduce a novel annotation schema to capture complex entities, and explore the effects of distant supervision on our deep-learning sequence labelling model","offset":2,"pro":0.4,"labels":"PUR"},{"idx":12,"sentence":"For BMNER task, our annotation schema outperforms other BIO-based annotation schemes on the same model","offset":3,"pro":0.6,"labels":"RST"},{"idx":12,"sentence":"We also achieve higher F1-scores than state-of-the-art models on multiple corpora without fine-tuning embeddings, highlighting the efficacy of neural feature extraction using our model.","offset":4,"pro":0.8,"labels":"RST"},{"idx":13,"sentence":"This paper proposes novel drug-protein relation extraction models that indirectly utilize distant supervision data","offset":0,"pro":0,"labels":"PUR"},{"idx":13,"sentence":"Concretely, instead of adding distant supervision data to the manually annotated training data, our models incorporate distantly supervised models that are relation extraction models trained with distant supervision data","offset":1,"pro":0.125,"labels":"MTD"},{"idx":13,"sentence":"Distantly supervised learning has been proposed to generate a large amount of pseudo-training data at low cost","offset":2,"pro":0.25,"labels":"BAC"},{"idx":13,"sentence":"However, there is still a problem of low prediction performance due to the inclusion of mislabeled data","offset":3,"pro":0.375,"labels":"BAC"},{"idx":13,"sentence":"herefore, several methods have been proposed to suppress the effects of noisy cases by utilizing some manually annotated training data","offset":4,"pro":0.5,"labels":"BAC"},{"idx":13,"sentence":"However, their performance is lower than that of supervised learning on manually annotated data because mislabeled data that cannot be fully suppressed becomes noise when training the model.","offset":5,"pro":0.625,"labels":"GAP"},{"idx":13,"sentence":"To overcome this issue, our methods indirectly utilize distant supervision data with manually annotated training data","offset":6,"pro":0.75,"labels":"MTD"},{"idx":13,"sentence":"The experimental results on the DrugProt corpus in the BioCreative VII Track 1 showed that our proposed model can consistently improve the supervised models in different settings","offset":7,"pro":0.875,"labels":"RST"},{"idx":14,"sentence":"Cancer immunology research involves several important cell and protein factors","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"Extracting the information of such cells and proteins and the interactions between them from text are crucial in text mining for cancer immunology research","offset":1,"pro":0.125,"labels":"BAC"},{"idx":14,"sentence":"However, there are few available datasets for these entities, and the amount of annotated documents is not sufficient compared with other major named entity types","offset":2,"pro":0.25,"labels":"GAP"},{"idx":14,"sentence":"In this work, we introduce our automatically annotated dataset of key named entities, i.e., T-cells, cytokines, and transcription factors, which engages the recent cancer immunotherapy","offset":3,"pro":0.375,"labels":"PUR"},{"idx":14,"sentence":"The entities are annotated based on the UniProtKB knowledge base using dictionary matching","offset":4,"pro":0.5,"labels":"MTD"},{"idx":14,"sentence":"We build a neural named entity recognition (NER) model to be trained on this dataset and evaluate it on a manually-annotated data","offset":5,"pro":0.625,"labels":"MTD"},{"idx":14,"sentence":"Experimental results show that we can achieve a promising NER performance even though our data is automatically annotated","offset":6,"pro":0.75,"labels":"RST"},{"idx":14,"sentence":"Our dataset also enhances the NER performance when combined with existing data, especially gaining improvement in yet investigated named entities such as cytokines and transcription factors","offset":7,"pro":0.875,"labels":"RST"},{"idx":15,"sentence":"We present a deep learning based information extraction system that can extract the design and results of a published abstract describing a Randomized Controlled Trial (RCT)","offset":0,"pro":0,"labels":"PUR"},{"idx":15,"sentence":" In contrast to other approaches, our system does not regard the PICO elements as flat objects or labels but as structured objects","offset":1,"pro":0.14285714285714285,"labels":"MTD"},{"idx":15,"sentence":"We thus model the task as the one of filling a set of templates and slots","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":15,"sentence":"our two-step approach recognizes relevant slot candidates as a first step and assigns them to a corresponding template as second step, relying on a learned pairwise scoring function that models the compatibility of the different slot values","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":15,"sentence":"We evaluate the approach on a dataset of 211 manually annotated abstracts for type 2 Diabetes and Glaucoma","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":15,"sentence":"showing the positive impact of modelling intra-template entity compatibility","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":15,"sentence":"As main benefit, our approach yields a structured object for every RCT abstract that supports the aggregation and summarization of clinical trial results across published studies and can facilitate the task of creating a systematic review or meta-analysis.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":16,"sentence":"This work presents the first large-scale biomedical Spanish language models trained from scratch, using large biomedical corpora consisting of a total of 1.1B tokens and an EHR corpus of 95M tokens","offset":0,"pro":0,"labels":"PUR"},{"idx":16,"sentence":"We compared them against general-domain and other domain-specific models for Spanish on three clinical NER tasks","offset":1,"pro":0.2,"labels":"MTD"},{"idx":16,"sentence":"As main results, our models are superior across the NER tasks, rendering them more convenient for clinical NLP applications","offset":2,"pro":0.4,"labels":"RST"},{"idx":16,"sentence":"Furthermore, our findings indicate that when enough data is available, pre-training from scratch is better than continual pre-training when tested on clinical tasks, raising an exciting research question about which approach is optimal","offset":3,"pro":0.6,"labels":"RST"},{"idx":16,"sentence":"Our models and fine-tuning scripts are publicly available at HuggingFace and GitHub.","offset":4,"pro":0.8,"labels":"CTN"},{"idx":17,"sentence":"Despite the advances in digital healthcare systems offering curated structured knowledge, much of the critical information still lies in large volumes of unlabeled and unstructured clinical texts","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"These texts, which often contain protected health information (PHI), are exposed to information extraction tools for downstream applications, risking patient identification","offset":1,"pro":0.125,"labels":"BAC"},{"idx":17,"sentence":"Existing works in de-identification rely on using large-scale annotated corpora in English, which often are not suitable in real-world multilingual settings","offset":2,"pro":0.25,"labels":"BAC"},{"idx":17,"sentence":"Pre-trained language models (LM) have shown great potential for cross-lingual transfer in low-resource settings","offset":3,"pro":0.375,"labels":"BAC"},{"idx":17,"sentence":"In this work, we empirically show the few-shot cross-lingual transfer property of LMs for named entity recognition (NER) and apply it to solve a low-resource and real-world challenge of code-mixed (Spanish-Catalan) clinical notes de-identification in the stroke domain.","offset":4,"pro":0.5,"labels":"PUR"},{"idx":17,"sentence":"We annotate a gold evaluation dataset to assess few-shot setting performance where we only use a few hundred labeled examples for training","offset":5,"pro":0.625,"labels":"MTD"},{"idx":17,"sentence":"Our model improves the zero-shot F1-score from 73.7% to 91.2% on the gold evaluation set when adapting Multilingual BERT (mBERT) (CITATION) from the MEDDOCAN (CITATION) corpus with our few-shot cross-lingual target corpus","offset":6,"pro":0.75,"labels":"RST"},{"idx":17,"sentence":"When generalized to an out-of-sample test set, the best model achieves a human-evaluation F1-score of 97.2%.","offset":7,"pro":0.875,"labels":"RST"},{"idx":18,"sentence":"This paper introduces the approach of VPAI{_Lab team's experiments on BioNLP 2022 shared task 1 Medical Video Classification (MedVidCL)","offset":0,"pro":0,"labels":"PUR"},{"idx":18,"sentence":"Given an input video, the MedVidCL task aims to correctly classify it into one of three following categories: Medical Instructional, Medical Non-instructional, and Non-medical","offset":1,"pro":0.1111111111111111,"labels":"MTD"},{"idx":18,"sentence":"Inspired by its dataset construction process, we divide the classification process into two stages.","offset":2,"pro":0.2222222222222222,"labels":"MTD"},{"idx":18,"sentence":"The first stage is to classify videos into medical videos and non-medical videos. In the second stage, for those samples classified as medical videos, we further classify them into instructional videos and non-instructional videos","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":18,"sentence":"In addition, we also propose the cross-modal fusion method to solve the video classification, such as fusing the text features (question and subtitles) from the pre-training language models and visual features from image frames","offset":4,"pro":0.4444444444444444,"labels":"PUR"},{"idx":18,"sentence":"Specifically, we use textual information to concatenate and query the visual information for obtaining better feature representation","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":18,"sentence":"Extensive experiments show that the proposed method significantly outperforms the official baseline method by 15.4% in the F1 score, which shows its effectiveness","offset":6,"pro":0.6666666666666666,"labels":"RST"},{"idx":18,"sentence":"Finally, the online results show that our method ranks the Top-1 on the online unseen test set","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":18,"sentence":"All the experimental codes are open-sourced at https://github.com/Lireanstar/MedVidCL.","offset":8,"pro":0.8888888888888888,"labels":"CTN"},{"idx":19,"sentence":"Text summarization (TS) is an important NLP task. Pre-trained Language Models (PLMs) have been used to improve the performance of TS","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"However, PLMs are limited by their need of labelled training data and by their attention mechanism, which often makes them unsuitable for use on long documents","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":19,"sentence":"To this end, we propose a hybrid, unsupervised, abstractive-extractive approach, in which we walk through a document, generating salient textual fragments representing its key points.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":19,"sentence":"We then select the most important sentences of the document by choosing the most similar sentences to the generated texts, calculated using BERTScore","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":19,"sentence":"We evaluate the efficacy of generating and using salient textual fragments to guide extractive summarization on documents from the biomedical and general scientific domains. ","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":19,"sentence":"We compare the performance between long and short documents using different generative text models, which are finetuned to generate relevant queries or document titles","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":19,"sentence":"We show that our hybrid approach out-performs existing unsupervised methods, as well as state-of-the-art supervised methods, despite not needing a vast amount of labelled training data.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":0,"sentence":"Research papers reflect scientific advances","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"Citations are widely used in research publications to support the new findings and show their benefits, while also regulating the information flow to make the contents clearer for the audience","offset":1,"pro":0.125,"labels":"BAC"},{"idx":0,"sentence":"A citation in a research article refers to the information's source, but not the specific text span from that source article. In biomedical research articles, this task is challenging as the same chemical or biological component can be represented in multiple ways in different papers from various domains","offset":2,"pro":0.25,"labels":"GAP"},{"idx":0,"sentence":"This paper suggests a mechanism for linking citing sentences in a publication with cited sentences in referenced sources.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":0,"sentence":"The framework presented here pairs the citing sentence with all of the sentences in the reference text, and then tries to retrieve the semantically equivalent pairs.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":0,"sentence":"These semantically related sentences from the reference paper are chosen as the cited statements. ","offset":5,"pro":0.625,"labels":"MTD"},{"idx":0,"sentence":"This effort involves designing a citation linkage framework utilizing sequential and tree-structured siamese deep learning models.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":0,"sentence":"This paper also provides a method to create a synthetic corpus for such a task.","offset":7,"pro":0.875,"labels":"MTD"},{"idx":1,"sentence":"Recognizing causal precedence relations among the chemical interactions in biomedical literature is crucial to understanding the underlying biological mechanisms. ","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"However, detecting such causal relation can be hard because: (1) many times, such causal relations among events are not explicitly expressed by certain phrases but implicitly implied by very diverse expressions in the text, and (2) annotating such causal relation detection datasets requires considerable expert knowledge and effort.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":1,"sentence":"In this paper, we propose a strategy to address both challenges by training neural models with in-domain pre-training and knowledge distillation. ","offset":2,"pro":0.4,"labels":"PUR"},{"idx":1,"sentence":"We show that, by using very limited amount of labeled data, and sufficient amount of unlabeled data, ","offset":3,"pro":0.6,"labels":"MTD"},{"idx":1,"sentence":"the neural models outperform previous baselines on the causal precedence detection task, and are ten times faster at inference compared to the BERT base model.","offset":4,"pro":0.8,"labels":"RST"},{"idx":2,"sentence":"In this paper, we present an overview of the MedVidQA 2022 shared task, collocated with the 21st BioNLP workshop at ACL 2022.","offset":0,"pro":0,"labels":"PUR"},{"idx":2,"sentence":"The shared task addressed two of the challenges faced by medical video question answering: (I) a video classification task that explores new approaches to medical video understanding (labeling), and (ii) a visual answer localization task.","offset":1,"pro":0.14285714285714285,"labels":"CTN"},{"idx":2,"sentence":"Visual answer localization refers to the identification of the relevant temporal segments (start and end timestamps) in the video where the answer to the medical question is being shown or illustrated.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":2,"sentence":"A total of thirteen teams participated in the shared task challenges, with eleven system descriptions submitted to the workshop.","offset":3,"pro":0.42857142857142855,"labels":"RST"},{"idx":2,"sentence":"The descriptions present monomodal and multi-modal approaches developed for medical video classification and visual answer localization.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":2,"sentence":"This paper describes the tasks, the datasets, evaluation metrics, and baseline systems for both tasks. ","offset":5,"pro":0.7142857142857143,"labels":"PUR"},{"idx":2,"sentence":"Finally, the paper summarizes the techniques and results of the evaluation of the various approaches explored by the participating teams.","offset":6,"pro":0.8571428571428571,"labels":"PUR"},{"idx":3,"sentence":"It is commonly claimed that inter-annotator agreement (IAA) is the ceiling of machine learning (ML) performance, i.e., that the agreement between an ML system's predictions and an annotator can not be higher than the agreement between two annotators","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"Although Boguslav {& Cohen (2017) showed that this claim is falsified by many real-world ML systems, the claim has persisted.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":3,"sentence":"As a complement to this real-world evidence, we conducted a comprehensive set of simulations,","offset":2,"pro":0.4,"labels":"PUR"},{"idx":3,"sentence":"and show that an ML model can beat IAA even if (and especially if) annotators are noisy and differ in their underlying classification functions, as long as the ML model is reasonably well-specified.","offset":3,"pro":0.6,"labels":"RST"},{"idx":3,"sentence":"Although the latter condition has long been elusive, leading ML models to underperform IAA, we anticipate that this condition will be increasingly met in the era of big data and deep learning.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":4,"sentence":"Conversational bots have become non-traditional methods for therapy among individuals suffering from psychological illnesses","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"Leveraging deep neural generative language models, we propose a deep trainable neural conversational model for therapy-oriented response generation.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":4,"sentence":"We leverage transfer learning methods during training on therapy and counseling based data from Reddit and AlexanderStreet","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":4,"sentence":"This was done to adapt existing generative models - GPT2 and DialoGPT - to the task of automated dialog generation.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":4,"sentence":"Through quantitative evaluation of the linguistic quality, ","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":4,"sentence":"we observe that the dialog generation model - DialoGPT (345M) with transfer learning on video data attains scores similar to a human response baseline.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":4,"sentence":"However, human evaluation of responses by conversational bots show mostly signs of generic advice or information sharing instead of therapeutic interaction.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":5,"sentence":"Automatic extraction of event structures from text is a promising way to extract important facts from the evergrowing amount of biomedical literature","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"We propose BEEDS, a new approach on how to mine event structures from PubMed based on a question-answering paradigm","offset":1,"pro":0.2,"labels":"PUR"},{"idx":5,"sentence":"Using a three-step pipeline comprising a document retriever, a document reader, and an entity normalizer, BEEDS is able to fully automatically extract event triples involving a query protein or gene and to store this information directly in a knowledge base","offset":2,"pro":0.4,"labels":"MTD"},{"idx":5,"sentence":"BEEDS applies a transformer-based architecture for event extraction and uses distant supervision to augment the scarce training data in event mining","offset":3,"pro":0.6,"labels":"MTD"},{"idx":5,"sentence":"In a knowledge base population setting, it outperforms a strong baseline in finding post-translational modification events consisting of enzyme-substrate-site triples while achieving competitive results in extracting binary relations consisting of protein-protein and protein-site interactions.","offset":4,"pro":0.8,"labels":"RST"},{"idx":6,"sentence":"We study the problem of entity detection and normalization applied to patient self-reports of symptoms that arise as side-effects of vaccines","offset":0,"pro":0,"labels":"PUR"},{"idx":6,"sentence":"Our application domain presents unique challenges that render traditional classification methods ineffective: the number of entity types is large; and many symptoms are rare, resulting in a long-tail distribution of training examples per entity type","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":6,"sentence":"We tackle these challenges with an autoregressive model that generates standardized names of symptoms","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":6,"sentence":"We introduce a data augmentation technique to increase the number of training examples for rare symptoms","offset":3,"pro":0.5,"labels":"MTD"},{"idx":6,"sentence":"Experiments on real-life patient vaccine symptom self-reports show that our approach outperforms strong baselines","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":6,"sentence":"and that additional examples improve performance on the long-tail entities","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":7,"sentence":"Recognition of named entities present in text is an important step towards information extraction and natural language understanding","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"This work presents a named entity recognition system for the Romanian biomedical domain.","offset":1,"pro":0.25,"labels":"PUR"},{"idx":7,"sentence":"The system makes use of a new and extended version of SiMoNERo corpus, that is open sourced","offset":2,"pro":0.5,"labels":"MTD"},{"idx":7,"sentence":"Also, the best system is available for direct usage in the RELATE platform","offset":3,"pro":0.75,"labels":"MTD"},{"idx":8,"sentence":"Recognizing biomedical entities in the text has significance in biomedical and health science research, as it benefits myriad downstream tasks, including entity linking, relation extraction, or entity resolution","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"While English and a few other widely used languages enjoy ample resources for automatic biomedical entity recognition, it is not the case for Bangla, a low-resource language","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":8,"sentence":"On that account, in this paper, we introduce BanglaBioMed, a Bangla biomedical named entity (NE) annotated dataset in standard IOB format, the first of its kind, consisting of over 12000 tokens annotated with the biomedical entities","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":8,"sentence":"The corpus is created by collecting Bangla text from a list of health articles and then annotated with four distinct types of entities: Anatomy (AN), Chemical and Drugs (CD), Disease and Symptom (DS), and Medical Procedure (MP)","offset":3,"pro":0.5,"labels":"MTD"},{"idx":8,"sentence":"We provide the details of the entire data collection and annotation procedure and illustrate various statistics of the created corpus","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":8,"sentence":"Our developed corpus is a much-needed addition to the Bangla NLP resource that will facilitate biomedical NLP research in Bangla","offset":5,"pro":0.8333333333333334,"labels":"CTN"},{"idx":9,"sentence":"The International Classification of Diseases (ICD) system is the international standard for classifying diseases and procedures during a healthcare encounter and is widely used for healthcare reporting and management purposes","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"Assigning correct codes for clinical procedures is important for clinical, operational and financial decision-making in healthcare","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":9,"sentence":"Contextual word embedding models have achieved state-of-the-art results in multiple NLP tasks","offset":2,"pro":0.3333333333333333,"labels":"BAC"},{"idx":9,"sentence":"However, these models have yet to achieve state-of-the-art results in the ICD classification task since one of their main disadvantages is that they can only process documents that contain a small number of tokens which is rarely the case with real patient notes","offset":3,"pro":0.5,"labels":"GAP"},{"idx":9,"sentence":"In this paper, we introduce ICDBigBird a BigBird-based model which can integrate a Graph Convolutional Network (GCN), that takes advantage of the relations between ICD codes in order to create enriched' representations of their embeddings, with a BigBird contextual model that can process larger documents","offset":4,"pro":0.6666666666666666,"labels":"PUR"},{"idx":9,"sentence":"Our experiments on a real-world clinical dataset demonstrate the effectiveness of our BigBird-based model on the ICD classification task as it outperforms the previous state-of-the-art models","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":10,"sentence":"We introduce Doctor XAvIer - a BERT-based diagnostic system that extracts relevant clinical data from transcribed patient-doctor dialogues and explains predictions using feature attribution methods","offset":0,"pro":0,"labels":"PUR"},{"idx":10,"sentence":"We present a novel performance plot and evaluation metric for feature attribution methods - Feature Attribution Dropping (FAD) curve and its Normalized Area Under the Curve (N-AUC)","offset":1,"pro":0.25,"labels":"MTD"},{"idx":10,"sentence":"FAD curve analysis shows that integrated gradients outperforms Shapley values in explaining diagnosis classification","offset":2,"pro":0.5,"labels":"RST"},{"idx":10,"sentence":"Doctor XAvIer outperforms the baseline with 0.97 F1-score in named entity recognition and symptom pertinence classification and 0.91 F1-score in diagnosis classification","offset":3,"pro":0.75,"labels":"RST"},{"idx":11,"sentence":"PICO recognition is an information extraction task for identifying participant, intervention, comparator, and outcome information from clinical literature","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"Manually identifying PICO information is the most time-consuming step for conducting systematic reviews (SR), which is already labor-intensive","offset":1,"pro":0.125,"labels":"BAC"},{"idx":11,"sentence":"A lack of diversified and large, annotated corpora restricts innovation and adoption of automated PICO recognition systems","offset":2,"pro":0.25,"labels":"BAC"},{"idx":11,"sentence":"The largest-available PICO entity/span corpus is manually annotated which is too expensive for a majority of the scientific community","offset":3,"pro":0.375,"labels":"GAP"},{"idx":11,"sentence":"To break through the bottleneck, we propose DISTANT-CTO, a novel distantly supervised PICO entity extraction approach using the clinical trials literature, to generate a massive weakly-labeled dataset with more than a million Intervention' and Comparator' entity annotations","offset":4,"pro":0.5,"labels":"PUR"},{"idx":11,"sentence":"We train distant NER (named-entity recognition) models using this weakly-labeled dataset and demonstrate that it outperforms even the sophisticated models trained on the manually annotated dataset with a 2% F1 improvement over the Intervention entity of the PICO benchmark and more than 5% improvement when combined with the manually annotated dataset","offset":5,"pro":0.625,"labels":"MTD"},{"idx":11,"sentence":"We investigate the generalizability of our approach and gain an impressive F1 score on another domain-specific PICO benchmark","offset":6,"pro":0.75,"labels":"RST"},{"idx":11,"sentence":"The approach is not only zero-cost but is also scalable for a constant stream of PICO entity annotations","offset":7,"pro":0.875,"labels":"CLN"},{"idx":12,"sentence":"Generating a summary from findings has been recently explored (Zhang et al., 2018, 2020) in note types such as radiology reports that typically have short length","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"In this work, we focus on echocardiogram notes that is longer and more complex compared to previous note types","offset":1,"pro":0.1,"labels":"PUR"},{"idx":12,"sentence":"We formally define the task of echocardiography conclusion generation (EchoGen) as generating a conclusion given the findings section, with emphasis on key cardiac findings","offset":2,"pro":0.2,"labels":"MTD"},{"idx":12,"sentence":"To promote the development of EchoGen methods","offset":3,"pro":0.3,"labels":"PUR"},{"idx":12,"sentence":"we present a new benchmark, which consists of two datasets collected from two hospitals","offset":4,"pro":0.4,"labels":"MTD"},{"idx":12,"sentence":"We further compare both standard and start-of-the-art methods on this new benchmark, with an emphasis on factual consistency","offset":5,"pro":0.5,"labels":"MTD"},{"idx":12,"sentence":"To accomplish this","offset":6,"pro":0.6,"labels":"PUR"},{"idx":12,"sentence":"we develop a tool to automatically extract concept-attribute tuples from the sentence","offset":7,"pro":0.7,"labels":"MTD"},{"idx":12,"sentence":"We then propose an evaluation metric, FactComp, to compare concept-attribute tuples between the human reference and generated conclusions","offset":8,"pro":0.8,"labels":"MTD"},{"idx":12,"sentence":"Both automatic and human evaluations show that there is still a significant gap between human-written and machine-generated conclusions on echo reports in terms of factuality and overall quality","offset":9,"pro":0.9,"labels":"CLN"},{"idx":13,"sentence":"A wealth of important clinical information lies untouched in the Electronic Health Record, often in the form of unstructured textual documents","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"For patients with Epilepsy, such information includes outcome measures like Seizure Frequency and Dates of Last Seizure, key parameters that guide all therapy for these patients","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":13,"sentence":"Transformer models have been able to extract such outcome measures from unstructured clinical note text as sentences with human-like accuracy; however, these sentences are not yet usable in a quantitative analysis for large-scale studies","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":13,"sentence":"In this study, we developed a pipeline to quantify these outcome measures","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":13,"sentence":"We used text summarization models to convert unstructured sentences into specific formats, and then employed rules-based quantifiers to calculate seizure frequencies and dates of last seizure","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":13,"sentence":"We demonstrated that our pipeline of models does not excessively propagate errors and we analyzed its mistakes","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":13,"sentence":"We anticipate that our methods can be generalized outside of epilepsy to other disorders to drive large-scale clinical research","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":14,"sentence":"Biomedical relation extraction, aiming to automatically discover high-quality and semantic relations between the entities from free text, is becoming a vital step for automated knowledge discovery","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"Pretrained language models have achieved impressive performance on various natural language processing tasks, including relation extraction","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":14,"sentence":"In this paper, we perform extensive empirical comparisons of encoder-only transformers with the encoder-decoder transformer, specifically T5, on ten public biomedical relation extraction datasets","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":14,"sentence":"We study the relation extraction task from four major biomedical tasks, namely chemical-protein relation extraction, disease-protein relation extraction, drug-drug interaction, and protein-protein interaction","offset":3,"pro":0.5,"labels":"MTD"},{"idx":14,"sentence":"We also explore the use of multi-task fine-tuning to investigate the correlation among major biomedical relation extraction tasks","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":14,"sentence":"We report performance (micro F-score) using T5, BioBERT and PubMedBERT, demonstrating that T5 and multi-task learning can improve the performance of the biomedical relation extraction task","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":15,"sentence":"Electronic health records contain valuable information about symptoms, diagnosis, treatment and outcomes of the treatments of individual patients","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"However, the records may also contain information that can reveal the identity of the patients","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":15,"sentence":"Removing these identifiers - the Protected Health Information (PHI) - can protect the identity of the patient","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":15,"sentence":"Automatic de-identification is a process which employs machine learning techniques to detect and remove PHI","offset":3,"pro":0.42857142857142855,"labels":"BAC"},{"idx":15,"sentence":"However, automatic techniques are imperfect in their precision and introduce noise into the data","offset":4,"pro":0.5714285714285714,"labels":"PUR"},{"idx":15,"sentence":"This study examines the impact of this noise on the utility of Swedish de-identified clinical data by using human evaluators and by training and testing BERT models","offset":5,"pro":0.7142857142857143,"labels":"PUR"},{"idx":15,"sentence":"Our results indicate that de-identification does not harm the utility for clinical NLP and that human evaluators are less sensitive to noise from de-identification than expected","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":16,"sentence":"Medical document coding is the process of assigning labels from a structured label space (ontology - e.g., ICD-9) to medical documents","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"This process is laborious, costly, and error-prone","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":16,"sentence":"In recent years, efforts have been made to automate this process with neural models","offset":2,"pro":0.2222222222222222,"labels":"BAC"},{"idx":16,"sentence":"The label spaces are large (in the order of thousands of labels) and follow a big-head long-tail label distribution, giving rise to few-shot and zero-shot scenarios","offset":3,"pro":0.3333333333333333,"labels":"BAC"},{"idx":16,"sentence":"Previous efforts tried to address these scenarios within the model, leading to improvements on rare labels, but worse results on frequent ones","offset":4,"pro":0.4444444444444444,"labels":"GAP"},{"idx":16,"sentence":"We propose data augmentation and synthesis techniques in order to address these scenarios","offset":5,"pro":0.5555555555555556,"labels":"PUR"},{"idx":16,"sentence":"We further introduce an analysis technique for this setting inspired by confusion matrices","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":16,"sentence":"This analysis technique points to the positive impact of data augmentation and synthesis","offset":7,"pro":0.7777777777777778,"labels":"CLN"},{"idx":16,"sentence":"but also highlights more general issues of confusion within families of codes, and underprediction","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":17,"sentence":"Antibiotic resistance has become a growing worldwide concern as new resistance mechanisms are emerging and spreading globally, and thus detecting and collecting the cause - Antibiotic Resistance Genes (ARGs), have been more critical than ever","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"In this work, we aim to automate the curation of ARGs by extracting ARG-related assertive statements from scientific papers","offset":1,"pro":0.1111111111111111,"labels":"PUR"},{"idx":17,"sentence":"To support the research towards this direction","offset":2,"pro":0.2222222222222222,"labels":"PUR"},{"idx":17,"sentence":"we build SciARG, a new benchmark dataset containing 2,000 manually annotated statements as the evaluation set and 12,516 silver-standard training statements that are automatically created from scientific papers by a set of rules","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":17,"sentence":"To set up the baseline performance on SciARG","offset":4,"pro":0.4444444444444444,"labels":"PUR"},{"idx":17,"sentence":"o the best of our knowledge, we are the first to leverage natural language processing techniques to curate all validated ARGs from scientific papers","offset":5,"pro":0.5555555555555556,"labels":"CTN"},{"idx":17,"sentence":"Both the code and data are publicly available at https://github.com/VT-NLP/SciARG","offset":6,"pro":0.6666666666666666,"labels":"CTN"},{"idx":17,"sentence":"we exploit three state-of-the-art neural architectures based on pre-trained language models and prompt tuning","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":17,"sentence":"and further ensemble them to attain the highest 77.0% F-score","offset":8,"pro":0.8888888888888888,"labels":"RST"},{"idx":18,"sentence":"Machine learning models that offer excellent predictive performance often lack the interpretability necessary to support integrated human machine decision-making","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"In clinical medicine and other high-risk settings, domain experts may be unwilling to trust model predictions without explanations","offset":1,"pro":0.2,"labels":"GAP"},{"idx":18,"sentence":"Work in explainable AI must balance competing objectives along two different axes: 1) Models should ideally be both accurate and simple. 2) Explanations must balance faithfulness to the model's decision-making with their plausibility to a domain expert","offset":2,"pro":0.4,"labels":"GAP"},{"idx":18,"sentence":"We propose to use knowledge distillation, or training a student model that mimics the behavior of a trained teacher model, as a technique to generate faithful and plausible explanations","offset":3,"pro":0.6,"labels":"PUR"},{"idx":18,"sentence":"We evaluate our approach on the task of assigning ICD codes to clinical notes to demonstrate that the student model is faithful to the teacher model's behavior and produces quality natural language explanations","offset":4,"pro":0.8,"labels":"CLN"},{"idx":19,"sentence":"Clinical risk scores enable clinicians to tabulate a set of patient data into simple scores to stratify patients into risk categories","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"Although risk scores are widely used to inform decision-making at the point-of-care, collecting the information necessary to calculate such scores requires considerable time and effort","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":19,"sentence":"Previous studies have focused on specific risk scores and involved manual curation of relevant terms or codes and heuristics for each data element of a risk score","offset":2,"pro":0.3333333333333333,"labels":"BAC"},{"idx":19,"sentence":"To support more generalizable methods for risk score calculation, we annotate 100 patients in MIMIC-III with elements of CHA2DS2-VASc and PERC scores, and explore using question answering (QA) and off-the-shelf tools","offset":3,"pro":0.5,"labels":"PUR"},{"idx":19,"sentence":"We show that QA models can achieve comparable or better performance for certain risk score elements as compared to heuristic-based methods, and demonstrate the potential for more scalable risk score automation without the need for expert-curated heuristics","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":19,"sentence":"Our annotated dataset will be released to the community to encourage efforts in generalizable methods for automating risk scores","offset":5,"pro":0.8333333333333334,"labels":"CTN"},{"idx":0,"sentence":" Images are often more significant than only the pixels to human eyes, as we can infer, associate, and reason with contextual information from other sources to establish a more complete picture","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"For example, in Figure 1, we can find a way to identify the news articles related to the picture through segment-wise understandings of the signs, the buildings, the crowds, and more","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":0,"sentence":"This reasoning could provide the time and place the image was taken, which will help us in subsequent tasks, such as automatic storyline construction, correction of image source in intended effect photographs, and upper-stream processing such as image clustering for certain location or time","offset":2,"pro":0.2857142857142857,"labels":"BAC"},{"idx":0,"sentence":"In this work, we formulate this problem and introduce TARA: a dataset with 16k images with their associated news, time, and location, automatically extracted from New York Times, and an additional 61k examples as distant supervision from WIT.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":0,"sentence":"On top of the extractions, we present a crowdsourced subset in which we believe it is possible to find the images' spatio-temporal information for evaluation purpose.","offset":4,"pro":0.5714285714285714,"labels":"PUR"},{"idx":0,"sentence":"We show that there exists a 70% gap between a state-of-the-art joint model and human performance, which is slightly filled by our proposed model that uses segment-wise reasoning, motivating higher-level vision-language joint models that can conduct open-ended reasoning with world knowledge","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":0,"sentence":"The data and code are publicly available at https://github.com/zeyofu/TARA.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":1,"sentence":"Tables store rich numerical data, but numerical reasoning over tables is still a challenge","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"In this paper, we find that the spreadsheet formula, a commonly used language to perform computations on numerical values in spreadsheets, is a valuable supervision for numerical reasoning in tables","offset":1,"pro":0.14285714285714285,"labels":"RST"},{"idx":1,"sentence":"Considering large amounts of spreadsheets available on the web, we propose FORTAP, the first exploration to leverage spreadsheet formulas for table pretraining","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":1,"sentence":"Two novel self-supervised pretraining objectives are derived from formulas, numerical reference prediction (NRP) and numerical calculation prediction (NCP)","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":1,"sentence":"While our proposed objectives are generic for encoders, to better capture spreadsheet table layouts and structures, FORTAP is built upon TUTA, the first transformer-based method for spreadsheet table pretraining with tree attention","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":1,"sentence":"FORTAP outperforms state-of-the-art methods by large margins on three representative datasets of formula prediction, question answering, and cell type classification","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":1,"sentence":"showing the great potential of leveraging formulas for table pretraining","offset":6,"pro":0.8571428571428571,"labels":"IMP"},{"idx":2,"sentence":"Information integration from different modalities is an active area of research","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"Human beings and, in general, biological neural systems are quite adept at using a multitude of signals from different sensory perceptive fields to interact with the environment and each other","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":2,"sentence":"Recent work in deep fusion models via neural networks has led to substantial improvements over unimodal approaches in areas like speech recognition, emotion recognition and analysis, captioning and image description.","offset":2,"pro":0.3333333333333333,"labels":"BAC"},{"idx":2,"sentence":"However, such research has mostly focused on architectural changes allowing for fusion of different modalities while keeping the model complexity manageable","offset":3,"pro":0.5,"labels":"GAP"},{"idx":2,"sentence":"Inspired by neuroscientific ideas about multisensory integration and processing, we investigate the effect of introducing neural dependencies in the loss functions","offset":4,"pro":0.6666666666666666,"labels":"PUR"},{"idx":2,"sentence":"Experiments on multimodal sentiment analysis tasks with different models show that our approach provides a consistent performance boost.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":3,"sentence":"Procedural Multimodal Documents (PMDs) organize textual instructions and corresponding images step by step","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"Comprehending PMDs and inducing their representations for the downstream reasoning tasks is designated as Procedural MultiModal Machine Comprehension (M3C)","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":3,"sentence":"In this study, we approach Procedural M3C at a fine-grained level (compared with existing explorations at a document or sentence level), that is, entity","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":3,"sentence":"With delicate consideration, we model entity both in its temporal and cross-modal relation and propose a novel Temporal-Modal Entity Graph (TMEG)","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":3,"sentence":"Specifically, graph structure is formulated to capture textual and visual entities and trace their temporal-modal evolution","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":3,"sentence":"In addition, a graph aggregation module is introduced to conduct graph encoding and reasoning","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":3,"sentence":"Comprehensive experiments across three Procedural M3C tasks are conducted on a traditional dataset RecipeQA and our new dataset CraftQA, which can better evaluate the generalization of TMEG","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":4,"sentence":"Pre-trained sequence-to-sequence language models have led to widespread success in many natural language generation tasks","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"However, there has been relatively less work on analyzing their ability to generate structured outputs such as graphs","offset":1,"pro":0.1,"labels":"GAP"},{"idx":4,"sentence":"Unlike natural language, graphs have distinct structural and semantic properties in the context of a downstream NLP task, e.g., generating a graph that is connected and acyclic can be attributed to its structural constraints, while the semantics of a graph can refer to how meaningfully an edge represents the relation between two node concepts","offset":2,"pro":0.2,"labels":"BAC"},{"idx":4,"sentence":"In this work, we study pre-trained language models that generate explanation graphs in an end-to-end manner and analyze their ability to learn the structural constraints and semantics of such graphs","offset":3,"pro":0.3,"labels":"PUR"},{"idx":4,"sentence":"We first show that with limited supervision, pre-trained language models often generate graphs that either violate these constraints or are semantically incoherent","offset":4,"pro":0.4,"labels":"MTD"},{"idx":4,"sentence":"Since curating large amount of human-annotated graphs is expensive and tedious, we propose simple yet effective ways of graph perturbations via node and edge edit operations that lead to structurally and semantically positive and negative graphs","offset":5,"pro":0.5,"labels":"MTD"},{"idx":4,"sentence":"Next, we leverage these graphs in different contrastive learning models with Max-Margin and InfoNCE losses.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":4,"sentence":"Our methods lead to significant improvements in both structural and semantic accuracy of explanation graphs and also generalize to other similar graph generation tasks.","offset":7,"pro":0.7,"labels":"RST"},{"idx":4,"sentence":"and also that automatically generating more such human-like negative graphs can lead to further improvements","offset":8,"pro":0.8,"labels":"IMP"},{"idx":4,"sentence":"Lastly, we show that human errors are the best negatives for contrastive learning","offset":9,"pro":0.9,"labels":"RST"},{"idx":5,"sentence":"Opinion summarization is the task of automatically generating summaries that encapsulate information expressed in multiple user reviews.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"We present Semantic Autoencoder (SemAE) to perform extractive opinion summarization in an unsupervised manner","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":5,"sentence":"SemAE uses dictionary learning to implicitly capture semantic information from the review text and learns a latent representation of each sentence over semantic units","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":5,"sentence":"Our extractive summarization algorithm leverages the representations to identify representative opinions among hundreds of reviews.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":5,"sentence":"SemAE is also able to perform controllable summarization to generate aspect-specific summaries using only a few samples","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":5,"sentence":"We report strong performance on SPACE and AMAZON datasets and perform experiments to investigate the functioning of our model.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":6,"sentence":"Lexical substitution is the task of generating meaningful substitutes for a word in a given textual consentence","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"Contextual word embedding models have achieved state-of-the-art results in the lexical substitution task by relying on contextual information extracted from the replaced word within the sentence","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":6,"sentence":"However, such models do not take into account structured knowledge that exists in external lexical databases","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":6,"sentence":"We introduce LexSubCon, an end-to-end lexical substitution framework based on contextual embedding models that can identify highly-accurate substitute candidates.","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":6,"sentence":"This is achieved by combining contextual information with knowledge from structured lexical resources","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":6,"sentence":"Our approach involves: (i) introducing a novel mix-up embedding strategy to the target word's embedding through linearly interpolating the pair of the target input embedding and the average embedding of its probable synonyms","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":6,"sentence":"(ii) considering the similarity of the sentence-definition embeddings of the target word and its proposed candidates","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":6,"sentence":"and, (iii) calculating the effect of each substitution on the semantics of the sentence through a fine-tuned sentence similarity model","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":6,"sentence":"Our experiments show that LexSubCon outperforms previous state-of-the-art methods by at least 2% over all the official lexical substitution metrics on LS07 and CoInCo benchmark datasets that are widely used for lexical substitution tasks.","offset":8,"pro":0.8888888888888888,"labels":"RST"},{"idx":7,"sentence":"Implicit knowledge, such as common sense, is key to fluid human conversations","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"Current neural response generation (RG) models are trained to generate responses directly, omitting unstated implicit knowledge","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":7,"sentence":"In this paper, we present Think-Before-Speaking (TBS), a generative approach to first externalize implicit commonsense knowledge (think) and use this knowledge to generate responses (speak)","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":7,"sentence":"We argue that externalizing implicit knowledge allows more efficient learning, produces more informative responses, and enables more explainable models","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":7,"sentence":"We analyze different choices to collect knowledge-aligned dialogues, represent implicit knowledge, and transition between knowledge and dialogues.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":7,"sentence":"Empirical results show TBS models outperform end-to-end and knowledge-augmented RG baselines on most automatic metrics and generate more informative, specific, and commonsense-following responses, as evaluated by human annotators","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":7,"sentence":"TBS also generates knowledge that makes sense and is relevant to the dialogue around 85% of the time","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":8,"sentence":"In this work, we propose a flow-adapter architecture for unsupervised NMT.","offset":0,"pro":0,"labels":"PUR"},{"idx":8,"sentence":"It leverages normalizing flows to explicitly model the distributions of sentence-level latent representations, which are subsequently used in conjunction with the attention mechanism for the translation task.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":8,"sentence":"The primary novelties of our model are: (a) capturing language-specific sentence representations separately for each language using normalizing flows and (b) using a simple transformation of these latent representations for translating from one language to another","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":8,"sentence":"This architecture allows for unsupervised training of each language independently","offset":3,"pro":0.5,"labels":"MTD"},{"idx":8,"sentence":"While there is prior work on latent variables for supervised MT, to the best of our knowledge, this is the first work that uses latent variables and normalizing flows for unsupervised MT","offset":4,"pro":0.6666666666666666,"labels":"CTN"},{"idx":8,"sentence":"We obtain competitive results on several unsupervised MT benchmarks","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":9,"sentence":"Sentence compression reduces the length of text by removing non-essential content while preserving important facts and grammaticality.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"Unsupervised objective driven methods for sentence compression can be used to create customized models without the need for ground-truth training data, while allowing flexibility in the objective function(s) that are used for learning and inference","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":9,"sentence":"Recent unsupervised sentence compression approaches use custom objectives to guide discrete search;","offset":2,"pro":0.2857142857142857,"labels":"BAC"},{"idx":9,"sentence":"however, guided search is expensive at inference time","offset":3,"pro":0.42857142857142855,"labels":"BAC"},{"idx":9,"sentence":"In this work, we explore the use of reinforcement learning to train effective sentence compression models that are also fast when generating predictions.","offset":4,"pro":0.5714285714285714,"labels":"PUR"},{"idx":9,"sentence":"In particular, we cast the task as binary sequence labelling and fine-tune a pre-trained transformer using a simple policy gradient approach","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":9,"sentence":"Our approach outperforms other unsupervised models while also being more efficient at inference time","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":10,"sentence":"Machine reading comprehension is a heavily-studied research and test field for evaluating new pre-trained language models (PrLMs) and fine-tuning strategies, and recent studies have enriched the pre-trained language models with syntactic, semantic and other linguistic information to improve the performance of the models. ","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"We use two strategies to fine-tune a pre-trained language model, namely, placing an additional encoder layer after a pre-trained language model to focus on the coreference mentions or constructing a relational graph convolutional network to model the coreference relations","offset":1,"pro":0.3333333333333333,"labels":"MTD"},{"idx":10,"sentence":"We demonstrate that the explicit incorporation of coreference information in the fine-tuning stage performs better than the incorporation of the coreference information in pre-training a language model","offset":2,"pro":0.6666666666666666,"labels":"RST"},{"idx":11,"sentence":"We contribute a new dataset for the task of automated fact checking and an evaluation of state of the art algorithms","offset":0,"pro":0,"labels":"PUR"},{"idx":11,"sentence":"The dataset includes claims (from speeches, interviews, social media and news articles), review articles published by professional fact checkers and premise articles used by those professional fact checkers to support their review and verify the veracity of the claims","offset":1,"pro":0.2,"labels":"MTD"},{"idx":11,"sentence":"An important challenge in the use of premise articles is the identification of relevant passages that will help to infer the veracity of a claim","offset":2,"pro":0.4,"labels":"MTD"},{"idx":11,"sentence":"We show that transferring a dense passage retrieval model trained with review articles improves the retrieval quality of passages in premise articles","offset":3,"pro":0.6,"labels":"RST"},{"idx":11,"sentence":"We report results for the prediction of claim veracity by inference from premise articles.","offset":4,"pro":0.8,"labels":"RST"},{"idx":12,"sentence":"Fast and reliable evaluation metrics are key to R{&D progress","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"While traditional natural language generation metrics are fast, they are not very reliable","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":12,"sentence":"Conversely, new metrics based on large pretrained language models are much more reliable, but require significant computational resources","offset":2,"pro":0.2857142857142857,"labels":"BAC"},{"idx":12,"sentence":"In this paper, we propose FrugalScore, an approach to learn a fixed, low cost version of any expensive NLG metric, while retaining most of its original performance","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":12,"sentence":"Experiments with BERTScore and MoverScore on summarization and translation show that FrugalScore is on par with the original metrics (and sometimes better), while having several orders of magnitude less parameters and running several times faster.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":12,"sentence":"On average over all learned metrics, tasks, and variants, FrugalScore retains 96.8% of the performance, runs 24 times faster, and has 35 times less parameters than the original metrics","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":12,"sentence":"We make our trained metrics publicly available, to benefit the entire NLP community and in particular researchers and practitioners with limited resources","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":13,"sentence":"We propose Composition Sampling, a simple but effective method to generate diverse outputs for conditional generation of higher quality compared to previous stochastic decoding strategies","offset":0,"pro":0,"labels":"PUR"},{"idx":13,"sentence":" It builds on recently proposed plan-based neural generation models (FROST, Narayan et al, 2021) that are trained to first create a composition of the output and then generate by conditioning on it and the input","offset":1,"pro":0.25,"labels":"MTD"},{"idx":13,"sentence":"Our approach avoids text degeneration by first sampling a composition in the form of an entity chain and then using beam search to generate the best possible text grounded to this entity chain","offset":2,"pro":0.5,"labels":"MTD"},{"idx":13,"sentence":"Experiments on summarization (CNN/DailyMail and XSum) and question generation (SQuAD), using existing and newly proposed automaticmetrics together with human-based evaluation, demonstrate that Composition Sampling is currently the best available decoding strategy for generating diverse meaningful outputs","offset":3,"pro":0.75,"labels":"RST"},{"idx":14,"sentence":"Synthesizing QA pairs with a question generator (QG) on the target domain has become a popular approach for domain adaptation of question answering (QA) models","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"Since synthetic questions are often noisy in practice, existing work adapts scores from a pretrained QA (or QG) model as criteria to select high-quality questions","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":14,"sentence":"However, these scores do not directly serve the ultimate goal of improving QA performance on the target domain","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":14,"sentence":"In this paper, we introduce a novel idea of training a question value estimator (QVE) that directly estimates the usefulness of synthetic questions for improving the target-domain QA performance","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":14,"sentence":"By conducting comprehensive experiments","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":14,"sentence":"we show that the synthetic questions selected by QVE can help achieve better target-domain QA performance, in comparison with existing techniques","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":14,"sentence":"We additionally show that by using such questions and only around 15% of the human annotations on the target domain, we can achieve comparable performance to the fully-supervised baselines","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":15,"sentence":"Class-based language models (LMs) have been long devised to address context sparsity in n-gram LMs","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"In this study, we revisit this approach in the context of neural LMs","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":15,"sentence":"We hypothesize that class-based prediction leads to an implicit context aggregation for similar words and thus can improve generalization for rare words","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":15,"sentence":"We map words that have a common WordNet hypernym to the same class and train large neural LMs by gradually annealing from predicting the class to token prediction during training","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":15,"sentence":"Empirically, this curriculum learning strategy consistently improves perplexity over various large, highly-performant state-of-the-art Transformer-based models on two datasets, WikiText-103 and ARXIV","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":15,"sentence":"Our analysis shows that the performance improvement is achieved without sacrificing performance on rare words","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":15,"sentence":"Finally, we document other attempts that failed to yield empirical gains, and discuss future directions for the adoption of class-based LMs on a larger scale","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":16,"sentence":"Easy access, variety of content, and fast widespread interactions are some of the reasons making social media increasingly popular","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"However, this rise has also enabled the propagation of fake news, text published by news sources with an intent to spread misinformation and sway beliefs.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":16,"sentence":"Detecting it is an important and challenging problem to prevent large scale misinformation and maintain a healthy society","offset":2,"pro":0.3333333333333333,"labels":"BAC"},{"idx":16,"sentence":"We view fake news detection as reasoning over the relations between sources, articles they publish, and engaging users on social media in a graph framework","offset":3,"pro":0.5,"labels":"MTD"},{"idx":16,"sentence":"After embedding this information, we formulate inference operators which augment the graph edges by revealing unobserved interactions between its elements, such as similarity between documents' contents and users' engagement patterns","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":16,"sentence":"Our experiments over two challenging fake news detection tasks show that using inference operators leads to a better understanding of the social media framework enabling fake news spread, resulting in improved performance","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":17,"sentence":"Knowledge base (KB) embeddings have been shown to contain gender biases","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"In this paper, we study two questions regarding these biases: how to quantify them, and how to trace their origins in KB?","offset":1,"pro":0.2,"labels":"PUR"},{"idx":17,"sentence":"Specifically, first, we develop two novel bias measures respectively for a group of person entities and an individual person entity. ","offset":2,"pro":0.4,"labels":"MTD"},{"idx":17,"sentence":"Second, we use the influence function to inspect the contribution of each triple in KB to the overall group bias","offset":3,"pro":0.6,"labels":"MTD"},{"idx":17,"sentence":"To exemplify the potential applications of our study, we also present two strategies (by adding and removing KB triples) to mitigate gender biases in KB embeddings","offset":4,"pro":0.8,"labels":"PUR"},{"idx":18,"sentence":"South Asia is home to a plethora of languages, many of which severely lack access to new language technologies","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"This linguistic diversity also results in a research environment conducive to the study of comparative, contact, and historical linguistics-fields which necessitate the gathering of extensive data from many languages","offset":1,"pro":0.2,"labels":"BAC"},{"idx":18,"sentence":"We review recent developments in and at the intersection of South Asian NLP and historical-comparative linguistics, describing our and others' current efforts in this area","offset":2,"pro":0.4,"labels":"MTD"},{"idx":18,"sentence":"We also offer new strategies towards breaking the data barrier","offset":3,"pro":0.6,"labels":"MTD"},{"idx":18,"sentence":"We claim that data scatteredness (rather than scarcity) is the primary obstacle in the development of South Asian language technology, and suggest that the study of language history is uniquely aligned with surmounting this obstacle","offset":4,"pro":0.8,"labels":"PUR"},{"idx":19,"sentence":"Despite recent progress in abstractive summarization, systems still suffer from faithfulness errors","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"While prior work has proposed models that improve faithfulness, it is unclear whether the improvement comes from an increased level of extractiveness of the model outputs as one naive way to improve faithfulness is to make summarization models more extractive","offset":1,"pro":0.125,"labels":"GAP"},{"idx":19,"sentence":"In this work, we present a framework for evaluating the effective faithfulness of summarization systems","offset":2,"pro":0.25,"labels":"PUR"},{"idx":19,"sentence":"by generating a faithfulness-abstractiveness trade-off curve that serves as a control at different operating points on the abstractiveness spectrum","offset":3,"pro":0.375,"labels":"MTD"},{"idx":19,"sentence":"We then show that the Maximum Likelihood Estimation (MLE) baseline as well as recently proposed methods for improving faithfulness, fail to consistently improve over the control at the same level of abstractiveness","offset":4,"pro":0.5,"labels":"RST"},{"idx":19,"sentence":"Finally, we learn a selector to identify the most faithful and abstractive summary for a given document","offset":5,"pro":0.625,"labels":"MTD"},{"idx":19,"sentence":"and show that this system can attain higher faithfulness scores in human evaluations while being more abstractive than the baseline system on two datasets","offset":6,"pro":0.75,"labels":"RST"},{"idx":19,"sentence":"Moreover, we show that our system is able to achieve a better faithfulness-abstractiveness trade-off than the control at the same level of abstractiveness","offset":7,"pro":0.875,"labels":"RST"},{"idx":0,"sentence":"Pre-trained language models have shown stellar performance in various downstream tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"But, this usually comes at the cost of high latency and computation, hindering their usage in resource-limited settings.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":0,"sentence":"In this work, we propose a novel approach for reducing the computational cost of BERT with minimal loss in downstream performance.","offset":2,"pro":0.2222222222222222,"labels":"PUR"},{"idx":0,"sentence":"Our method dynamically eliminates less contributing tokens through layers, resulting in shorter lengths and consequently lower computational cost.","offset":3,"pro":0.3333333333333333,"labels":"RST"},{"idx":0,"sentence":"To determine the importance of each token representation, we train a Contribution Predictor for each layer using a gradient-based saliency method.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":0,"sentence":"Our experiments on several diverse classification tasks show speedups up to 22x during inference time without much sacrifice in performance.","offset":5,"pro":0.5555555555555556,"labels":"RST"},{"idx":0,"sentence":"We also validate the quality of the selected tokens in our method using human annotations in the ERASER benchmark.","offset":6,"pro":0.6666666666666666,"labels":"CLN"},{"idx":0,"sentence":"In comparison to other widely used strategies for selecting important tokens, such as saliency and attention, our proposed method has a significantly lower false positive rate in generating rationales.","offset":7,"pro":0.7777777777777778,"labels":"CLN"},{"idx":0,"sentence":"Our code is freely available at https://github.com/amodaresi/AdapLeR.","offset":8,"pro":0.8888888888888888,"labels":"CTN"},{"idx":1,"sentence":"This paper describes and tests a method for carrying out quantified reproducibility assessment (QRA) that is based on concepts and definitions from metrology.","offset":0,"pro":0,"labels":"PUR"},{"idx":1,"sentence":"QRA produces a single score estimating the degree of reproducibility of a given system and evaluation measure, on the basis of the scores from, and differences between, different reproductions.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":1,"sentence":"We test QRA on 18 different system and evaluation measure combinations (involving diverse NLP tasks and types of evaluation), for each of which we have the original results and one to seven reproduction results.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":1,"sentence":"The proposed QRA method produces degree-of-reproducibility scores that are comparable across multiple reproductions not only of the same, but also of different, original studies.","offset":3,"pro":0.6,"labels":"RST"},{"idx":1,"sentence":"We find that the proposed method facilitates insights into causes of variation between reproductions, and as a result, allows conclusions to be drawn about what aspects of system and/or evaluation design need to be changed in order to improve reproducibility.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":2,"sentence":"Recent studies have determined that the learned token embeddings of large-scale neural language models are degenerated to be anisotropic with a narrow-cone shape.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"This phenomenon, called the representation degeneration problem, facilitates an increase in the overall similarity between token embeddings that negatively affect the performance of the models.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":2,"sentence":"Although the existing methods that address the degeneration problem based on observations of the phenomenon triggered by the problem improves the performance of the text generation, the training dynamics of token embeddings behind the degeneration problem are still not explored.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":2,"sentence":"In this study, we analyze the training dynamics of the token embeddings focusing on rare token embedding.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":2,"sentence":"We demonstrate that the specific part of the gradient for rare token embeddings is the key cause of the degeneration problem for all tokens during training stage.","offset":4,"pro":0.5,"labels":"PUR"},{"idx":2,"sentence":"Based on the analysis, we propose a novel method called, adaptive gradient gating(AGG).","offset":5,"pro":0.625,"labels":"MTD"},{"idx":2,"sentence":"AGG addresses the degeneration problem by gating the specific part of the gradient for rare token embeddings.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":2,"sentence":"Experimental results from language modeling, word similarity, and machine translation tasks quantitatively and qualitatively verify the effectiveness of AGG.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":3,"sentence":"Large Pre-trained Language Models (PLMs) have become ubiquitous in the development of language understanding technology and lie at the heart of many artificial intelligence advances.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"While advances reported for English using PLMs are unprecedented, reported advances using PLMs for Hebrew are few and far between.","offset":1,"pro":0.09090909090909091,"labels":"GAP"},{"idx":3,"sentence":"The problem is twofold.","offset":2,"pro":0.18181818181818182,"labels":"GAP"},{"idx":3,"sentence":"First, so far, Hebrew resources for training large language models are not of the same magnitude as their English counterparts.","offset":3,"pro":0.2727272727272727,"labels":"GAP"},{"idx":3,"sentence":"Second, most benchmarks available to evaluate progress in Hebrew NLP require morphological boundaries which are not available in the output of standard PLMs.","offset":4,"pro":0.36363636363636365,"labels":"GAP"},{"idx":3,"sentence":"In this work we remedy both aspects.","offset":5,"pro":0.45454545454545453,"labels":"PUR"},{"idx":3,"sentence":"We present AlephBERT, a large PLM for Modern Hebrew, trained on larger vocabulary and a larger dataset than any Hebrew PLM before.","offset":6,"pro":0.5454545454545454,"labels":"MTD"},{"idx":3,"sentence":"Moreover, we introduce a novel neural architecture that recovers the morphological segments encoded in contextualized embedding vectors.","offset":7,"pro":0.6363636363636364,"labels":"MTD"},{"idx":3,"sentence":"Based on this new morphological component we offer an evaluation suite consisting of multiple tasks and benchmarks that cover sentence-level, word-level and sub-word level analyses.","offset":8,"pro":0.7272727272727273,"labels":"MTD"},{"idx":3,"sentence":"On all tasks, AlephBERT obtains state-of-the-art results beyond contemporary Hebrew baselines.","offset":9,"pro":0.8181818181818182,"labels":"RST"},{"idx":3,"sentence":"We make our AlephBERT model, the morphological extraction model, and the Hebrew evaluation suite publicly available, for evaluating future Hebrew PLMs.","offset":10,"pro":0.9090909090909091,"labels":"CTN"},{"idx":4,"sentence":"Neural discrete reasoning (NDR) has shown remarkable progress in combining deep models with discrete reasoning.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"However, we find that existing NDR solution suffers from large performance drop on hypothetical questions, e.g. what the annualized rate of return would be if the revenue in 2020 was doubled.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":4,"sentence":"The key to hypothetical question answering (HQA) is counterfactual thinking, which is a natural ability of human reasoning but difficult for deep models.","offset":2,"pro":0.2857142857142857,"labels":"BAC"},{"idx":4,"sentence":"In this work, we devise a Learning to Imagine (L2I) module, which can be seamlessly incorporated into NDR models to perform the imagination of unseen counterfactual.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":4,"sentence":"In particular, we formulate counterfactual thinking into two steps: 1) identifying the fact to intervene, and 2) deriving the counterfactual from the fact and assumption, which are designed as neural networks.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":4,"sentence":"Based on TAT-QA, we construct a very challenging HQA dataset with 8,283 hypothetical questions.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":4,"sentence":"We apply the proposed L2I to TAGOP, the state-of-the-art solution on TAT-QA, validating the rationality and effectiveness of our approach.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":5,"sentence":"Complex word identification (CWI) is a cornerstone process towards proper text simplification.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"CWI is highly dependent on context, whereas its difficulty is augmented by the scarcity of available datasets which vary greatly in terms of domains and languages.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":5,"sentence":"As such, it becomes increasingly more difficult to develop a robust model that generalizes across a wide array of input examples.","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":5,"sentence":"In this paper, we propose a novel training technique for the CWI task based on domain adaptation to improve the target character and context representations.","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":5,"sentence":"This technique addresses the problem of working with multiple domains, inasmuch as it creates a way of smoothing the differences between the explored datasets.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":5,"sentence":"Moreover, we also propose a similar auxiliary task, namely text simplification, that can be used to complement lexical complexity prediction.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":5,"sentence":"Our model obtains a boost of up to 2.42% in terms of Pearson Correlation Coefficients in contrast to vanilla training techniques, when considering the CompLex from the Lexical Complexity Prediction 2021 dataset.","offset":6,"pro":0.6666666666666666,"labels":"RST"},{"idx":5,"sentence":"At the same time, we obtain an increase of 3% in Pearson scores, while considering a cross-lingual setup relying on the Complex Word Identification 2018 dataset.","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":5,"sentence":"In addition, our model yields state-of-the-art results in terms of Mean Absolute Error.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":6,"sentence":"Zero-shot stance detection (ZSSD) aims to detect the stance for an unseen target during the inference stage.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"In this paper, we propose a joint contrastive learning (JointCL) framework, which consists of stance contrastive learning and target-aware prototypical graph contrastive learning.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":6,"sentence":"Specifically, a stance contrastive learning strategy is employed to better generalize stance features for unseen targets.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":6,"sentence":"Further, we build a prototypical graph for each instance to learn the target-based representation, in which the prototypes are deployed as a bridge to share the graph structures between the known targets and the unseen ones.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":6,"sentence":"Then a novel target-aware prototypical graph contrastive learning strategy is devised to generalize the reasoning ability of target-based stance representations to the unseen targets.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":6,"sentence":"Extensive experiments on three benchmark datasets show that the proposed approach achieves state-of-the-art performance in the ZSSD task.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":7,"sentence":"CASPI includes a mechanism to learn fine-grained reward that captures intention behind human response and also offers guarantee on dialogue policy's performance against a baseline.","offset":0,"pro":0,"labels":"MTD"},{"idx":7,"sentence":"We demonstrate the effectiveness of this framework on end-to-end dialogue task of the Multiwoz2.0 dataset.","offset":1,"pro":0.14285714285714285,"labels":"MTD"},{"idx":7,"sentence":"The proposed method outperforms the current state of the art.","offset":2,"pro":0.2857142857142857,"labels":"RST"},{"idx":7,"sentence":"Further more we demonstrate sample efficiency, where our method trained only on 20% of the data, are comparable to current state of the art method trained on 100% data on two out of there evaluation metrics.","offset":3,"pro":0.42857142857142855,"labels":"RST"},{"idx":7,"sentence":"Unfortunately, RL policy trained on off-policy data are prone to issues of bias and generalization, which are further exacerbated by stochasticity in human response and non-markovian nature of ","offset":4,"pro":0.5714285714285714,"labels":"GAP"},{"idx":7,"sentence":"nnotated belief state of a dialogue management system.","offset":5,"pro":0.7142857142857143,"labels":"GAP"},{"idx":7,"sentence":"o this end, we propose a batch-RL framework for ToD policy learning: Causal-aware Safe Policy Improvement (CASPI)","offset":6,"pro":0.8571428571428571,"labels":"PUR"},{"idx":8,"sentence":"As a more natural and intelligent interaction manner, multimodal task-oriented dialog system recently has received great attention and many remarkable progresses have been achieved.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"Nevertheless, almost all existing studies follow the pipeline to first learn intra-modal features separately and then conduct simple feature concatenation or attention-based feature fusion to generate responses, which hampers them from learning inter-modal interactions and conducting cross-modal feature alignment for generating more intention-aware responses.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":8,"sentence":"To address these issues, we propose UniTranSeR, a Unified Transformer Semantic Representation framework with feature alignment and intention reasoning for multimodal dialog systems.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":8,"sentence":"Specifically, we first embed the multimodal features into a unified Transformer semantic space to prompt inter-modal interactions, and then devise a feature alignment and intention reasoning (FAIR) layer to perform cross-modal entity alignment and fine-grained key-value reasoning, so as to effectively identify user's intention for generating more accurate responses.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":8,"sentence":"Experimental results verify the effectiveness of UniTranSeR, showing that it significantly outperforms state-of-the-art approaches on the representative MMD dataset.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":9,"sentence":"Dialogue State Tracking (DST) aims to keep track of users' intentions during the course of a conversation.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"In DST, modelling the relations among domains and slots is still an under-studied problem.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":9,"sentence":"Existing approaches that have considered such relations generally fall short in: (1) fusing prior slot-domain membership relations and dialogue-aware dynamic slot relations explicitly, and (2) generalizing to unseen domains.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":9,"sentence":"To address these issues, we propose a novel Dynamic Schema Graph Fusion Network (DSGFNet), which generates a dynamic schema graph to explicitly fuse the prior slot-domain membership relations and dialogue-aware dynamic slot relations.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":9,"sentence":"It also uses the schemata to facilitate knowledge transfer to new domains.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":9,"sentence":"DSGFNet consists of a dialogue utterance encoder, a schema graph encoder, a dialogue-aware schema graph evolving network, and a schema graph enhanced dialogue state decoder.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":9,"sentence":"Empirical results on benchmark datasets (i.e., SGD, MultiWOZ2.1, and MultiWOZ2.2), show that DSGFNet outperforms existing methods.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":10,"sentence":"Recent progress of abstractive text summarization largely relies on large pre-trained sequence-to-sequence Transformer models, which are computationally expensive.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"This paper aims to distill these large models into smaller ones for faster inference and with minimal performance loss.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":10,"sentence":"Pseudo-labeling based methods are popular in sequence-to-sequence model distillation.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":10,"sentence":"In this paper, we find simply manipulating attention temperatures in Transformers can make pseudo labels easier to learn for student models.","offset":3,"pro":0.5,"labels":"RST"},{"idx":10,"sentence":"Our experiments on three summarization datasets show our proposed method consistently improves vanilla pseudo-labeling based methods.","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":10,"sentence":"Further empirical analysis shows that both pseudo labels and summaries produced by our students are shorter and more abstractive.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":11,"sentence":"This paper demonstrates that multilingual pretraining and multilingual fine-tuning are both critical for facilitating cross-lingual transfer in zero-shot translation, where the neural machine translation (NMT) model is tested on source languages unseen during supervised training.","offset":0,"pro":0,"labels":"PUR"},{"idx":11,"sentence":"Following this idea, we present SixT+, a strong many-to-English NMT model that supports 100 source languages but is trained with a parallel dataset in only six source languages.","offset":1,"pro":0.1111111111111111,"labels":"MTD"},{"idx":11,"sentence":"SixT+ initializes the decoder embedding and the full encoder with XLM-R large and then trains the encoder and decoder layers with a simple two-stage training strategy.","offset":2,"pro":0.2222222222222222,"labels":"MTD"},{"idx":11,"sentence":"SixT+ achieves impressive performance on many-to-English translation.","offset":3,"pro":0.3333333333333333,"labels":"RST"},{"idx":11,"sentence":"It significantly outperforms CRISS and m2m-100, two strong multilingual NMT systems, with an average gain of 7.2 and 5.0 BLEU respectively.","offset":4,"pro":0.4444444444444444,"labels":"RST"},{"idx":11,"sentence":"Additionally, SixT+ offers a set of model parameters that can be further fine-tuned to other unsupervised tasks.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":11,"sentence":"We demonstrate that adding SixT+ initialization outperforms state-of-the-art explicitly designed unsupervised NMT models on Siless-greaterEn and Neless-greaterEn by over 1.2 average BLEU.","offset":6,"pro":0.6666666666666666,"labels":"CLN"},{"idx":11,"sentence":"When applied to zero-shot cross-lingual abstractive summarization, it produces an average performance gain of 12.3 ROUGE-L over mBART-ft.","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":11,"sentence":"We conduct detailed analyses to understand the key ingredients of SixT+, including multilinguality of the auxiliary parallel data, positional disentangled encoder, and the cross-lingual transferability of its encoder.","offset":8,"pro":0.8888888888888888,"labels":"MTD"},{"idx":12,"sentence":"Processing open-domain Chinese texts has been a critical bottleneck in computational linguistics for decades, partially because text segmentation and word discovery often entangle with each other in this challenging scenario.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"No existing methods yet can achieve effective text segmentation and word discovery simultaneously in open domain.","offset":1,"pro":0.25,"labels":"GAP"},{"idx":12,"sentence":"This study fills in this gap by proposing a novel method called TopWORDS-Seg based on Bayesian inference, which enjoys robust performance and transparent interpretation when no training corpus and domain vocabulary are available.","offset":2,"pro":0.5,"labels":"PUR"},{"idx":12,"sentence":"Advantages of TopWORDS-Seg are demonstrated by a series of experimental studies.","offset":3,"pro":0.75,"labels":"MTD"},{"idx":13,"sentence":"Cross-lingual named entity recognition task is one of the critical problems for evaluating the potential transfer learning techniques on low resource languages.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"Knowledge distillation using pre-trained multilingual language models between source and target languages have shown their superiority in transfer.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":13,"sentence":"However, existing cross-lingual distillation models merely consider the potential transferability between two identical single tasks across both domains.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":13,"sentence":"Other possible auxiliary tasks to improve the learning performance have not been fully investigated.","offset":3,"pro":0.375,"labels":"GAP"},{"idx":13,"sentence":"In this study, based on the knowledge distillation framework and multi-task learning, we introduce the similarity metric model as an auxiliary task to improve the cross-lingual NER performance on the target domain.","offset":4,"pro":0.5,"labels":"PUR"},{"idx":13,"sentence":"Specifically, an entity recognizer and a similarity evaluator are first trained in parallel as two teachers from the source domain.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":13,"sentence":"Then, two tasks in the student model are supervised by these teachers simultaneously.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":13,"sentence":"Empirical studies on the three datasets across 7 different languages confirm the effectiveness of the proposed model.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":14,"sentence":"Although current state-of-the-art Transformer-based solutions succeeded in a wide range for single-document NLP tasks, they still struggle to address multi-input tasks such as multi-document summarization.","offset":0,"pro":0,"labels":"GAP"},{"idx":14,"sentence":"Many solutions truncate the inputs, thus ignoring potential summary-relevant contents, which is unacceptable in the medical domain where each information can be vital.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":14,"sentence":"Others leverage linear model approximations to apply multi-input concatenation, worsening the results because all information is considered, even if it is conflicting or noisy with respect to a shared background.","offset":2,"pro":0.2857142857142857,"labels":"BAC"},{"idx":14,"sentence":"Despite the importance and social impact of medicine, there are no ad-hoc solutions for multi-document summarization.","offset":3,"pro":0.42857142857142855,"labels":"GAP"},{"idx":14,"sentence":"For this reason, we propose a novel discriminative marginalized probabilistic method (DAMEN) trained to discriminate critical information from a cluster of topic-related medical documents and generate a multi-document summary via token probability marginalization.","offset":4,"pro":0.5714285714285714,"labels":"PUR"},{"idx":14,"sentence":"Results prove we outperform the previous state-of-the-art on a biomedical dataset for multi-document summarization of systematic literature reviews.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":14,"sentence":"Moreover, we perform extensive ablation studies to motivate the design choices and prove the importance of each module of our method.","offset":6,"pro":0.8571428571428571,"labels":"IMP"},{"idx":15,"sentence":"Conventional wisdom in pruning Transformer-based language models is that pruning reduces the model expressiveness and thus is more likely to underfit rather than overfit.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"However, under the trending pretrain-and-finetune paradigm, we postulate a counter-traditional hypothesis, that is: pruning increases the risk of overfitting when performed at the fine-tuning phase.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":15,"sentence":"In this paper, we aim to address the overfitting problem and improve pruning performance via progressive knowledge distillation with error-bound properties.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":15,"sentence":"We show for the first time that reducing the risk of overfitting can help the effectiveness of pruning under the pretrain-and-finetune paradigm.","offset":3,"pro":0.6,"labels":"CLN"},{"idx":15,"sentence":"Ablation studies and experiments on the GLUE benchmark show that our method outperforms the leading competitors across different tasks.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":16,"sentence":"We propose a novel data-augmentation technique for neural machine translation based on ROT-k ciphertexts.","offset":0,"pro":0,"labels":"PUR"},{"idx":16,"sentence":"ROT-k is a simple letter substitution cipher that replaces a letter in the plaintext with the kth letter after it in the alphabet.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":16,"sentence":"We first generate multiple ROT-k ciphertexts using different values of k for the plaintext which is the source side of the parallel data.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":16,"sentence":"We then leverage this enciphered training data along with the original parallel data via multi-source training to improve neural machine translation.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":16,"sentence":"Our method, CipherDAug, uses a co-regularization-inspired training procedure, requires no external data sources other than the original training data, and uses a standard Transformer to outperform strong data augmentation techniques on several datasets by a significant margin.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":16,"sentence":"This technique combines easily with existing approaches to data augmentation, and yields particularly strong results in low-resource settings.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":17,"sentence":"Pre-trained multilingual language models such as mBERT and XLM-R have demonstrated great potential for zero-shot cross-lingual transfer to low web-resource languages (LRL).","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"However, due to limited model capacity, the large difference in the sizes of available monolingual corpora between high web-resource languages (HRL) and LRLs does not provide enough scope of co-embedding the LRL with the HRL, thereby affecting the downstream task performance of LRLs.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":17,"sentence":"In this paper, we argue that relatedness among languages in a language family along the dimension of lexical overlap may be leveraged to overcome some of the corpora limitations of LRLs.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":17,"sentence":"We propose Overlap BPE (OBPE), a simple yet effective modification to the BPE vocabulary generation algorithm which enhances overlap across related languages.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":17,"sentence":"Through extensive experiments on multiple NLP tasks and datasets, we observe that OBPE generates a vocabulary that increases the representation of LRLs via tokens shared with HRLs.","offset":4,"pro":0.5,"labels":"RST"},{"idx":17,"sentence":"Unlike previous studies that dismissed the importance of token-overlap, we show that in the low-resource related language setting, token overlap matters.","offset":5,"pro":0.625,"labels":"CLN"},{"idx":17,"sentence":"Synthetically reducing the overlap to zero can cause as much as a four-fold drop in zero-shot transfer accuracy.","offset":6,"pro":0.75,"labels":"RST"},{"idx":17,"sentence":"This results in improved zero-shot transfer from related HRLs to LRLs without reducing HRL representation and accuracy","offset":7,"pro":0.875,"labels":"CLN"},{"idx":18,"sentence":"Due to the sparsity of the attention matrix, much computation is redundant.","offset":0,"pro":0,"labels":"GAP"},{"idx":18,"sentence":"Therefore, in this paper, we design an efficient Transformer architecture, named Fourier Sparse Attention for Transformer (FSAT), for fast long-range sequence modeling.","offset":1,"pro":0.1,"labels":"PUR"},{"idx":18,"sentence":"We provide a brand-new perspective for constructing sparse attention matrix, i.e. making the sparse attention matrix predictable.","offset":2,"pro":0.2,"labels":"MTD"},{"idx":18,"sentence":"Two core sub-modules are: (1) A fast Fourier transform based hidden state cross module, which captures and pools L^2 semantic combinations in mathcal{O(Llog L) time complexity.","offset":3,"pro":0.3,"labels":"MTD"},{"idx":18,"sentence":"(2) A sparse attention matrix estimation module, which predicts dominant elements of an attention matrix based on the output of the previous hidden state cross module.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":18,"sentence":"By reparameterization and gradient truncation, FSAT successfully learned the index of dominant elements.","offset":5,"pro":0.5,"labels":"RST"},{"idx":18,"sentence":"The overall complexity about the sequence length is reduced from mathcal{O(L^2) to mathcal{O(Llog L).","offset":6,"pro":0.6,"labels":"MTD"},{"idx":18,"sentence":"Extensive experiments (natural language, vision, and math) show that FSAT remarkably outperforms the standard multi-head attention and its variants in various long-sequence tasks with low computational costs, and achieves new state-of-the-art results on the Long Range Arena benchmark.","offset":7,"pro":0.7,"labels":"CLN"},{"idx":18,"sentence":"Self-attention mechanism has been shown to be an effective approach for capturing global context dependencies in sequence modeling,","offset":8,"pro":0.8,"labels":"BAC"},{"idx":18,"sentence":"but it suffers from quadratic complexity in time and memory usage.","offset":9,"pro":0.9,"labels":"GAP"},{"idx":19,"sentence":"In modern recommender systems, there are usually comments or reviews from users that justify their ratings for different items.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"Trained on such textual corpus, explainable recommendation models learn to discover user interests and generate personalized explanations.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":19,"sentence":"Though able to provide plausible explanations, existing models tend to generate repeated sentences for different items or empty sentences with insufficient details.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":19,"sentence":"To this end, we propose a visually-enhanced approach named METER with the help of visualization generation and text-image matching discrimination: the explainable recommendation model is encouraged to visualize what it refers to while incurring a penalty if the visualization is incongruent with the textual explanation.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":19,"sentence":"Experimental results and a manual assessment demonstrate that our approach can improve not only the text quality but also the diversity and explainability of the generated explanations.","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":19,"sentence":"This begs an interesting question: can we immerse the models in a multimodal environment to gain proper awareness of real-world concepts and alleviate above shortcomings?","offset":5,"pro":0.8333333333333334,"labels":"GAP"},{"idx":0,"sentence":"New intent discovery aims to uncover novel intent categories from user utterances to expand the set of supported intent classes.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"It is a critical task for the development and service expansion of a practical dialogue system.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":0,"sentence":"Despite its importance, this problem remains under-explored in the literature.","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":0,"sentence":"In this paper, we provide new solutions to two important research questions for new intent discovery: (1) how to learn semantic utterance representations and (2) how to better cluster utterances.","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":0,"sentence":"Particularly, we first propose a multi-task pre-training strategy to leverage rich unlabeled data along with external labeled data for representation learning.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":0,"sentence":"Then, we design a new contrastive loss to exploit self-supervisory signals in unlabeled data for clustering.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":0,"sentence":"Extensive experiments on three intent recognition benchmarks demonstrate the high effectiveness of our proposed method, which outperforms state-of-the-art methods by a large margin in both unsupervised and semi-supervised scenarios.","offset":6,"pro":0.6666666666666666,"labels":"CLN"},{"idx":0,"sentence":"The source code will be available at url{https://github.com/zhang-yu-wei/MTP-CLNN.","offset":7,"pro":0.7777777777777778,"labels":"CTN"},{"idx":0,"sentence":"Existing approaches typically rely on a large amount of labeled utterances and employ pseudo-labeling methods for representation learning and clustering, which are label-intensive, inefficient, and inaccurate.","offset":8,"pro":0.8888888888888888,"labels":"GAP"},{"idx":1,"sentence":"Decisions on state-level policies have a deep effect on many aspects of our everyday life, such as health-care and education access.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"However, there is little understanding of how these policies and decisions are being formed in the legislative process.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":1,"sentence":"We take a data-driven approach by decoding the impact of legislation on relevant stakeholders (e.g., teachers in education bills) to understand legislators' decision-making process and votes.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":1,"sentence":"We build a new dataset for multiple US states that interconnects multiple sources of data including bills, stakeholders, legislators, and money donors.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":1,"sentence":"Next, we develop a textual graph-based model to embed and analyze state bills.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":1,"sentence":"Our model predicts winners/losers of bills and then utilizes them to better determine the legislative body's vote breakdown according to demographic/ideological criteria, e.g., gender.","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":2,"sentence":"Tangled multi-party dialogue contexts lead to challenges for dialogue reading comprehension, where multiple dialogue threads flow simultaneously within a common dialogue record, increasing difficulties in understanding the dialogue history for both human and machine.","offset":0,"pro":0,"labels":"GAP"},{"idx":2,"sentence":"Previous studies mainly focus on utterance encoding methods with carefully designed features but pay inadequate attention to characteristic features of the structure of dialogues.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":2,"sentence":"Based on the fact that dialogues are constructed on successive participation and interactions between speakers, we model structural information of dialogues in two aspects: 1)speaker property that indicates whom a message is from, and 2) reference dependency that shows whom a message may refer to.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":2,"sentence":"The proposed method achieves new state-of-the-art on the Ubuntu IRC benchmark dataset and contributes to dialogue-related comprehension.","offset":3,"pro":0.6,"labels":"CLN"},{"idx":2,"sentence":"We specially take structure factors into account and design a novel model for dialogue disentangling.","offset":4,"pro":0.8,"labels":"PUR"},{"idx":3,"sentence":"Empathetic dialogue assembles emotion understanding, feeling projection, and appropriate response generation.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"Existing work for empathetic dialogue generation concentrates on the two-party conversation scenario.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":3,"sentence":"Multi-party dialogues, however, are pervasive in reality.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":3,"sentence":"Furthermore, emotion and sensibility are typically confused; a refined empathy analysis is needed for comprehending fragile and nuanced human feelings.","offset":3,"pro":0.5,"labels":"GAP"},{"idx":3,"sentence":"Additionally, a Static-Dynamic model for Multi-Party Empathetic Dialogue Generation, SDMPED, is introduced as a baseline by exploring the static sensibility and dynamic emotion for the multi-party empathetic dialogue learning, the aspects that help SDMPED achieve the state-of-the-art performance.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":3,"sentence":"We address these issues ","offset":5,"pro":0.8333333333333334,"labels":"PUR"},{"idx":4,"sentence":"Applying existing methods to emotional support conversation-which provides valuable assistance to people who are in need-has two major limitations: (a) they generally employ a conversation-level emotion label, which is too coarse-grained to capture user's instant mental state; (b) most of them focus on expressing empathy in the response(s) rather than gradually reducing user's distress.","offset":0,"pro":0,"labels":"GAP"},{"idx":4,"sentence":"To address the problems, we propose a novel model MISC, which firstly infers the user's fine-grained emotional status, and then responds skillfully using a mixture of strategy.","offset":1,"pro":0.3333333333333333,"labels":"PUR"},{"idx":4,"sentence":"Experimental results on the benchmark dataset demonstrate the effectiveness of our method and reveal the benefits of fine-grained emotion understanding as well as mixed-up strategy modeling.","offset":2,"pro":0.6666666666666666,"labels":"CLN"},{"idx":5,"sentence":"There have been various types of pretraining architectures including autoencoding models (e.g., BERT), autoregressive models (e.g., GPT), and encoder-decoder models (e.g., T5).","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"We propose a General Language Model (GLM) based on autoregressive blank infilling to address this challenge.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":5,"sentence":"GLM improves blank filling pretraining by adding 2D positional encodings and allowing an arbitrary order to predict spans, which results in performance gains over BERT and T5 on NLU tasks.","offset":2,"pro":0.4,"labels":"RST"},{"idx":5,"sentence":"Meanwhile, GLM can be pretrained for different types of tasks by varying the number and lengths of blanks.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":5,"sentence":"On a wide range of tasks across NLU, conditional and unconditional generation, GLM outperforms BERT, T5, and GPT given the same model sizes and data, and achieves the best performance from a single pretrained model with 1.25{mbox{times parameters of BERT Large , demonstrating its generalizability to different downstream tasks.","offset":4,"pro":0.8,"labels":"RST"},{"idx":6,"sentence":"It is very common to use quotations (quotes) to make our writings more elegant or convincing.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"Any part of it is larger than previous unpublished counterparts.","offset":1,"pro":0.125,"labels":"MTD"},{"idx":6,"sentence":"We conduct an extensive evaluation of existing quote recommendation methods on QuoteR.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":6,"sentence":"Furthermore, we propose a new quote recommendation model that significantly outperforms previous methods on all three parts of QuoteR.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":6,"sentence":"All the code and data of this paper can be obtained at https://github.com/thunlp/QuoteR.","offset":4,"pro":0.5,"labels":"CTN"},{"idx":6,"sentence":"To help people find appropriate quotes efficiently, the task of quote recommendation is presented, aiming to recommend quotes that fit the current context of writing.","offset":5,"pro":0.625,"labels":"PUR"},{"idx":6,"sentence":"There have been various quote recommendation approaches, but they are evaluated on different unpublished datasets.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":6,"sentence":"To facilitate the research on this task, we build a large and fully open quote recommendation dataset called QuoteR, which comprises three parts including English, standard Chinese and classical Chinese.","offset":7,"pro":0.875,"labels":"MTD"},{"idx":7,"sentence":"To address this issue, we propose a novel framework that unifies the document classifier with handcrafted features, particularly time-dependent novelty scores.","offset":0,"pro":0,"labels":"PUR"},{"idx":7,"sentence":"Specifically, we formulate the novelty scores by comparing each application with millions of prior arts using a hybrid of efficient filters and a neural bi-encoder.","offset":1,"pro":0.125,"labels":"MTD"},{"idx":7,"sentence":"Moreover, we impose a new regularization term into the classification objective to enforce the monotonic change of approval prediction w.r.t. novelty scores.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":7,"sentence":"From extensive experiments on a large-scale USPTO dataset, we find that standard BERT fine-tuning can partially learn the correct relationship between novelty and approvals from inconsistent data.","offset":3,"pro":0.375,"labels":"RST"},{"idx":7,"sentence":"However, our time-dependent novelty features offer a boost on top of it.","offset":4,"pro":0.5,"labels":"CLN"},{"idx":7,"sentence":"Also, our monotonic regularization, while shrinking the search space, can drive the optimizer to better local optima, yielding a further small performance gain.","offset":5,"pro":0.625,"labels":"RST"},{"idx":7,"sentence":"Predicting the approval chance of a patent application is a challenging problem involving multiple facets. ","offset":6,"pro":0.75,"labels":"BAC"},{"idx":7,"sentence":"Such novelty evaluations differ the patent approval prediction from conventional document classification - Successful patent applications may share similar writing patterns; ","offset":7,"pro":0.875,"labels":"BAC"},{"idx":8,"sentence":"Knowledge-based visual question answering (QA) aims to answer a question which requires visually-grounded external knowledge beyond image content itself.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"Answering complex questions that require multi-hop reasoning under weak supervision is considered as a challenging problem since i) no supervision is given to the reasoning process and ii) high-order semantics of multi-hop knowledge facts need to be captured.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":8,"sentence":"In this paper, we introduce a concept of hypergraph to encode high-level semantics of a question and a knowledge base, and to learn high-order associations between them.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":8,"sentence":"The proposed model, Hypergraph Transformer, constructs a question hypergraph and a query-aware knowledge hypergraph, and infers an answer by encoding inter-associations between two hypergraphs and intra-associations in both hypergraph itself.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":8,"sentence":"Extensive experiments on two knowledge-based visual QA and two knowledge-based textual QA demonstrate the effectiveness of our method, especially for multi-hop reasoning problem.","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":8,"sentence":"Our source code is available at https://github.com/yujungheo/kbvqa-public.","offset":5,"pro":0.8333333333333334,"labels":"CTN"},{"idx":9,"sentence":"Modelling prosody variation is critical for synthesizing natural and expressive speech in end-to-end text-to-speech (TTS) systems.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"In this paper, a cross-utterance conditional VAE (CUC-VAE) is proposed to estimate a posterior probability distribution of the latent prosody features for each phoneme by conditioning on acoustic features, speaker information, and text features obtained from both past and future sentences.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":9,"sentence":"At inference time, instead of the standard Gaussian distribution used by VAE, CUC-VAE allows sampling from an utterance-specific prior distribution conditioned on cross-utterance information, which allows the prosody features generated by the TTS system to be related to the context and is more similar to how humans naturally produce prosody.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":9,"sentence":"The performance of CUC-VAE is evaluated via a qualitative listening test for naturalness, intelligibility and quantitative measurements, including word error rates and the standard deviation of prosody attributes.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":9,"sentence":"Experimental results on LJ-Speech and LibriTTS data show that the proposed CUC-VAE TTS system improves naturalness and prosody diversity with clear margins.","offset":4,"pro":0.8,"labels":"RST"},{"idx":10,"sentence":"Recent work on controlled text generation has either required attribute-based fine-tuning of the base language model (LM), or has restricted the parameterization of the attribute discriminator to be compatible with the base autoregressive LM.","offset":0,"pro":0,"labels":"GAP"},{"idx":10,"sentence":"In this work, we propose Mix and Match LM, a global score-based alternative for controllable text generation that combines arbitrary pre-trained black-box models for achieving the desired attributes in the generated text without involving any fine-tuning or structural assumptions about the black-box models.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":10,"sentence":"We interpret the task of controllable generation as drawing samples from an energy-based model whose energy values are a linear combination of scores from black-box models that are separately responsible for fluency, the control attribute, and faithfulness to any conditioning context.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":10,"sentence":"We use a Metropolis-Hastings sampling scheme to sample from this energy-based model using bidirectional context and global attribute features.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":10,"sentence":"We validate the effectiveness of our approach on various controlled generation and style-based text revision tasks by outperforming recently proposed methods that involve extra training, fine-tuning, or restrictive assumptions over the form of models.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":11,"sentence":"Automatic transfer of text between domains has become popular in recent times.","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"However, it does not explicitly maintain other attributes between the source and translated text: e.g., text length and descriptiveness.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":11,"sentence":"Maintaining constraints in transfer has several downstream applications, including data augmentation and debiasing.","offset":2,"pro":0.25,"labels":"BAC"},{"idx":11,"sentence":"Unlike the competing losses used in GANs, we introduce cooperative losses where the discriminator and the generator cooperate and reduce the same loss.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":11,"sentence":"We demonstrate that such training retains lexical, syntactic and domain-specific constraints between domains for multiple benchmark datasets, including ones where more than one attribute change.","offset":4,"pro":0.5,"labels":"CLN"},{"idx":11,"sentence":"We show that the complementary cooperative losses improve text quality, according to both automated and human evaluation measures.","offset":5,"pro":0.625,"labels":"CLN"},{"idx":11,"sentence":"One of its aims is to preserve the semantic content while adapting to the target domain.","offset":6,"pro":0.75,"labels":"BAC"},{"idx":11,"sentence":"We introduce a method for such constrained unsupervised text style transfer ","offset":7,"pro":0.875,"labels":"PUR"},{"idx":12,"sentence":"Understanding causality has vital importance for various Natural Language Processing (NLP) applications.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"Beyond the labeled instances, conceptual explanations of the causality can provide deep understanding of the causal fact to facilitate the causal reasoning process.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":12,"sentence":"However, such explanation information still remains absent in existing causal reasoning resources.","offset":2,"pro":0.4,"labels":"GAP"},{"idx":12,"sentence":"In this paper, we fill this gap by presenting a human-annotated explainable CAusal REasoning dataset (e-CARE), which contains over 20K causal reasoning questions, together with natural language formed explanations of the causal questions.","offset":3,"pro":0.6,"labels":"PUR"},{"idx":12,"sentence":"Experimental results show that generating valid explanations for causal facts still remains especially challenging for the state-of-the-art models, and the explanation information can be helpful for promoting the accuracy and stability of causal reasoning models.","offset":4,"pro":0.8,"labels":"RST"},{"idx":13,"sentence":"In particular, existing datasets rarely distinguish fine-grained reading skills, such as the understanding of varying narrative elements.","offset":0,"pro":0,"labels":"GAP"},{"idx":13,"sentence":"Generated by educational experts based on an evidence-based theoretical framework, FairytaleQA consists of 10,580 explicit and implicit questions derived from 278 children-friendly stories, covering seven types of narrative elements or relations.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":13,"sentence":"Our dataset is valuable in two folds: First, we ran existing QA models on our dataset and confirmed that this annotation helps assess models' fine-grained learning skills.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":13,"sentence":"Second, the dataset supports question generation (QG) task in the education domain.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":13,"sentence":"Through benchmarking with QG models, we show that the QG model trained on FairytaleQA is capable of asking high-quality and more diverse questions.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":13,"sentence":"Question answering (QA) is a fundamental means to facilitate assessment and training of narrative comprehension skills for both machines and young children, ","offset":5,"pro":0.8333333333333334,"labels":"BAC"},{"idx":14,"sentence":"In this paper, we study two issues of semantic parsing approaches to conversational question answering over a large-scale knowledge base: (1) The actions defined in grammar are not sufficient to handle uncertain reasoning common in real-world scenarios.","offset":0,"pro":0,"labels":"PUR"},{"idx":14,"sentence":"It defines fuzzy comparison operations in the grammar system for uncertain reasoning based on the fuzzy set theory.","offset":1,"pro":0.125,"labels":"MTD"},{"idx":14,"sentence":"In order to enhance the interaction between semantic parsing and knowledge base, we incorporate entity triples from the knowledge base into a knowledge-aware entity disambiguation module.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":14,"sentence":"Additionally, we propose a multi-label classification framework to not only capture correlations between entity types and relations but also detect knowledge base information relevant to the current utterance.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":14,"sentence":"Both enhancements are based on pre-trained language models.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":14,"sentence":"Experiments on a large-scale conversational question answering benchmark demonstrate that the proposed KaFSP achieves significant improvements over previous state-of-the-art models, setting new SOTA results on 8 out of 10 question types, gaining improvements of over 10% F1 or accuracy on 3 question types, and improving overall F1 from 83.01% to 85.33%.","offset":5,"pro":0.625,"labels":"RST"},{"idx":14,"sentence":"The source code of KaFSP is available at https://github.com/tjunlp-lab/KaFSP.","offset":6,"pro":0.75,"labels":"CTN"},{"idx":14,"sentence":"(2) Knowledge base information is not well exploited and incorporated into semantic parsing.","offset":7,"pro":0.875,"labels":"PUR"},{"idx":15,"sentence":"In this paper, we explore multilingual KG completion, which leverages limited seed alignment as a bridge, to embrace the collective knowledge from multiple languages.","offset":0,"pro":0,"labels":"PUR"},{"idx":15,"sentence":"However, language alignment used in prior works is still not fully exploited: (1) alignment pairs are treated equally to maximally push parallel entities to be close, which ignores KG capacity inconsistency; (2) seed alignment is scarce and new alignment identification is usually in a noisily unsupervised manner.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":15,"sentence":"To tackle these issues, we propose a novel self-supervised adaptive graph alignment (SS-AGA) method.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":15,"sentence":"Specifically, SS-AGA fuses all KGs as a whole graph by regarding alignment as a new edge type.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":15,"sentence":"As such, information propagation and noise influence across KGs can be adaptively controlled via relation-aware attention weights.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":15,"sentence":"Meanwhile, SS-AGA features a new pair generator that dynamically captures potential alignment pairs in a self-supervised paradigm.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":15,"sentence":"Extensive experiments on both the public multilingual DBPedia KG and newly-created industrial multilingual E-commerce KG empirically demonstrate the effectiveness of SS-AGA","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":16,"sentence":"Automatic code summarization, which aims to describe the source code in natural language, has become an essential task in software maintenance.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"One key challenge keeping these approaches from being practical lies in the lacking of retaining the semantic structure of source code, which has unfortunately been overlooked by the state-of-the-art.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":16,"sentence":"Existing approaches resort to representing the syntax structure of code by modeling the Abstract Syntax Trees (ASTs).","offset":2,"pro":0.25,"labels":"BAC"},{"idx":16,"sentence":"However, the hierarchical structures of ASTs have not been well explored.","offset":3,"pro":0.375,"labels":"GAP"},{"idx":16,"sentence":"In this paper, we propose CODESCRIBE to model the hierarchical syntax structure of code by introducing a novel triplet position for code summarization.","offset":4,"pro":0.5,"labels":"PUR"},{"idx":16,"sentence":"Specifically, CODESCRIBE leverages the graph neural network and Transformer to preserve the structural and sequential information of code, respectively.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":16,"sentence":"In addition, we propose a pointer-generator network that pays attention to both the structure and sequential tokens of code for a better summary generation.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":16,"sentence":"Experiments on two real-world datasets in Java and Python demonstrate the effectiveness of our proposed approach when compared with several state-of-the-art baselines.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":17,"sentence":"The few-shot natural language understanding (NLU) task has attracted much recent attention.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"However, prior methods have been evaluated under a disparate set of protocols, which hinders fair comparison and measuring the progress of the field.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":17,"sentence":"To address this issue, we introduce an evaluation framework that improves previous evaluation procedures in three key aspects, i.e., test performance, dev-test correlation, and stability.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":17,"sentence":"Under this new evaluation framework, we re-evaluate several state-of-the-art few-shot methods for NLU tasks.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":17,"sentence":"Our framework reveals new insights: (1) both the absolute performance and relative gap of the methods were not accurately estimated in prior literature; (2) no single method dominates most tasks with consistent performance; (3) improvements of some methods diminish with a larger pretrained model; and (4) gains from different methods are often complementary and the best combined model performs close to a strong fully-supervised baseline.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":17,"sentence":"We open-source our toolkit, FewNLU, that implements our evaluation framework along with a number of state-of-the-art methods.","offset":5,"pro":0.8333333333333334,"labels":"CTN"},{"idx":18,"sentence":"Generalized zero-shot text classification aims to classify textual instances from both previously seen classes and incrementally emerging unseen classes.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"Most existing methods generalize poorly since the learned parameters are only optimal for seen classes rather than for both classes, and the parameters keep stationary in predicting procedures.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":18,"sentence":"To address these challenges, we propose a novel Learn to Adapt (LTA) network using a variant meta-learning framework.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":18,"sentence":"Specifically, LTA trains an adaptive classifier by using both seen and virtual unseen classes to simulate a generalized zero-shot learning (GZSL) scenario in accordance with the test time, and simultaneously learns to calibrate the class prototypes and sample representations to make the learned parameters adaptive to incoming unseen classes.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":18,"sentence":"We claim that the proposed model is capable of representing all prototypes and samples from both classes to a more consistent distribution in a global space.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":18,"sentence":"Extensive experiments on five text classification datasets show that our model outperforms several competitive previous approaches by large margins.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":18,"sentence":"The code and the whole datasets are available at https://github.com/Quareia/LTA.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":19,"sentence":"Understanding tables is an important aspect of natural language understanding.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"Existing models for table understanding require linearization of the table structure, where row or column order is encoded as an unwanted bias.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":19,"sentence":"Such spurious biases make the model vulnerable to row and column order perturbations.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":19,"sentence":"Additionally, prior work has not thoroughly modeled the table structures or table-text alignments, hindering the table-text understanding ability.","offset":3,"pro":0.42857142857142855,"labels":"GAP"},{"idx":19,"sentence":"In this work, we propose a robust and structurally aware table-text encoding architecture TableFormer, where tabular structural biases are incorporated completely through learnable attention biases.","offset":4,"pro":0.5714285714285714,"labels":"PUR"},{"idx":19,"sentence":"TableFormer is (1) strictly invariant to row and column orders, and, (2) could understand tables better due to its tabular inductive biases.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":19,"sentence":"Our evaluations showed that TableFormer outperforms strong baselines in all settings on SQA, WTQ and TabFact table reasoning datasets, and achieves state-of-the-art performance on SQA, especially when facing answer-invariant row and column order perturbations (6% improvement over the best baseline), because previous SOTA models' performance drops by 4% - 6% when facing such perturbations while TableFormer is not affected.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":0,"sentence":"Text-based games provide an interactive way to study natural language processing.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"While deep reinforcement learning has shown effectiveness in developing the game playing agent, the low sample efficiency and the large action space remain to be the two major challenges that hinder the DRL from being applied in the real world.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":0,"sentence":"In this paper, we address the challenges by introducing world-perceiving modules, which automatically decompose tasks and prune actions by answering questions about the environment.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":0,"sentence":"We then propose a two-phase training framework to decouple language learning from reinforcement learning, which further improves the sample efficiency.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":0,"sentence":"Besides, it shows robustness against compound error and limited pre-training data.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":0,"sentence":"The experimental results show that the proposed method significantly improves the performance and sample efficiency.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":1,"sentence":"In zero-shot multilingual extractive text summarization, a model is typically trained on English summarization dataset and then applied on summarization datasets of other languages.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"Given English gold summaries and documents, sentence-level labels for extractive summarization are usually generated using heuristics.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":1,"sentence":"However, these monolingual labels created on English datasets may not be optimal on datasets of other languages, for that there is the syntactic or semantic discrepancy between different languages.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":1,"sentence":"To fully leverage the information of these different sets of labels, we propose NLSSum (Neural Label Search for Summarization), which jointly learns hierarchical weights for these different sets of labels together with our summarization model.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":1,"sentence":"We conduct multilingual zero-shot summarization experiments on MLSUM and WikiLingua datasets, and we achieve state-of-the-art results using both human and automatic evaluations across these two datasets.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":1,"sentence":"In this way, it is possible to translate the English dataset to other languages and obtain different sets of labels again using heuristics.","offset":5,"pro":0.8333333333333334,"labels":"GAP"},{"idx":2,"sentence":"Previous work of class-incremental learning for Named Entity Recognition (NER) relies on the assumption that there exists abundance of labeled data for the training of new classes.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"In this work, we study a more challenging but practical problem, i.e., few-shot class-incremental learning for NER, where an NER model is trained with only few labeled samples of the new classes, without forgetting knowledge of the old ones.","offset":1,"pro":0.25,"labels":"PUR"},{"idx":2,"sentence":"We further develop a framework that distills from the existing model with both synthetic data, and real data from the current training set.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":2,"sentence":"To alleviate the problem of catastrophic forgetting in few-shot class-incremental learning, we reconstruct synthetic training data of the old classes using the trained NER model, augmenting the training of new classes.","offset":3,"pro":0.75,"labels":"MTD"},{"idx":3,"sentence":"Building models of natural language processing (NLP) is challenging in low-resource scenarios where limited data are available.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"Optimization-based meta-learning algorithms achieve promising results in low-resource scenarios by adapting a well-generalized model initialization to handle new tasks.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":3,"sentence":"Nonetheless, these approaches suffer from the memorization overfitting issue, where the model tends to memorize the meta-training tasks while ignoring support sets when adapting to new tasks.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":3,"sentence":"To address this issue, we propose a memory imitation meta-learning (MemIML) method that enhances the model's reliance on support sets for task adaptation.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":3,"sentence":"Specifically, we introduce a task-specific memory module to store support set information and construct an imitation module to force query sets to imitate the behaviors of support sets stored in the memory.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":3,"sentence":"A theoretical analysis is provided to prove the effectiveness of our method, and empirical results also demonstrate that our method outperforms competitive baselines on both text classification and generation tasks.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":4,"sentence":"Paraphrase generation has been widely used in various downstream tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"Most tasks benefit mainly from high quality paraphrases, namely those that are semantically similar to, yet linguistically diverse from, the original sentence.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":4,"sentence":"Recent works achieve nice results by controlling specific aspects of the paraphrase, such as its syntactic tree.","offset":2,"pro":0.2222222222222222,"labels":"BAC"},{"idx":4,"sentence":"However, they do not allow to directly control the quality of the generated paraphrase, and suffer from low flexibility and scalability.","offset":3,"pro":0.3333333333333333,"labels":"GAP"},{"idx":4,"sentence":"Here we propose QCPG, a quality-guided controlled paraphrase generation model, that allows directly controlling the quality dimensions.","offset":4,"pro":0.4444444444444444,"labels":"PUR"},{"idx":4,"sentence":"Furthermore, we suggest a method that given a sentence, identifies points in the quality control space that are expected to yield optimal generated paraphrases.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":4,"sentence":"The models, the code, and the data can be found in https://github.com/IBM/quality-controlled-paraphrase-generation.","offset":6,"pro":0.6666666666666666,"labels":"CTN"},{"idx":4,"sentence":"Generating high-quality paraphrases is challenging as it becomes increasingly hard to preserve meaning as linguistic diversity increases.","offset":7,"pro":0.7777777777777778,"labels":"BAC"},{"idx":4,"sentence":"We show that our method is able to generate paraphrases which maintain the original meaning while achieving higher diversity than the uncontrolled baseline.","offset":8,"pro":0.8888888888888888,"labels":"RST"},{"idx":5,"sentence":"Example sentences for targeted words in a dictionary play an important role to help readers understand the usage of words.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"In this paper, we introduce the problem of dictionary example sentence generation, aiming to automatically generate dictionary example sentences for targeted words according to the corresponding definitions.","offset":1,"pro":0.125,"labels":"PUR"},{"idx":5,"sentence":"Our proposed model can generate reasonable examples for targeted words, even for polysemous words.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":5,"sentence":"In addition, our model allows users to provide explicit control over attributes related to readability, such as length and lexical complexity, thus generating suitable examples for targeted audiences.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":5,"sentence":" To solve these problems, we propose a controllable target-word-aware model for this task.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":5,"sentence":"Traditionally, example sentences in a dictionary are usually created by linguistics experts, which are labor-intensive and knowledge-intensive.","offset":5,"pro":0.625,"labels":"GAP"},{"idx":5,"sentence":"This task is challenging especially for polysemous words, because the generated sentences need to reflect different usages and meanings of these targeted words.","offset":6,"pro":0.75,"labels":"PUR"},{"idx":5,"sentence":"Targeted readers may also have different backgrounds and educational levels.","offset":7,"pro":0.875,"labels":"PUR"},{"idx":6,"sentence":"Transfer learning with a unified Transformer framework (T5) that converts all language problems into a text-to-text format was recently proposed as a simple and effective transfer learning approach.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"Although a multilingual version of the T5 model (mT5) was also introduced, it is not clear how well it can fare on non-English tasks involving diverse data.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":6,"sentence":"To investigate this question, we apply mT5 on a language with a wide variety of dialects-Arabic.","offset":2,"pro":0.2222222222222222,"labels":"PUR"},{"idx":6,"sentence":"For model comparison, we pre-train three powerful Arabic T5-style models and evaluate them on ARGEN.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":6,"sentence":"Although pre-trained with asciitilde49 less data, our new models perform significantly better than mT5 on all ARGEN tasks (in 52 out of 59 test sets) and set several new SOTAs.","offset":4,"pro":0.4444444444444444,"labels":"RST"},{"idx":6,"sentence":"Our models also establish new SOTA on the recently-proposed, large Arabic language understanding evaluation benchmark ARLUE (Abdul-Mageed et al., 2021).","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":6,"sentence":"Our new models are publicly available.","offset":6,"pro":0.6666666666666666,"labels":"CTN"},{"idx":6,"sentence":"We also link to ARGEN datasets through our repository: https://github.com/UBC-NLP/araT5.","offset":7,"pro":0.7777777777777778,"labels":"CTN"},{"idx":6,"sentence":"For evaluation, we introduce a novel benchmark for ARabic language GENeration (ARGEN), covering seven important tasks.","offset":8,"pro":0.8888888888888888,"labels":"MTD"},{"idx":7,"sentence":"While significant progress has been made on the task of Legal Judgment Prediction (LJP) in recent years, the incorrect predictions made by SOTA LJP models can be attributed in part to their failure to (1) locate the key event information that determines the judgment, and (2) exploit the cross-task consistency constraints that exist among the subtasks of LJP.","offset":0,"pro":0,"labels":"GAP"},{"idx":7,"sentence":"To address these weaknesses, we propose EPM, an Event-based Prediction Model with constraints,","offset":1,"pro":0.5,"labels":"PUR"},{"idx":8,"sentence":"Pre-trained language models have recently shown that training on large corpora using the language modeling objective enables few-shot and zero-shot capabilities on a variety of NLP tasks, including commonsense reasoning tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"This is achieved using text interactions with the model, usually by posing the task as a natural language text completion problem.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":8,"sentence":"While using language model probabilities to obtain task specific scores has been generally useful, it often requires task-specific heuristics such as length normalization, or probability calibration.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":8,"sentence":"In this work, we consider the question answering format, where we need to choose from a set of (free-form) textual choices of unspecified lengths given a context.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":8,"sentence":"We analyze such biases using an associated F1-score.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":8,"sentence":"Our analysis indicates that answer-level calibration is able to remove such biases and leads to a more robust measure of model capability.","offset":5,"pro":0.625,"labels":"CLN"},{"idx":8,"sentence":"We present ALC (Answer-Level Calibration), where our main suggestion is to model context-independent biases in terms of the probability of a choice without the associated context and to subsequently remove it using an unsupervised estimate of similarity with the full context.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":8,"sentence":"We show that our unsupervised answer-level calibration consistently improves over or is competitive with baselines using standard evaluation metrics on a variety of tasks including commonsense reasoning tasks.","offset":7,"pro":0.875,"labels":"MTD"},{"idx":9,"sentence":"How to find proper moments to generate partial sentence translation given a streaming speech input?","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"Existing approaches waiting-and-translating for a fixed duration often break the acoustic units in speech, since the boundaries between acoustic units in speech are not even.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":9,"sentence":"In this paper, we propose MoSST, a simple yet effective method for translating streaming speech content.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":9,"sentence":"Given a usually long speech sequence, we develop an efficient monotonic segmentation module inside an encoder-decoder model to accumulate acoustic information incrementally and detect proper speech unit boundaries for the input in speech translation task.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":9,"sentence":"Our code is available at https://github.com/dqqcasia/mosst.","offset":4,"pro":0.6666666666666666,"labels":"CTN"},{"idx":9,"sentence":"Experiments on multiple translation directions of the MuST-C dataset show that outperforms existing methods and achieves the best trade-off between translation quality (BLEU) and latency.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":10,"sentence":"Transformer based re-ranking models can achieve high search relevance through context- aware soft matching of query tokens with document tokens.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"To alleviate runtime complexity of such inference, previous work has adopted a late interaction architecture with pre-computed contextual token representations at the cost of a large online storage.","offset":1,"pro":0.25,"labels":"GAP"},{"idx":10,"sentence":"This paper proposes contextual quantization of token embeddings by decoupling document-specific and document-independent ranking contributions during codebook-based compression.","offset":2,"pro":0.5,"labels":"PUR"},{"idx":10,"sentence":"This allows effective online decompression and embedding composition for better search relevance.","offset":3,"pro":0.75,"labels":"CTN"},{"idx":11,"sentence":"Early stopping, which is widely used to prevent overfitting, is generally based on a separate validation set.","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"However, in low resource settings, validation-based stopping can be risky because a small validation set may not be sufficiently representative, and the reduction in the number of samples by validation split may result in insufficient samples for training.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":11,"sentence":"In this study, we propose an early stopping method that uses unlabeled samples.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":11,"sentence":"The proposed method is based on confidence and class distribution similarities.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":11,"sentence":"Extensive experiments are conducted on five text classification datasets and several stop-methods are compared.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":11,"sentence":"Our code is available at https://github.com/DMCB-GIST/BUS-stop.","offset":5,"pro":0.625,"labels":"CTN"},{"idx":11,"sentence":"To further improve the performance, we present a calibration method to better estimate the class distribution of the unlabeled samples.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":11,"sentence":"The proposed method is advantageous because it does not require a separate validation set and provides a better stopping point by using a large unlabeled set.","offset":7,"pro":0.875,"labels":"MTD"},{"idx":12,"sentence":"The goal of meta-learning is to learn to adapt to a new task with only a few labeled examples.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"Compared to MAML which adapts the model through gradient descent, our method leverages the inductive bias of pre-trained LMs to perform pattern matching, and outperforms MAML by an absolute 6% average AUC-ROC score on BinaryClfs, gaining more advantage with increasing model size.","offset":1,"pro":0.2,"labels":"RST"},{"idx":12,"sentence":"On BinaryClfs, ICT improves the average AUC-ROC score by an absolute 10%, and reduces the variance due to example ordering by 6x and example choices by 2x.","offset":2,"pro":0.4,"labels":"RST"},{"idx":12,"sentence":" to form the input sequence, we concatenate the task instruction, labeled in-context examples, and the target input to predict; ","offset":3,"pro":0.6,"labels":"MTD"},{"idx":12,"sentence":"We benchmark our method on two collections of text classification tasks: LAMA and BinaryClfs.","offset":4,"pro":0.8,"labels":"MTD"},{"idx":13,"sentence":"Existing question answering (QA) techniques are created mainly to answer questions asked by humans.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"But in educational applications, teachers often need to decide what questions they should ask, in order to help students to improve their narrative understanding capabilities.","offset":1,"pro":0.25,"labels":"GAP"},{"idx":13,"sentence":"Our proposed QAG model architecture is demonstrated using a new expert-annotated FairytaleQA dataset, which has 278 child-friendly storybooks with 10,580 QA pairs.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":13,"sentence":"Automatic and human evaluations show that our model outperforms state-of-the-art QAG baseline systems. ","offset":3,"pro":0.75,"labels":"RST"},{"idx":14,"sentence":"Weakly-supervised learning (WSL) has shown promising results in addressing label scarcity on many NLP tasks, but manually designing a comprehensive, high-quality labeling rule set is tedious and difficult.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"We study interactive weakly-supervised learning-the problem of iteratively and automatically discovering novel labeling rules from data to improve the WSL model.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":14,"sentence":"Our proposed model, named PRBoost, achieves this goal via iterative prompt-based rule discovery and model boosting.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":14,"sentence":"It uses boosting to identify large-error instances and discovers candidate rules from them by prompting pre-trained LMs with rule templates.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":14,"sentence":"The candidate rules are judged by human experts, and the accepted rules are used to generate complementary weak labels and strengthen the current model.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":14,"sentence":"Experiments on four tasks show PRBoost outperforms state-of-the-art WSL baselines up to 7.1%, and bridges the gaps with fully supervised models.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":15,"sentence":"We examine the extent to which supervised bridging resolvers can be improved without employing additional labeled bridging data","offset":0,"pro":0,"labels":"PUR"},{"idx":15,"sentence":"by proposing a novel constrained multi-task learning framework for bridging resolution, within which we (1) design cross-task consistency constraints to guide the learning process; (2) pre-train the entity coreference model in the multi-task framework on the large amount of publicly available coreference data; and (3) integrating prior knowledge encoded in rule-based resolvers.","offset":1,"pro":0.3333333333333333,"labels":"MTD"},{"idx":15,"sentence":"Our approach achieves state-of-the-art results on three standard evaluation corpora.","offset":2,"pro":0.6666666666666666,"labels":"RST"},{"idx":16,"sentence":"Automatic evaluation metrics are essential for the rapid development of open-domain dialogue systems as they facilitate hyper-parameter tuning and comparison between models.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"Although recently proposed trainable conversation-level metrics have shown encouraging results, the quality of the metrics is strongly dependent on the quality of training data.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":16,"sentence":"Prior works mainly resort to heuristic text-level manipulations (e.g. utterances shuffling) to bootstrap incoherent conversations (negative examples) from coherent dialogues (positive examples).","offset":2,"pro":0.2222222222222222,"labels":"BAC"},{"idx":16,"sentence":"Such approaches are insufficient to appropriately reflect the incoherence that occurs in interactions between advanced dialogue models and humans.","offset":3,"pro":0.3333333333333333,"labels":"GAP"},{"idx":16,"sentence":"To tackle this problem, we propose DEAM, a Dialogue coherence Evaluation metric that relies on Abstract Meaning Representation (AMR) to apply semantic-level Manipulations for incoherent (negative) data generation.","offset":4,"pro":0.4444444444444444,"labels":"PUR"},{"idx":16,"sentence":"Our experiments show that DEAM achieves higher correlations with human judgments compared to baseline methods on several dialog datasets by significant margins.","offset":5,"pro":0.5555555555555556,"labels":"RST"},{"idx":16,"sentence":"We also show that DEAM can distinguish between coherent and incoherent dialogues generated by baseline manipulations, whereas those baseline models cannot detect incoherent examples generated by DEAM.","offset":6,"pro":0.6666666666666666,"labels":"RST"},{"idx":16,"sentence":"Our results demonstrate the potential of AMR-based semantic manipulations for natural negative example generation.","offset":7,"pro":0.7777777777777778,"labels":"CLN"},{"idx":16,"sentence":"AMRs naturally facilitate the injection of various types of incoherence sources, such as coreference inconsistency, irrelevancy, contradictions, and decrease engagement, at the semantic level, thus resulting in more natural incoherent samples.","offset":8,"pro":0.8888888888888888,"labels":"MTD"},{"idx":17,"sentence":"Document structure is critical for efficient information consumption.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"However, it is challenging to encode it efficiently into the modern Transformer architecture.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":17,"sentence":"In this work, we present HIBRIDS, which injects Hierarchical Biases foR Incorporating Document Structure into attention score calculation.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":17,"sentence":"We further present a new task, hierarchical question-summary generation, for summarizing salient content in the source document into a hierarchy of questions and summaries, where each follow-up question inquires about the content of its parent question-summary pair.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":17,"sentence":"We also annotate a new dataset with 6,153 question-summary hierarchies labeled on government reports.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":17,"sentence":"Experiment results show that our model produces better question-summary hierarchies than comparisons on both hierarchy quality and content coverage, a finding also echoed by human judges.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":17,"sentence":"Additionally, our model improves the generation of long-form summaries from long government reports and Wikipedia articles, as measured by ROUGE scores.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":18,"sentence":"Named entity recognition (NER) is a fundamental task to recognize specific types of entities from a given sentence.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"However, when the generative model is applied to NER, its optimization objective is not consistent with the task, which makes the model vulnerable to the incorrect biases.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":18,"sentence":"In this paper, we analyze the incorrect biases in the generation process from a causality perspective and attribute them to two confounders: pre-context confounder and entity-order confounder.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":18,"sentence":"Furthermore, we design Intra- and Inter-entity Deconfounding Data Augmentation methods to eliminate the above confounders according to the theory of backdoor adjustment.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":18,"sentence":"Depending on how the entities appear in the sentence, it can be divided into three subtasks, namely, Flat NER, Nested NER, and Discontinuous NER.","offset":4,"pro":0.5714285714285714,"labels":"BAC"},{"idx":18,"sentence":"Among the existing approaches, only the generative model can be uniformly adapted to these three subtasks.","offset":5,"pro":0.7142857142857143,"labels":"BAC"},{"idx":18,"sentence":"Experiments show that our method can improve the performance of the generative NER model in various datasets.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":19,"sentence":"Pre-trained language models derive substantial linguistic and factual knowledge from the massive corpora on which they are trained, and prompt engineering seeks to align these models to specific tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"Unfortunately, existing prompt engineering methods require significant amounts of labeled data, access to model parameters, or both.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":19,"sentence":"We introduce a new method for selecting prompt templates without labeled examples and without direct access to the model.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":19,"sentence":"Specifically, over a set of candidate templates, we choose the template that maximizes the mutual information between the input and the corresponding model output.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":19,"sentence":"Across 8 datasets representing 7 distinct NLP tasks, we show that when a template has high mutual information, it also has high accuracy on the task.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":19,"sentence":"On the largest model, selecting prompts with our method gets 90% of the way from the average prompt accuracy to the best prompt accuracy and requires no ground truth labels.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":0,"sentence":"The performance of multilingual pretrained models is highly dependent on the availability of monolingual or parallel text present in a target language.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"Thus, the majority of the world's languages cannot benefit from recent progress in NLP as they have no or limited textual data.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":0,"sentence":"To expand possibilities of using NLP technology in these under-represented languages, we systematically study strategies that relax the reliance on conventional language resources through the use of bilingual lexicons, an alternative resource with much better language coverage.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":0,"sentence":"We analyze different strategies to synthesize textual or labeled data using lexicons, and how this data can be combined with monolingual or parallel text when available.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":0,"sentence":"For 19 under-represented languages across 3 tasks, our methods lead to consistent improvements of up to 5 and 15 points with and without extra monolingual text respectively.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":0,"sentence":"Overall, our study highlights how NLP methods can be adapted to thousands more languages that are under-served by current technology.","offset":5,"pro":0.8333333333333334,"labels":"IMP"},{"idx":1,"sentence":"While BERT is an effective method for learning monolingual sentence embeddings for semantic similarity and embedding based transfer learning BERT based cross-lingual sentence embeddings have yet to be explored.","offset":0,"pro":0,"labels":"GAP"},{"idx":1,"sentence":"We systematically investigate methods for learning multilingual sentence embeddings by combining the best methods for learning monolingual and cross-lingual representations including: masked language modeling (MLM), translation language modeling (TLM), dual encoder translation ranking, and additive margin softmax.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":1,"sentence":"We show that introducing a pre-trained multilingual language model dramatically reduces the amount of parallel training data required to achieve good performance by 80%.","offset":2,"pro":0.3333333333333333,"labels":"CLN"},{"idx":1,"sentence":"Composing the best of these methods produces a model that achieves 83.7% bi-text retrieval accuracy over 112 languages on Tatoeba, well above the 65.5% achieved by LASER, while still performing competitively on monolingual transfer learning benchmarks.","offset":3,"pro":0.5,"labels":"RST"},{"idx":1,"sentence":"Parallel data mined from CommonCrawl using our best model is shown to train competitive NMT models for en-zh and en-de.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":1,"sentence":"We publicly release our best multilingual sentence embedding model for 109+ languages at https://tfhub.dev/google/LaBSE.","offset":5,"pro":0.8333333333333334,"labels":"CTN"},{"idx":2,"sentence":"Span-based methods with the neural networks backbone have great potential for the nested named entity recognition (NER) problem.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"However, they face problems such as degenerating when positive instances and negative instances largely overlap.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":2,"sentence":"Besides, the generalization ability matters a lot in nested NER, as a large proportion of entities in the test set hardly appear in the training set.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":2,"sentence":"In this work, we try to improve the span representation by utilizing retrieval-based span-level graphs, connecting spans and entities in the training data based on n-gram features.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":2,"sentence":"Specifically, we build the entity-entity graph and span-entity graph globally based on n-gram similarity to integrate the information of similar neighbor entities into the span representation.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":2,"sentence":"To evaluate our method, we conduct experiments on three common nested NER datasets, ACE2004, ACE2005, and GENIA datasets.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":2,"sentence":"Experimental results show that our method achieves general improvements on all three benchmarks (+0.30 sim 0.85 micro-F1), and obtains special superiority on low frequency entities (+0.56 sim 2.08 recall).","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":3,"sentence":"Is there a principle to guide transfer learning across tasks in natural language processing (NLP)?","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"Taxonomy (Zamir et al., 2018) finds that a structure exists among visual tasks, as a principle underlying transfer learning for them.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":3,"sentence":"In this paper, we propose a cognitively inspired framework, CogTaskonomy, to learn taxonomy for NLP tasks.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":3,"sentence":"The framework consists of Cognitive Representation Analytics (CRA) and Cognitive-Neural Mapping (CNM).","offset":3,"pro":0.375,"labels":"MTD"},{"idx":3,"sentence":"The former employs Representational Similarity Analysis, which is commonly used in computational neuroscience to find a correlation between brain-activity measurement and computational modeling, to estimate task similarity with task-specific sentence representations.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":3,"sentence":"The latter learns to detect task relations by projecting neural representations from NLP models to cognitive signals (i.e., fMRI voxels).","offset":5,"pro":0.625,"labels":"MTD"},{"idx":3,"sentence":"Experiments on 12 NLP tasks, where BERT/TinyBERT are used as the underlying models for transfer learning, demonstrate that the proposed CogTaxonomy is able to guide transfer learning, achieving performance competitive to the Analytic Hierarchy Process (Saaty, 1987) used in visual Taskonomy (Zamir et al., 2018) but without requiring exhaustive pairwise O(m^2) task transferring.","offset":6,"pro":0.75,"labels":"CLN"},{"idx":3,"sentence":"Analyses further discover that CNM is capable of learning model-agnostic task taxonomy.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":4,"sentence":"Large-scale pretrained language models have achieved SOTA results on NLP tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"However, they have been shown vulnerable to adversarial attacks especially for logographic languages like Chinese.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":4,"sentence":"The model takes as input multimodal information including the semantic, phonetic and visual features.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":4,"sentence":"We show all these features areimportant to the model robustness since the attack can be performed in all the three forms.","offset":3,"pro":0.375,"labels":"CLN"},{"idx":4,"sentence":"Across 5 Chinese NLU tasks, RoCBert outperforms strong baselines under three blackbox adversarial algorithms without sacrificing the performance on clean testset.","offset":4,"pro":0.5,"labels":"RST"},{"idx":4,"sentence":"It also performs the best in the toxic content detection task under human-made attacks.","offset":5,"pro":0.625,"labels":"RST"},{"idx":4,"sentence":"In this work, we propose RoCBert: a pretrained Chinese Bert that is robust to various forms of adversarial attacks like word perturbation, synonyms, typos, etc.","offset":6,"pro":0.75,"labels":"PUR"},{"idx":4,"sentence":"It is pretrained with the contrastive learning objective which maximizes the label consistency under different synthesized adversarial examples.","offset":7,"pro":0.875,"labels":"MTD"},{"idx":5,"sentence":"It is a common practice for recent works in vision language cross-modal reasoning to adopt a binary or multi-choice classification formulation taking as input a set of source image(s) and textual query.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"In this work, we take a sober look at such an unconditional formulation in the sense that no prior knowledge is specified with respect to the source image(s).","offset":1,"pro":0.25,"labels":"PUR"},{"idx":5,"sentence":"With selected high-quality movie screenshots and human-curated premise templates from 6 pre-defined categories, we ask crowd-source workers to write one true hypothesis and three distractors (4 choices) given the premise and image through a cross-check procedure.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":5,"sentence":"Inspired by the designs of both visual commonsense reasoning and natural language inference tasks, we propose a new task termed Premise-based Multi-modal Reasoning (PMR) where a textual premise is the background presumption on each source image.","offset":3,"pro":0.75,"labels":"MTD"},{"idx":6,"sentence":"Named entity recognition (NER) is a fundamental task in natural language processing.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"Recent works treat named entity recognition as a reading comprehension task, constructing type-specific queries manually to extract entities.","offset":1,"pro":0.09090909090909091,"labels":"BAC"},{"idx":6,"sentence":"This paradigm suffers from three issues.","offset":2,"pro":0.18181818181818182,"labels":"GAP"},{"idx":6,"sentence":"First, type-specific queries can only extract one type of entities per inference, which is inefficient.","offset":3,"pro":0.2727272727272727,"labels":"GAP"},{"idx":6,"sentence":"Second, the extraction for different types of entities is isolated, ignoring the dependencies between them.","offset":4,"pro":0.36363636363636365,"labels":"GAP"},{"idx":6,"sentence":"Third, query construction relies on external knowledge and is difficult to apply to realistic scenarios with hundreds of entity types.","offset":5,"pro":0.45454545454545453,"labels":"GAP"},{"idx":6,"sentence":"To deal with them, we propose Parallel Instance Query Network (PIQN), which sets up global and learnable instance queries to extract entities from a sentence in a parallel manner.","offset":6,"pro":0.5454545454545454,"labels":"PUR"},{"idx":6,"sentence":"Each instance query predicts one entity, and by feeding all instance queries simultaneously, we can query all entities in parallel.","offset":7,"pro":0.6363636363636364,"labels":"MTD"},{"idx":6,"sentence":"Instead of being constructed from external knowledge, instance queries can learn their different query semantics during training.","offset":8,"pro":0.7272727272727273,"labels":"MTD"},{"idx":6,"sentence":"For training the model, we treat label assignment as a one-to-many Linear Assignment Problem (LAP) and dynamically assign gold entities to instance queries with minimal assignment cost.","offset":9,"pro":0.8181818181818182,"labels":"MTD"},{"idx":6,"sentence":"Experiments on both nested and flat NER datasets demonstrate that our proposed method outperforms previous state-of-the-art models.","offset":10,"pro":0.9090909090909091,"labels":"CLN"},{"idx":7,"sentence":"Typical generative dialogue models utilize the dialogue history to generate the response.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"However, since one dialogue utterance can often be appropriately answered by multiple distinct responses, generating a desired response solely based on the historical information is not easy.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":7,"sentence":"Intuitively, if the chatbot can foresee in advance what the user would talk about (i.e., the dialogue future) after receiving its response, it could possibly provide a more informative response.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":7,"sentence":"Accordingly, we propose a novel dialogue generation framework named ProphetChat that utilizes the simulated dialogue futures in the inference phase to enhance response generation.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":7,"sentence":"With the simulated futures, we then utilize the ensemble of a history-to-response generator and a future-to-response generator to jointly generate a more informative response.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":7,"sentence":"Experiments on two popular open-domain dialogue datasets demonstrate that ProphetChat can generate better responses over strong baselines, which validates the advantages of incorporating the simulated dialogue futures.","offset":5,"pro":0.625,"labels":"CLN"},{"idx":7,"sentence":"To enable the chatbot to foresee the dialogue future","offset":6,"pro":0.75,"labels":"PUR"},{"idx":7,"sentence":"we design a beam-search-like roll-out strategy for dialogue future simulation using a typical dialogue generation model and a dialogue selector","offset":7,"pro":0.875,"labels":"MTD"},{"idx":8,"sentence":"Fusion-in-decoder (Fid) (Izacard and Grave, 2020) is a generative question answering (QA) model that leverages passage retrieval with a pre-trained transformer and pushed the state of the art on single-hop QA.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"However, the complexity of multi-hop QA hinders the effectiveness of the generative QA approach.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":8,"sentence":"In this work, we propose a simple generative approach (PathFid) that extends the task beyond just answer generation by explicitly modeling the reasoning process to resolve the answer for multi-hop questions.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":8,"sentence":"By linearizing the hierarchical reasoning path of supporting passages, their key sentences, and finally the factoid answer, we cast the problem as a single sequence prediction task.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":8,"sentence":"Our extensive experiments demonstrate that PathFid leads to strong performance gains on two multi-hop QA datasets: HotpotQA and IIRC.","offset":4,"pro":0.5714285714285714,"labels":"CLN"},{"idx":8,"sentence":"Besides the performance gains, PathFid is more interpretable, which in turn yields answers that are more faithfully grounded to the supporting passages and facts compared to the baseline Fid model.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":8,"sentence":"To facilitate complex reasoning with multiple clues, ","offset":6,"pro":0.8571428571428571,"labels":"PUR"},{"idx":9,"sentence":"Multilingual pre-trained models are able to zero-shot transfer knowledge from rich-resource to low-resource languages in machine reading comprehension (MRC).","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"However, inherent linguistic discrepancies in different languages could make answer spans predicted by zero-shot transfer violate syntactic constraints of the target language.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":9,"sentence":"In this paper, we propose a novel multilingual MRC framework equipped with a Siamese Semantic Disentanglement Model (S2DM) to disassociate semantics from syntax in representations learned by multilingual pre-trained models.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":9,"sentence":"Experimental results on three multilingual MRC datasets (i.e., XQuAD, MLQA, and TyDi QA) demonstrate the effectiveness of our proposed approach over models based on mBERT and XLM-100.","offset":3,"pro":0.5,"labels":"CLN"},{"idx":9,"sentence":"To explicitly transfer only semantic knowledge to the target language","offset":4,"pro":0.6666666666666666,"labels":"PUR"},{"idx":9,"sentence":"we propose two groups of losses tailored for semantic and syntactic encoding and disentanglement","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":10,"sentence":"Transferring the knowledge to a small model through distillation has raised great interest in recent years.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"Prevailing methods transfer the knowledge derived from mono-granularity language units (e.g., token-level or sample-level), which is not enough to represent the rich semantics of a text and may lose some vital knowledge.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":10,"sentence":"Besides, these methods form the knowledge as individual representations or their simple dependencies, neglecting abundant structural relations among intermediate representations.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":10,"sentence":"To overcome the problems, we present a novel knowledge distillation framework that gathers intermediate representations from multiple semantic granularities (e.g., tokens, spans and samples) and forms the knowledge as more sophisticated structural relations specified as the pair-wise interactions and the triplet-wise geometric angles based on multi-granularity representations.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":10,"sentence":"Moreover, we propose distilling the well-organized multi-granularity structural knowledge to the student hierarchically across layers.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":10,"sentence":"Experimental results on GLUE benchmark demonstrate that our method outperforms advanced distillation methods.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":11,"sentence":"Human-like biases and undesired social stereotypes exist in large pretrained language models.","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"Given the wide adoption of these models in real-world applications, mitigating such biases has become an emerging and important task.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":11,"sentence":"In this paper, we propose an automatic method to mitigate the biases in pretrained language models.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":11,"sentence":"Different from previous debiasing work that uses external corpora to fine-tune the pretrained models, we instead directly probe the biases encoded in pretrained models through prompts.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":11,"sentence":"Specifically, we propose a variant of the beam search method to automatically search for biased prompts such that the cloze-style completions are the most different with respect to different demographic groups.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":11,"sentence":"Given the identified biased prompts, we then propose a distribution alignment loss to mitigate the biases.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":11,"sentence":"Experiment results on standard datasets and metrics show that our proposed Auto-Debias approach can significantly reduce biases, including gender and racial bias, in pretrained language models such as BERT, RoBERTa and ALBERT.","offset":6,"pro":0.75,"labels":"CLN"},{"idx":11,"sentence":"Moreover, the improvement in fairness does not decrease the language models' understanding abilities, as shown using the GLUE benchmark.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":12,"sentence":"Most dialog systems posit that users have figured out clear and specific goals before starting an interaction.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"For example, users have determined the departure, the destination, and the travel time for booking a flight.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":12,"sentence":"However, in many scenarios, limited by experience and knowledge, users may know what they need, but still struggle to figure out clear and specific goals by determining all the necessary slots.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":12,"sentence":"In this paper, we identify this challenge, and make a step forward by collecting a new human-to-human mixed-type dialog corpus.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":12,"sentence":"It contains 5k dialog sessions and 168k utterances for 4 dialog types and 5 domains.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":12,"sentence":"Within each session, an agent first provides user-goal-related knowledge to help figure out clear and specific goals, and then help achieve them.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":12,"sentence":"Furthermore, we propose a mixed-type dialog model with a novel Prompt-based continual learning mechanism.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":12,"sentence":"Specifically, the mechanism enables the model to continually strengthen its ability on any specific type by utilizing existing dialog corpora effectively.","offset":7,"pro":0.875,"labels":"MTD"},{"idx":13,"sentence":"Supervised parsing models have achieved impressive results on in-domain texts.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"However, their performances drop drastically on out-of-domain texts due to the data distribution shift.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":13,"sentence":"The shared-private model has shown its promising advantages for alleviating this problem via feature separation, whereas prior works pay more attention to enhance shared features but neglect the in-depth relevance of specific ones.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":13,"sentence":"To address this issue, we for the first time apply a dynamic matching network on the shared-private model for semi-supervised cross-domain dependency parsing.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":13,"sentence":"Meanwhile, considering the scarcity of target-domain labeled data, we leverage unlabeled data from two aspects, i.e., designing a new training strategy to improve the capability of the dynamic matching network and fine-tuning BERT to obtain domain-related contextualized representations.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":13,"sentence":"Experiments on benchmark datasets show that our proposed model consistently outperforms various baselines, leading to new state-of-the-art results on all domains.","offset":5,"pro":0.625,"labels":"CLN"},{"idx":13,"sentence":"Detailed analysis on different matching strategies demonstrates that it is essential to learn suitable matching weights to emphasize useful features and ignore useless or even harmful ones.","offset":6,"pro":0.75,"labels":"CLN"},{"idx":13,"sentence":"Besides, our proposed model can be directly extended to multi-source domain adaptation and achieves best performances among various baselines, further verifying the effectiveness and robustness.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":14,"sentence":"Given the prevalence of pre-trained contextualized representations in today's NLP, there have been many efforts to understand what information they contain, and why they seem to be universally successful.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"The most common approach to use these representations involves fine-tuning them for an end task.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":14,"sentence":"Yet, how fine-tuning changes the underlying embedding space is less studied.","offset":2,"pro":0.25,"labels":"BAC"},{"idx":14,"sentence":"In this work, we study the English BERT family and use two probing techniques to analyze how fine-tuning changes the space.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":14,"sentence":"We hypothesize that fine-tuning affects classification performance by increasing the distances between examples associated with different labels.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":14,"sentence":"We confirm this hypothesis with carefully designed experiments on five different NLP tasks.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":14,"sentence":"Via these experiments, we also discover an exception to the prevailing wisdom that fine-tuning always improves performance.","offset":6,"pro":0.75,"labels":"CLN"},{"idx":14,"sentence":"Finally, by comparing the representations before and after fine-tuning, we discover that fine-tuning does not introduce arbitrary changes to representations; instead, it adjusts the representations to downstream tasks while largely preserving the original spatial structure of the data points.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":15,"sentence":"Training dense passage representations via contrastive learning has been shown effective for Open-Domain Passage Retrieval (ODPR).","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"Existing studies focus on further optimizing by improving negative sampling strategy or extra pretraining.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":15,"sentence":"However, these studies keep unknown in capturing passage with internal representation conflicts from improper modeling granularity.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":15,"sentence":"Specifically, under our observation that a passage can be organized by multiple semantically different sentences, modeling such a passage as a unified dense vector is not optimal.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":15,"sentence":"This work thus presents a refined model on the basis of a smaller granularity, contextual sentences, to alleviate the concerned conflicts.","offset":4,"pro":0.5,"labels":"PUR"},{"idx":15,"sentence":"In detail, we introduce an in-passage negative sampling strategy to encourage a diverse generation of sentence representations within the same passage.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":15,"sentence":"Experiments on three benchmark datasets verify the efficacy of our method, especially on datasets where conflicts are severe.","offset":6,"pro":0.75,"labels":"CLN"},{"idx":15,"sentence":"Extensive experiments further present good transferability of our method across datasets.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":16,"sentence":"Transformers have been shown to be able to perform deductive reasoning on a logical rulebase containing rules and statements written in natural language.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"Recent works show that such models can also produce the reasoning steps (i.e., the proof graph) that emulate the model's logical reasoning process.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":16,"sentence":"Currently, these black-box models generate both the proof graph and intermediate inferences within the same model and thus may be unfaithful.","offset":2,"pro":0.2222222222222222,"labels":"BAC"},{"idx":16,"sentence":"In this work, we frame the deductive logical reasoning task by defining three modular components: rule selection, fact selection, and knowledge composition.","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":16,"sentence":"The rule and fact selection steps select the candidate rule and facts to be used and then the knowledge composition combines them to generate new inferences.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":16,"sentence":"This ensures model faithfulness by assured causal relation from the proof step to the inference reasoning.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":16,"sentence":"To test our framework, we propose FaiRR (Faithful and Robust Reasoner) where the above three components are independently modeled by transformers.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":16,"sentence":"We observe that FaiRR is robust to novel language perturbations, and is faster at inference than previous works on existing reasoning datasets.","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":16,"sentence":"Additionally, in contrast to black-box generative models, the errors made by FaiRR are more interpretable due to the modular approach.","offset":8,"pro":0.8888888888888888,"labels":"RST"},{"idx":17,"sentence":"Tables are often created with hierarchies, but existing works on table reasoning mainly focus on flat tables and neglect hierarchical tables.","offset":0,"pro":0,"labels":"GAP"},{"idx":17,"sentence":"Hierarchical tables challenge numerical reasoning by complex hierarchical indexing, as well as implicit relationships of calculation and semantics.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":17,"sentence":"We present a new dataset, HiTab, to study question answering (QA) and natural language generation (NLG) over hierarchical tables.","offset":2,"pro":0.2222222222222222,"labels":"PUR"},{"idx":17,"sentence":"HiTab is a cross-domain dataset constructed from a wealth of statistical reports and Wikipedia pages, and has unique characteristics: (1) nearly all tables are hierarchical, and (2) QA pairs are not proposed by annotators from scratch, but are revised from real and meaningful sentences authored by analysts.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":17,"sentence":"Experiments suggest that this HiTab presents a strong challenge for existing baselines and a valuable benchmark for future research.","offset":4,"pro":0.4444444444444444,"labels":"CLN"},{"idx":17,"sentence":"(3) to reveal complex numerical reasoning in statistical reports,","offset":5,"pro":0.5555555555555556,"labels":"PUR"},{"idx":17,"sentence":"Targeting table reasoning, we leverage entity and quantity alignment to explore partially supervised training in QA and conditional generation in NLG,","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":17,"sentence":"and largely reduce spurious predictions in QA and produce better descriptions in NLG.","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":17,"sentence":"Targeting hierarchical structure, we devise a hierarchy-aware logical form for symbolic reasoning over tables, ","offset":8,"pro":0.8888888888888888,"labels":"MTD"},{"idx":18,"sentence":"Huge volumes of patient queries are daily generated on online health forums, rendering manual doctor allocation a labor-intensive task.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"To better help patients, this paper studies a novel task of doctor recommendation to enable automatic pairing of a patient to a doctor with relevant expertise.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":18,"sentence":"While most prior work in recommendation focuses on modeling target users from their past behavior, we can only rely on the limited words in a query to infer a patient's needs for privacy reasons.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":18,"sentence":"For doctor modeling, we study the joint effects of their profiles and previous dialogues with other patients and explore their interactions via self-learning.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":18,"sentence":"The learned doctor embeddings are further employed to estimate their capabilities of handling a patient query with a multi-head attention mechanism.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":18,"sentence":"For experiments, a large-scale dataset is collected from Chunyu Yisheng, a Chinese online health forum, where our model exhibits the state-of-the-art results, ","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":19,"sentence":"A desirable dialog system should be able to continually learn new skills without forgetting old ones, and thereby adapt to new domains or tasks in its life cycle.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"However, continually training a model often leads to a well-known catastrophic forgetting issue.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":19,"sentence":"In this paper, we present Continual Prompt Tuning, a parameter-efficient framework that not only avoids forgetting but also enables knowledge transfer between tasks.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":19,"sentence":"To avoid forgetting, we only learn and store a few prompt tokens' embeddings for each task while freezing the backbone pre-trained model.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":19,"sentence":"Extensive experiments demonstrate the effectiveness and efficiency of our proposed method on continual learning for dialog state tracking, compared with state-of-the-art baselines.","offset":4,"pro":0.5714285714285714,"labels":"CLN"},{"idx":19,"sentence":"To achieve bi-directional knowledge transfer among tasks,","offset":5,"pro":0.7142857142857143,"labels":"PUR"},{"idx":19,"sentence":"we propose several techniques (continual prompt initialization, query fusion, and memory replay) to transfer knowledge from preceding tasks and a memory-guided technique to transfer knowledge from subsequent tasks.","offset":6,"pro":0.8571428571428571,"labels":"MTD"},{"idx":0,"sentence":"Images are often more significant than only the pixels to human eyes, as we can infer, associate, and reason with contextual information from other sources to establish a more complete picture.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"For example, in Figure 1, we can find a way to identify the news articles related to the picture through segment-wise understandings of the signs, the buildings, the crowds, and more.","offset":1,"pro":0.2,"labels":"RST"},{"idx":0,"sentence":"This reasoning could provide the time and place the image was taken, which will help us in subsequent tasks, such as automatic storyline construction, correction of image source in intended effect photographs, and upper-stream processing such as image clustering for certain location or time.In this work, we formulate this problem and introduce TARA: a dataset with 16k images with their associated news, time, and location, automatically extracted from New York Times, and an additional 61k examples as distant supervision from WIT.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":0,"sentence":"On top of the extractions, we present a crowdsourced subset in which we believe it is possible to find the images' spatio-temporal information for evaluation purpose.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":0,"sentence":"We show that there exists a 70% gap between a state-of-the-art joint model and human performance, which is slightly filled by our proposed model that uses segment-wise reasoning, motivating higher-level vision-language joint models that can conduct open-ended reasoning with world knowledge.The data and code are publicly available at https://github.com/zeyofu/TARA.","offset":4,"pro":0.8,"labels":"RST"},{"idx":1,"sentence":"Tables store rich numerical data, but numerical reasoning over tables is still a challenge.","offset":0,"pro":0,"labels":"GAP"},{"idx":1,"sentence":"In this paper, we find that the spreadsheet formula, a commonly used language to perform computations on numerical values in spreadsheets, is a valuable supervision for numerical reasoning in tables.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":1,"sentence":"Considering large amounts of spreadsheets available on the web, we propose FORTAP, the first exploration to leverage spreadsheet formulas for table pretraining.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":1,"sentence":"Two novel self-supervised pretraining objectives are derived from formulas, numerical reference prediction (NRP) and numerical calculation prediction (NCP).","offset":3,"pro":0.5,"labels":"MTD"},{"idx":1,"sentence":"While our proposed objectives are generic for encoders, to better capture spreadsheet table layouts and structures, FORTAP is built upon TUTA, the first transformer-based method for spreadsheet table pretraining with tree attention.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":1,"sentence":"FORTAP outperforms state-of-the-art methods by large margins on three representative datasets of formula prediction, question answering, and cell type classification, showing the great potential of leveraging formulas for table pretraining.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":2,"sentence":"Information integration from different modalities is an active area of research.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"Human beings and, in general, biological neural systems are quite adept at using a multitude of signals from different sensory perceptive fields to interact with the environment and each other.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":2,"sentence":"Recent work in deep fusion models via neural networks has led to substantial improvements over unimodal approaches in areas like speech recognition, emotion recognition and analysis, captioning and image description.","offset":2,"pro":0.4,"labels":"BAC"},{"idx":2,"sentence":"However, such research has mostly focused on architectural changes allowing for fusion of different modalities while keeping the model complexity manageable.Inspired by neuroscientific ideas about multisensory integration and processing, we investigate the effect of introducing neural dependencies in the loss functions.","offset":3,"pro":0.6,"labels":"GAP"},{"idx":2,"sentence":"Experiments on multimodal sentiment analysis tasks with different models show that our approach provides a consistent performance boost.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":3,"sentence":"Procedural Multimodal Documents (PMDs) organize textual instructions and corresponding images step by step.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"Comprehending PMDs and inducing their representations for the downstream reasoning tasks is designated as Procedural MultiModal Machine Comprehension (M3C).","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":3,"sentence":"In this study, we approach Procedural M3C at a fine-grained level (compared with existing explorations at a document or sentence level), that is, entity.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":3,"sentence":"With delicate consideration, we model entity both in its temporal and cross-modal relation and propose a novel Temporal-Modal Entity Graph (TMEG).","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":3,"sentence":"Specifically, graph structure is formulated to capture textual and visual entities and trace their temporal-modal evolution.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":3,"sentence":"In addition, a graph aggregation module is introduced to conduct graph encoding and reasoning.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":3,"sentence":"Comprehensive experiments across three Procedural M3C tasks are conducted on a traditional dataset RecipeQA and our new dataset CraftQA, which can better evaluate the generalization of TMEG.","offset":6,"pro":0.8571428571428571,"labels":"MTD"},{"idx":4,"sentence":"Pre-trained sequence-to-sequence language models have led to widespread success in many natural language generation tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"However, there has been relatively less work on analyzing their ability to generate structured outputs such as graphs.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":4,"sentence":"Unlike natural language, graphs have distinct structural and semantic properties in the context of a downstream NLP task, e.g., generating a graph that is connected and acyclic can be attributed to its structural constraints, while the semantics of a graph can refer to how meaningfully an edge represents the relation between two node concepts.","offset":2,"pro":0.2222222222222222,"labels":"BAC"},{"idx":4,"sentence":"In this work, we study pre-trained language models that generate explanation graphs in an end-to-end manner and analyze their ability to learn the structural constraints and semantics of such graphs.","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":4,"sentence":"We first show that with limited supervision, pre-trained language models often generate graphs that either violate these constraints or are semantically incoherent.","offset":4,"pro":0.4444444444444444,"labels":"RST"},{"idx":4,"sentence":"Since curating large amount of human-annotated graphs is expensive and tedious, we propose simple yet effective ways of graph perturbations via node and edge edit operations that lead to structurally and semantically positive and negative graphs.","offset":5,"pro":0.5555555555555556,"labels":"PUR"},{"idx":4,"sentence":"Next, we leverage these graphs in different contrastive learning models with Max-Margin and InfoNCE losses.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":4,"sentence":"Our methods lead to significant improvements in both structural and semantic accuracy of explanation graphs and also generalize to other similar graph generation tasks.","offset":7,"pro":0.7777777777777778,"labels":"CLN"},{"idx":4,"sentence":"Lastly, we show that human errors are the best negatives for contrastive learning and also that automatically generating more such human-like negative graphs can lead to further improvements.","offset":8,"pro":0.8888888888888888,"labels":"RST"},{"idx":5,"sentence":"Opinion summarization is the task of automatically generating summaries that encapsulate information expressed in multiple user reviews.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"We present Semantic Autoencoder (SemAE) to perform extractive opinion summarization in an unsupervised manner.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":5,"sentence":"SemAE uses dictionary learning to implicitly capture semantic information from the review text and learns a latent representation of each sentence over semantic units.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":5,"sentence":"Our extractive summarization algorithm leverages the representations to identify representative opinions among hundreds of reviews.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":5,"sentence":"SemAE is also able to perform controllable summarization to generate aspect-specific summaries using only a few samples.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":5,"sentence":"We report strong performance on SPACE and AMAZON datasets and perform experiments to investigate the functioning of our model.","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":6,"sentence":"Lexical substitution is the task of generating meaningful substitutes for a word in a given textual context.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"Contextual word embedding models have achieved state-of-the-art results in the lexical substitution task by relying on contextual information extracted from the replaced word within the sentence.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":6,"sentence":"However, such models do not take into account structured knowledge that exists in external lexical databases.We introduce LexSubCon, an end-to-end lexical substitution framework based on contextual embedding models that can identify highly-accurate substitute candidates.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":6,"sentence":"This is achieved by combining contextual information with knowledge from structured lexical resources.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":6,"sentence":"Our approach involves: (i) introducing a novel mix-up embedding strategy to the target word's embedding through linearly interpolating the pair of the target input embedding and the average embedding of its probable synonyms; (ii) considering the similarity of the sentence-definition embeddings of the target word and its proposed candidates; and, (iii) calculating the effect of each substitution on the semantics of the sentence through a fine-tuned sentence similarity model.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":6,"sentence":"Our experiments show that LexSubCon outperforms previous state-of-the-art methods by at least 2% over all the official lexical substitution metrics on LS07 and CoInCo benchmark datasets that are widely used for lexical substitution tasks.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":7,"sentence":"Implicit knowledge, such as common sense, is key to fluid human conversations.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"Current neural response generation (RG) models are trained to generate responses directly, omitting unstated implicit knowledge.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":7,"sentence":"In this paper, we present Think-Before-Speaking (TBS), a generative approach to first externalize implicit commonsense knowledge (think) and use this knowledge to generate responses (speak).","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":7,"sentence":"We argue that externalizing implicit knowledge allows more efficient learning, produces more informative responses, and enables more explainable models.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":7,"sentence":"We analyze different choices to collect knowledge-aligned dialogues, represent implicit knowledge, and transition between knowledge and dialogues.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":7,"sentence":"Empirical results show TBS models outperform end-to-end and knowledge-augmented RG baselines on most automatic metrics and generate more informative, specific, and commonsense-following responses, as evaluated by human annotators.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":7,"sentence":"TBS also generates knowledge that makes sense and is relevant to the dialogue around 85% of the time","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":8,"sentence":"In this work, we propose a flow-adapter architecture for unsupervised NMT.","offset":0,"pro":0,"labels":"PUR"},{"idx":8,"sentence":"It leverages normalizing flows to explicitly model the distributions of sentence-level latent representations, which are subsequently used in conjunction with the attention mechanism for the translation task.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":8,"sentence":"The primary novelties of our model are: (a) capturing language-specific sentence representations separately for each language using normalizing flows and (b) using a simple transformation of these latent representations for translating from one language to another.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":8,"sentence":"This architecture allows for unsupervised training of each language independently.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":8,"sentence":"While there is prior work on latent variables for supervised MT, to the best of our knowledge, this is the first work that uses latent variables and normalizing flows for unsupervised MT.","offset":4,"pro":0.6666666666666666,"labels":"CTN"},{"idx":8,"sentence":"We obtain competitive results on several unsupervised MT benchmarks.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":9,"sentence":"Sentence compression reduces the length of text by removing non-essential content while preserving important facts and grammaticality.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"Unsupervised objective driven methods for sentence compression can be used to create customized models without the need for ground-truth training data, while allowing flexibility in the objective function(s) that are used for learning and inference.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":9,"sentence":"Recent unsupervised sentence compression approaches use custom objectives to guide discrete search; however, guided search is expensive at inference time.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":9,"sentence":"In this work, we explore the use of reinforcement learning to train effective sentence compression models that are also fast when generating predictions.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":9,"sentence":"In particular, we cast the task as binary sequence labelling and fine-tune a pre-trained transformer using a simple policy gradient approach.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":9,"sentence":"Our approach outperforms other unsupervised models while also being more efficient at inference time.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":10,"sentence":"Machine reading comprehension is a heavily-studied research and test field for evaluating new pre-trained language models (PrLMs) and fine-tuning strategies, and recent studies have enriched the pre-trained language models with syntactic, semantic and other linguistic information to improve the performance of the models.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"In this paper, we imitate the human reading process in connecting the anaphoric expressions and explicitly leverage the coreference information of the entities to enhance the word embeddings from the pre-trained language model, in order to highlight the coreference mentions of the entities that must be identified for coreference-intensive question answering in QUOREF, a relatively new dataset that is specifically designed to evaluate the coreference-related performance of a model.","offset":1,"pro":0.25,"labels":"PUR"},{"idx":10,"sentence":"We use two strategies to fine-tune a pre-trained language model, namely, placing an additional encoder layer after a pre-trained language model to focus on the coreference mentions or constructing a relational graph convolutional network to model the coreference relations.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":10,"sentence":"We demonstrate that the explicit incorporation of coreference information in the fine-tuning stage performs better than the incorporation of the coreference information in pre-training a language model.","offset":3,"pro":0.75,"labels":"CLN"},{"idx":11,"sentence":"We contribute a new dataset for the task of automated fact checking and an evaluation of state of the art algorithms.","offset":0,"pro":0,"labels":"MTD"},{"idx":11,"sentence":"The dataset includes claims (from speeches, interviews, social media and news articles), review articles published by professional fact checkers and premise articles used by those professional fact checkers to support their review and verify the veracity of the claims.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":11,"sentence":"An important challenge in the use of premise articles is the identification of relevant passages that will help to infer the veracity of a claim.","offset":2,"pro":0.4,"labels":"BAC"},{"idx":11,"sentence":"We show that transferring a dense passage retrieval model trained with review articles improves the retrieval quality of passages in premise articles.","offset":3,"pro":0.6,"labels":"CLN"},{"idx":11,"sentence":"We report results for the prediction of claim veracity by inference from premise articles.","offset":4,"pro":0.8,"labels":"RST"},{"idx":12,"sentence":"Fast and reliable evaluation metrics are key to R{&D progress.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"While traditional natural language generation metrics are fast, they are not very reliable.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":12,"sentence":"Conversely, new metrics based on large pretrained language models are much more reliable, but require significant computational resources.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":12,"sentence":"In this paper, we propose FrugalScore, an approach to learn a fixed, low cost version of any expensive NLG metric, while retaining most of its original performance.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":12,"sentence":"Experiments with BERTScore and MoverScore on summarization and translation show that FrugalScore is on par with the original metrics (and sometimes better), while having several orders of magnitude less parameters and running several times faster.","offset":4,"pro":0.5714285714285714,"labels":"CLN"},{"idx":12,"sentence":"On average over all learned metrics, tasks, and variants, FrugalScore retains 96.8% of the performance, runs 24 times faster, and has 35 times less parameters than the original metrics.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":12,"sentence":"We make our trained metrics publicly available, to benefit the entire NLP community and in particular researchers and practitioners with limited resources.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":13,"sentence":"We propose Composition Sampling, a simple but effective method to generate diverse outputs for conditional generation of higher quality compared to previous stochastic decoding strategies.","offset":0,"pro":0,"labels":"MTD"},{"idx":13,"sentence":"It builds on recently proposed plan-based neural generation models (FROST, Narayan et al, 2021) that are trained to first create a composition of the output and then generate by conditioning on it and the input.","offset":1,"pro":0.25,"labels":"MTD"},{"idx":13,"sentence":"Our approach avoids text degeneration by first sampling a composition in the form of an entity chain and then using beam search to generate the best possible text grounded to this entity chain.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":13,"sentence":"Experiments on summarization (CNN/DailyMail and XSum) and question generation (SQuAD), using existing and newly proposed automaticmetrics together with human-based evaluation, demonstrate that Composition Sampling is currently the best available decoding strategy for generating diverse meaningful outputs.","offset":3,"pro":0.75,"labels":"CLN"},{"idx":14,"sentence":"Synthesizing QA pairs with a question generator (QG) on the target domain has become a popular approach for domain adaptation of question answering (QA) models.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"Since synthetic questions are often noisy in practice, existing work adapts scores from a pretrained QA (or QG) model as criteria to select high-quality questions.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":14,"sentence":"However, these scores do not directly serve the ultimate goal of improving QA performance on the target domain.","offset":2,"pro":0.3333333333333333,"labels":"RST"},{"idx":14,"sentence":"In this paper, we introduce a novel idea of training a question value estimator (QVE) that directly estimates the usefulness of synthetic questions for improving the target-domain QA performance.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":14,"sentence":"By conducting comprehensive experiments, we show that the synthetic questions selected by QVE can help achieve better target-domain QA performance, in comparison with existing techniques.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":14,"sentence":"We additionally show that by using such questions and only around 15% of the human annotations on the target domain, we can achieve comparable performance to the fully-supervised baselines.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":15,"sentence":"Class-based language models (LMs) have been long devised to address context sparsity in n-gram LMs.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"In this study, we revisit this approach in the context of neural LMs.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":15,"sentence":"We hypothesize that class-based prediction leads to an implicit context aggregation for similar words and thus can improve generalization for rare words.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":15,"sentence":"We map words that have a common WordNet hypernym to the same class and train large neural LMs by gradually annealing from predicting the class to token prediction during training.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":15,"sentence":"Empirically, this curriculum learning strategy consistently improves perplexity over various large, highly-performant state-of-the-art Transformer-based models on two datasets, WikiText-103 and ARXIV.","offset":4,"pro":0.5714285714285714,"labels":"CLN"},{"idx":15,"sentence":"Our analysis shows that the performance improvement is achieved without sacrificing performance on rare words.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":15,"sentence":"Finally, we document other attempts that failed to yield empirical gains, and discuss future directions for the adoption of class-based LMs on a larger scale.","offset":6,"pro":0.8571428571428571,"labels":"GAP"},{"idx":16,"sentence":"Easy access, variety of content, and fast widespread interactions are some of the reasons making social media increasingly popular.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"However, this rise has also enabled the propagation of fake news, text published by news sources with an intent to spread misinformation and sway beliefs.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":16,"sentence":"Detecting it is an important and challenging problem to prevent large scale misinformation and maintain a healthy society.","offset":2,"pro":0.3333333333333333,"labels":"BAC"},{"idx":16,"sentence":"We view fake news detection as reasoning over the relations between sources, articles they publish, and engaging users on social media in a graph framework.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":16,"sentence":"After embedding this information, we formulate inference operators which augment the graph edges by revealing unobserved interactions between its elements, such as similarity between documents' contents and users' engagement patterns.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":16,"sentence":"Our experiments over two challenging fake news detection tasks show that using inference operators leads to a better understanding of the social media framework enabling fake news spread, resulting in improved performance.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":17,"sentence":"Knowledge base (KB) embeddings have been shown to contain gender biases.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"In this paper, we study two questions regarding these biases: how to quantify them, and how to trace their origins in KB?","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":17,"sentence":"Specifically, first, we develop two novel bias measures respectively for a group of person entities and an individual person entity.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":17,"sentence":"Evidence of their validity is observed by comparison with real-world census data.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":17,"sentence":"Second, we use the influence function to inspect the contribution of each triple in KB to the overall group bias.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":17,"sentence":"To exemplify the potential applications of our study, we also present two strategies (by adding and removing KB triples) to mitigate gender biases in KB embeddings.","offset":5,"pro":0.8333333333333334,"labels":"PUR"},{"idx":18,"sentence":"South Asia is home to a plethora of languages, many of which severely lack access to new language technologies.","offset":0,"pro":0,"labels":"GAP"},{"idx":18,"sentence":"This linguistic diversity also results in a research environment conducive to the study of comparative, contact, and historical linguistics-fields which necessitate the gathering of extensive data from many languages.","offset":1,"pro":0.2,"labels":"CTN"},{"idx":18,"sentence":"We claim that data scatteredness (rather than scarcity) is the primary obstacle in the development of South Asian language technology, and suggest that the study of language history is uniquely aligned with surmounting this obstacle.","offset":2,"pro":0.4,"labels":"CLN"},{"idx":18,"sentence":"We review recent developments in and at the intersection of South Asian NLP and historical-comparative linguistics, describing our and others' current efforts in this area.","offset":3,"pro":0.6,"labels":"PUR"},{"idx":18,"sentence":"We also offer new strategies towards breaking the data barrier.","offset":4,"pro":0.8,"labels":"MTD"},{"idx":19,"sentence":"Despite recent progress in abstractive summarization, systems still suffer from faithfulness errors.","offset":0,"pro":0,"labels":"GAP"},{"idx":19,"sentence":"While prior work has proposed models that improve faithfulness, it is unclear whether the improvement comes from an increased level of extractiveness of the model outputs as one naive way to improve faithfulness is to make summarization models more extractive.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":19,"sentence":"In this work, we present a framework for evaluating the effective faithfulness of summarization systems, by generating a faithfulness-abstractiveness trade-off curve that serves as a control at different operating points on the abstractiveness spectrum.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":19,"sentence":"We then show that the Maximum Likelihood Estimation (MLE) baseline as well as recently proposed methods for improving faithfulness, fail to consistently improve over the control at the same level of abstractiveness.","offset":3,"pro":0.5,"labels":"RST"},{"idx":19,"sentence":"Finally, we learn a selector to identify the most faithful and abstractive summary for a given document, and show that this system can attain higher faithfulness scores in human evaluations while being more abstractive than the baseline system on two datasets.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":19,"sentence":"Moreover, we show that our system is able to achieve a better faithfulness-abstractiveness trade-off than the control at the same level of abstractiveness.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":0,"sentence":"Languages are continuously undergoing changes, and the mechanisms that underlie these changes are still a matter of debate.","offset":0,"pro":0,"labels":"GAP"},{"idx":0,"sentence":"In this work, we approach language evolution through the lens of causality in order to model not only how various distributional factors associate with language change, but how they causally affect it.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":0,"sentence":"In particular, we study slang, which is an informal language that is typically restricted to a specific group or social setting.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":0,"sentence":"We analyze the semantic change and frequency shift of slang words and compare them to those of standard, nonslang words.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":0,"sentence":"With causal discovery and causal inference techniques, we measure the effect that word type (slang/nonslang) has on both semantic change and frequency shift, as well as its relationship to frequency, polysemy and part of speech.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":0,"sentence":"Our analysis provides some new insights in the study of language change, e.g., we show that slang words undergo less semantic change but tend to have larger frequency shifts over time.","offset":5,"pro":0.8333333333333334,"labels":"CTN"},{"idx":1,"sentence":"Model-based, reference-free evaluation metricshave been proposed as a fast and cost-effectiveapproach to evaluate Natural Language Generation(NLG) systems.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"Despite promising recentresults, we find evidence that reference-freeevaluation metrics of summarization and dialoggeneration may be relying on spuriouscorrelations with measures such as word overlap,perplexity, and length.","offset":1,"pro":0.3333333333333333,"labels":"RST"},{"idx":1,"sentence":"We further observethat for text summarization, these metrics havehigh error rates when ranking current state-ofthe-art abstractive summarization systems.","offset":2,"pro":0.6666666666666666,"labels":"RST"},{"idx":2,"sentence":"Semantic parsers map natural language utterances into meaning representations (e.g., programs).","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"Such models are typically bottlenecked by the paucity of training data due to the required laborious annotation efforts.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":2,"sentence":"Recent studies have performed zero-shot learning by synthesizing training examples of canonical utterances and programs from a grammar, and further paraphrasing these utterances to improve linguistic diversity.","offset":2,"pro":0.2857142857142857,"labels":"BAC"},{"idx":2,"sentence":"However, such synthetic examples cannot fully capture patterns in real data.","offset":3,"pro":0.42857142857142855,"labels":"GAP"},{"idx":2,"sentence":"In this paper we analyze zero-shot parsers through the lenses of the language and logical gaps (Herzig and Berant, 2019), which quantify the discrepancy of language and programmatic patterns between the canonical examples and real-world user-issued ones.","offset":4,"pro":0.5714285714285714,"labels":"PUR"},{"idx":2,"sentence":"We propose bridging these gaps using improved grammars, stronger paraphrasers, and efficient learning methods using canonical examples that most likely reflect real user intents.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":2,"sentence":"Our model achieves strong performance on two semantic parsing benchmarks (Scholar, Geo) with zero labeled data.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":3,"sentence":"Machine Translation Quality Estimation (QE) aims to build predictive models to assess the quality of machine-generated translations in the absence of reference translations.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"While state-of-the-art QE models have been shown to achieve good results, they over-rely on features that do not have a causal impact on the quality of a translation.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":3,"sentence":"Two approaches use additional data to inform and support the main task, while the other two are adversarial, actively discouraging the model from learning the bias.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":3,"sentence":"We compare the methods with respect to their ability to reduce the partial input bias while maintaining the overall performance.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":3,"sentence":"We find that training a multitask architecture with an auxiliary binary classification task that utilises additional augmented data best achieves the desired effects and generalises well to different languages and quality metrics.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":3,"sentence":"In particular, there appears to be a partial input bias, i.e., a tendency to assign high-quality scores to translations that are fluent and grammatically correct, even though they do not preserve the meaning of the source. ","offset":5,"pro":0.8333333333333334,"labels":"GAP"},{"idx":4,"sentence":"In this work, we describe a method to jointly pre-train speech and text in an encoder-decoder modeling framework for speech translation and recognition.","offset":0,"pro":0,"labels":"PUR"},{"idx":4,"sentence":"The proposed method utilizes multi-task learning to integrate four self-supervised and supervised subtasks for cross modality learning.","offset":1,"pro":0.125,"labels":"MTD"},{"idx":4,"sentence":"A self-supervised speech subtask, which leverages unlabelled speech data, and a (self-)supervised text to text subtask, which makes use of abundant text training data, take up the majority of the pre-training time.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":4,"sentence":"Two auxiliary supervised speech tasks are included to unify speech and text modeling space.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":4,"sentence":"Detailed analysis reveals learning interference among subtasks.","offset":4,"pro":0.5,"labels":"CLN"},{"idx":4,"sentence":"In order to alleviate the subtask interference, two pre-training configurations are proposed for speech translation and speech recognition respectively.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":4,"sentence":"Our experiments show the proposed method can effectively fuse speech and text information into one model.","offset":6,"pro":0.75,"labels":"CLN"},{"idx":4,"sentence":"It achieves between 1.7 and 2.3 BLEU improvement above the state of the art on the MuST-C speech translation dataset and comparable WERs to wav2vec 2.0 on the Librispeech speech recognition task.","offset":7,"pro":0.875,"labels":"RST"},{"idx":5,"sentence":"Pretrained multilingual models enable zero-shot learning even for unseen languages, and that performance can be further improved via adaptation prior to finetuning.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"However, it is unclear how the number of pretraining languages influences a model's zero-shot learning for languages unseen during pretraining.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":5,"sentence":"To fill this gap, we ask the following research questions: (1) How does the number of pretraining languages influence zero-shot performance on unseen target languages? (2) Does the answer to that question change with model adaptation? (3) Do the findings for our first question change if the languages used for pretraining are all related?","offset":2,"pro":0.4,"labels":"PUR"},{"idx":5,"sentence":"Our experiments on pretraining with related languages indicate that choosing a diverse set of languages is crucial.","offset":3,"pro":0.6,"labels":"CLN"},{"idx":5,"sentence":"Without model adaptation, surprisingly, increasing the number of pretraining languages yields better results up to adding related languages, after which performance plateaus.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":6,"sentence":"The growing size of neural language models has led to increased attention in model compression.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"Pruning methods can significantly reduce the model size but hardly achieve large speedups as distillation.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":6,"sentence":"However, distillation methods require large amounts of unlabeled data and are expensive to train.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":6,"sentence":"In this work, we propose a task-specific structured pruning method CoFi (Coarse- and Fine-grained Pruning), which delivers highly parallelizable subnetworks and matches the distillation methods in both accuracy and latency, without resorting to any unlabeled data.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":6,"sentence":"Our key insight is to jointly prune coarse-grained (e.g., layers) and fine-grained (e.g., heads and hidden units) modules, which controls the pruning decision of each parameter with masks of different granularity.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":6,"sentence":"We also devise a layerwise distillation strategy to transfer knowledge from unpruned to pruned models during optimization.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":6,"sentence":"Our experiments on GLUE and SQuAD datasets show that CoFi yields models with over 10X speedups with a small accuracy drop, showing its effectiveness and efficiency compared to previous pruning and distillation approaches.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":7,"sentence":"Saving and revitalizing endangered languages has become very important for maintaining the cultural diversity on our planet.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"In this work, we focus on discussing how NLP can help revitalize endangered languages.","offset":1,"pro":0.125,"labels":"PUR"},{"idx":7,"sentence":"We first suggest three principles that may help NLP practitioners to foster mutual understanding and collaboration with language communities, and we discuss three ways in which NLP can potentially assist in language education.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":7,"sentence":"We then take Cherokee, a severely-endangered Native American language, as a case study.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":7,"sentence":"After reviewing the language's history, linguistic features, and existing resources, we (in collaboration with Cherokee community members) arrive at a few meaningful ways NLP practitioners can collaborate with community partners.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":7,"sentence":"We hope that our work serves not only to inform the NLP community about Cherokee, but also to provide inspiration for future work on endangered languages in general.","offset":5,"pro":0.625,"labels":"IMP"},{"idx":7,"sentence":"More than 43% of the languages spoken in the world are endangered, and language loss currently occurs at an accelerated rate because of globalization and neocolonialism.","offset":6,"pro":0.75,"labels":"BAC"},{"idx":7,"sentence":"We suggest two approaches to enrich the Cherokee language's resources with machine-in-the-loop processing, and discuss several NLP tools that people from the Cherokee community have shown interest in.","offset":7,"pro":0.875,"labels":"RST"},{"idx":8,"sentence":"The IMPRESSIONS section of a radiology report about an imaging study is a summary of the radiologist's reasoning and conclusions, and it also aids the referring physician in confirming or excluding certain diagnoses.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"Prior research on radiology report summarization has focused on single-step end-to-end models - which subsume the task of salient content acquisition.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":8,"sentence":"To fully explore the cascade structure and explainability of radiology report summarization, we introduce two innovations.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":8,"sentence":"First, we design a two-step approach: extractive summarization followed by abstractive summarization.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":8,"sentence":"Second, we additionally break down the extractive part into two independent tasks: extraction of salient (1) sentences and (2) keywords.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":8,"sentence":"Experiments on English radiology reports from two clinical sites show our novel approach leads to a more precise summary compared to single-step and to two-step-with-single-extractive-process baselines with an overall improvement in F1 score of 3-4%.","offset":5,"pro":0.625,"labels":"RST"},{"idx":8,"sentence":"A cascade of tasks are required to automatically generate an abstractive summary of the typical information-rich radiology report.","offset":6,"pro":0.75,"labels":"BAC"},{"idx":8,"sentence":"hese tasks include acquisition of salient content from the report and generation of a concise, easily consumable IMPRESSIONS section.","offset":7,"pro":0.875,"labels":"BAC"},{"idx":9,"sentence":"Standard conversational semantic parsing maps a complete user utterance into an executable program, after which the program is executed to respond to the user.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"This could be slow when the program contains expensive function calls.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":9,"sentence":"We investigate the opportunity to reduce latency by predicting and executing function calls while the user is still speaking.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":9,"sentence":"We introduce the task of online semantic parsing for this purpose, with a formal latency reduction metric inspired by simultaneous machine translation.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":9,"sentence":"We propose a general framework with first a learned prefix-to-program prediction module, and then a simple yet effective thresholding heuristic for subprogram selection for early execution.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":9,"sentence":"Experiments on the SMCalFlow and TreeDST datasets show our approach achieves large latency reduction with good parsing quality, with a 30%-65% latency reduction depending on function execution time and allowed cost.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":10,"sentence":"The enrichment of tabular datasets using external sources has gained significant attention in recent years.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"Existing solutions, however, either ignore external unstructured data completely or devise dataset-specific solutions.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":10,"sentence":"In this study we proposed Few-Shot Transformer based Enrichment (FeSTE), a generic and robust framework for the enrichment of tabular datasets using unstructured data.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":10,"sentence":"By training over multiple datasets, our approach is able to develop generic models that can be applied to additional datasets with minimal training (i.e., few-shot).","offset":3,"pro":0.5,"labels":"RST"},{"idx":10,"sentence":"Our approach is based on an adaptation of BERT, for which we present a novel fine-tuning approach that reformulates the tuples of the datasets as sentences.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":10,"sentence":"Our evaluation, conducted on 17 datasets, shows that FeSTE is able to generate high quality features and significantly outperform existing fine-tuning solutions.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":11,"sentence":"Text summarization helps readers capture salient information from documents, news, interviews, and meetings.","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"However, most state-of-the-art pretrained language models (LM) are unable to efficiently process long text for many summarization tasks.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":11,"sentence":"In this paper, we propose Summ^N, a simple, flexible, and effective multi-stage framework for input texts that are longer than the maximum context length of typical pretrained LMs.","offset":2,"pro":0.2222222222222222,"labels":"PUR"},{"idx":11,"sentence":"Summ^N first splits the data samples and generates a coarse summary in multiple stages and then produces the final fine-grained summary based on it.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":11,"sentence":"Our framework can process input text of arbitrary length by adjusting the number of stages while keeping the LM input size fixed.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":11,"sentence":"Moreover, it can deal with both single-source documents and dialogues, and it can be used on top of different backbone abstractive summarization models.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":11,"sentence":"To the best of our knowledge, Summ^N is the first multi-stage split-then-summarize framework for long input summarization.","offset":6,"pro":0.6666666666666666,"labels":"CTN"},{"idx":11,"sentence":"Our experiments demonstrate that Summ^N outperforms previous state-of-the-art methods by improving ROUGE scores on three long meeting summarization datasets AMI, ICSI, and QMSum, two long TV series datasets from SummScreen, and a long document summarization dataset GovReport.","offset":7,"pro":0.7777777777777778,"labels":"CLN"},{"idx":11,"sentence":"Our data and code are available at https://github.com/psunlpgroup/Summ-N.","offset":8,"pro":0.8888888888888888,"labels":"CTN"},{"idx":12,"sentence":"While data-to-text generation has the potential to serve as a universal interface for data and text, its feasibility for downstream tasks remains largely unknown.","offset":0,"pro":0,"labels":"GAP"},{"idx":12,"sentence":"In this work, we bridge this gap and use the data-to-text method as a means for encoding structured knowledge for open-domain question answering.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":12,"sentence":"Specifically, we propose a verbalizer-retriever-reader framework for ODQA over data and text where verbalized tables from Wikipedia and graphs from Wikidata are used as augmented knowledge sources.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":12,"sentence":"We show that our Unified Data and Text QA, UDT-QA, can effectively benefit from the expanded knowledge index, leading to large gains over text-only baselines.","offset":3,"pro":0.42857142857142855,"labels":"CLN"},{"idx":12,"sentence":"Notably, our approach sets the single-model state-of-the-art on Natural Questions.","offset":4,"pro":0.5714285714285714,"labels":"CLN"},{"idx":12,"sentence":"Furthermore, our analyses indicate that verbalized knowledge is preferred for answer reasoning for both adapted and hot-swap settings.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":12,"sentence":"The retriever-reader framework is popular for open-domain question answering (ODQA) due to its ability to use explicit knowledge.","offset":6,"pro":0.8571428571428571,"labels":"BAC"},{"idx":13,"sentence":"Round-trip Machine Translation (MT) is a popular choice for paraphrase generation, which leverages readily available parallel corpora for supervision.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"In this paper, we formalize the implicit similarity function induced by this approach, and show that it is susceptible to non-paraphrase pairs sharing a single ambiguous translation.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":13,"sentence":"Based on these insights, we design an alternative similarity metric that mitigates this issue by requiring the entire translation distribution to match, and implement a relaxation of it through the Information Bottleneck method.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":13,"sentence":"Our approach incorporates an adversarial term into MT training in order to learn representations that encode as much information about the reference translation as possible, while keeping as little information about the input as possible.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":13,"sentence":"Paraphrases can be generated by decoding back to the source from this representation, without having to generate pivot translations.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":13,"sentence":"In addition to being more principled and efficient than round-trip MT, our approach offers an adjustable parameter to control the fidelity-diversity trade-off, and obtains better results in our experiments.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":14,"sentence":"Over the last few years, there has been a move towards data curation for multilingual task-oriented dialogue (ToD) systems that can serve people speaking different languages.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"However, existing multilingual ToD datasets either have a limited coverage of languages due to the high cost of data curation, or ignore the fact that dialogue entities barely exist in countries speaking these languages.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":14,"sentence":"To tackle these limitations, we introduce a novel data curation method that generates GlobalWoZ - a large-scale multilingual ToD dataset globalized from an English ToD dataset for three unexplored use cases of multilingual ToD systems.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":14,"sentence":"Our method is based on translating dialogue templates and filling them with local entities in the target-language countries.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":14,"sentence":"Besides, we extend the coverage of target languages to 20 languages.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":14,"sentence":"We will release our dataset and a set of strong baselines to encourage research on multilingual ToD systems for real use cases.","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":15,"sentence":"Since the development and wide use of pretrained language models (PLMs), several approaches have been applied to boost their performance on downstream tasks in specific domains, such as biomedical or scientific domains.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"Additional pre-training with in-domain texts is the most common approach for providing domain-specific knowledge to PLMs.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":15,"sentence":"However, these pre-training methods require considerable in-domain data and training resources and a longer training time.","offset":2,"pro":0.2,"labels":"GAP"},{"idx":15,"sentence":"Moreover, the training must be re-performed whenever a new PLM emerges.","offset":3,"pro":0.3,"labels":"GAP"},{"idx":15,"sentence":"In this study, we propose a domain knowledge transferring (DoKTra) framework for PLMs without additional in-domain pretraining.","offset":4,"pro":0.4,"labels":"PUR"},{"idx":15,"sentence":"Specifically, we extract the domain knowledge from an existing in-domain pretrained language model and transfer it to other PLMs by applying knowledge distillation.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":15,"sentence":"In particular, we employ activation boundary distillation, which focuses on the activation of hidden neurons.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":15,"sentence":"We also apply an entropy regularization term in both teacher training and distillation to encourage the model to generate reliable output probabilities, and thus aid the distillation.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":15,"sentence":"By applying the proposed DoKTra framework to downstream tasks in the biomedical, clinical, and financial domains, our student models can retain a high percentage of teacher performance and even outperform the teachers in certain tasks.","offset":8,"pro":0.8,"labels":"RST"},{"idx":15,"sentence":"Our code is available at https://github.com/DMCB-GIST/DoKTra.","offset":9,"pro":0.9,"labels":"CTN"},{"idx":16,"sentence":"Deep NLP models have been shown to be brittle to input perturbations.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"Recent work has shown that data augmentation using counterfactuals - i.e. minimally perturbed inputs - can help ameliorate this weakness.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":16,"sentence":"Using an open-domain QA framework and question generation model trained on original task data, we create counterfactuals that are fluent, semantically diverse, and automatically labeled.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":16,"sentence":"Data augmentation with RGF counterfactuals improves performance on out-of-domain and challenging evaluation sets over and above existing methods, in both the reading comprehension and open-domain QA settings.","offset":3,"pro":0.6,"labels":"CLN"},{"idx":16,"sentence":"We focus on the task of creating counterfactuals for question answering, which presents unique challenges related to world knowledge, semantic diversity, and answerability. ","offset":4,"pro":0.8,"labels":"PUR"},{"idx":17,"sentence":"Transformer-based models have achieved state-of-the-art performance on short-input summarization.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"However, they still struggle with summarizing longer text.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":17,"sentence":"In this paper, we present DYLE, a novel dynamic latent extraction approach for abstractive long-input summarization.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":17,"sentence":"DYLE jointly trains an extractor and a generator and treats the extracted text snippets as the latent variable, allowing dynamic snippet-level attention weights during decoding.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":17,"sentence":"To provide adequate supervision, we propose simple yet effective heuristics for oracle extraction as well as a consistency loss term, which encourages the extractor to approximate the averaged dynamic weights predicted by the generator.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":17,"sentence":"We evaluate our method on different long-document and long-dialogue summarization tasks: GovReport, QMSum, and arXiv.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":17,"sentence":"Experiment results show that DYLE outperforms all existing methods on GovReport and QMSum, with gains up to 6.1 ROUGE, while yielding strong results on arXiv.","offset":6,"pro":0.75,"labels":"RST"},{"idx":17,"sentence":"Further analysis shows that the proposed dynamic weights provide interpretability of our generation process.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":18,"sentence":"Natural language processing for sign language video-including tasks like recognition, translation, and search-is crucial for making artificial intelligence technologies accessible to deaf individuals, and is gaining research interest in recent years.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"In this paper, we address the problem of searching for fingerspelled keywords or key phrases in raw sign language videos.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":18,"sentence":"Our experiments, done on a large public dataset of ASL fingerspelling in the wild, show the importance of fingerspelling detection as a component of a search and retrieval model.","offset":2,"pro":0.3333333333333333,"labels":"CLN"},{"idx":18,"sentence":"Our model significantly outperforms baseline methods adapted from prior work on related tasks.","offset":3,"pro":0.5,"labels":"RST"},{"idx":18,"sentence":"We propose an end-to-end model for this task, FSS-Net, that jointly detects fingerspelling and matches it to a text sequence.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":18,"sentence":"This is an important task since significant content in sign language is often conveyed via fingerspelling, and to our knowledge the task has not been studied before.","offset":5,"pro":0.8333333333333334,"labels":"CTN"},{"idx":19,"sentence":"We formulate a generative model of action sequences in which goals generate sequences of high-level subtask descriptions, and these descriptions generate sequences of low-level actions.","offset":0,"pro":0,"labels":"MTD"},{"idx":19,"sentence":"We describe how to train this model using primarily unannotated demonstrations by parsing demonstrations into sequences of named high-level sub-tasks, using only a small number of seed annotations to ground language in action.","offset":1,"pro":0.25,"labels":"MTD"},{"idx":19,"sentence":"We evaluate this approach in the ALFRED household simulation environment, providing natural language annotations for only 10% of demonstrations.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":19,"sentence":"It achieves performance comparable state-of-the-art models on ALFRED success rate, outperforming several recent methods with access to ground-truth plans during training and evaluation.","offset":3,"pro":0.75,"labels":"RST"},{"idx":0,"sentence":"A language-independent representation of meaning is one of the most coveted dreams in Natural Language Understanding.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"With this goal in mind, several formalisms have been proposed as frameworks for meaning representation in Semantic Parsing.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":0,"sentence":"And yet, the dependencies these formalisms share with respect to language-specific repositories of knowledge make the objective of closing the gap between high- and low-resourced languages hard to accomplish.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":0,"sentence":"In this paper, we present the BabelNet Meaning Representation (BMR), an interlingual formalism that abstracts away from language-specific constraints by taking advantage of the multilingual semantic resources of BabelNet and VerbAtlas.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":0,"sentence":"We describe the rationale behind the creation of BMR and put forward BMR 1.0, a dataset labeled entirely according to the new formalism.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":0,"sentence":"Moreover, we show how BMR is able to outperform previous formalisms thanks to its fully-semantic framing, which enables top-notch multilingual parsing and generation.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":0,"sentence":"We release the code at https://github.com/SapienzaNLP/bmr.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":1,"sentence":"Personalized language models are designed and trained to capture language patterns specific to individual users.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"This makes them more accurate at predicting what a user will write.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":1,"sentence":"However, when a new user joins a platform and not enough text is available, it is harder to build effective personalized language models.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":1,"sentence":"We propose a solution for this problem, using a model trained on users that are similar to a new user.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":1,"sentence":"In this paper, we explore strategies for finding the similarity between new users and existing ones and methods for using the data from existing users who are a good match.","offset":4,"pro":0.6666666666666666,"labels":"PUR"},{"idx":1,"sentence":"We further explore the trade-off between available data for new users and how well their language can be modeled.","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":2,"sentence":"It has been shown that machine translation models usually generate poor translations for named entities that are infrequent in the training corpus.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"Earlier named entity translation methods mainly focus on phonetic transliteration, which ignores the sentence context for translation and is limited in domain and language coverage.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":2,"sentence":"To address this limitation, we propose DEEP, a DEnoising Entity Pre-training method that leverages large amounts of monolingual data and a knowledge base to improve named entity translation accuracy within sentences.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":2,"sentence":"Besides, we investigate a multi-task learning strategy that finetunes a pre-trained neural machine translation model on both entity-augmented monolingual data and parallel data to further improve entity translation.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":2,"sentence":"Experimental results on three language pairs demonstrate that DEEP results in significant improvements over strong denoising auto-encoding baselines, with a gain of up to 1.3 BLEU and up to 9.2 entity accuracy points for English-Russian translation.","offset":4,"pro":0.8,"labels":"RST"},{"idx":3,"sentence":"With the increasing popularity of posting multimodal messages online, many recent studies have been carried out utilizing both textual and visual information for multi-modal sarcasm detection.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"In this paper, we investigate multi-modal sarcasm detection from a novel perspective by constructing a cross-modal graph for each instance to explicitly draw the ironic relations between textual and visual modalities.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":3,"sentence":"Specifically, we first detect the objects paired with descriptions of the image modality, enabling the learning of important visual information.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":3,"sentence":"Then, the descriptions of the objects are served as a bridge to determine the importance of the association between the objects of image modality and the contextual words of text modality, so as to build a cross-modal graph for each multi-modal instance.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":3,"sentence":"Furthermore, we devise a cross-modal graph convolutional network to make sense of the incongruity relations between modalities for multi-modal sarcasm detection.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":3,"sentence":"Extensive experimental results and in-depth analysis show that our model achieves state-of-the-art performance in multi-modal sarcasm detection.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":4,"sentence":"Fine-tuning the entire set of parameters of a large pretrained model has become the mainstream approach for transfer learning.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"To increase its efficiency and prevent catastrophic forgetting and interference, techniques like adapters and sparse fine-tuning have been developed.","offset":1,"pro":0.08333333333333333,"labels":"BAC"},{"idx":4,"sentence":"Adapters are modular, as they can be combined to adapt a model towards different facets of knowledge (e.g., dedicated language and/or task adapters).","offset":2,"pro":0.16666666666666666,"labels":"BAC"},{"idx":4,"sentence":"Sparse fine-tuning is expressive, as it controls the behavior of all model components.","offset":3,"pro":0.25,"labels":"BAC"},{"idx":4,"sentence":"In this work, we introduce a new fine-tuning method with both these desirable properties.","offset":4,"pro":0.3333333333333333,"labels":"PUR"},{"idx":4,"sentence":"In particular, we learn sparse, real-valued masks based on a simple variant of the Lottery Ticket Hypothesis.","offset":5,"pro":0.4166666666666667,"labels":"MTD"},{"idx":4,"sentence":"Task-specific masks are obtained from annotated data in a source language, and language-specific masks from masked language modeling in a target language.","offset":6,"pro":0.5,"labels":"MTD"},{"idx":4,"sentence":"Both these masks can then be composed with the pretrained model.","offset":7,"pro":0.5833333333333334,"labels":"MTD"},{"idx":4,"sentence":"Unlike adapter-based fine-tuning, this method neither increases the number of parameters at inference time nor alters the original model architecture.","offset":8,"pro":0.6666666666666666,"labels":"MTD"},{"idx":4,"sentence":"Most importantly, it outperforms adapters in zero-shot cross-lingual transfer by a large margin in a series of multilingual benchmarks, including Universal Dependencies, MasakhaNER, and AmericasNLI.","offset":9,"pro":0.75,"labels":"RST"},{"idx":4,"sentence":"Based on an in-depth analysis, we additionally find that sparsity is crucial to prevent both 1) interference between the fine-tunings to be composed and 2) overfitting.","offset":10,"pro":0.8333333333333334,"labels":"RST"},{"idx":4,"sentence":"We release the code and models at https://github.com/cambridgeltl/composable-sft.","offset":11,"pro":0.9166666666666666,"labels":"CTN"},{"idx":5,"sentence":"Crowdsourcing has emerged as a popular approach for collecting annotated data to train supervised machine learning models.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"However, annotator bias can lead to defective annotations.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":5,"sentence":"Though there are a few works investigating individual annotator bias, the group effects in annotators are largely overlooked.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":5,"sentence":"In this work, we reveal that annotators within the same demographic group tend to show consistent group bias in annotation tasks and thus we conduct an initial study on annotator group bias.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":5,"sentence":"We first empirically verify the existence of annotator group bias in various real-world crowdsourcing datasets.","offset":4,"pro":0.5,"labels":"RST"},{"idx":5,"sentence":"Then, we develop a novel probabilistic graphical framework GroupAnno to capture annotator group bias with an extended Expectation Maximization (EM) algorithm.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":5,"sentence":"We conduct experiments on both synthetic and real-world datasets.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":5,"sentence":"Experimental results demonstrate the effectiveness of our model in modeling annotator group bias in label aggregation and model learning over competitive baselines.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":6,"sentence":"Gender bias is largely recognized as a problematic phenomenon affecting language technologies, with recent studies underscoring that it might surface differently across languages.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"However, most of current evaluation practices adopt a word-level focus on a narrow set of occupational nouns under synthetic conditions.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":6,"sentence":"Such protocols overlook key features of grammatical gender languages, which are characterized by morphosyntactic chains of gender agreement, marked on a variety of lexical items and parts-of-speech (POS).","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":6,"sentence":"To overcome this limitation, we enrich the natural, gender-sensitive MuST-SHE corpus (Bentivogli et al., 2020) with two new linguistic annotation layers (POS and agreement chains), and explore to what extent different lexical categories and agreement phenomena are impacted by gender skews.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":6,"sentence":"Focusing on speech translation, we conduct a multifaceted evaluation on three language directions (English-French/Italian/Spanish), with models trained on varying amounts of data and different word segmentation techniques.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":6,"sentence":"By shedding light on model behaviours, gender bias, and its detection at several levels of granularity, our findings emphasize the value of dedicated analyses beyond aggregated overall results.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":7,"sentence":"Open-domain questions are likely to be open-ended and ambiguous, leading to multiple valid answers.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"Existing approaches typically adopt the rerank-then-read framework, where a reader reads top-ranking evidence to predict answers.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":7,"sentence":"According to our empirical analysis, this framework faces three problems: first, to leverage a large reader under a memory constraint, the reranker should select only a few relevant passages to cover diverse answers, while balancing relevance and diversity is non-trivial; second, the small reading budget prevents the reader from accessing valuable retrieved evidence filtered out by the reranker; third, when using a generative reader to predict answers all at once based on all selected evidence, whether a valid answer will be predicted also pathologically depends on evidence of some other valid answer(s).","offset":2,"pro":0.4,"labels":"GAP"},{"idx":7,"sentence":"To address these issues, we propose to answer open-domain multi-answer questions with a recall-then-verify framework, which separates the reasoning process of each answer so that we can make better use of retrieved evidence while also leveraging large models under the same memory constraint.","offset":3,"pro":0.6,"labels":"PUR"},{"idx":7,"sentence":"Our framework achieves state-of-the-art results on two multi-answer datasets, and predicts significantly more gold answers than a rerank-then-read system that uses an oracle reranker.","offset":4,"pro":0.8,"labels":"RST"},{"idx":8,"sentence":"Pre-trained contextual representations have led to dramatic performance improvements on a range of downstream tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"Such performance improvements have motivated researchers to quantify and understand the linguistic information encoded in these representations.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":8,"sentence":"In general, researchers quantify the amount of linguistic information through probing, an endeavor which consists of training a supervised model to predict a linguistic property directly from the contextual representations.","offset":2,"pro":0.2222222222222222,"labels":"BAC"},{"idx":8,"sentence":"Unfortunately, this definition of probing has been subject to extensive criticism in the literature, and has been observed to lead to paradoxical and counter-intuitive results.","offset":3,"pro":0.3333333333333333,"labels":"GAP"},{"idx":8,"sentence":"In the theoretical portion of this paper, we take the position that the goal of probing ought to be measuring the amount of inductive bias that the representations encode on a specific task.","offset":4,"pro":0.4444444444444444,"labels":"PUR"},{"idx":8,"sentence":"We further describe a Bayesian framework that operationalizes this goal and allows us to quantify the representations' inductive bias.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":8,"sentence":"In the empirical portion of the paper, we apply our framework to a variety of NLP tasks.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":8,"sentence":"Our results suggest that our proposed framework alleviates many previous problems found in probing.","offset":7,"pro":0.7777777777777778,"labels":"CLN"},{"idx":8,"sentence":"Moreover, we are able to offer concrete evidence that-for some tasks-fastText can offer a better inductive bias than BERT.","offset":8,"pro":0.8888888888888888,"labels":"RST"},{"idx":9,"sentence":"Structured pruning has been extensively studied on monolingual pre-trained language models and is yet to be fully evaluated on their multilingual counterparts.","offset":0,"pro":0,"labels":"GAP"},{"idx":9,"sentence":"This work investigates three aspects of structured pruning on multilingual pre-trained language models: settings, algorithms, and efficiency.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":9,"sentence":"Experiments on nine downstream tasks show several counter-intuitive phenomena: for settings, individually pruning for each language does not induce a better result; for algorithms, the simplest method performs the best; for efficiency, a fast model does not imply that it is also small.","offset":2,"pro":0.4,"labels":"RST"},{"idx":9,"sentence":"To facilitate the comparison on all sparsity levels, we present Dynamic Sparsification, a simple approach that allows training the model once and adapting to different model sizes at inference.","offset":3,"pro":0.6,"labels":"CLN"},{"idx":9,"sentence":"We hope this work fills the gap in the study of structured pruning on multilingual pre-trained models and sheds light on future research.","offset":4,"pro":0.8,"labels":"CTN"},{"idx":10,"sentence":"Deep learning (DL) techniques involving fine-tuning large numbers of model parameters have delivered impressive performance on the task of discriminating between language produced by cognitively healthy individuals, and those with Alzheimer's disease (AD).","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"However, questions remain about their ability to generalize beyond the small reference sets that are publicly available for research.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":10,"sentence":"As an alternative to fitting model parameters directly, we propose a novel method by which a Transformer DL model (GPT-2) pre-trained on general English text is paired with an artificially degraded version of itself (GPT-D), to compute the ratio between these two models' perplexities on language from cognitively healthy and impaired individuals.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":10,"sentence":"This technique approaches state-of-the-art performance on text data from a widely used Cookie Theft picture description task, and unlike established alternatives also generalizes well to spontaneous conversations.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":10,"sentence":"Furthermore, GPT-D generates text with characteristics known to be associated with AD, demonstrating the induction of dementia-related linguistic anomalies.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":10,"sentence":"Our study is a step toward better understanding of the relationships between the inner workings of generative neural language models, the language that they produce, and the deleterious effects of dementia on human speech and language characteristics.","offset":5,"pro":0.8333333333333334,"labels":"CTN"},{"idx":11,"sentence":"Recent work has shown pre-trained language models capture social biases from the large amounts of text they are trained on.","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"This has attracted attention to developing techniques that mitigate such biases.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":11,"sentence":"In this work, we perform an empirical survey of five recently proposed bias mitigation techniques: Counterfactual Data Augmentation (CDA), Dropout, Iterative Nullspace Projection, Self-Debias, and SentenceDebias.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":11,"sentence":"We quantify the effectiveness of each technique using three intrinsic bias benchmarks while also measuring the impact of these techniques on a model's language modeling ability, as well as its performance on downstream NLU tasks.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":11,"sentence":"We experimentally find that: (1) Self-Debias is the strongest debiasing technique, obtaining improved scores on all bias benchmarks; (2) Current debiasing techniques perform less consistently when mitigating non-gender biases; And (3) improvements on bias benchmarks such as StereoSet and CrowS-Pairs by using debiasing strategies are often accompanied by a decrease in language modeling ability, making it difficult to determine whether the bias mitigation was effective.","offset":4,"pro":0.8,"labels":"RST"},{"idx":12,"sentence":"While GPT has become the de-facto method for text generation tasks, its application to pinyin input method remains unexplored.","offset":0,"pro":0,"labels":"GAP"},{"idx":12,"sentence":"We find that a frozen GPT achieves state-of-the-art performance on perfect pinyin","offset":1,"pro":0.16666666666666666,"labels":"RST"},{"idx":12,"sentence":"owever, the performance drops dramatically when the input includes abbreviated pinyin","offset":2,"pro":0.3333333333333333,"labels":"RST"},{"idx":12,"sentence":"A reason is that an abbreviated pinyin can be mapped to many perfect pinyin, which links to even larger number of Chinese characters","offset":3,"pro":0.5,"labels":"RST"},{"idx":12,"sentence":"We mitigate this issue with two strategies,including enriching the context with pinyin and optimizing the training process to help distinguish homophones.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":12,"sentence":"Results show that our approach improves the performance on abbreviated pinyin across all domains","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":13,"sentence":"Cross-lingual natural language inference (XNLI) is a fundamental task in cross-lingual natural language understanding.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"Recently this task is commonly addressed by pre-trained cross-lingual language models.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":13,"sentence":"Existing methods usually enhance pre-trained language models with additional data, such as annotated parallel corpora.","offset":2,"pro":0.25,"labels":"BAC"},{"idx":13,"sentence":"These additional data, however, are rare in practice, especially for low-resource languages.","offset":3,"pro":0.375,"labels":"GAP"},{"idx":13,"sentence":"Inspired by recent promising results achieved by prompt-learning, this paper proposes a novel prompt-learning based framework for enhancing XNLI.","offset":4,"pro":0.5,"labels":"PUR"},{"idx":13,"sentence":"It reformulates the XNLI problem to a masked language modeling problem by constructing cloze-style questions through cross-lingual templates.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":13,"sentence":"To enforce correspondence between different languages, the framework augments a new question for every question using a sampled template in another language and then introduces a consistency loss to make the answer probability distribution obtained from the new question as similar as possible with the corresponding distribution obtained from the original question.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":13,"sentence":"Experimental results on two benchmark datasets demonstrate that XNLI models enhanced by our proposed framework significantly outperform original ones under both the full-shot and few-shot cross-lingual transfer settings.","offset":7,"pro":0.875,"labels":"RST"},{"idx":14,"sentence":"Sense embedding learning methods learn different embeddings for the different senses of an ambiguous word.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"One sense of an ambiguous word might be socially biased while its other senses remain unbiased.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":14,"sentence":"In comparison to the numerous prior work evaluating the social biases in pretrained word embeddings, the biases in sense embeddings have been relatively understudied.","offset":2,"pro":0.3333333333333333,"labels":"BAC"},{"idx":14,"sentence":"We create a benchmark dataset for evaluating the social biases in sense embeddings and propose novel sense-specific bias evaluation measures.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":14,"sentence":"We conduct an extensive evaluation of multiple static and contextualised sense embeddings for various types of social biases using the proposed measures.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":14,"sentence":"Our experimental results show that even in cases where no biases are found at word-level, there still exist worrying levels of social biases at sense-level, which are often ignored by the word-level bias evaluation measures.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":15,"sentence":"We consider the problem of generating natural language given a communicative goal and a world description.","offset":0,"pro":0,"labels":"PUR"},{"idx":15,"sentence":"We ask the question: is it possible to combine complementary meaning representations to scale a goal-directed NLG system without losing expressiveness?","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":15,"sentence":"In particular, we consider using two meaning representations, one based on logical semantics and the other based on distributional semantics.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":15,"sentence":"We build upon an existing goal-directed generation system, S-STRUCT, which models sentence generation as planning in a Markov decision process.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":15,"sentence":"We develop a hybrid approach, which uses distributional semantics to quickly and imprecisely add the main elements of the sentence and then uses first-order logic based semantics to more slowly add the precise details.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":15,"sentence":"We find that our hybrid method allows S-STRUCT's generation to scale significantly better in early phases of generation and that the hybrid can often generate sentences with the same quality as S-STRUCT in substantially less time.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":15,"sentence":"However, we also observe and give insight into cases where the imprecision in distributional semantics leads to generation that is not as good as using pure logical semantics.","offset":6,"pro":0.8571428571428571,"labels":"MTD"},{"idx":16,"sentence":"Clinical trials offer a fundamental opportunity to discover new treatments and advance the medical knowledge.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"However, the uncertainty of the outcome of a trial can lead to unforeseen costs and setbacks.","offset":1,"pro":0.1,"labels":"GAP"},{"idx":16,"sentence":"In this study, we propose a new method to predict the effectiveness of an intervention in a clinical trial.","offset":2,"pro":0.2,"labels":"PUR"},{"idx":16,"sentence":"Our method relies on generating an informative summary from multiple documents available in the literature about the intervention under study.","offset":3,"pro":0.3,"labels":"MTD"},{"idx":16,"sentence":"Specifically, our method first gathers all the abstracts of PubMed articles related to the intervention.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":16,"sentence":"Then, an evidence sentence, which conveys information about the effectiveness of the intervention, is extracted automatically from each abstract.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":16,"sentence":"Based on the set of evidence sentences extracted from the abstracts, a short summary about the intervention is constructed.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":16,"sentence":"Finally, the produced summaries are used to train a BERT-based classifier, in order to infer the effectiveness of an intervention.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":16,"sentence":"To evaluate our proposed method, we introduce a new dataset which is a collection of clinical trials together with their associated PubMed articles.","offset":8,"pro":0.8,"labels":"MTD"},{"idx":16,"sentence":"Our experiments, demonstrate the effectiveness of producing short informative summaries and using them to predict the effectiveness of an intervention.","offset":9,"pro":0.9,"labels":"CLN"},{"idx":17,"sentence":"Interactive neural machine translation (INMT) is able to guarantee high-quality translations by taking human interactions into account.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"Existing IMT systems relying on lexical constrained decoding (LCD) enable humans to translate in a flexible translation order beyond the left-to-right.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":17,"sentence":"However, they typically suffer from two significant limitations in translation efficiency and quality due to the reliance on LCD.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":17,"sentence":"In this work, we propose a novel BiTIIMT system, Bilingual Text-Infilling for Interactive Neural Machine Translation.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":17,"sentence":"The key idea to BiTIIMT is Bilingual Text-infilling (BiTI) which aims to fill missing segments in a manually revised translation for a given source sentence.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":17,"sentence":"We propose a simple yet effective solution by casting this task as a sequence-to-sequence task.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":17,"sentence":"In this way, our system performs decoding without explicit constraints and makes full use of revised words for better translation prediction.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":17,"sentence":"Experiment results show that BiTiIMT performs significantly better and faster than state-of-the-art LCD-based IMT on three translation tasks.","offset":7,"pro":0.875,"labels":"RST"},{"idx":18,"sentence":"In this study, we investigate robustness against covariate drift in spoken language understanding (SLU).","offset":0,"pro":0,"labels":"PUR"},{"idx":18,"sentence":"Covariate drift can occur in SLUwhen there is a drift between training and testing regarding what users request or how they request it.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":18,"sentence":"To study this we propose a method that exploits natural variations in data to create a covariate drift in SLU datasets.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":18,"sentence":"Experiments show that a state-of-the-art BERT-based model suffers performance loss under this drift.","offset":3,"pro":0.5,"labels":"RST"},{"idx":18,"sentence":"To mitigate the performance loss, we investigate distributionally robust optimization (DRO) for finetuning BERT-based models.","offset":4,"pro":0.6666666666666666,"labels":"PUR"},{"idx":18,"sentence":"We discuss some recent DRO methods, propose two new variants and empirically show that DRO improves robustness under drift.","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":19,"sentence":"Chinese pre-trained language models usually exploit contextual character information to learn representations, while ignoring the linguistics knowledge, e.g., word and sentence information.","offset":0,"pro":0,"labels":"GAP"},{"idx":19,"sentence":"Hence, we propose a task-free enhancement module termed as Heterogeneous Linguistics Graph (HLG) to enhance Chinese pre-trained language models by integrating linguistics knowledge.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":19,"sentence":"Specifically, we construct a hierarchical heterogeneous graph to model the characteristics linguistics structure of Chinese language, and conduct a graph-based method to summarize and concretize information on different granularities of Chinese linguistics hierarchies.Experimental results demonstrate our model has the ability to improve the performance of vanilla BERT, BERTwwm and ERNIE 1.0 on 6 natural language processing tasks with 10 benchmark datasets.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":19,"sentence":"Meanwhile, our model introduces far fewer parameters (about half of MWA) and the training/inference speed is about 7x faster than MWA.","offset":3,"pro":0.6,"labels":"RST"},{"idx":19,"sentence":"Further, the detailed experimental analyses have proven that this kind of modelization achieves more improvements compared with previous strong baseline MWA","offset":4,"pro":0.8,"labels":"CLN"},{"idx":0,"sentence":"Fine-grained Entity Typing (FET) has made great progress based on distant supervision but still suffers from label noise.","offset":0,"pro":0,"labels":"GAP"},{"idx":0,"sentence":"In this work, we propose a clustering-based loss correction framework named Feature Cluster Loss Correction (FCLC), to address these two problems.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":0,"sentence":"FCLC first train a coarse backbone model as a feature extractor and noise estimator.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":0,"sentence":"Loss correction is then applied to each feature cluster, learning directly from the noisy labels.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":0,"sentence":"Experimental results on three public datasets show that FCLC achieves the best performance over existing competitive systems.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":0,"sentence":"We also find that in the extreme case of no clean data, the FCLC framework still achieves competitive performance.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":0,"sentence":"Existing FET noise learning methods rely on prediction distributions in an instance-independent manner, which causes the problem of confirmation bias.","offset":6,"pro":0.8571428571428571,"labels":"GAP"},{"idx":1,"sentence":"The robustness of Text-to-SQL parsers against adversarial perturbations plays a crucial role in delivering highly reliable applications.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"Previous studies along this line primarily focused on perturbations in the natural language question side, neglecting the variability of tables.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":1,"sentence":"Motivated by this, we propose the Adversarial Table Perturbation (ATP) as a new attacking paradigm to measure robustness of Text-to-SQL models.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":1,"sentence":"Following this proposition, we curate ADVETA, the first robustness evaluation benchmark featuring natural and realistic ATPs.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":1,"sentence":"All tested state-of-the-art models experience dramatic performance drops on ADVETA, revealing significant room of improvement.","offset":4,"pro":0.5,"labels":"RST"},{"idx":1,"sentence":"We will release ADVETA and code to facilitate future research.","offset":5,"pro":0.625,"labels":"CTN"},{"idx":1,"sentence":"To defense against ATP, we build a systematic adversarial training example generation framework tailored for better contextualization of tabular data.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":1,"sentence":"Experiments show that our approach brings models best robustness improvement against ATP, while also substantially boost model robustness against NL-side perturbations.","offset":7,"pro":0.875,"labels":"RST"},{"idx":2,"sentence":"Neural networks, especially neural machine translation models, suffer from catastrophic forgetting even if they learn from a static training set.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"The underlying cause is that training samples do not get balanced training in each model update, so we name this problem imbalanced training.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":2,"sentence":"Experimental results on multiple machine translation tasks show that our method successfully alleviates the problem of imbalanced training and achieves substantial improvements over strong baseline systems.","offset":2,"pro":0.3333333333333333,"labels":"CLN"},{"idx":2,"sentence":"Neural networks tend to gradually forget the previously learned knowledge when learning multiple tasks sequentially from dynamic data distributions.","offset":3,"pro":0.5,"labels":"BAC"},{"idx":2,"sentence":"To alleviate this problem,","offset":4,"pro":0.6666666666666666,"labels":"PUR"},{"idx":2,"sentence":"we propose Complementary Online Knowledge Distillation (COKD), which uses dynamically updated teacher models trained on specific data orders to iteratively provide complementary knowledge to the student model.","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":3,"sentence":"Human languages are full of metaphorical expressions.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"Metaphors help people understand the world by connecting new concepts and domains to more familiar ones.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":3,"sentence":"Large pre-trained language models (PLMs) are therefore assumed to encode metaphorical knowledge useful for NLP systems.","offset":2,"pro":0.2857142857142857,"labels":"BAC"},{"idx":3,"sentence":"We present studies in multiple metaphor detection datasets and in four languages (i.e., English, Spanish, Russian, and Farsi).","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":3,"sentence":"Our extensive experiments suggest that contextual representations in PLMs do encode metaphorical knowledge, and mostly in their middle layers.","offset":4,"pro":0.5714285714285714,"labels":"CLN"},{"idx":3,"sentence":"by probing metaphoricity information in their encodings, and by measuring the cross-lingual and cross-dataset generalization of this information.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":3,"sentence":"Our findings give helpful insights for both cognitive and NLP scientists.","offset":6,"pro":0.8571428571428571,"labels":"IMP"},{"idx":4,"sentence":"Dependency trees have been intensively used with graph neural networks for aspect-based sentiment classification.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"Though being effective, such methods rely on external dependency parsers, which can be unavailable for low-resource languages or perform worse in low-resource domains.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":4,"sentence":"In addition, dependency trees are also not optimized for aspect-based sentiment classification.","offset":2,"pro":0.4,"labels":"GAP"},{"idx":4,"sentence":"In this paper, we propose an aspect-specific and language-agnostic discrete latent opinion tree model as an alternative structure to explicit dependency trees.","offset":3,"pro":0.6,"labels":"PUR"},{"idx":4,"sentence":"To ease the learning of complicated structured latent variables, we build a connection between aspect-to-context attention scores and syntactic distances, inducing trees from the attention scores. ","offset":4,"pro":0.8,"labels":"MTD"},{"idx":5,"sentence":"Thanks to the strong representation power of neural encoders, neural chart-based parsers have achieved highly competitive performance by using local features.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"Recently, it has been shown that non-local features in CRF structures lead to improvements.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":5,"sentence":"Results show that our simple method gives better results than the self-attentive parser on both PTB and CTB.","offset":2,"pro":0.2857142857142857,"labels":"RST"},{"idx":5,"sentence":"Besides, our method achieves state-of-the-art BERT-based performance on PTB (95.92 F1) and strong performance on CTB (92.31 F1).","offset":3,"pro":0.42857142857142855,"labels":"RST"},{"idx":5,"sentence":"Our parser also outperforms the self-attentive parser in multi-lingual and zero-shot cross-domain settings.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":5,"sentence":"In this paper, we investigate injecting non-local features into the training process of a local span-based parser,","offset":5,"pro":0.7142857142857143,"labels":"PUR"},{"idx":5,"sentence":"by predicting constituent n-gram non-local patterns and ensuring consistency between non-local patterns and local constituents.","offset":6,"pro":0.8571428571428571,"labels":"MTD"},{"idx":6,"sentence":"In this paper, we firstly empirically find that existing models struggle to handle hard mentions due to their insufficient contexts, which consequently limits their overall typing performance.","offset":0,"pro":0,"labels":"PUR"},{"idx":6,"sentence":"Moreover, our experiments indeed prove the superiority of sibling mentions in helping clarify the types for hard mentions.","offset":1,"pro":0.25,"labels":"CLN"},{"idx":6,"sentence":"To this end, we propose to exploit sibling mentions for enhancing the mention representations.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":6,"sentence":"The proposed graph model is scalable in that unseen test mentions are allowed to be added as new nodes for inference.","offset":3,"pro":0.75,"labels":"RST"},{"idx":7,"sentence":"The goal of the cross-lingual summarization (CLS) is to convert a document in one language (e.g., English) to a summary in another one (e.g., Chinese).","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"The CLS task is essentially the combination of machine translation (MT) and monolingual summarization (MS), and thus there exists the hierarchical relationship between MT{&MS and CLS.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":7,"sentence":"However, it is very challenging for the model to directly conduct CLS as it requires both the abilities to translate and summarize.","offset":2,"pro":0.2,"labels":"GAP"},{"idx":7,"sentence":"To address this issue, we propose a hierarchical model for the CLS task, based on the conditional variational auto-encoder.","offset":3,"pro":0.3,"labels":"PUR"},{"idx":7,"sentence":"The hierarchical model contains two kinds of latent variables at the local and global levels, respectively.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":7,"sentence":"At the local level, there are two latent variables, one for translation and the other for summarization.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":7,"sentence":"Existing studies on CLS mainly focus on utilizing pipeline methods or jointly training an end-to-end model through an auxiliary MT or MS objective.","offset":6,"pro":0.6,"labels":"BAC"},{"idx":7,"sentence":"As for the global level, there is another latent variable for cross-lingual summarization conditioned on the two local-level variables.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":7,"sentence":"In addition, we show that our model is able to generate better cross-lingual summaries than comparison models in the few-shot setting.","offset":8,"pro":0.8,"labels":"CLN"},{"idx":7,"sentence":"Experiments on two language directions (English-Chinese) verify the effectiveness and superiority of the proposed approach.","offset":9,"pro":0.9,"labels":"CLN"},{"idx":8,"sentence":"In conversational question answering (CQA), the task of question rewriting (QR) in context aims to rewrite a context-dependent question into an equivalent self-contained question that gives the same answer.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"In this paper, we are interested in the robustness of a QR system to questions varying in rewriting hardness or difficulty.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":8,"sentence":"Since there is a lack of questions classified based on their rewriting hardness, we first propose a heuristic method to automatically classify questions into subsets of varying hardness, by measuring the discrepancy between a question and its rewrite.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":8,"sentence":"To find out what makes questions hard or easy for rewriting, we then conduct a human evaluation to annotate the rewriting hardness of questions.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":8,"sentence":"Finally, to enhance the robustness of QR systems to questions of varying hardness, we propose a novel learning framework for QR that first trains a QR model independently on each subset of questions of a certain level of hardness, then combines these QR models as one joint model for inference.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":8,"sentence":"Experimental results on two datasets show that our framework improves the overall performance compared to the baselines.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":9,"sentence":"AI technologies for Natural Languages have made tremendous progress recently.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"However, commensurate progress has not been made on Sign Languages, in particular, in recognizing signs as individual words or as complete sentences.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":9,"sentence":"First, we propose using pose extracted through pretrained models as the standard modality of data in this work to reduce training time and enable efficient inference, and we release standardized pose datasets for different existing sign language datasets.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":9,"sentence":"Second, we train and release checkpoints of 4 pose-based isolated sign language recognition models across 6 languages (American, Argentinian, Chinese, Greek, Indian, and Turkish), providing baselines and ready checkpoints for deployment.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":9,"sentence":"We open-source all models and datasets in OpenHands with a hope that it makes research in sign languages reproducible and more accessible.","offset":4,"pro":0.5,"labels":"CTN"},{"idx":9,"sentence":"We introduce OpenHands, a library where we take four key ideas from the NLP community for low-resource languages and apply them to sign languages for word-level recognition.","offset":5,"pro":0.625,"labels":"PUR"},{"idx":9,"sentence":"Third, to address the lack of labelled data, we propose self-supervised pretraining on unlabelled data. ","offset":6,"pro":0.75,"labels":"MTD"},{"idx":9,"sentence":"Fourth, we compare different pretraining strategies and for the first time establish that pretraining is effective for sign language recognition by demonstrating (a) improved fine-tuning performance especially in low-resource settings, and (b) high crosslingual transfer from Indian-SL to few other sign languages.","offset":7,"pro":0.875,"labels":"MTD"},{"idx":10,"sentence":"In recent years, researchers tend to pre-train ever-larger language models to explore the upper limit of deep models.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"However, large language model pre-training costs intensive computational resources, and most of the models are trained from scratch without reusing the existing pre-trained models, which is wasteful.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":10,"sentence":"In this paper, we propose bert2BERT, which can effectively transfer the knowledge of an existing smaller pre-trained model to a large model through parameter initialization and significantly improve the pre-training efficiency of the large model.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":10,"sentence":"Specifically, we extend the previous function-preserving method proposed in computer vision on the Transformer-based language model, and further improve it by proposing a novel method, advanced knowledge for large model's initialization.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":10,"sentence":"In addition, a two-stage learning method is proposed to further accelerate the pre-training.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":10,"sentence":"We conduct extensive experiments on representative PLMs (e.g., BERT and GPT) and demonstrate that (1) our method can save a significant amount of training cost compared with baselines including learning from scratch, StackBERT and MSLT; (2) our method is generic and applicable to different types of pre-trained models.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":10,"sentence":"In particular, bert2BERT saves about 45% and 47% computational cost of pre-training BERT_{rm BASE and GPT_{rm BASE by reusing the models of almost their half sizes.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":11,"sentence":"As an important task in sentiment analysis, Multimodal Aspect-Based Sentiment Analysis (MABSA) has attracted increasing attention inrecent years.","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"However, previous approaches either (i) use separately pre-trained visual and textual models, which ignore the crossmodalalignment or (ii) use vision-language models pre-trained with general pre-training tasks, which are inadequate to identify fine-grainedaspects, opinions, and their alignments across modalities.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":11,"sentence":"To tackle these limitations, we propose a task-specific Vision-LanguagePre-training framework for MABSA (VLP-MABSA), which is a unified multimodal encoder-decoder architecture for all the pretrainingand downstream tasks.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":11,"sentence":"We further design three types of task-specific pre-training tasks from the language, vision, and multimodalmodalities, respectively.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":11,"sentence":"Experimental results show that our approach generally outperforms the state-of-the-art approaches on three MABSA subtasks.","offset":4,"pro":0.5714285714285714,"labels":"CLN"},{"idx":11,"sentence":"Further analysis demonstrates the effectiveness of each pre-training task.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":11,"sentence":"The source code is publicly released at https://github.com/NUSTM/VLP-MABSA.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":12,"sentence":"Hedges have an important role in the management of rapport.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"We compared approaches relying on pre-trained resources with others that integrate insights from the social science literature.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":12,"sentence":"We employ a model explainability tool to explore the features that characterize hedges in peer-tutoring conversations, and we identify some novel features, and the benefits of a such a hybrid model approach.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":12,"sentence":"In peer-tutoring, they are notably used by tutors in dyads experiencing low rapport to tone down the impact of instructions and negative feedback.","offset":3,"pro":0.5,"labels":"BAC"},{"idx":12,"sentence":"we used a multimodal peer-tutoring dataset to construct a computational framework for identifying hedges.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":12,"sentence":"Our best performance involved a hybrid approach that outperforms the existing baseline while being easier to interpret.","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":13,"sentence":"k-Nearest-Neighbor Machine Translation (kNN-MT) has been recently proposed as a non-parametric solution for domain adaptation in neural machine translation (NMT).","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"Previous studies (Khandelwal et al., 2021; Zheng et al., 2021) have already demonstrated that non-parametric NMT is even superior to models fine-tuned on out-of-domain data.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":13,"sentence":"In spite of this success, kNN retrieval is at the expense of high latency, in particular for large datastores.","offset":2,"pro":0.2,"labels":"GAP"},{"idx":13,"sentence":"To make it practical, in this paper, we explore a more efficient kNN-MT and propose to use clustering to improve the retrieval efficiency.","offset":3,"pro":0.3,"labels":"PUR"},{"idx":13,"sentence":"Concretely, we first propose a cluster-based Compact Network for feature reduction in a contrastive learning manner to compress context features into 90+% lower dimensional vectors.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":13,"sentence":"We then suggest a cluster-based pruning solution to filter out 10% 40% redundant nodes in large datastores while retaining translation quality.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":13,"sentence":"Our proposed methods achieve better or comparable performance while reducing up to 57% inference latency against the advanced non-parametric MT model on several machine translation benchmarks.","offset":6,"pro":0.6,"labels":"RST"},{"idx":13,"sentence":"Experimental results indicate that the proposed methods maintain the most useful information of the original datastore and the Compact Network shows good generalization on unseen domains.","offset":7,"pro":0.7,"labels":"CLN"},{"idx":13,"sentence":"Codes are available at https://github.com/tjunlp-lab/PCKMT.","offset":8,"pro":0.8,"labels":"CTN"},{"idx":13,"sentence":"It aims to alleviate the performance degradation of advanced MT systems in translating out-of-domain sentences by coordinating with an additional token-level feature-based retrieval module constructed from in-domain data.","offset":9,"pro":0.9,"labels":"BAC"},{"idx":14,"sentence":"We propose a new method for projective dependency parsing based on headed spans.","offset":0,"pro":0,"labels":"PUR"},{"idx":14,"sentence":"In a projective dependency tree, the largest subtree rooted at each word covers a contiguous sequence (i.e., a span) in the surface order.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":14,"sentence":"A projective dependency tree can be represented as a collection of headed spans.","offset":2,"pro":0.3333333333333333,"labels":"BAC"},{"idx":14,"sentence":"We decompose the score of a dependency tree into the scores of the headed spans and design a novel O(n^3) dynamic programming algorithm to enable global training and exact inference.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":14,"sentence":"Our model achieves state-of-the-art or competitive results on PTB, CTB, and UD","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":14,"sentence":"We call such a span marked by a root word headed span.","offset":5,"pro":0.8333333333333334,"labels":"BAC"},{"idx":15,"sentence":"This work explores techniques to predict Part-of-Speech (PoS) tags from neural signals measured at millisecond resolution with electroencephalography (EEG) during text reading.","offset":0,"pro":0,"labels":"PUR"},{"idx":15,"sentence":"We first show that information about word length, frequency and word class is encoded by the brain at different post-stimulus latencies.","offset":1,"pro":0.25,"labels":"MTD"},{"idx":15,"sentence":"We then demonstrate that pre-training on averaged EEG data and data augmentation techniques boost PoS decoding accuracy for single EEG trials.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":15,"sentence":"Finally, applying optimised temporally-resolved decoding techniques","offset":3,"pro":0.75,"labels":"MTD"},{"idx":16,"sentence":"Recent works on Lottery Ticket Hypothesis have shown that pre-trained language models (PLMs) contain smaller matching subnetworks(winning tickets) which are capable of reaching accuracy comparable to the original models.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"However, these tickets are proved to be notrobust to adversarial examples, and even worse than their PLM counterparts.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":16,"sentence":"To address this problem, we propose a novel method based on learning binary weight masks to identify robust tickets hidden in the original PLMs.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":16,"sentence":"Experimental results show the significant improvement of the proposed method over previous work on adversarial robustness evaluation.","offset":3,"pro":0.6,"labels":"CLN"},{"idx":16,"sentence":"Since the loss is not differentiable for the binary mask, we assign the hard concrete distribution to the masks and encourage their sparsity using a smoothing approximation of L0 regularization.","offset":4,"pro":0.8,"labels":"MTD"},{"idx":17,"sentence":"Tuning pre-trained language models (PLMs) with task-specific prompts has been a promising approach for text classification.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"Particularly, previous studies suggest that prompt-tuning has remarkable superiority in the low-data scenario over the generic fine-tuning methods with extra classifiers.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":17,"sentence":"The core idea of prompt-tuning is to insert text pieces, i.e., template, to the input and transform a classification problem into a masked language modeling problem, where a crucial step is to construct a projection, i.e., verbalizer, between a label space and a label word space.","offset":2,"pro":0.2857142857142857,"labels":"BAC"},{"idx":17,"sentence":"A verbalizer is usually handcrafted or searched by gradient descent, which may lack coverage and bring considerable bias and high variances to the results.","offset":3,"pro":0.42857142857142855,"labels":"GAP"},{"idx":17,"sentence":"In this work, we focus on incorporating external knowledge into the verbalizer, forming a knowledgeable prompttuning (KPT), to improve and stabilize prompttuning.","offset":4,"pro":0.5714285714285714,"labels":"PUR"},{"idx":17,"sentence":"Specifically, we expand the label word space of the verbalizer using external knowledge bases (KBs) and refine the expanded label word space with the PLM itself before predicting with the expanded label word space.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":17,"sentence":"Extensive experiments on zero and few-shot text classification tasks demonstrate the effectiveness of knowledgeable prompt-tuning.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":18,"sentence":"Fine-grained entity typing (FGET) aims to classify named entity mentions into fine-grained entity types, which is meaningful for entity-related NLP tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"For FGET, a key challenge is the low-resource problem - the complex entity type hierarchy makes it difficult to manually label data.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":18,"sentence":"Especially for those languages other than English, human-labeled data is extremely scarce.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":18,"sentence":"In this paper, we propose a cross-lingual contrastive learning framework to learn FGET models for low-resource languages.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":18,"sentence":"Specifically, we use multi-lingual pre-trained language models (PLMs) as the backbone to transfer the typing knowledge from high-resource languages (such as English) to low-resource languages (such as Chinese).","offset":4,"pro":0.5,"labels":"MTD"},{"idx":18,"sentence":"Furthermore, we introduce entity-pair-oriented heuristic rules as well as machine translation to obtain cross-lingual distantly-supervised data, and apply cross-lingual contrastive learning on the distantly-supervised data to enhance the backbone PLMs.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":18,"sentence":"Experimental results show that by applying our framework, we can easily learn effective FGET models for low-resource languages, even without any language-specific human-labeled data.","offset":6,"pro":0.75,"labels":"RST"},{"idx":18,"sentence":"Our code is also available at url{https://github.com/thunlp/CrossET.","offset":7,"pro":0.875,"labels":"CTN"},{"idx":19,"sentence":"Data augmentation is an effective solution to data scarcity in low-resource scenarios.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"However, when applied to token-level tasks such as NER, data augmentation methods often suffer from token-label misalignment, which leads to unsatsifactory performance.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":19,"sentence":"In this work, we propose Masked Entity Language Modeling (MELM) as a novel data augmentation framework for low-resource NER.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":19,"sentence":"Thereby, MELM generates high-quality augmented data with novel entities, which provides rich entity regularity knowledge and boosts NER performance.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":19,"sentence":"When training data from multiple languages are available, we also integrate MELM with code-mixing for further improvement.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":19,"sentence":"Experimental results show that our MELM consistently outperforms the baseline methods.","offset":5,"pro":0.625,"labels":"RST"},{"idx":19,"sentence":"To alleviate the token-label misalignment issue, we explicitly inject NER labels into sentence context, and thus the fine-tuned MELM is able to predict masked entity tokens by explicitly conditioning on their labels.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":19,"sentence":"We demonstrate the effectiveness of MELM on monolingual, cross-lingual and multilingual NER across various low-resource levels.","offset":7,"pro":0.875,"labels":"MTD"},{"idx":0,"sentence":"Many relationships between words can be expressed set-theoretically, for example, adjective-noun compounds (eg. red cars⊆cars) and homographs (eg. tongue∩body should be similar to mouth, while tongue∩language should be similar to dialect) have natural set-theoretic interpretations.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"Box embeddings are a novel region-based representation which provide the capability to perform these set-theoretic operations.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":0,"sentence":"In this work, we provide a fuzzy-set interpretation of box embeddings, and learn box representations of words using a set-theoretic training objective.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":0,"sentence":"We demonstrate improved performance on various word similarity tasks, particularly on less common words, and perform a quantitative and qualitative analysis exploring the additional unique expressivity provided by Word2Box.","offset":3,"pro":0.5,"labels":"RST"},{"idx":0,"sentence":"Learning representations of words in a continuous space is perhaps the most fundamental task in NLP","offset":4,"pro":0.6666666666666666,"labels":"BAC"},{"idx":0,"sentence":"however words interact in ways much richer than vector dot product similarity can provide","offset":5,"pro":0.8333333333333334,"labels":"BAC"},{"idx":1,"sentence":"Traditionally, a debate usually requires a manual preparation process, including reading plenty of articles, selecting the claims, identifying the stances of the claims, seeking the evidence for the claims, etc. As the AI debate attracts more attention these years, it is worth exploring the methods to automate the tedious process involved in the debating system.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"Near 70k sentences in the dataset are fully annotated based on their argument properties (e.g., claims, stances, evidence, etc.).","offset":1,"pro":0.14285714285714285,"labels":"MTD"},{"idx":1,"sentence":"We further propose two new integrated argument mining tasks associated with the debate preparation process: (1) claim extraction with stance classification (CESC) and (2) claim-evidence pair extraction (CEPE).","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":1,"sentence":"We adopt a pipeline approach and an end-to-end method for each integrated task separately.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":1,"sentence":"Promising experimental results are reported to show the values and challenges of our proposed tasks, and motivate future research on argument mining.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":1,"sentence":"In this work, we introduce a comprehensive and large dataset named IAM, which can be applied to a series of argument mining tasks, including claim extraction, stance classification, evidence extraction, etc","offset":5,"pro":0.7142857142857143,"labels":"PUR"},{"idx":1,"sentence":"Our dataset is collected from over 1k articles related to 123 topics","offset":6,"pro":0.8571428571428571,"labels":"MTD"},{"idx":2,"sentence":"Despite recent progress of pre-trained language models on generating fluent text, existing methods still suffer from incoherence problems in long-form text generation tasks that require proper content control and planning to form a coherent high-level logical flow.","offset":0,"pro":0,"labels":"GAP"},{"idx":2,"sentence":"In this work, we propose PLANET, a novel generation framework leveraging autoregressive self-attention mechanism to conduct content planning and surface realization dynamically.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":2,"sentence":"To guide the generation of output sentences, our framework enriches the Transformer decoder with latent representations to maintain sentence-level semantic plans grounded by bag-of-words.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":2,"sentence":"Moreover, we introduce a new coherence-based contrastive learning objective to further improve the coherence of output.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":2,"sentence":"Extensive experiments are conducted on two challenging long-form text generation tasks including counterargument generation and opinion article generation.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":2,"sentence":"Both automatic and human evaluations show that our method significantly outperforms strong baselines and generates more coherent texts with richer contents.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":3,"sentence":"Existing reference-free metrics have obvious limitations for evaluating controlled text generation models.","offset":0,"pro":0,"labels":"GAP"},{"idx":3,"sentence":"Unsupervised metrics can only provide a task-agnostic evaluation result which correlates weakly with human judgments, whereas supervised ones may overfit task-specific data with poor generalization ability to other datasets.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":3,"sentence":"In this paper, we propose an unsupervised reference-free metric called CTRLEval, which evaluates controlled text generation from different aspects by formulating each aspect into multiple text infilling tasks.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":3,"sentence":"On top of these tasks, the metric assembles the generation probabilities from a pre-trained language model without any model training.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":3,"sentence":"Experimental results show that our metric has higher correlations with human judgments than other baselines, while obtaining better generalization of evaluating generated texts from different models and with different qualities.","offset":4,"pro":0.8,"labels":"RST"},{"idx":4,"sentence":"In dialogue state tracking, dialogue history is a crucial material, and its utilization varies between different models.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"However, no matter how the dialogue history is used, each existing model uses its own consistent dialogue history during the entire state tracking process, regardless of which slot is updated.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":4,"sentence":"Apparently, it requires different dialogue history to update different slots in different turns.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":4,"sentence":"Therefore, using consistent dialogue contents may lead to insufficient or redundant information for different slots, which affects the overall performance.","offset":3,"pro":0.375,"labels":"GAP"},{"idx":4,"sentence":"To address this problem, we devise DiCoS-DST to dynamically select the relevant dialogue contents corresponding to each slot for state updating.","offset":4,"pro":0.5,"labels":"PUR"},{"idx":4,"sentence":"Specifically, it first retrieves turn-level utterances of dialogue history and evaluates their relevance to the slot from a combination of three perspectives: (1) its explicit connection to the slot name; (2) its relevance to the current turn dialogue; (3) Implicit Mention Oriented Reasoning.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":4,"sentence":"Then these perspectives are combined to yield a decision, and only the selected dialogue contents are fed into State Generator, which explicitly minimizes the distracting information passed to the downstream state prediction.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":4,"sentence":"Experimental results show that our approach achieves new state-of-the-art performance on MultiWOZ 2.1 and MultiWOZ 2.2, and achieves superior performance on multiple mainstream benchmark datasets (including Sim-M, Sim-R, and DSTC2).","offset":7,"pro":0.875,"labels":"RST"},{"idx":5,"sentence":"Finetuning large pre-trained language models with a task-specific head has advanced the state-of-the-art on many natural language understanding benchmarks.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"However, models with a task-specific head require a lot of training data, making them susceptible to learning and exploiting dataset-specific superficial cues that do not generalize to other datasets.Prompting has reduced the data requirement by reusing the language model head and formatting the task input to match the pre-training objective.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":5,"sentence":"While the models perform well on instances with superficial cues, they often underperform or only marginally outperform random accuracy on instances without superficial cues.","offset":2,"pro":0.3333333333333333,"labels":"RST"},{"idx":5,"sentence":"Therefore, it is expected that few-shot prompt-based models do not exploit superficial cues","offset":3,"pro":0.5,"labels":"GAP"},{"idx":5,"sentence":"This paper presents an empirical examination of whether few-shot prompt-based models also exploit superficial cues","offset":4,"pro":0.6666666666666666,"labels":"PUR"},{"idx":5,"sentence":"Analyzing few-shot prompt-based models on MNLI, SNLI, HANS, and COPA has revealed that prompt-based models also exploit superficial cues","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":6,"sentence":"Confidence estimation aims to quantify the confidence of the model prediction, providing an expectation of success.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"A well-calibrated confidence estimate enables accurate failure prediction and proper risk measurement when given noisy samples and out-of-distribution data in real-world settings.","offset":1,"pro":0.09090909090909091,"labels":"BAC"},{"idx":6,"sentence":"However, this task remains a severe challenge for neural machine translation (NMT), where probabilities from softmax distribution fail to describe when the model is probably mistaken.","offset":2,"pro":0.18181818181818182,"labels":"GAP"},{"idx":6,"sentence":"To address this problem, we propose an unsupervised confidence estimate learning jointly with the training of the NMT model.","offset":3,"pro":0.2727272727272727,"labels":"PUR"},{"idx":6,"sentence":"We explain confidence as how many hints the NMT model needs to make a correct prediction, and more hints indicate low confidence.","offset":4,"pro":0.36363636363636365,"labels":"MTD"},{"idx":6,"sentence":"Specifically, the NMT model is given the option to ask for hints to improve translation accuracy at the cost of some slight penalty.","offset":5,"pro":0.45454545454545453,"labels":"MTD"},{"idx":6,"sentence":"Then, we approximate their level of confidence by counting the number of hints the model uses.","offset":6,"pro":0.5454545454545454,"labels":"MTD"},{"idx":6,"sentence":"We demonstrate that our learned confidence estimate achieves high accuracy on extensive sentence/word-level quality estimation tasks.","offset":7,"pro":0.6363636363636364,"labels":"RST"},{"idx":6,"sentence":"Analytical results verify that our confidence estimate can correctly assess underlying risk in two real-world scenarios: (1) discovering noisy samples and (2) detecting out-of-domain data.","offset":8,"pro":0.7272727272727273,"labels":"RST"},{"idx":6,"sentence":"We further propose a novel confidence-based instance-specific label smoothing approach based on our learned confidence estimate","offset":9,"pro":0.8181818181818182,"labels":"MTD"},{"idx":6,"sentence":"which outperforms standard label smoothing.","offset":10,"pro":0.9090909090909091,"labels":"RST"},{"idx":7,"sentence":"Spatial commonsense, the knowledge about spatial position and relationship between objects (like the relative size of a lion and a girl, and the position of a boy relative to a bicycle when cycling), is an important part of commonsense knowledge.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"Although pretrained language models (PLMs) succeed in many NLP tasks, they are shown to be ineffective in spatial commonsense reasoning.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":7,"sentence":"Starting from the observation that images are more likely to exhibit spatial commonsense than texts, we explore whether models with visual signals learn more spatial commonsense than text-based PLMs.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":7,"sentence":"The spatial knowledge from image synthesis models also helps in natural language understanding tasks that require spatial commonsense.","offset":3,"pro":0.42857142857142855,"labels":"CLN"},{"idx":7,"sentence":"We propose a spatial commonsense benchmark that focuses on the relative scales of objects, and the positional relationship between people and objects under different actions","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":7,"sentence":"We probe PLMs and models with visual signals, including vision-language pretrained models and image synthesis models, on this benchmark","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":7,"sentence":"and find that image synthesis models are more capable of learning accurate and consistent spatial knowledge than other models.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":8,"sentence":"Token-level adaptive training approaches can alleviate the token imbalance problem and thus improve neural machine translation, through re-weighting the losses of different target tokens based on specific statistical metrics (e.g., token frequency or mutual information).","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"While one possible solution is to directly take target contexts into these statistical metrics, the target-context-aware statistical computing is extremely expensive, and the corresponding storage overhead is unrealistic.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":8,"sentence":"To solve the above issues, we propose a target-context-aware metric, named conditional bilingual mutual information (CBMI), which makes it feasible to supplement target context information for statistical metrics.","offset":2,"pro":0.2222222222222222,"labels":"PUR"},{"idx":8,"sentence":"Particularly, our CBMI can be formalized as the log quotient of the translation model probability and language model probability by decomposing the conditional joint distribution.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":8,"sentence":"Thus CBMI can be efficiently calculated during model training without any pre-specific statistical calculations and large storage overhead.","offset":4,"pro":0.4444444444444444,"labels":"RST"},{"idx":8,"sentence":"Furthermore, we propose an effective adaptive training approach based on both the token- and sentence-level CBMI.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":8,"sentence":"Experimental results on WMT14 English-German and WMT19 Chinese-English tasks show our approach can significantly outperform the Transformer baseline and other related methods.","offset":6,"pro":0.6666666666666666,"labels":"RST"},{"idx":8,"sentence":"Given that standard translation models make predictions on the condition of previous target contexts","offset":7,"pro":0.7777777777777778,"labels":"BAC"},{"idx":8,"sentence":"we argue that the above statistical metrics ignore target context information and may assign inappropriate weights to target tokens","offset":8,"pro":0.8888888888888888,"labels":"GAP"},{"idx":9,"sentence":"Recently, a lot of research has been carried out to improve the efficiency of Transformer.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"Among them, the sparse pattern-based method is an important branch of efficient Transformers.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":9,"sentence":"However, some existing sparse methods usually use fixed patterns to select words, without considering similarities between words.","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":9,"sentence":"Other sparse methods use clustering patterns to select words, but the clustering process is separate from the training process of the target task, which causes a decrease in effectiveness.","offset":3,"pro":0.3333333333333333,"labels":"GAP"},{"idx":9,"sentence":"To address these limitations, we design a neural clustering method, which can be seamlessly integrated into the Self-Attention Mechanism in Transformer.","offset":4,"pro":0.4444444444444444,"labels":"PUR"},{"idx":9,"sentence":"The clustering task and the target task are jointly trained and optimized to benefit each other, leading to significant effectiveness improvement.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":9,"sentence":"In addition, our method groups the words with strong dependencies into the same cluster and performs the attention mechanism for each cluster independently, which improves the efficiency.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":9,"sentence":"We verified our method on machine translation, text classification, natural language inference, and text matching tasks.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":9,"sentence":"Experimental results show that our method outperforms two typical sparse attention methods, Reformer and Routing Transformer while having a comparable or even better time and memory efficiency.","offset":8,"pro":0.8888888888888888,"labels":"RST"},{"idx":10,"sentence":"Constituency parsing and nested named entity recognition (NER) are similar tasks since they both aim to predict a collection of nested and non-crossing spans.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"In this work, we cast nested NER to constituency parsing and propose a novel pointing mechanism for bottom-up parsing to tackle both tasks.","offset":1,"pro":0.125,"labels":"PUR"},{"idx":10,"sentence":"The key idea is based on the observation that if we traverse a constituency tree in post-order, i.e., visiting a parent after its children, then two consecutively visited spans would share a boundary.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":10,"sentence":"Our model tracks the shared boundaries and predicts the next boundary at each step by leveraging a pointer network.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":10,"sentence":"As a result, it needs only linear steps to parse and thus is efficient.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":10,"sentence":"It also maintains a parsing configuration for structural consistency, i.e., always outputting valid trees.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":10,"sentence":"Experimentally, our model achieves the state-of-the-art performance on PTB among all BERT-based models (96.01 F1 score) and competitive performance on CTB7 in constituency parsing; and it also achieves strong performance on three benchmark datasets of nested NER: ACE2004, ACE2005, and GENIA.","offset":6,"pro":0.75,"labels":"RST"},{"idx":10,"sentence":"Our code will be available at url{https://github.com/xxxxx.","offset":7,"pro":0.875,"labels":"CTN"},{"idx":11,"sentence":"Knowledge distillation (KD) is the preliminary step for training non-autoregressive translation (NAT) models, which eases the training of NAT models at the cost of losing important information for translating low-frequency words.","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"In this work, we provide an appealing alternative for NAT - monolingual KD, which trains NAT student on external monolingual data with AT teacher trained on the original bilingual data.","offset":1,"pro":0.125,"labels":"PUR"},{"idx":11,"sentence":"Monolingual KD is able to transfer both the knowledge of the original bilingual data (implicitly encoded in the trained AT teacher model) and that of the new monolingual data to the NAT student model.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":11,"sentence":"Extensive experiments on eight WMT benchmarks over two advanced NAT models show that monolingual KD consistently outperforms the standard KD by improving low-frequency word translation, without introducing any computational cost.","offset":3,"pro":0.375,"labels":"RST"},{"idx":11,"sentence":"Monolingual KD enjoys desirable expandability, which can be further enhanced (when given more computational budget) by combining with the standard KD, a reverse monolingual KD, or enlarging the scale of monolingual data.","offset":4,"pro":0.5,"labels":"IMP"},{"idx":11,"sentence":"Extensive analyses demonstrate that these techniques can be used together profitably to further recall the useful information lost in the standard KD.","offset":5,"pro":0.625,"labels":"IMP"},{"idx":11,"sentence":"Encouragingly, combining with standard KD, our approach achieves 30.4 and 34.1 BLEU points on the WMT14 English-German and German-English datasets, respectively.","offset":6,"pro":0.75,"labels":"RST"},{"idx":11,"sentence":"Our code and trained models are freely available at url{https://github.com/alphadl/RLFW-NAT.mono.","offset":7,"pro":0.875,"labels":"CTN"},{"idx":12,"sentence":"Higher-order methods for dependency parsing can partially but not fully address the issue that edges in dependency trees should be constructed at the text span/subtree level rather than word level.","offset":0,"pro":0,"labels":"GAP"},{"idx":12,"sentence":"In this paper, we propose a new method for dependency parsing to address this issue.","offset":1,"pro":0.125,"labels":"PUR"},{"idx":12,"sentence":"The proposed method constructs dependency trees by directly modeling span-span (in other words, subtree-subtree) relations.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":12,"sentence":"It consists of two modules: the text span proposal module which proposes candidate text spans, each of which represents a subtree in the dependency tree denoted by (root, start, end); and the span linking module, which constructs links between proposed spans.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":12,"sentence":"We use the machine reading comprehension (MRC) framework as the backbone to formalize the span linking module, where one span is used as query to extract the text span/subtree it should be linked to.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":12,"sentence":"The proposed method has the following merits: (1) it addresses the fundamental problem that edges in a dependency tree should be constructed between subtrees; (2) the MRC framework allows the method to retrieve missing spans in the span proposal stage, which leads to higher recall for eligible spans.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":12,"sentence":"Extensive experiments on the PTB, CTB and Universal Dependencies (UD) benchmarks demonstrate the effectiveness of the proposed method.","offset":6,"pro":0.75,"labels":"RST"},{"idx":12,"sentence":"The code is available at url{https://github.com/ShannonAI/mrc-for-dependency-parsing","offset":7,"pro":0.875,"labels":"CTN"},{"idx":13,"sentence":"Cross-domain sentiment analysis has achieved promising results with the help of pre-trained language models.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"As GPT-3 appears, prompt tuning has been widely explored to enable better semantic modeling in many natural language processing tasks.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":13,"sentence":"However, directly using a fixed predefined template for cross-domain research cannot model different distributions of the operatorname{[MASK] token in different domains, thus making underuse of the prompt tuning technique.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":13,"sentence":"In this paper, we propose a novel Adversarial Soft Prompt Tuning method (AdSPT) to better model cross-domain sentiment analysis.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":13,"sentence":"On the one hand, AdSPT adopts separate soft prompts instead of hard templates to learn different vectors for different domains, thus alleviating the domain discrepancy of the operatorname{[MASK] token in the masked language modeling task.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":13,"sentence":"On the other hand, AdSPT uses a novel domain adversarial training strategy to learn domain-invariant representations between each source domain and the target domain.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":13,"sentence":"Experiments on a publicly available sentiment analysis dataset show that our model achieves the new state-of-the-art results for both single-source domain adaptation and multi-source domain adaptation.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":14,"sentence":"Automated scientific fact checking is difficult due to the complexity of scientific language and a lack of significant amounts of training data, as annotation requires domain expertise.","offset":0,"pro":0,"labels":"GAP"},{"idx":14,"sentence":"To address this challenge, we propose scientific claim generation, the task of generating one or more atomic and verifiable claims from scientific sentences, and demonstrate its usefulness in zero-shot fact checking for biomedical claims.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":14,"sentence":"We propose CLAIMGEN-BART, a new supervised method for generating claims supported by the literature, as well as KBIN, a novel method for generating claim negations.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":14,"sentence":"Additionally, we adapt an existing unsupervised entity-centric method of claim generation to biomedical claims, which we call CLAIMGEN-ENTITY.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":14,"sentence":"Experiments on zero-shot fact checking demonstrate that both CLAIMGEN-ENTITY and CLAIMGEN-BART, coupled with KBIN, achieve up to 90% performance of fully supervised models trained on manually annotated claims and evidence.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":14,"sentence":"A rigorous evaluation study demonstrates significant improvement in generated claim and negation quality over existing baselines","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":15,"sentence":"Simultaneous machine translation (SiMT) outputs translation while reading source sentence and hence requires a policy to decide whether to wait for the next source word (READ) or generate a target word (WRITE), the actions of which form a read/write path.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"Although the read/write path is essential to SiMT performance, no direct supervision is given to the path in the existing methods.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":15,"sentence":"In this paper, we propose a method of dual-path SiMT which introduces duality constraints to direct the read/write path.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":15,"sentence":"According to duality constraints, the read/write path in source-to-target and target-to-source SiMT models can be mapped to each other.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":15,"sentence":"As a result, the two SiMT models can be optimized jointly by forcing their read/write paths to satisfy the mapping.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":15,"sentence":"Experiments on En-Vi and De-En tasks show that our method can outperform strong baselines under all latency.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":16,"sentence":"Local models for Entity Disambiguation (ED) have today become extremely powerful, in most part thanks to the advent of large pre-trained language models.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"However, despite their significant performance achievements, most of these approaches frame ED through classification formulations that have intrinsic limitations, both computationally and from a modeling perspective.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":16,"sentence":"ExtEnD outperforms its alternatives by as few as 6 F1 points on the more constrained of the two data regimes and, when moving to the other higher-resourced regime, sets a new state of the art on 4 out of 4 benchmarks under consideration, with average improvements of 0.7 F1 points overall and 1.1 F1 points out of domain.","offset":2,"pro":0.25,"labels":"RST"},{"idx":16,"sentence":"In addition, to gain better insights from our results, we also perform a fine-grained evaluation of our performances on different classes of label frequency, along with an ablation study of our architectural choices and an error analysis.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":16,"sentence":"We release our code and models for research purposes at https://github.com/SapienzaNLP/extend.","offset":4,"pro":0.5,"labels":"CTN"},{"idx":16,"sentence":"In contrast with this trend, here we propose ExtEnD, a novel local formulation for ED where we frame this task as a text extraction problem, and present two Transformer-based architectures that implement it","offset":5,"pro":0.625,"labels":"PUR"},{"idx":16,"sentence":"Based on experiments in and out of domain, and training over two different data regimes","offset":6,"pro":0.75,"labels":"MTD"},{"idx":16,"sentence":"we find our approach surpasses all its competitors in terms of both data efficiency and raw performance","offset":7,"pro":0.875,"labels":"RST"},{"idx":17,"sentence":"We propose a generative model of paraphrase generation, that encourages syntactic diversity by conditioning on an explicit syntactic sketch.","offset":0,"pro":0,"labels":"PUR"},{"idx":17,"sentence":"We introduce Hierarchical Refinement Quantized Variational Autoencoders (HRQ-VAE), a method for learning decompositions of dense encodings as a sequence of discrete latent variables that make iterative refinements of increasing granularity.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":17,"sentence":"This hierarchy of codes is learned through end-to-end training, and represents fine-to-coarse grained information about the input.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":17,"sentence":"We use HRQ-VAE to encode the syntactic form of an input sentence as a path through the hierarchy, allowing us to more easily predict syntactic sketches at test time.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":17,"sentence":"Extensive experiments, including a human evaluation, confirm that HRQ-VAE learns a hierarchical representation of the input space, and generates paraphrases of higher quality than previous systems.","offset":4,"pro":0.8,"labels":"RST"},{"idx":18,"sentence":"Progress with supervised Open Information Extraction (OpenIE) has been primarily limited to English due to the scarcity of training data in other languages.","offset":0,"pro":0,"labels":"GAP"},{"idx":18,"sentence":"In this paper, we explore techniques to automatically convert English text for training OpenIE systems in other languages.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":18,"sentence":"We introduce the Alignment-Augmented Constrained Translation (AACTrans) model to translate English sentences and their corresponding extractions consistently with each other - with no changes to vocabulary or semantic meaning which may result from independent translations.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":18,"sentence":"Using the data generated with AACTrans, we train a novel two-stage generative OpenIE model, which we call Gen2OIE, that outputs for each sentence: 1) relations in the first stage and 2) all extractions containing the relation in the second stage.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":18,"sentence":"Gen2OIE increases relation coverage using a training data transformation technique that is generalizable to multiple languages, in contrast to existing models that use an English-specific training loss.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":18,"sentence":"Evaluations on 5 languages - Spanish, Portuguese, Chinese, Hindi and Telugu - show that the Gen2OIE with AACTrans data outperforms prior systems by a margin of 6-25% in F1.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":19,"sentence":"We study a new problem setting of information extraction (IE), referred to as text-to-table.","offset":0,"pro":0,"labels":"PUR"},{"idx":19,"sentence":"In text-to-table, given a text, one creates a table or several tables expressing the main content of the text, while the model is learned from text-table pair data.","offset":1,"pro":0.07142857142857142,"labels":"BAC"},{"idx":19,"sentence":"The problem setting differs from those of the existing methods for IE.","offset":2,"pro":0.14285714285714285,"labels":"BAC"},{"idx":19,"sentence":"First, the extraction can be carried out from long texts to large tables with complex structures.","offset":3,"pro":0.21428571428571427,"labels":"MTD"},{"idx":19,"sentence":"Second, the extraction is entirely data-driven, and there is no need to explicitly define the schemas.","offset":4,"pro":0.2857142857142857,"labels":"MTD"},{"idx":19,"sentence":"As far as we know, there has been no previous work that studies the problem.","offset":5,"pro":0.35714285714285715,"labels":"GAP"},{"idx":19,"sentence":"In this work, we formalize text-to-table as a sequence-to-sequence (seq2seq) problem.","offset":6,"pro":0.42857142857142855,"labels":"PUR"},{"idx":19,"sentence":"We first employ a seq2seq model fine-tuned from a pre-trained language model to perform the task.","offset":7,"pro":0.5,"labels":"MTD"},{"idx":19,"sentence":"We also develop a new method within the seq2seq approach, exploiting two additional techniques in table generation: table constraint and table relation embeddings.","offset":8,"pro":0.5714285714285714,"labels":"MTD"},{"idx":19,"sentence":"We consider text-to-table as an inverse problem of the well-studied table-to-text, and make use of four existing table-to-text datasets in our experiments on text-to-table.","offset":9,"pro":0.6428571428571429,"labels":"MTD"},{"idx":19,"sentence":"Experimental results show that the vanilla seq2seq model can outperform the baseline methods of using relation extraction and named entity extraction.","offset":10,"pro":0.7142857142857143,"labels":"RST"},{"idx":19,"sentence":"The results also show that our method can further boost the performances of the vanilla seq2seq model.","offset":11,"pro":0.7857142857142857,"labels":"RST"},{"idx":19,"sentence":"We further discuss the main challenges of the proposed task.","offset":12,"pro":0.8571428571428571,"labels":"MTD"},{"idx":19,"sentence":"The code and data are available at https://github.com/shirley-wu/text{_to{_table.","offset":13,"pro":0.9285714285714286,"labels":"CTN"},{"idx":0,"sentence":"Code search is to search reusable code snippets from source code corpus based on natural languages queries.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"Deep learning-based methods on code search have shown promising results.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":0,"sentence":"However, previous methods focus on retrieval accuracy, but lacked attention to the efficiency of the retrieval process.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":0,"sentence":"Extensive experimental results indicate that compared with previous code search baselines, CoSHC can save more than 90% of retrieval time meanwhile preserving at least 99% of retrieval accuracy.","offset":3,"pro":0.42857142857142855,"labels":"RST"},{"idx":0,"sentence":"We propose a novel method CoSHC to accelerate code search with deep hashing and code classification, aiming to perform efficient code search without sacrificing too much accuracy.","offset":4,"pro":0.5714285714285714,"labels":"PUR"},{"idx":0,"sentence":"To evaluate the effectiveness of CoSHC","offset":5,"pro":0.7142857142857143,"labels":"PUR"},{"idx":0,"sentence":"we apply our methodon five code search models.","offset":6,"pro":0.8571428571428571,"labels":"MTD"},{"idx":1,"sentence":"Role-oriented dialogue summarization is to generate summaries for different roles in the dialogue, e.g., merchants and consumers.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"Existing methods handle this task by summarizing each role's content separately and thus are prone to ignore the information from other roles.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":1,"sentence":"However, we believe that other roles' content could benefit the quality of summaries, such as the omitted information mentioned by other roles.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":1,"sentence":"Therefore, we propose a novel role interaction enhanced method for role-oriented dialogue summarization.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":1,"sentence":"It adopts cross attention and decoder self-attention interactions to interactively acquire other roles' critical information.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":1,"sentence":"The cross attention interaction aims to select other roles' critical dialogue utterances, while the decoder self-attention interaction aims to obtain key information from other roles' summaries.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":1,"sentence":"Experimental results have shown that our proposed method significantly outperforms strong baselines on two public role-oriented dialogue summarization datasets.","offset":6,"pro":0.75,"labels":"RST"},{"idx":1,"sentence":"Extensive analyses have demonstrated that other roles' content could help generate summaries with more complete semantics and correct topic structures.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":2,"sentence":"Generating new events given context with correlated ones plays a crucial role in many event-centric reasoning tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"Existing works either limit their scope to specific scenarios or overlook event-level correlations.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":2,"sentence":"To achieve this, we propose three novel event-centric objectives, i.e., whole event recovering, contrastive event-correlation encoding and prompt-based event locating, which highlight event-level correlations with effective training.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":2,"sentence":"The proposed ClarET is applicable to a wide range of event-centric reasoning scenarios, considering its versatility of (i) event-correlation types (e.g., causal, temporal, contrast), (ii) application formulations (i.e., generation and classification), and (iii) reasoning types (e.g., abductive, counterfactual and ending reasoning).","offset":3,"pro":0.5,"labels":"MTD"},{"idx":2,"sentence":"Empirical fine-tuning results, as well as zero- and few-shot learning, on 9 benchmarks (5 generation and 4 classification tasks covering 4 reasoning types with diverse event correlations), verify its effectiveness and generalization ability.","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":2,"sentence":"In this paper, we propose to pre-train a general Correlation-aware context-to-Event Transformer (ClarET) for event-centric reasoning.","offset":5,"pro":0.8333333333333334,"labels":"PUR"},{"idx":3,"sentence":"Neural Machine Translation (NMT) systems exhibit problematic biases, such as stereotypical gender bias in the translation of occupation terms into languages with grammatical gender.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"In this paper we describe a new source of bias prevalent in NMT systems, relating to translations of sentences containing person names.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":3,"sentence":"To correctly translate such sentences, a NMT system needs to determine the gender of the name.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":3,"sentence":"We show that leading systems are particularly poor at this task, especially for female given names.","offset":3,"pro":0.5,"labels":"CLN"},{"idx":3,"sentence":"This bias is deeper than given name gender: we show that the translation of terms with ambiguous sentiment can also be affected by person names, and the same holds true for proper nouns denoting race.","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":3,"sentence":"To mitigate these biases we propose a simple but effective data augmentation method based on randomly switching entities during translation, which effectively eliminates the problem without any effect on translation quality.","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":4,"sentence":"In this paper, we present a substantial step in better understanding the SOTA sequence-to-sequence (Seq2Seq) pretraining for neural machine translation (NMT).","offset":0,"pro":0,"labels":"PUR"},{"idx":4,"sentence":"We focus on studying the impact of the jointly pretrained decoder, which is the main difference between Seq2Seq pretraining and previous encoder-based pretraining approaches for NMT.","offset":1,"pro":0.14285714285714285,"labels":"MTD"},{"idx":4,"sentence":"On the other hand, the discrepancies between Seq2Seq pretraining and NMT finetuning limit the translation quality (i.e., domain discrepancy) and induce the over-estimation issue (i.e., objective discrepancy).","offset":2,"pro":0.2857142857142857,"labels":"RST"},{"idx":4,"sentence":"Based on these observations, we further propose simple and effective strategies, named in-domain pretraining and input adaptation to remedy the domain and objective discrepancies, respectively.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":4,"sentence":"Experimental results on several language pairs show that our approach can consistently improve both translation performance and model robustness upon Seq2Seq pretraining.","offset":4,"pro":0.5714285714285714,"labels":"CLN"},{"idx":4,"sentence":"By carefully designing experiments on three language pairs,","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":4,"sentence":"we find that Seq2Seq pretraining is a double-edged sword: On one hand, it helps NMT models to produce more diverse translations and reduce adequacy-related translation errors.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":5,"sentence":"Multimodal machine translation and textual chat translation have received considerable attention in recent years.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"Although the conversation in its natural form is usually multimodal, there still lacks work on multimodal machine translation in conversations.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":5,"sentence":"In this work, we introduce a new task named Multimodal Chat Translation (MCT), aiming to generate more accurate translations with the help of the associated dialogue history and visual context.","offset":2,"pro":0.2222222222222222,"labels":"PUR"},{"idx":5,"sentence":"To this end, we firstly construct a Multimodal Sentiment Chat Translation Dataset (MSCTD) containing 142,871 English-Chinese utterance pairs in 14,762 bilingual dialogues.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":5,"sentence":"Each utterance pair, corresponding to the visual context that reflects the current conversational scene, is annotated with a sentiment label.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":5,"sentence":"Then, we benchmark the task by establishing multiple baseline systems that incorporate multimodal and sentiment features for MCT.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":5,"sentence":"Preliminary experiments on two language directions (English-Chinese) verify the potential of contextual and multimodal information fusion and the positive impact of sentiment on the MCT task.","offset":6,"pro":0.6666666666666666,"labels":"CLN"},{"idx":5,"sentence":"Additionally, we provide a new benchmark on multimodal dialogue sentiment analysis with the constructed MSCTD.","offset":7,"pro":0.7777777777777778,"labels":"CTN"},{"idx":5,"sentence":"Our work can facilitate researches on both multimodal chat translation and multimodal dialogue sentiment analysis.","offset":8,"pro":0.8888888888888888,"labels":"CTN"},{"idx":6,"sentence":"When working with textual data, a natural application of disentangled representations is the fair classification where the goal is to make predictions without being biased (or influenced) by sensible attributes that may be present in the data (e.g., age, gender or race).","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"Dominant approaches to disentangle a sensitive attribute from textual representations rely on learning simultaneously a penalization term that involves either an adversary loss (e.g., a discriminator) or an information measure (e.g., mutual information).","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":6,"sentence":"However, these methods require the training of a deep neural network with several parameter updates for each update of the representation model.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":6,"sentence":"As a matter of fact, the resulting nested optimization loop is both times consuming, adding complexity to the optimization dynamic, and requires a fine hyperparameter selection (e.g., learning rates, architecture).","offset":3,"pro":0.42857142857142855,"labels":"GAP"},{"idx":6,"sentence":"In this work, we introduce a family of regularizers for learning disentangled representations that do not require training.","offset":4,"pro":0.5714285714285714,"labels":"PUR"},{"idx":6,"sentence":"These regularizers are based on statistical measures of similarity between the conditional probability distributions with respect to the sensible attributes.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":6,"sentence":"Our novel regularizers do not require additional training, are faster and do not involve additional tuning while achieving better results both when combined with pretrained and randomly initialized text encoders.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":7,"sentence":"Recent years have witnessed the emergence of a variety of post-hoc interpretations that aim to uncover how natural language processing (NLP) models make predictions.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"Despite the surge of new interpretation methods, it remains an open problem how to define and quantitatively measure the faithfulness of interpretations, i.e., to what extent interpretations reflect the reasoning process by a model.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":7,"sentence":"We propose two new criteria, sensitivity and stability, that provide complementary notions of faithfulness to the existed removal-based criteria.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":7,"sentence":"Our results show that the conclusion for how faithful interpretations are could vary substantially based on different notions.","offset":3,"pro":0.375,"labels":"CLN"},{"idx":7,"sentence":"Motivated by the desiderata of sensitivity and stability, we introduce a new class of interpretation methods that adopt techniques from adversarial robustness.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":7,"sentence":"Empirical results show that our proposed methods are effective under the new criteria and overcome limitations of gradient-based methods on removal-based criteria.","offset":5,"pro":0.625,"labels":"CLN"},{"idx":7,"sentence":"Besides text classification, we also apply interpretation methods and metrics to dependency parsing.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":7,"sentence":"Our results shed light on understanding the diverse set of interpretations.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":8,"sentence":"Solving crossword puzzles requires diverse reasoning capabilities, access to a vast amount of knowledge about language and the world, and the ability to satisfy the constraints imposed by the structure of the puzzle.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"In this work, we introduce solving crossword puzzles as a new natural language understanding task.","offset":1,"pro":0.125,"labels":"PUR"},{"idx":8,"sentence":"We release a corpus of crossword puzzles collected from the New York Times daily crossword spanning 25 years and comprised of a total of around nine thousand puzzles.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":8,"sentence":"These puzzles include a diverse set of clues: historic, factual, word meaning, synonyms/antonyms, fill-in-the-blank, abbreviations, prefixes/suffixes, wordplay, and cross-lingual, as well as clues that depend on the answers to other clues.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":8,"sentence":"We separately release the clue-answer pairs from these puzzles as an open-domain question answering dataset containing over half a million unique clue-answer pairs.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":8,"sentence":"For the question answering task, our baselines include several sequence-to-sequence and retrieval-based generative models.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":8,"sentence":"We also introduce a non-parametric constraint satisfaction baseline for solving the entire crossword puzzle.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":8,"sentence":"Finally, we propose an evaluation framework which consists of several complementary performance metrics.","offset":7,"pro":0.875,"labels":"MTD"},{"idx":9,"sentence":"Natural language processing models often exploit spurious correlations between task-independent features and labels in datasets to perform well only within the distributions they are trained on, while not generalising to different task distributions.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"We propose to tackle this problem by generating a debiased version of a dataset, which can then be used to train a debiased, off-the-shelf model, by simply replacing its training data.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":9,"sentence":"Our approach consists of 1) a method for training data generators to generate high-quality, label-consistent data samples; and 2) a filtering mechanism for removing data points that contribute to spurious correlations, measured in terms of z-statistics.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":9,"sentence":"We generate debiased versions of the SNLI and MNLI datasets, and we evaluate on a large suite of debiased, out-of-distribution, and adversarial test sets.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":9,"sentence":"Results show that models trained on our debiased datasets generalise better than those trained on the original datasets in all settings.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":9,"sentence":"On the majority of the datasets, our method outperforms or performs comparably to previous state-of-the-art debiasing strategies, and when combined with an orthogonal technique, product-of-experts, it improves further and outperforms previous best results of SNLI-hard and MNLI-hard.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":10,"sentence":"Due to high data demands of current methods, attention to zero-shot cross-lingual spoken language understanding (SLU) has grown, as such approaches greatly reduce human annotation effort.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"However, existing models solely rely on shared parameters, which can only perform implicit alignment across languages.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":10,"sentence":"We present Global-Local Contrastive Learning Framework (GL-CLeF) to address this shortcoming.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":10,"sentence":"Specifically, we employ contrastive learning, leveraging bilingual dictionaries to construct multilingual views of the same utterance, then encourage their representations to be more similar than negative example pairs, which achieves to explicitly align representations of similar sentences across languages.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":10,"sentence":"In addition, a key step in GL-CLeF is a proposed Local and Global component, which achieves a fine-grained cross-lingual transfer (i.e., sentence-level Local intent transfer, token-level Local slot transfer, and semantic-level Global transfer across intent and slot).","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":10,"sentence":"Experiments on MultiATIS++ show that GL-CLeF achieves the best performance and successfully pulls representations of similar sentences across languages closer.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":11,"sentence":"Recent advances in prompt-based learning have shown strong results on few-shot text classification by using cloze-style templates.Similar attempts have been made on named entity recognition (NER) which manually design templates to predict entity types for every text span in a sentence.","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"However, such methods may suffer from error propagation induced by entity span detection, high cost due to enumeration of all possible text spans, and omission of inter-dependencies among token labels in a sentence.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":11,"sentence":"Here we present a simple demonstration-based learning method for NER, which lets the input be prefaced by task demonstrations for in-context learning.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":11,"sentence":"We perform a systematic study on demonstration strategy regarding what to include (entity examples, with or without surrounding context), how to select the examples, and what templates to use.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":11,"sentence":"Results on in-domain learning and domain adaptation show that the model's performance in low-resource settings can be largely improved with a suitable demonstration strategy (e.g., a 4-17% improvement on 25 train instances).","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":11,"sentence":"We also find that good demonstration can save many labeled examples and consistency in demonstration contributes to better performance.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":12,"sentence":"Currently, masked language modeling (e.g., BERT) is the prime choice to learn contextualized representations.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"Due to the pervasiveness, it naturally raises an interesting question: how do masked language models (MLMs) learn contextual representations?","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":12,"sentence":"In this work, we analyze the learning dynamics of MLMs and find that it adopts sampled embeddings as anchors to estimate and inject contextual semantics to representations, which limits the efficiency and effectiveness of MLMs.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":12,"sentence":"To be specific, TACO extracts and aligns contextual semantics hidden in contextualized representations to encourage models to attend global semantics when generating contextualized representations.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":12,"sentence":"Experiments on the GLUE benchmark show that TACO achieves up to 5x speedup and up to 1.2 points average improvement over MLM.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":12,"sentence":"To address these problems, we propose TACO, a simple yet effective representation learning approach to directly model global semantics.","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":13,"sentence":"While hyper-parameters (HPs) are important for knowledge graph (KG) learning, existing methods fail to search them efficiently.","offset":0,"pro":0,"labels":"GAP"},{"idx":13,"sentence":"To solve this problem, we first analyze the properties of different HPs and measure the transfer ability from small subgraph to the full graph.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":13,"sentence":"Based on the analysis, we propose an efficient two-stage search algorithm KGTuner, which efficiently explores HP configurations on small subgraph at the first stage and transfers the top-performed configurations for fine-tuning on the large full graph at the second stage.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":13,"sentence":"Experiments show that our method can consistently find better HPs than the baseline algorithms within the same time budget, which achieves 9.1% average relative improvement for four embedding models on the large-scale KGs in open graph benchmark.","offset":3,"pro":0.6,"labels":"RST"},{"idx":13,"sentence":"Our code is released in https://github. com/AutoML-Research/KGTuner.","offset":4,"pro":0.8,"labels":"CTN"},{"idx":14,"sentence":"News events are often associated with quantities (e.g., the number of COVID-19 patients or the number of arrests in a protest), and it is often important to extract their type, time, and location from unstructured text in order to analyze these quantity events.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"This paper thus formulates the NLP problem of spatiotemporal quantity extraction, and proposes the first meta-framework for solving it.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":14,"sentence":"This meta-framework contains a formalism that decomposes the problem into several information extraction tasks, a shareable crowdsourcing pipeline, and transformer-based baseline models.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":14,"sentence":"We demonstrate the meta-framework in three domains-the COVID-19 pandemic, Black Lives Matter protests, and 2020 California wildfires-to show that the formalism is general and extensible, the crowdsourcing pipeline facilitates fast and high-quality data annotation, and the baseline system can handle spatiotemporal quantity extraction well enough to be practically useful.","offset":3,"pro":0.6,"labels":"CLN"},{"idx":14,"sentence":"We release all resources for future research on this topic at https://github.com/steqe.","offset":4,"pro":0.8,"labels":"CTN"},{"idx":15,"sentence":"Pre-trained language models are still far from human performance in tasks that need understanding of properties (e.g. appearance, measurable quantity) and affordances of everyday objects in the real world since the text lacks such information due to reporting bias.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"We investigate two types of knowledge transfer: (1) text knowledge transfer using image captions that may contain enriched visual knowledge and (2) cross-modal knowledge transfer using both images and captions with vision-language training objectives.","offset":1,"pro":0.5,"labels":"MTD"},{"idx":16,"sentence":"Our code is publicly available at url{https://github.com/woojeongjin/FewVLM","offset":0,"pro":0,"labels":"CTN"},{"idx":16,"sentence":"Large pre-trained vision-language (VL) models can learn a new task with a handful of examples and generalize to a new task without fine-tuning.","offset":1,"pro":0.25,"labels":"BAC"},{"idx":16,"sentence":"To solve this limitation, we study prompt-based low-resource learning of VL tasks with our proposed method, FewVLM, relatively smaller than recent few-shot learners.","offset":2,"pro":0.5,"labels":"PUR"},{"idx":16,"sentence":"Furthermore, we analyze the effect of diverse prompts for few-shot tasks.","offset":3,"pro":0.75,"labels":"MTD"},{"idx":17,"sentence":"Existing continual relation learning (CRL) methods rely on plenty of labeled training data for learning a new task, which can be hard to acquire in real scenario as getting large and representative labeled data is often expensive and time-consuming.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"It is therefore necessary for the model to learn novel relational patterns with very few labeled data while avoiding catastrophic forgetting of previous task knowledge.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":17,"sentence":"In this paper, we formulate this challenging yet practical problem as continual few-shot relation learning (CFRL).","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":17,"sentence":"Based on the finding that learning for new emerging few-shot tasks often results in feature distributions that are incompatible with previous tasks' learned distributions, we propose a novel method based on embedding space regularization and data augmentation.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":17,"sentence":"Our method generalizes to new few-shot tasks and avoids catastrophic forgetting of previous tasks by enforcing extra constraints on the relational embeddings and by adding extra relevant data in a self-supervised manner.","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":17,"sentence":"With extensive experiments we demonstrate that our method can significantly outperform previous state-of-the-art methods in CFRL task settings.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":18,"sentence":"Coreference resolution over semantic graphs like AMRs aims to group the graph nodes that represent the same entity.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"This is a crucial step for making document-level formal semantic representations.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":18,"sentence":"With annotated data on AMR coreference resolution, deep learning approaches have recently shown great potential for this task, yet they are usually data hunger and annotations are costly.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":18,"sentence":"We propose a general pretraining method using variational graph autoencoder (VGAE) for AMR coreference resolution, which can leverage any general AMR corpus and even automatically parsed AMR data.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":18,"sentence":"Experiments on benchmarks show that the pretraining approach achieves performance gains of up to 6% absolute F1 points.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":18,"sentence":"Moreover, our model significantly improves on the previous state-of-the-art model by up to 11% F1.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":19,"sentence":"Recent works of opinion expression identification (OEI) rely heavily on the quality and scale of the manually-constructed training corpus, which could be extremely difficult to satisfy.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"Crowdsourcing is one practical solution for this problem, aiming to create a large-scale but quality-unguaranteed corpus.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":19,"sentence":"In this work, we investigate Chinese OEI with extremely-noisy crowdsourcing annotations, constructing a dataset at a very low cost.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":19,"sentence":"Following Zhang el al. (2021), we train the annotator-adapter model by regarding all annotations as gold-standard in terms of crowd annotators, and test the model by using a synthetic expert, which is a mixture of all annotators.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":19,"sentence":"As this annotator-mixture for testing is never modeled explicitly in the training phase, we propose to generate synthetic training samples by a pertinent mixup strategy to make the training and testing highly consistent.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":19,"sentence":"The simulation experiments on our constructed dataset show that crowdsourcing is highly promising for OEI, and our proposed annotator-mixup can further enhance the crowdsourcing modeling.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":0,"sentence":"Knowledge graph embedding (KGE) models represent each entity and relation of a knowledge graph (KG) with low-dimensional embedding vectors.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"These methods have recently been applied to KG link prediction and question answering over incomplete KGs (KGQA).","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":0,"sentence":"KGEs typically create an embedding for each entity in the graph, which results in large model sizes on real-world graphs with millions of entities.","offset":2,"pro":0.2857142857142857,"labels":"BAC"},{"idx":0,"sentence":"For downstream tasks these atomic entity representations often need to be integrated into a multi stage pipeline, limiting their utility.","offset":3,"pro":0.42857142857142855,"labels":"GAP"},{"idx":0,"sentence":"We achieve this by posing KG link prediction as a sequence-to-sequence task and exchange the triple scoring approach taken by prior KGE methods with autoregressive decoding.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":0,"sentence":"Such a simple but powerful method reduces the model size up to 98% compared to conventional KGE models while keeping inference time tractable.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":0,"sentence":"After finetuning this model on the task of KGQA over incomplete KGs, our approach outperforms baselines on multiple large-scale datasets without extensive hyperparameter tuning.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":1,"sentence":"Human communication is a collaborative process.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"Speakers, on top of conveying their own intent, adjust the content and language expressions by taking the listeners into account, including their knowledge background, personalities, and physical capabilities.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":1,"sentence":"Towards building AI agents with similar abilities in language communication, we propose a novel rational reasoning framework, Pragmatic Rational Speaker (PRS), where the speaker attempts to learn the speaker-listener disparity and adjust the speech accordingly, by adding a light-weighted disparity adjustment layer into working memory on top of speaker's long-term memory system.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":1,"sentence":"By fixing the long-term memory, the PRS only needs to update its working memory to learn and adapt to different types of listeners.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":1,"sentence":"Our empirical results demonstrate that the PRS is able to shift its output towards the language that listeners are able to understand, significantly improve the collaborative task outcome, and learn the disparity more efficiently than joint training.","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":1,"sentence":"o validate our framework, we create a dataset that simulates different types of speaker-listener disparities in the context of referential games.","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":2,"sentence":"Recent research demonstrates the effectiveness of using fine-tuned language models (LM) for dense retrieval.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"However, dense retrievers are hard to train, typically requiring heavily engineered fine-tuning pipelines to realize their full potential.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":2,"sentence":"In this paper, we identify and address two underlying problems of dense retrievers: i) fragility to training data noise and ii) requiring large batches to robustly learn the embedding space.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":2,"sentence":"We use the recently proposed Condenser pre-training architecture, which learns to condense information into the dense vector through LM pre-training.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":2,"sentence":"On top of it, we propose coCondenser, which adds an unsupervised corpus-level contrastive loss to warm up the passage embedding space.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":2,"sentence":"Experiments on MS-MARCO, Natural Question, and Trivia QA datasets show that coCondenser removes the need for heavy data engineering such as augmentation, synthesis, or filtering, and the need for large batch training.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":2,"sentence":"It shows comparable performance to RocketQA, a state-of-the-art, heavily engineered system, using simple small batch fine-tuning.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":3,"sentence":"Responsing with image has been recognized as an important capability for an intelligent conversational agent.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"Yet existing works only focus on exploring the multimodal dialogue models which depend on retrieval-based methods, but neglecting generation methods.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":3,"sentence":"To fill in the gaps, we first present a new task: multimodal dialogue response generation (MDRG) - given the dialogue history, one model needs to generate a text sequence or an image as response.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":3,"sentence":"Motivated by the challenge in practice, we consider MDRG under a natural assumption that only limited training examples are available.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":3,"sentence":"In such a low-resource setting, we devise a novel conversational agent, Divter, in order to isolate parameters that depend on multimodal dialogues from the entire generation model.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":3,"sentence":"By this means, the major part of the model can be learned from a large number of text-only dialogues and text-image pairs respectively, then the whole parameters can be well fitted using the limited training examples.","offset":5,"pro":0.625,"labels":"RST"},{"idx":3,"sentence":"Extensive experiments demonstrate our method achieves state-of-the-art results in both automatic and human evaluation, and can generate informative text and high-resolution image responses.","offset":6,"pro":0.75,"labels":"CLN"},{"idx":3,"sentence":"Learning such a MDRG model often requires multimodal dialogues containing both texts and images which are difficult to obtain.","offset":7,"pro":0.875,"labels":"MTD"},{"idx":4,"sentence":"Knowledge graphs store a large number of factual triples while they are still incomplete, inevitably.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"The previous knowledge graph completion (KGC) models predict missing links between entities merely relying on fact-view data, ignoring the valuable commonsense knowledge.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":4,"sentence":"The previous knowledge graph embedding (KGE) techniques suffer from invalid negative sampling and the uncertainty of fact-view link prediction, limiting KGC's performance.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":4,"sentence":"To address the above challenges, we propose a novel and scalable Commonsense-Aware Knowledge Embedding (CAKE) framework to automatically extract commonsense from factual triples with entity concepts.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":4,"sentence":"Experimental results on the KGC task demonstrate that assembling our framework could enhance the performance of the original KGE models, and the proposed commonsense-aware NS module is superior to other NS techniques.","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":4,"sentence":"Besides, our proposed framework could be easily adaptive to various KGE models and explain the predicted results.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":5,"sentence":"Most dominant neural machine translation (NMT) models are restricted to make predictions only according to the local context of preceding words in a left-to-right manner.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"Although many previous studies try to incorporate global information into NMT models, there still exist limitations on how to effectively exploit bidirectional global context.","offset":1,"pro":0.1,"labels":"GAP"},{"idx":5,"sentence":"In this paper, we propose a Confidence Based Bidirectional Global Context Aware (CBBGCA) training framework for NMT, where the NMT model is jointly trained with an auxiliary conditional masked language model (CMLM).","offset":2,"pro":0.2,"labels":"PUR"},{"idx":5,"sentence":"The training consists of two stages: (1) multi-task joint training; (2) confidence based knowledge distillation.","offset":3,"pro":0.3,"labels":"MTD"},{"idx":5,"sentence":"At the first stage, by sharing encoder parameters, the NMT model is additionally supervised by the signal from the CMLM decoder that contains bidirectional global contexts.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":5,"sentence":"Moreover, at the second stage, using the CMLM as teacher, we further pertinently incorporate bidirectional global context to the NMT model on its unconfidently-predicted target words via knowledge distillation.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":5,"sentence":"Experimental results show that our proposed CBBGCA training framework significantly improves the NMT model by +1.","offset":6,"pro":0.6,"labels":"RST"},{"idx":5,"sentence":"02, +1.","offset":7,"pro":0.7,"labels":"RST"},{"idx":5,"sentence":"30 and +0.","offset":8,"pro":0.8,"labels":"RST"},{"idx":5,"sentence":"57 BLEU scores on three large-scale translation datasets, namely WMT'14 English-to-German, WMT'19 Chinese-to-English and WMT'14 English-to-French, respectively.","offset":9,"pro":0.9,"labels":"RST"},{"idx":6,"sentence":"Abstractive summarization models are commonly trained using maximum likelihood estimation, which assumes a deterministic (one-point) target distribution in which an ideal model will assign all the probability mass to the reference summary.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"This assumption may lead to performance degradation during inference, where the model needs to compare several system-generated (candidate) summaries that have deviated from the reference summary.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":6,"sentence":"To address this problem, we propose a novel training paradigm which assumes a non-deterministic distribution so that different candidate summaries are assigned probability mass according to their quality.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":6,"sentence":"Our method achieves a new state-of-the-art result on the CNN/DailyMail (47.","offset":3,"pro":0.42857142857142855,"labels":"RST"},{"idx":6,"sentence":"78 ROUGE-1) and XSum (49.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":6,"sentence":"07 ROUGE-1) datasets.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":6,"sentence":"Further analysis also shows that our model can estimate probabilities of candidate summaries that are more correlated with their level of quality.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":7,"sentence":"In sequence modeling, certain tokens are usually less ambiguous than others, and representations of these tokens require fewer refinements for disambiguation.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"However, given the nature of attention-based models like Transformer and UT (universal transformer), all tokens are equally processed towards depth.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":7,"sentence":"Our lazy transition is deployed on top of UT to build LT (lazy transformer), where all tokens are processed unequally towards depth.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":7,"sentence":"Eventually, LT is encouraged to oscillate around a relaxed equilibrium.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":7,"sentence":"Our experiments show that LT outperforms baseline models on several tasks of machine translation, pre-training, Learning to Execute, and LAMBADA.","offset":4,"pro":0.8,"labels":"RST"},{"idx":8,"sentence":"The fill-in-the-blanks setting tests a model's understanding of a video by requiring it to predict a masked noun phrase in the caption of the video, given the video and the surrounding text.","offset":0,"pro":0,"labels":"MTD"},{"idx":8,"sentence":"The FIBER benchmark does not share the weaknesses of the current state-of-the-art language-informed video understanding tasks, namely: (1) video question answering using multiple-choice questions, where models perform relatively well because they exploit linguistic biases in the task formulation, thus making our framework challenging for the current state-of-the-art systems to solve; and (2) video captioning, which relies on an open-ended evaluation framework that is often inaccurate because system answers may be perceived as incorrect if they differ in form from the ground truth.","offset":1,"pro":0.25,"labels":"MTD"},{"idx":8,"sentence":"The FIBER dataset and our code are available at https://lit. eecs. umich. edu/fiber/.","offset":2,"pro":0.5,"labels":"CTN"},{"idx":8,"sentence":"We propose fill-in-the-blanks as a video understanding evaluation framework and introduce FIBER - a novel dataset consisting of 28,000 videos and descriptions in support of this evaluation framework.","offset":3,"pro":0.75,"labels":"PUR"},{"idx":9,"sentence":"With the rapid growth of the PubMed database, large-scale biomedical document indexing becomes increasingly important.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"Experimental results show the proposed method achieves state-of-the-art performance on a number of measures.","offset":1,"pro":0.2,"labels":"CLN"},{"idx":9,"sentence":"Currently, Medical Subject Headings (MeSH) are manually assigned to every biomedical article published and subsequently recorded in the PubMed database to facilitate retrieving relevant information.","offset":2,"pro":0.4,"labels":"BAC"},{"idx":9,"sentence":"MeSH indexing is a challenging task for machine learning, as it needs to assign multiple labels to each article from an extremely large hierachically organized collection. ","offset":3,"pro":0.6,"labels":"GAP"},{"idx":9,"sentence":"an end-to-end model that combines new text features and a dynamic knowledge-enhanced mask attention that integrates document features with MeSH label hierarchy and journal correlation features to index MeSH terms.","offset":4,"pro":0.8,"labels":"MTD"},{"idx":10,"sentence":"Effective question-asking is a crucial component of a successful conversational chatbot.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"However, current dialog generation approaches do not model this subtle emotion regulation technique due to the lack of a taxonomy of questions and their purpose in social chitchat.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":10,"sentence":"To address this gap, we have developed an empathetic question taxonomy (EQT), with special attention paid to questions' ability to capture communicative acts and their emotion-regulation intents.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":10,"sentence":"We further design a crowd-sourcing task to annotate a large subset of the EmpatheticDialogues dataset with the established labels.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":10,"sentence":"We use the crowd-annotated data to develop automatic labeling tools and produce labels for the whole dataset.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":10,"sentence":"Finally, we employ information visualization techniques to summarize co-occurrences of question acts and intents and their role in regulating interlocutor's emotion.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":10,"sentence":"These results reveal important question-asking strategies in social dialogs.","offset":6,"pro":0.75,"labels":"CLN"},{"idx":10,"sentence":"More importantly, it can inform future efforts in empathetic question generation using neural or hybrid methods.","offset":7,"pro":0.875,"labels":"IMP"},{"idx":11,"sentence":"Aspect Sentiment Triplet Extraction (ASTE) is an emerging sentiment analysis task.","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"Most of the existing studies focus on devising a new tagging scheme that enables the model to extract the sentiment triplets in an end-to-end fashion.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":11,"sentence":"However, these methods ignore the relations between words for ASTE task.","offset":2,"pro":0.2,"labels":"GAP"},{"idx":11,"sentence":"In this paper, we propose an Enhanced Multi-Channel Graph Convolutional Network model (EMC-GCN) to fully utilize the relations between words.","offset":3,"pro":0.3,"labels":"PUR"},{"idx":11,"sentence":"Specifically, we first define ten types of relations for ASTE task, and then adopt a biaffine attention module to embed these relations as an adjacent tensor between words in a sentence.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":11,"sentence":"After that, our EMC-GCN transforms the sentence into a multi-channel graph by treating words and the relation adjacent tensor as nodes and edges, respectively.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":11,"sentence":"Thus, relation-aware node representations can be learnt.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":11,"sentence":"Furthermore, we consider diverse linguistic features to enhance our EMC-GCN model.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":11,"sentence":"Finally, we design an effective refining strategy on EMC-GCN for word-pair representation refinement, which considers the implicit results of aspect and opinion extraction when determining whether word pairs match or not.","offset":8,"pro":0.8,"labels":"MTD"},{"idx":11,"sentence":"Extensive experimental results on the benchmark datasets demonstrate that the effectiveness and robustness of our proposed model, which outperforms state-of-the-art methods significantly.","offset":9,"pro":0.9,"labels":"CLN"},{"idx":12,"sentence":"We present ProtoTEx, a novel white-box NLP classification architecture based on prototype networks (Li et al. , 2018).","offset":0,"pro":0,"labels":"PUR"},{"idx":12,"sentence":"ProtoTEx faithfully explains model decisions based on prototype tensors that encode latent clusters of training examples.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":12,"sentence":"At inference time, classification decisions are based on the distances between the input text and the prototype tensors, explained via the training examples most similar to the most influential prototypes.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":12,"sentence":"We also describe a novel interleaved training algorithm that effectively handles classes characterized by ProtoTEx indicative features.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":12,"sentence":"On a propaganda detection task, ProtoTEx accuracy matches BART-large and exceeds BERTlarge with the added benefit of providing faithful explanations.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":12,"sentence":"A user study also shows that prototype-based explanations help non-experts to better recognize propaganda in online news.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":13,"sentence":"Procedures are inherently hierarchical.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"To make videos, one may need to purchase a camera, which in turn may require one to set a budget.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":13,"sentence":"While such hierarchical knowledge is critical for reasoning about complex procedures, most existing work has treated procedures as shallow structures without modeling the parent-child relation.","offset":2,"pro":0.4,"labels":"GAP"},{"idx":13,"sentence":"In this work, we attempt to construct an open-domain hierarchical knowledge-base (KB) of procedures based on wikiHow, a website containing more than 110k instructional articles, each documenting the steps to carry out a complex procedure.","offset":3,"pro":0.6,"labels":"PUR"},{"idx":13,"sentence":"Our method significantly outperforms several strong baselines according to automatic evaluation, human judgment, and application to downstream tasks such as instructional video retrieval.","offset":4,"pro":0.8,"labels":"RST"},{"idx":14,"sentence":"In contrast to recent advances focusing on high-level representation learning across modalities, in this work we present a self-supervised learning framework that is able to learn a representation that captures finer levels of granularity across different modalities such as concepts or events represented by visual objects or spoken words.","offset":0,"pro":0,"labels":"PUR"},{"idx":14,"sentence":"Our framework relies on a discretized embedding space created via vector quantization that is shared across different modalities.","offset":1,"pro":0.25,"labels":"MTD"},{"idx":14,"sentence":"Beyond the shared embedding space, we propose a Cross-Modal Code Matching objective that forces the representations from different views (modalities) to have a similar distribution over the discrete embedding space such that cross-modal objects/actions localization can be performed without direct supervision.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":14,"sentence":"We show that the proposed discretized multi-modal fine-grained representation (e. g. , pixel/word/frame) can complement high-level summary representations (e. g. , video/sentence/waveform) for improved performance on cross-modal retrieval tasks.","offset":3,"pro":0.75,"labels":"CLN"},{"idx":15,"sentence":"Representations of events described in text are important for various tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"In this work, we present SWCC: a Simultaneous Weakly supervised Contrastive learning and Clustering framework for event representation learning.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":15,"sentence":"SWCC learns event representations by making better use of co-occurrence information of events.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":15,"sentence":"Specifically, we introduce a weakly supervised contrastive learning method that allows us to consider multiple positives and multiple negatives, and a prototype-based clustering method that avoids semantically related events being pulled apart.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":15,"sentence":"For model training, SWCC learns representations by simultaneously performing weakly supervised contrastive learning and prototype-based clustering.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":15,"sentence":"Experimental results show that SWCC outperforms other baselines on Hard Similarity and Transitive Sentence Similarity tasks.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":15,"sentence":"In addition, a thorough analysis of the prototype-based clustering method demonstrates that the learned prototype vectors are able to implicitly capture various relations between events.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":16,"sentence":"Our results indicate that high anisotropy is not an inevitable consequence of contextualization, and that visual semantic pretraining is beneficial not only for ordering visual representations, but also for encoding useful semantic representations of language, both on the word level and the sentence level.","offset":0,"pro":0,"labels":"CLN"},{"idx":16,"sentence":"We examine the effects of contrastive visual semantic pretraining by comparing the geometry and semantic properties of contextualized English language representations formed by GPT-2 and CLIP, a zero-shot multimodal image classifier which adapts the GPT-2 architecture to encode image captions. ","offset":1,"pro":0.3333333333333333,"labels":"PUR"},{"idx":16,"sentence":"CLIP word embeddings outperform GPT-2 on word-level semantic intrinsic evaluation tasks, and achieve a new corpus-based state of the art for the RG65 evaluation, at . 88. ","offset":2,"pro":0.6666666666666666,"labels":"RST"},{"idx":17,"sentence":"The mainstream machine learning paradigms for NLP often work with two underlying presumptions.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"The system is required to (i) generate the expected outputs of a new task by learning from its instruction, (ii) transfer the knowledge acquired from upstream tasks to help solve downstream tasks (i. e. , forward-transfer), and (iii) retain or even improve the performance on earlier tasks after learning new tasks (i. e. , backward-transfer).","offset":1,"pro":0.14285714285714285,"labels":"MTD"},{"idx":17,"sentence":"This new problem is studied on a stream of more than 60 tasks, each equipped with an instruction.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":17,"sentence":"Technically, our method InstructionSpeak contains two strategies that make full use of task instructions to improve forward-transfer and backward-transfer: one is to learn from negative outputs, the other is to re-visit instructions of previous tasks.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":17,"sentence":"To our knowledge, this is the first time to study ConTinTin in NLP.","offset":4,"pro":0.5714285714285714,"labels":"CTN"},{"idx":17,"sentence":"First, the target task is predefined and static; a system merely needs to learn to solve it exclusively. ","offset":5,"pro":0.7142857142857143,"labels":"BAC"},{"idx":17,"sentence":"A question arises: how to build a system that can keep learning new tasks from their instructions?","offset":6,"pro":0.8571428571428571,"labels":"BAC"},{"idx":18,"sentence":"Our system works by generating answer candidates for each crossword clue using neural question answering models and then combines loopy belief propagation with local search to find full puzzle solutions.","offset":0,"pro":0,"labels":"MTD"},{"idx":18,"sentence":"Compared to existing approaches, our system improves exact puzzle accuracy from 57% to 82% on crosswords from The New York Times and obtains 99.","offset":1,"pro":0.2,"labels":"RST"},{"idx":18,"sentence":"9% letter accuracy on themeless puzzles.","offset":2,"pro":0.4,"labels":"RST"},{"idx":18,"sentence":"Our system also won first place at the top human crossword tournament, which marks the first time that a computer program has surpassed human performance at this event.","offset":3,"pro":0.6,"labels":"RST"},{"idx":18,"sentence":"We present the Berkeley Crossword Solver, a state-of-the-art approach for automatically solving crossword puzzles.","offset":4,"pro":0.8,"labels":"PUR"},{"idx":19,"sentence":"We present an incremental syntactic representation that consists of assigning a single discrete label to each word in a sentence, where the label is predicted using strictly incremental processing of a prefix of the sentence, and the sequence of labels for a sentence fully determines a parse tree.","offset":0,"pro":0,"labels":"MTD"},{"idx":19,"sentence":"Our goal is to induce a syntactic representation that commits to syntactic choices only as they are incrementally revealed by the input, in contrast with standard representations that must make output choices such as attachments speculatively and later throw out conflicting analyses.","offset":1,"pro":0.3333333333333333,"labels":"PUR"},{"idx":19,"sentence":"Our learned representations achieve 93. 72 F1 on the Penn Treebank with as few as 5 bits per word, and at 8 bits per word they achieve 94. 97 F1, which is comparable with other state of the art parsing models when using the same pre-trained embeddings. ","offset":2,"pro":0.6666666666666666,"labels":"RST"},{"idx":0,"sentence":"In this paper, we study the effect of commonsense and domain knowledge while generating responses in counseling conversations using retrieval and generative methods for knowledge integration.","offset":0,"pro":0,"labels":"PUR"},{"idx":0,"sentence":"We propose a pipeline that collects domain knowledge through web mining, and show that retrieval from both domain-specific and commonsense knowledge bases improves the quality of generated responses.","offset":1,"pro":0.25,"labels":"MTD"},{"idx":0,"sentence":"We also present a model that incorporates knowledge generated by COMET using soft positional encoding and masked self-attention.We show that both retrieved and COMET-generated knowledge improve the system's performance as measured by automatic metrics and also by human evaluation.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":0,"sentence":"Lastly, we present a comparative study on the types of knowledge encoded by our system showing that causal and intentional relationships benefit the generation task more than other types of commonsense relations.","offset":3,"pro":0.75,"labels":"MTD"},{"idx":1,"sentence":"Even to a simple and short news headline, readers react in a multitude of ways: cognitively (e.g. inferring the writer's intent), emotionally (e.g. feeling distrust), and behaviorally (e.g. sharing the news with their friends).","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"In contrast to categorical schema, our free-text dimensions provide a more nuanced way of understanding intent beyond being benign or malicious.","offset":1,"pro":0.14285714285714285,"labels":"MTD"},{"idx":1,"sentence":"We also introduce a Misinfo Reaction Frames corpus, a crowdsourced dataset of reactions to over 25k news headlines focusing on global crises: the Covid-19 pandemic, climate change, and cancer.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":1,"sentence":"Empirical results confirm that it is indeed possible for neural models to predict the prominent patterns of readers' reactions to previously unseen news headlines.","offset":3,"pro":0.42857142857142855,"labels":"CLN"},{"idx":1,"sentence":"Additionally, our user study shows that displaying machine-generated MRF implications alongside news headlines to readers can increase their trust in real news while decreasing their trust in misinformation.","offset":4,"pro":0.5714285714285714,"labels":"CLN"},{"idx":1,"sentence":"Our work demonstrates the feasibility and importance of pragmatic inferences on news headlines to help enhance AI-guided misinformation detection and mitigation.","offset":5,"pro":0.7142857142857143,"labels":"CTN"},{"idx":1,"sentence":"Such reactions are instantaneous and yet complex, as they rely on factors that go beyond interpreting factual content of news.","offset":6,"pro":0.8571428571428571,"labels":"BAC"},{"idx":2,"sentence":"Real-world natural language processing (NLP) models need to be continually updated to fix the prediction errors in out-of-distribution (OOD) data streams while overcoming catastrophic forgetting.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"However, existing continual learning (CL) problem setups cannot cover such a realistic and complex scenario.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":2,"sentence":"In response to this, we propose a new CL problem formulation dubbed continual model refinement (CMR).","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":2,"sentence":"Compared to prior CL settings, CMR is more practical and introduces unique challenges (boundary-agnostic and non-stationary distribution shift, diverse mixtures of multiple OOD data clusters, error-centric streams, etc.).","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":2,"sentence":"We extend several existing CL approaches to the CMR setting and evaluate them extensively.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":2,"sentence":"For benchmarking and analysis, we propose a general sampling algorithm to obtain dynamic OOD data streams with controllable non-stationarity, as well as a suite of metrics measuring various aspects of online performance.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":2,"sentence":"Our experiments and detailed analysis reveal the promise and challenges of the CMR problem, supporting that studying CMR in dynamic OOD streams can benefit the longevity of deployed NLP models in production.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":3,"sentence":"A limitation of current neural dialog models is that they tend to suffer from a lack of specificity and informativeness in generated responses, primarily due to dependence on training data that covers a limited variety of scenarios and conveys limited knowledge.","offset":0,"pro":0,"labels":"GAP"},{"idx":3,"sentence":"One way to alleviate this issue is to extract relevant knowledge from external sources at decoding time and incorporate it into the dialog response.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":3,"sentence":"In this paper, we propose a post-hoc knowledge-injection technique where we first retrieve a diverse set of relevant knowledge snippets conditioned on both the dialog history and an initial response from an existing dialog model.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":3,"sentence":"We construct multiple candidate responses, individually injecting each retrieved snippet into the initial response using a gradient-based decoding method, and then select the final response with an unsupervised ranking step.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":3,"sentence":"Our experiments in goal-oriented and knowledge-grounded dialog settings demonstrate that human annotators judge the outputs from the proposed method to be more engaging and informative compared to responses from prior dialog systems.","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":3,"sentence":"We further show that knowledge-augmentation promotes success in achieving conversational goals in both experimental settings.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":4,"sentence":"It remains an open question whether incorporating external knowledge benefits commonsense reasoning while maintaining the flexibility of pretrained sequence models.","offset":0,"pro":0,"labels":"GAP"},{"idx":4,"sentence":"To investigate this question, we develop generated knowledge prompting, which consists of generating knowledge from a language model, then providing the knowledge as additional input when answering a question.","offset":1,"pro":0.25,"labels":"PUR"},{"idx":4,"sentence":"Our method does not require task-specific supervision for knowledge integration, or access to a structured knowledge base, yet it improves performance of large-scale, state-of-the-art models on four commonsense reasoning tasks, achieving state-of-the-art results on numerical commonsense (NumerSense), general commonsense (CommonsenseQA 2.0), and scientific commonsense (QASC) benchmarks.","offset":2,"pro":0.5,"labels":"CLN"},{"idx":4,"sentence":"Generated knowledge prompting highlights large-scale language models as flexible sources of external knowledge for improving commonsense reasoning.Our code is available at url{github.com/liujch1998/GKP","offset":3,"pro":0.75,"labels":"CTN"},{"idx":5,"sentence":"Retrieval-based methods have been shown to be effective in NLP tasks via introducing external knowledge.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"However, the indexing and retrieving of large-scale corpora bring considerable computational cost.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":5,"sentence":"Surprisingly, we found that REtrieving from the traINing datA (REINA) only can lead to significant gains on multiple NLG and NLU tasks.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":5,"sentence":"We retrieve the labeled training instances most similar to the input text and then concatenate them with the input to feed into the model to generate the output.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":5,"sentence":"Experimental results show that this simple method can achieve significantly better performance on a variety of NLU and NLG tasks, including summarization, machine translation, language modeling, and question answering tasks.","offset":4,"pro":0.5714285714285714,"labels":"CLN"},{"idx":5,"sentence":"For instance, our proposed method achieved state-of-the-art results on XSum, BigPatent, and CommonsenseQA.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":5,"sentence":"Our code is released, https://github.com/microsoft/REINA .","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":6,"sentence":"Existing pre-trained transformer analysis works usually focus only on one or two model families at a time, overlooking the variability of the architecture and pre-training objectives.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"In our work, we utilize the oLMpics bench- mark and psycholinguistic probing datasets for a diverse set of 29 models including T5, BART, and ALBERT.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":6,"sentence":"Additionally, we adapt the oLMpics zero-shot setup for autoregres- sive models and evaluate GPT networks of different sizes.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":6,"sentence":"Our findings show that none of these models can resolve compositional questions in a zero-shot fashion, suggesting that this skill is not learnable using existing pre-training objectives.","offset":3,"pro":0.6,"labels":"RST"},{"idx":6,"sentence":"Furthermore, we find that global model decisions such as architecture, directionality, size of the dataset, and pre-training objective are not predictive of a model's linguistic capabilities.","offset":4,"pro":0.8,"labels":"RST"},{"idx":7,"sentence":"Controlled text perturbation is useful for evaluating and improving model generalizability.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"However, current techniques rely on training a model for every target perturbation, which is expensive and hard to generalize.","offset":1,"pro":0.09090909090909091,"labels":"GAP"},{"idx":7,"sentence":"We present Tailor, a semantically-controlled text generation system.","offset":2,"pro":0.18181818181818182,"labels":"MTD"},{"idx":7,"sentence":"Tailor builds on a pretrained seq2seq model and produces textual outputs conditioned on control codes derived from semantic representations.","offset":3,"pro":0.2727272727272727,"labels":"MTD"},{"idx":7,"sentence":"We craft a set of operations to modify the control codes, which in turn steer generation towards targeted attributes.","offset":4,"pro":0.36363636363636365,"labels":"MTD"},{"idx":7,"sentence":"These operations can be further composed into higher-level ones, allowing for flexible perturbation strategies.","offset":5,"pro":0.45454545454545453,"labels":"MTD"},{"idx":7,"sentence":"We demonstrate the effectiveness of these perturbations in multiple applications.","offset":6,"pro":0.5454545454545454,"labels":"MTD"},{"idx":7,"sentence":"First, we use Tailor to automatically create high-quality contrast sets for four distinct natural language processing (NLP) tasks.","offset":7,"pro":0.6363636363636364,"labels":"MTD"},{"idx":7,"sentence":"These contrast sets contain fewer spurious artifacts and are complementary to manually annotated ones in their lexical diversity.","offset":8,"pro":0.7272727272727273,"labels":"RST"},{"idx":7,"sentence":"Second, we show that Tailor perturbations can improve model generalization through data augmentation.","offset":9,"pro":0.8181818181818182,"labels":"CLN"},{"idx":7,"sentence":"Perturbing just ∼2% of training data leads to a 5.8-point gain on an NLI challenge set measuring reliance on syntactic heuristics.","offset":10,"pro":0.9090909090909091,"labels":"RST"},{"idx":8,"sentence":"We propose a benchmark to measure whether a language model is truthful in generating answers to questions.","offset":0,"pro":0,"labels":"PUR"},{"idx":8,"sentence":"The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics.","offset":1,"pro":0.09090909090909091,"labels":"MTD"},{"idx":8,"sentence":"We crafted questions that some humans would answer falsely due to a false belief or misconception.","offset":2,"pro":0.18181818181818182,"labels":"MTD"},{"idx":8,"sentence":"To perform well, models must avoid generating false answers learned from imitating human texts.","offset":3,"pro":0.2727272727272727,"labels":"BAC"},{"idx":8,"sentence":"We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model.","offset":4,"pro":0.36363636363636365,"labels":"MTD"},{"idx":8,"sentence":"The best model was truthful on 58% of questions, while human performance was 94%.","offset":5,"pro":0.45454545454545453,"labels":"RST"},{"idx":8,"sentence":"Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans.","offset":6,"pro":0.5454545454545454,"labels":"BAC"},{"idx":8,"sentence":"The largest models were generally the least truthful.","offset":7,"pro":0.6363636363636364,"labels":"CLN"},{"idx":8,"sentence":"This contrasts with other NLP tasks, where performance improves with model size.","offset":8,"pro":0.7272727272727273,"labels":"CLN"},{"idx":8,"sentence":"However, this result is expected if false answers are learned from the training distribution.","offset":9,"pro":0.8181818181818182,"labels":"GAP"},{"idx":8,"sentence":"We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.","offset":10,"pro":0.9090909090909091,"labels":"CLN"},{"idx":9,"sentence":"Current approaches to testing and debugging NLP models rely on highly variable human creativity and extensive labor, or only work for a very restrictive class of bugs.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"We present AdaTest, a process which uses large scale language models (LMs) in partnership with human feedback to automatically write unit tests highlighting bugs in a target model.","offset":1,"pro":0.25,"labels":"MTD"},{"idx":9,"sentence":"Such bugs are then addressed through an iterative text-fix-retest loop, inspired by traditional software development.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":9,"sentence":"In experiments with expert and non-expert users and commercial / research models for 8 different tasks, AdaTest makes users 5-10x more effective at finding bugs than current approaches, and helps users effectively fix bugs without adding new bugs.","offset":3,"pro":0.75,"labels":"RST"},{"idx":10,"sentence":"When pre-trained contextualized embedding-based models developed for unstructured data are adapted for structured tabular data, they perform admirably.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"However, recent probing studies show that these models use spurious correlations, and often predict inference labels by focusing on false evidence or ignoring it altogether.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":10,"sentence":"To study this issue, we introduce the task of Trustworthy Tabular Reasoning, where a model needs to extract evidence to be used for reasoning, in addition to predicting the label.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":10,"sentence":"As a case study, we propose a two-stage sequential prediction approach, which includes an evidence extraction and an inference stage.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":10,"sentence":"First, we crowdsource evidence row labels and develop several unsupervised and supervised evidence extraction strategies for InfoTabS, a tabular NLI benchmark.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":10,"sentence":"Our evidence extraction strategy outperforms earlier baselines.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":10,"sentence":"On the downstream tabular inference task, using only the automatically extracted evidence as the premise, our approach outperforms prior benchmarks.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":11,"sentence":"The composition of richly-inflected words in morphologically complex languages can be a challenge for language learners developing literacy.","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"Accordingly, Lane and Bird (2020) proposed a finite state approach which maps prefixes in a language to a set of possible completions up to the next morpheme boundary, for the incremental building of complex words.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":11,"sentence":"In this work, we develop an approach to morph-based auto-completion based on a finite state morphological analyzer of Plains Cree (n{^ehiyaw{^ewin), showing the portability of the concept to a much larger, more complete morphological transducer.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":11,"sentence":"Additionally, we propose and compare various novel ranking strategies on the morph auto-complete output.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":11,"sentence":"The best weighting scheme ranks the target completion in the top 10 results in 64.9% of queries, and in the top 50 in 73.9% of queries.","offset":4,"pro":0.8,"labels":"RST"},{"idx":12,"sentence":"Semantic parsing is the task of producing structured meaning representations for natural language sentences.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"Recent research has pointed out that the commonly-used sequence-to-sequence (seq2seq) semantic parsers struggle to generalize systematically, i.e. to handle examples that require recombining known knowledge in novel settings.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":12,"sentence":"In this work, we show that better systematic generalization can be achieved by producing the meaning representation directly as a graph and not as a sequence.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":12,"sentence":"To this end we propose LAGr (Label Aligned Graphs), a general framework to produce semantic parses by independently predicting node and edge labels for a complete multi-layer input-aligned graph.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":12,"sentence":"The strongly-supervised LAGr algorithm requires aligned graphs as inputs, whereas weakly-supervised LAGr infers alignments for originally unaligned target graphs using approximate maximum-a-posteriori inference.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":12,"sentence":"Experiments demonstrate that LAGr achieves significant improvements in systematic generalization upon the baseline seq2seq parsers in both strongly- and weakly-supervised settings.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":13,"sentence":"Toxic language detection systems often falsely flag text that contains minority group mentions as toxic, as those groups are often the targets of online hate.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"Such over-reliance on spurious correlations also causes systems to struggle with detecting implicitly toxic language.To help mitigate these issues, we create ToxiGen, a new large-scale and machine-generated dataset of 274k toxic and benign statements about 13 minority groups.","offset":1,"pro":0.125,"labels":"PUR"},{"idx":13,"sentence":"We develop a demonstration-based prompting framework and an adversarial classifier-in-the-loop decoding method to generate subtly toxic and benign text with a massive pretrained language model.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":13,"sentence":"Controlling machine generation in this way allows ToxiGen to cover implicitly toxic text at a larger scale, and about more demographic groups, than previous resources of human-written text.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":13,"sentence":"We conduct a human evaluation on a challenging subset of ToxiGen and find that annotators struggle to distinguish machine-generated text from human-written language.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":13,"sentence":"We also find that 94.5% of toxic examples are labeled as hate speech by human annotators.","offset":5,"pro":0.625,"labels":"RST"},{"idx":13,"sentence":"Using three publicly-available datasets, we show that finetuning a toxicity classifier on our data improves its performance on human-written data substantially.","offset":6,"pro":0.75,"labels":"RST"},{"idx":13,"sentence":"We also demonstrate that ToxiGen can be used to fight machine-generated toxicity as finetuning improves the classifier significantly on our evaluation subset.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":14,"sentence":"We present a direct speech-to-speech translation (S2ST) model that translates speech from one language to speech in another language without relying on intermediate text generation.","offset":0,"pro":0,"labels":"PUR"},{"idx":14,"sentence":"We tackle the problem by first applying a self-supervised discrete speech encoder on the target speech and then training a sequence-to-sequence speech-to-unit translation (S2UT) model to predict the discrete representations of the target speech.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":14,"sentence":"When target text transcripts are available, we design a joint speech and text training framework that enables the model to generate dual modality output (speech and text) simultaneously in the same inference pass.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":14,"sentence":"Experiments on the Fisher Spanish-English dataset show that the proposed framework yields improvement of 6.7 BLEU compared with a baseline direct S2ST model that predicts spectrogram features.","offset":3,"pro":0.6,"labels":"RST"},{"idx":14,"sentence":"When trained without any text transcripts, our model performance is comparable to models that predict spectrograms and are trained with text supervision, showing the potential of our system for translation between unwritten languages.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":15,"sentence":"State-of-the-art abstractive summarization systems often generate hallucinations; i.e., content that is not directly inferable from the source text.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"Despite being assumed to be incorrect, we find that much hallucinated content is actually consistent with world knowledge, which we call factual hallucinations.","offset":1,"pro":0.16666666666666666,"labels":"RST"},{"idx":15,"sentence":"Including these factual hallucinations in a summary can be beneficial because they provide useful background information.","offset":2,"pro":0.3333333333333333,"labels":"CLN"},{"idx":15,"sentence":"In this work, we propose a novel detection approach that separates factual from non-factual hallucinations of entities.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":15,"sentence":"Our method is based on an entity's prior and posterior probabilities according to pre-trained and finetuned masked language models, respectively.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":15,"sentence":"Empirical results suggest that our method vastly outperforms two baselines in both accuracy and F1 scores and has a strong correlation with human judgments on factuality classification tasks.Furthermore, we use our method as a reward signal to train a summarization system using an off-line reinforcement learning (RL) algorithm that can significantly improve the factuality of generated summaries while maintaining the level of abstractiveness.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":16,"sentence":"We propose extensions to state-of-the-art summarization approaches that achieve substantially better results on our data set.","offset":0,"pro":0,"labels":"MTD"},{"idx":16,"sentence":"Our analysis and results show the challenging nature of this task and of the proposed data set.","offset":1,"pro":0.25,"labels":"CLN"},{"idx":16,"sentence":"ontrollable summarization aims to provide summaries that take into account user-specified aspects and preferences to better assist them with their information need, as opposed to the standard summarization setup which build a single generic summary of a document.","offset":2,"pro":0.5,"labels":"BAC"},{"idx":16,"sentence":"e introduce a human-annotated data set EntSUM for controllable summarization with a focus on named entities as the aspects to control.We conduct an extensive quantitative analysis to motivate the task of entity-centric summarization and show that existing methods for controllable summarization fail to generate entity-centric summaries.","offset":3,"pro":0.75,"labels":"PUR"},{"idx":17,"sentence":"User language data can contain highly sensitive personal content.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"As such, it is imperative to offer users a strong and interpretable privacy guarantee when learning from their data.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":17,"sentence":"In this work we propose SentDP, pure local differential privacy at the sentence level for a single user document.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":17,"sentence":"We propose a novel technique, DeepCandidate, that combines concepts from robust statistics and language modeling to produce high (768) dimensional, general epsilon-SentDP document embeddings.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":17,"sentence":"This guarantees that any single sentence in a document can be substituted with any other sentence while keeping the embedding epsilon-indistinguishable.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":17,"sentence":"Our experiments indicate that these private document embeddings are useful for downstream tasks like sentiment analysis and topic classification and even outperform baseline methods with weaker guarantees like word-level Metric DP.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":18,"sentence":"As language technologies become more ubiquitous, there are increasing efforts towards expanding the language diversity and coverage of natural language processing (NLP) systems.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"Arguably, the most important factor influencing the quality of modern NLP systems is data availability.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":18,"sentence":"In this work, we study the geographical representativeness of NLP datasets, aiming to quantify if and by how much do NLP datasets match the expected needs of the language speakers.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":18,"sentence":"In doing so, we use entity recognition and linking systems, also making important observations about their cross-lingual consistency and giving suggestions for more robust evaluation.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":18,"sentence":"Last, we explore some geographical and economic factors that may explain the observed dataset distributions.","offset":4,"pro":0.8,"labels":"MTD"},{"idx":19,"sentence":"Knowledge of difficulty level of questions helps a teacher in several ways, such as estimating students' potential quickly by asking carefully selected questions and improving quality of examination by modifying trivial and hard questions.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"Can we extract such benefits of instance difficulty in Natural Language Processing?","offset":1,"pro":0.2,"labels":"PUR"},{"idx":19,"sentence":"To this end, we conduct Instance-Level Difficulty Analysis of Evaluation data (ILDAE) in a large-scale setup of 23 datasets and demonstrate its five novel applications: 1) conducting efficient-yet-accurate evaluations with fewer instances saving computational cost and time, 2) improving quality of existing evaluation datasets by repairing erroneous and trivial instances, 3) selecting the best model based on application requirements, 4) analyzing dataset characteristics for guiding future data creation, 5) estimating Out-of-Domain performance reliably.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":19,"sentence":"Comprehensive experiments for these applications lead to several interesting results, such as evaluation using just 5% instances (selected via ILDAE) achieves as high as 0.93 Kendall correlation with evaluation using complete dataset and computing weighted accuracy using difficulty scores leads to 5.2% higher correlation with Out-of-Domain performance.","offset":3,"pro":0.6,"labels":"RST"},{"idx":19,"sentence":"We release the difficulty scores and hope our work will encourage research in this important yet understudied field of leveraging instance difficulty in evaluations.","offset":4,"pro":0.8,"labels":"CTN"},{"idx":0,"sentence":"The ability to integrate context, including perceptual and temporal cues, plays a pivotal role in grounding the meaning of a linguistic utterance.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"In order to measure to what extent current vision-and-language models master this ability, we devise a new multimodal challenge, Image Retrieval from Contextual Descriptions (ImageCoDe).","offset":1,"pro":0.2,"labels":"PUR"},{"idx":0,"sentence":"In particular, models are tasked with retrieving the correct image from a set of 10 minimally contrastive candidates based on a contextual description.As such, each description contains only the details that help distinguish between images.Because of this, descriptions tend to be complex in terms of syntax and discourse and require drawing pragmatic inferences.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":0,"sentence":"Images are sourced from both static pictures and video frames.We benchmark several state-of-the-art models, including both cross-encoders such as ViLBERT and bi-encoders such as CLIP, on ImageCoDe.Our results reveal that these models dramatically lag behind human performance: the best variant achieves an accuracy of 20.9 on video frames and 59.4 on static pictures, compared with 90.8 in humans.Furthermore, we experiment with new model variants that are better equipped to incorporate visual and temporal context into their representations, which achieve modest gains.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":0,"sentence":"Our hope is that ImageCoDE will foster progress in grounded language understanding by encouraging models to focus on fine-grained visual differences.","offset":4,"pro":0.8,"labels":"PUR"},{"idx":1,"sentence":"Molecular representation learning plays an essential role in cheminformatics.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"Recently, language model-based approaches have gained popularity as an alternative to traditional expert-designed features to encode molecules.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":1,"sentence":"However, these approaches only utilize a single molecular language for representation learning.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":1,"sentence":"MM-Deacon is pre-trained using SMILES and IUPAC as two different languages on large-scale molecules.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":1,"sentence":"We evaluated the robustness of our method on seven molecular property prediction tasks from MoleculeNet benchmark, zero-shot cross-lingual retrieval, and a drug-drug interaction prediction task.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":1,"sentence":"Motivated by the fact that a given molecule can be described using different languages such as Simplified Molecular Line Entry System (SMILES), The International Union of Pure and Applied Chemistry (IUPAC), and The IUPAC International Chemical Identifier (In","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":1,"sentence":"we propose a multilingual molecular embedding generation approach called MM-Deacon (multilingual molecular domain embedding analysis via contrastive learning","offset":6,"pro":0.8571428571428571,"labels":"PUR"},{"idx":2,"sentence":"Transformer-based models are the modern work horses for neural machine translation (NMT), reaching state of the art across several benchmarks.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"We find that even when the surrounding context provides unambiguous evidence of the appropriate grammatical gender marking, no tested model was able to accurately gender occupation nouns systematically.","offset":1,"pro":0.16666666666666666,"labels":"RST"},{"idx":2,"sentence":"We release an evaluation scheme and dataset for measuring the ability of NMT models to translate gender morphology correctly in unambiguous contexts across syntactically diverse sentences.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":2,"sentence":"Our dataset translates from an English source into 20 languages from several different language families.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":2,"sentence":"With the availability of this dataset, our hope is that the NMT community can iterate on solutions for this class of especially egregious errors.","offset":4,"pro":0.6666666666666666,"labels":"PUR"},{"idx":2,"sentence":"Despite their impressive accuracy, we observe a systemic and rudimentary class of errors made by current state-of-the-art NMT models with regards to translating from a language that doesn't mark gender on nouns into others that d","offset":5,"pro":0.8333333333333334,"labels":"GAP"},{"idx":3,"sentence":"Humans (e.g., crowdworkers) have a remarkable ability in solving different tasks, by simply reading textual instructions that define them and looking at a few examples.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"Despite the success of the conventional supervised learning on individual datasets, such models often struggle with generalization across tasks (e.g., a question-answering system cannot solve classification tasks).","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":3,"sentence":"A long-standing challenge in AI is to build a model that learns a new task by understanding the human-readable instructions that define it.","offset":2,"pro":0.2222222222222222,"labels":"BAC"},{"idx":3,"sentence":"To study this, we introduce NATURAL INSTRUCTIONS, a dataset of 61 distinct tasks, their human-authored instructions, and 193k task instances (input-output pairs).","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":3,"sentence":"The instructions are obtained from crowdsourcing instructions used to create existing NLP datasets and mapped to a unified schema.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":3,"sentence":"Using this meta-dataset, we measure cross-task generalization by training models on seen tasks and measuring generalization to the remaining unseen ones.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":3,"sentence":"We adopt generative pre-trained language models to encode task-specific instructions along with input and generate task output.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":3,"sentence":"Our results indicate that models benefit from instructions when evaluated in terms of generalization to unseen tasks (19% better for models utilizing instructions).","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":3,"sentence":"These models, however, are far behind an estimated performance upperbound indicating significant room for more progress in this direction.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":4,"sentence":"Moreover, it can be used in a plug-and-play fashion with FastText and BERT, where it significantly improves their robustness.","offset":0,"pro":0,"labels":"CLN"},{"idx":4,"sentence":"State-of-the-art NLP systems represent inputs with word embeddings","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":4,"sentence":"but these are brittle when faced with Out-of-Vocabulary (OOV) word","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":4,"sentence":"To address this issue, we follow the principle of mimick-like models to generate vectors for unseen word","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":4,"sentence":"by learning the behavior of pre-trained embeddings using only the surface form of word","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":4,"sentence":"We present a simple contrastive learning framework, LOVE, which extends the word representation of an existing pre-trained language model (such as BERT) and makes it robust to OOV with few additional parameters","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":4,"sentence":"Extensive evaluations demonstrate that our lightweight model achieves similar or even better performances than prior competitors, both on original datasets and on corrupted variant","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":5,"sentence":"Given the ubiquitous nature of numbers in text, reasoning with numbers to perform simple calculations is an important skill of AI systems.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"While many datasets and models have been developed to this end, state-of-the-art AI systems are brittle; failing to perform the underlying mathematical reasoning when they appear in a slightly different scenario.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":5,"sentence":"We show that this benchmark is far from being solved with neural models including state-of-the-art large-scale language models performing significantly worse than humans (lower by 46.4 %).","offset":2,"pro":0.2857142857142857,"labels":"RST"},{"idx":5,"sentence":"Further, NumGLUE promotes sharing knowledge across tasks, especially those with limited training data as evidenced by the superior performance (average gain of 3.4 % on each task) when a model is jointly trained on all the tasks as opposed to task-specific modeling.","offset":3,"pro":0.42857142857142855,"labels":"CLN"},{"idx":5,"sentence":"Finally, we hope that NumGLUE will encourage systems that perform robust and general arithmetic reasoning within language, a first step towards being able to perform more complex mathematical reasoning.","offset":4,"pro":0.5714285714285714,"labels":"CTN"},{"idx":5,"sentence":"Drawing inspiration from GLUE that was proposed in the context of natural language understanding","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":5,"sentence":"we propose NumGLUE, a multi-task benchmark that evaluates the performance of AI systems on eight different tasks, that at their core require simple arithmetic understanding","offset":6,"pro":0.8571428571428571,"labels":"PUR"},{"idx":6,"sentence":"A few large, homogenous, pre-trained models undergird many machine learning systems - and often, these models contain harmful stereotypes learned from the internet.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"We investigate the bias transfer hypothesis: the theory that social biases (such as stereotypes) internalized by large language models during pre-training transfer into harmful task-specific behavior after fine-tuning.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":6,"sentence":"For two classification tasks, we find that reducing intrinsic bias with controlled interventions before fine-tuning does little to mitigate the classifier's discriminatory behavior after fine-tuning.","offset":2,"pro":0.3333333333333333,"labels":"RST"},{"idx":6,"sentence":"Regression analysis suggests that downstream disparities are better explained by biases in the fine-tuning dataset.","offset":3,"pro":0.5,"labels":"CLN"},{"idx":6,"sentence":"Still, pre-training plays a role: simple alterations to co-occurrence rates in the fine-tuning dataset are ineffective when the model has been pre-trained.","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":6,"sentence":"Our results encourage practitioners to focus more on dataset quality and context-specific harms.","offset":5,"pro":0.8333333333333334,"labels":"CTN"},{"idx":7,"sentence":"A dialogue response is malevolent if it is grounded in negative emotions, inappropriate behavior, or an unethical value basis in terms of content and dialogue acts.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"The detection of malevolent dialogue responses is attracting growing interest.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":7,"sentence":"Current research on detecting dialogue malevolence has limitations in terms of datasets and methods.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":7,"sentence":"First, available dialogue datasets related to malevolence are labeled with a single category, but in practice assigning a single category to each utterance may not be appropriate as some malevolent utterances belong to multiple labels.","offset":3,"pro":0.375,"labels":"GAP"},{"idx":7,"sentence":"Second, current methods for detecting dialogue malevolence neglect label correlation.","offset":4,"pro":0.5,"labels":"GAP"},{"idx":7,"sentence":"Therefore, we propose the task of multi-label dialogue malevolence detection and crowdsource a multi-label dataset, multi-label dialogue malevolence detection (MDMD) for evaluation.","offset":5,"pro":0.625,"labels":"PUR"},{"idx":7,"sentence":"We also propose a multi-label malevolence detection model, multi-faceted label correlation enhanced CRF (MCRF), with two label correlation mechanisms, label correlation in taxonomy (LCT) and label correlation in context (LCC).","offset":6,"pro":0.75,"labels":"MTD"},{"idx":7,"sentence":"Experiments on MDMD show that our method outperforms the best performing baseline by a large margin, i.e., 16.1%, 11.9%, 12.0%, and 6.1% on precision, recall, F1, and Jaccard score, respectively.","offset":7,"pro":0.875,"labels":"RST"},{"idx":8,"sentence":"Long-form answers, consisting of multiple sentences, can provide nuanced and comprehensive answers to a broader set of questions.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"To better understand this complex and understudied task, we study the functional structure of long-form answers collected from three datasets, ELI5, WebGPT and Natural Questions.","offset":1,"pro":0.125,"labels":"PUR"},{"idx":8,"sentence":"Our main goal is to understand how humans organize information to craft complex answers.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":8,"sentence":"We develop an ontology of six sentence-level functional roles for long-form answers, and annotate 3.9k sentences in 640 answer paragraphs.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":8,"sentence":"Different answer collection methods manifest in different discourse structures.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":8,"sentence":"We further analyze model-generated answers - finding that annotators agree less with each other when annotating model-generated answers compared to annotating human-written answers.","offset":5,"pro":0.625,"labels":"RST"},{"idx":8,"sentence":"Our annotated data enables training a strong classifier that can be used for automatic analysis.","offset":6,"pro":0.75,"labels":"CTN"},{"idx":8,"sentence":"We hope our work can inspire future research on discourse-level modeling and evaluation of long-form QA systems.","offset":7,"pro":0.875,"labels":"IMP"},{"idx":9,"sentence":"Writing is, by nature, a strategic, adaptive, and, more importantly, an iterative process.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"A crucial part of writing is editing and revising the text.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":9,"sentence":"Previous works on text revision have focused on defining edit intention taxonomies within a single domain or developing computational models with a single level of edit granularity, such as sentence-level edits, which differ from human's revision cycles.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":9,"sentence":"This work describes IteraTeR: the first large-scale, multi-domain, edit-intention annotated corpus of iteratively revised text.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":9,"sentence":"In particular, IteraTeR is collected based on a new framework to comprehensively model the iterative text revisions that generalizes to a variety of domains, edit intentions, revision depths, and granularities.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":9,"sentence":"When we incorporate our annotated edit intentions, both generative and action-based text revision models significantly improve automatic evaluations.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":9,"sentence":"Through our work","offset":6,"pro":0.75,"labels":"MTD"},{"idx":9,"sentence":" we better understand the text revision process","offset":7,"pro":0.875,"labels":"CLN"},{"idx":10,"sentence":"Several studies have reported the inability of Transformer models to generalize compositionally, a key type of generalization in many NLP tasks such as semantic parsing.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"In this paper we explore the design space of Transformer models showing that the inductive biases given to the model by several design decisions significantly impact compositional generalization.","offset":1,"pro":0.25,"labels":"PUR"},{"idx":10,"sentence":"We identified Transformer configurations that generalize compositionally significantly better than previously reported in the literature in many compositional tasks.","offset":2,"pro":0.5,"labels":"RST"},{"idx":10,"sentence":"We achieve state-of-the-art results in a semantic parsing compositional generalization benchmark (COGS), and a string edit operation composition benchmark (PCFG).","offset":3,"pro":0.75,"labels":"RST"},{"idx":11,"sentence":"Unlike literal expressions, idioms' meanings do not directly follow from their parts, posing a challenge for neural machine translation (NMT).","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"NMT models are often unable to translate idioms accurately and over-generate compositional, literal translations.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":11,"sentence":"This manifests in idioms' parts being grouped through attention and in reduced interaction between idioms and their context.In the decoder's cross-attention, figurative inputs result in reduced attention on source-side tokens.","offset":2,"pro":0.2857142857142857,"labels":"RST"},{"idx":11,"sentence":"These results suggest that Transformer's tendency to process idioms as compositional expressions contributes to literal translations of idioms.","offset":3,"pro":0.42857142857142855,"labels":"CLN"},{"idx":11,"sentence":"In this work, we investigate whether the non-compositionality of idioms is reflected in the mechanics of the dominant NMT model, Transforme","offset":4,"pro":0.5714285714285714,"labels":"PUR"},{"idx":11,"sentence":"by analysing the hidden states and attention patterns for models with English as source language and one of seven European languages as target languag","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":11,"sentence":"When Transformer emits a non-literal translation - i.e. identifies the expression as idiomatic - the encoder processes idioms more strongly as single lexical units compared to literal expression","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":12,"sentence":"We describe a Question Answering (QA) dataset that contains complex questions with conditional answers, i.e. the answers are only applicable when certain conditions apply.","offset":0,"pro":0,"labels":"MTD"},{"idx":12,"sentence":"We call this dataset ConditionalQA.","offset":1,"pro":0.25,"labels":"MTD"},{"idx":12,"sentence":"In addition to conditional answers, the dataset also features:(1) long context documents with information that is related in logically complex ways;(2) multi-hop questions that require compositional logical reasoning;(3) a combination of extractive questions, yes/no questions, questions with multiple answers, and not-answerable questions;(4) questions asked without knowing the answers.We show that ConditionalQA is challenging for many of the existing QA models, especially in selecting answer conditions.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":12,"sentence":"We believe that this dataset will motivate further research in answering complex questions over long documents.","offset":3,"pro":0.75,"labels":"IMP"},{"idx":13,"sentence":"Current methods for few-shot fine-tuning of pretrained masked language models (PLMs) require carefully engineered prompts and verbalizers for each new task to convert examples into a cloze-format that the PLM can score.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"In this work, we propose Perfect, a simple and efficient method for few-shot fine-tuning of PLMs without relying on any such handcrafting, which is highly effective given as few as 32 data points.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":13,"sentence":"Perfect makes two key design choices: First, we show that manually engineered task prompts can be replaced with task-specific adapters that enable sample-efficient fine-tuning and reduce memory and storage costs by roughly factors of 5 and 100, respectively.","offset":2,"pro":0.2857142857142857,"labels":"RST"},{"idx":13,"sentence":"Second, instead of using handcrafted verbalizers, we learn new multi-token label embeddings during fine-tuning, which are not tied to the model vocabulary and which allow us to avoid complex auto-regressive decoding.","offset":3,"pro":0.42857142857142855,"labels":"RST"},{"idx":13,"sentence":"These embeddings are not only learnable from limited data but also enable nearly 100x faster training and inference.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":13,"sentence":"Experiments on a wide range of few shot NLP tasks demonstrate that Perfect, while being simple and efficient, also outperforms existing state-of-the-art few-shot learning methods.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":13,"sentence":"Our code is publicly available at https://github.com/rabeehk/perfect.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":14,"sentence":"Continual learning is essential for real-world deployment when there is a need to quickly adapt the model to new tasks without forgetting knowledge of old tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"Existing work on continual sequence generation either always reuses existing parameters to learn new tasks, which is vulnerable to catastrophic forgetting on dissimilar tasks, or blindly adds new parameters for every new task, which could prevent knowledge sharing between similar tasks.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":14,"sentence":"To get the best of both worlds, in this work, we propose continual sequence generation with adaptive compositional modules to adaptively add modules in transformer architectures and compose both old and new modules for new tasks.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":14,"sentence":"We also incorporate pseudo experience replay to facilitate knowledge transfer in those shared modules.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":14,"sentence":"Experiment results on various sequences of generation tasks show that our framework can adaptively add modules or reuse modules based on task similarity, outperforming state-of-the-art baselines in terms of both performance and parameter efficiency.","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":14,"sentence":"We make our code public at https://github.com/GT-SALT/Adaptive-Compositional-Modules.","offset":5,"pro":0.8333333333333334,"labels":"CTN"},{"idx":15,"sentence":"Recent work has explored using counterfactually-augmented data (CAD)-data generated by minimally perturbing examples to flip the ground-truth label-to identify robust features that are invariant under distribution shift.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"However, empirical results using CAD during training for OOD generalization have been mixed.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":15,"sentence":"To explain this discrepancy, through a toy theoretical example and empirical analysis on two crowdsourced CAD datasets, we show that: (a) while features perturbed in CAD are indeed robust features, it may prevent the model from learning unperturbed robust features; and (b) CAD may exacerbate existing spurious correlations in the data.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":15,"sentence":"Our results thus show that the lack of perturbation diversity limits CAD's effectiveness on OOD generalization, calling for innovative crowdsourcing procedures to elicit diverse perturbation of examples.","offset":3,"pro":0.5,"labels":"CLN"},{"idx":15,"sentence":"While pretrained language models achieve excellent performance on natural language understanding benchmark","offset":4,"pro":0.6666666666666666,"labels":"BAC"},{"idx":15,"sentence":"hey tend to rely on spurious correlations and generalize poorly to out-of-distribution (OOD) dat","offset":5,"pro":0.8333333333333334,"labels":"GAP"},{"idx":16,"sentence":"Sentiment transfer is one popular example of a text style transfer task, where the goal is to reverse the sentiment polarity of a text.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"With a sentiment reversal comes also a reversal in meaning.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":16,"sentence":"We introduce a different but related task called positive reframing in which we neutralize a negative point of view and generate a more positive perspective for the author without contradicting the original meaning.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":16,"sentence":"Our insistence on meaning preservation makes positive reframing a challenging and semantically rich task.","offset":3,"pro":0.5,"labels":"CTN"},{"idx":16,"sentence":"To facilitate rapid progress, we introduce a large-scale benchmark, Positive Psychology Frames, with 8,349 sentence pairs and 12,755 structured annotations to explain positive reframing in terms of six theoretically-motivated reframing strategies.","offset":4,"pro":0.6666666666666666,"labels":"PUR"},{"idx":16,"sentence":"Then we evaluate a set of state-of-the-art text style transfer models, and conclude by discussing key challenges and directions for future work.","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":17,"sentence":"English Natural Language Understanding (NLU) systems have achieved great performances and even outperformed humans on benchmarks like GLUE and SuperGLUE.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"However, these benchmarks contain only textbook Standard American English (SAE).","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":17,"sentence":"Other dialects have been largely overlooked in the NLP community.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":17,"sentence":"This leads to biased and inequitable NLU systems that serve only a sub-population of speakers.","offset":3,"pro":0.42857142857142855,"labels":"GAP"},{"idx":17,"sentence":"To understand disparities in current models and to facilitate more dialect-competent NLU systems, we introduce the VernAcular Language Understanding Evaluation (VALUE) benchmark, a challenging variant of GLUE that we created with a set of lexical and morphosyntactic transformation rules.","offset":4,"pro":0.5714285714285714,"labels":"PUR"},{"idx":17,"sentence":"In this initial release (V.1), we construct rules for 11 features of African American Vernacular English (AAVE), and we recruit fluent AAVE speakers to validate each feature transformation via linguistic acceptability judgments in a participatory design manner.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":17,"sentence":"Experiments show that these new dialectal features can lead to a drop in model performance.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":18,"sentence":"We study the task of toxic spans detection, which concerns the detection of the spans that make a text toxic, when detecting such spans is possible.","offset":0,"pro":0,"labels":"PUR"},{"idx":18,"sentence":"We introduce a dataset for this task, ToxicSpans, which we release publicly.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":18,"sentence":"By experimenting with several methods, we show that sequence labeling models perform best, but methods that add generic rationale extraction mechanisms on top of classifiers trained to predict if a post is toxic or not are also surprisingly promising.","offset":2,"pro":0.3333333333333333,"labels":"RST"},{"idx":18,"sentence":"Our work highlights challenges in finer toxicity detection and mitigation.","offset":3,"pro":0.5,"labels":"CTN"},{"idx":18,"sentence":"Finally, we use ToxicSpans and systems trained on it","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":18,"sentence":"to provide further analysis of state-of-the-art toxic to non-toxic transfer systems, as well as of human performance on that latter tas","offset":5,"pro":0.8333333333333334,"labels":"PUR"},{"idx":19,"sentence":"Sequence modeling has demonstrated state-of-the-art performance on natural language and document understanding tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"However, it is challenging to correctly serialize tokens in form-like documents in practice due to their variety of layout patterns.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":19,"sentence":"We propose FormNet, a structure-aware sequence model to mitigate the suboptimal serialization of forms.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":19,"sentence":"First, we design Rich Attention that leverages the spatial relationship between tokens in a form for more precise attention score calculation.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":19,"sentence":"Second, we construct Super-Tokens for each word by embedding representations from their neighboring tokens through graph convolutions.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":19,"sentence":"FormNet therefore explicitly recovers local syntactic information that may have been lost during serialization.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":19,"sentence":"In experiments, FormNet outperforms existing methods with a more compact model size and less pre-training data, establishing new state-of-the-art performance on CORD, FUNSD and Payment benchmarks.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":0,"sentence":"Conversational agents have come increasingly closer to human competence in open-domain dialogue settings; however, such models can reflect insensitive, hurtful, or entirely incoherent viewpoints that erode a user's trust in the moral integrity of the system.","offset":0,"pro":0,"labels":"GAP"},{"idx":0,"sentence":"Moral deviations are difficult to mitigate because moral judgments are not universal, and there may be multiple competing judgments that apply to a situation simultaneously.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":0,"sentence":"In this work, we introduce a new resource, not to authoritatively resolve moral ambiguities, but instead to facilitate systematic understanding of the intuitions, values and moral judgments reflected in the utterances of dialogue systems.","offset":2,"pro":0.2222222222222222,"labels":"PUR"},{"idx":0,"sentence":"The Moral Integrity Corpus, MIC, is such a resource, which captures the moral assumptions of 38k prompt-reply pairs, using 99k distinct Rules of Thumb (RoTs).","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":0,"sentence":"Each RoT reflects a particular moral conviction that can explain why a chatbot's reply may appear acceptable or problematic.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":0,"sentence":"We further organize RoTs with a set of 9 moral and social attributes and benchmark performance for attribute classification.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":0,"sentence":"Most importantly, we show that current neural language models can automatically generate new RoTs that reasonably describe previously unseen interactions, but they still struggle with certain scenarios.","offset":6,"pro":0.6666666666666666,"labels":"CLN"},{"idx":0,"sentence":"Our findings suggest that MIC will be a useful resource for understanding and language models' implicit moral assumptions and flexibly benchmarking the integrity of conversational agents.","offset":7,"pro":0.7777777777777778,"labels":"CLN"},{"idx":0,"sentence":"To download the data, see https://github.com/GT-SALT/mic","offset":8,"pro":0.8888888888888888,"labels":"CTN"},{"idx":1,"sentence":"Transformer-based models generally allocate the same amount of computation for each token in a given sequence.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"We develop a simple but effective token dropping method to accelerate the pretraining of transformer models, such as BERT, without degrading its performance on downstream tasks.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":1,"sentence":"In particular, we drop unimportant tokens starting from an intermediate layer in the model to make the model focus on important tokens more efficiently if with limited computational resource.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":1,"sentence":"The dropped tokens are later picked up by the last layer of the model so that the model still produces full-length sequences.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":1,"sentence":"In our experiments, this simple approach reduces the pretraining cost of BERT by 25% while achieving similar overall fine-tuning performance on standard downstream tasks.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":1,"sentence":"We leverage the already built-in masked language modeling (MLM) loss to identify unimportant tokens with practically no computational overhead","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":2,"sentence":"Fact-checking is an essential tool to mitigate the spread of misinformation and disinformation.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"We introduce the task of fact-checking in dialogue, which is a relatively unexplored area.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":2,"sentence":"We construct DialFact, a testing benchmark dataset of 22,245 annotated conversational claims, paired with pieces of evidence from Wikipedia.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":2,"sentence":"There are three sub-tasks in DialFact: 1) Verifiable claim detection task distinguishes whether a response carries verifiable factual information; 2) Evidence retrieval task retrieves the most relevant Wikipedia snippets as evidence; 3) Claim verification task predicts a dialogue response to be supported, refuted, or not enough information.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":2,"sentence":"We point out unique challenges in DialFact such as handling the colloquialisms, coreferences, and retrieval ambiguities in the error analysis to shed light on future research in this direction.","offset":4,"pro":0.5714285714285714,"labels":"CTN"},{"idx":2,"sentence":"We found that existing fact-checking models trained on non-dialogue data like FEVER fail to perform well on our task,","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":2,"sentence":"and thus, we propose a simple yet data-efficient solution to effectively improve fact-checking performance in dialogue.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":3,"sentence":"This work connects language model adaptation with concepts of machine learning theory.","offset":0,"pro":0,"labels":"PUR"},{"idx":3,"sentence":"We consider a training setup with a large out-of-domain set and a small in-domain set.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":3,"sentence":"We derive how the benefit of training a model on either set depends on the size of the sets and the distance between their underlying distributions.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":3,"sentence":"We analyze how out-of-domain pre-training before in-domain fine-tuning achieves better generalization than either solution independently.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":3,"sentence":"Finally, we present how adaptation techniques based on data selection, such as importance sampling, intelligent data selection and influence functions, can be presented in a common framework which highlights their similarity and also their subtle differences.","offset":4,"pro":0.8,"labels":"MTD"},{"idx":4,"sentence":"Aligning with ACL 2022 special Theme on Language Diversity: from Low Resource to Endangered Languages, we discuss the major linguistic and sociopolitical challenges facing development of NLP technologies for African languages.","offset":0,"pro":0,"labels":"PUR"},{"idx":4,"sentence":"Situating African languages in a typological framework, we discuss how the particulars of these languages can be harnessed.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":4,"sentence":"To facilitate future research, we also highlight current efforts, communities, venues, datasets, and tools.","offset":2,"pro":0.4,"labels":"IMP"},{"idx":4,"sentence":"Our main objective is to motivate and advocate for an Afrocentric approach to technology development.","offset":3,"pro":0.6,"labels":"PUR"},{"idx":4,"sentence":"With this in mind, we recommend what technologies to build and how to build, evaluate, and deploy them based on the needs of local African communities.","offset":4,"pro":0.8,"labels":"MTD"},{"idx":5,"sentence":"In this paper, we investigate improvements to the GEC sequence tagging architecture with a focus on ensembling of recent cutting-edge Transformer-based encoders in Large configurations.","offset":0,"pro":0,"labels":"PUR"},{"idx":5,"sentence":"We encourage ensembling models by majority votes on span-level edits because this approach is tolerant to the model architecture and vocabulary size.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":5,"sentence":"Our best ensemble achieves a new SOTA result with an F_{0.5 score of 76.05 on BEA-2019 (test), even without pre-training on synthetic datasets.","offset":2,"pro":0.3333333333333333,"labels":"RST"},{"idx":5,"sentence":"In addition, we perform knowledge distillation with a trained ensemble to generate new synthetic training datasets, Troy-Blogs and Troy-1BW.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":5,"sentence":"Our best single sequence tagging model that is pretrained on the generated Troy- datasets in combination with the publicly available synthetic PIE dataset achieves a near-SOTA result with an F_{0.5 score of 73.21 on BEA-2019 (test).","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":5,"sentence":"The code, datasets, and trained models are publicly available.","offset":5,"pro":0.8333333333333334,"labels":"CTN"},{"idx":6,"sentence":"Natural language processing (NLP) models trained on people-generated data can be unreliable because, without any constraints, they can learn from spurious correlations that are not relevant to the task.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"We hypothesize that enriching models with speaker information in a controlled, educated way can guide them to pick up on relevant inductive biases.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":6,"sentence":"For the speaker-driven task of predicting code-switching points in English-Spanish bilingual dialogues, we show that adding sociolinguistically-grounded speaker features as prepended prompts significantly improves accuracy.","offset":2,"pro":0.4,"labels":"CLN"},{"idx":6,"sentence":"We find that by adding influential phrases to the input, speaker-informed models learn useful and explainable linguistic information.","offset":3,"pro":0.6,"labels":"CLN"},{"idx":6,"sentence":"To our knowledge, we are the first to incorporate speaker characteristics in a neural model for code-switching, and more generally, take a step towards developing transparent, personalized models that use speaker information in a controlled way.","offset":4,"pro":0.8,"labels":"CTN"},{"idx":7,"sentence":"This work presents a new resource for borrowing identification and analyzes the performance and errors of several models on this task.","offset":0,"pro":0,"labels":"PUR"},{"idx":7,"sentence":"We introduce a new annotated corpus of Spanish newswire rich in unassimilated lexical borrowings-words from one language that are introduced into another without orthographic adaptation-and use it to evaluate how several sequence labeling models (CRF, BiLSTM-CRF, and Transformer-based models) perform.","offset":1,"pro":0.25,"labels":"MTD"},{"idx":7,"sentence":"The corpus contains 370,000 tokens and is larger, more borrowing-dense, OOV-rich, and topic-varied than previous corpora available for this task.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":7,"sentence":"Our results show that a BiLSTM-CRF model fed with subword embeddings along with either Transformer-based embeddings pretrained on codeswitched data or a combination of contextualized word embeddings outperforms results obtained by a multilingual BERT-based model.","offset":3,"pro":0.75,"labels":"CLN"},{"idx":8,"sentence":"The performance of deep learning models in NLP and other fields of machine learning has led to a rise in their popularity, and so the need for explanations of these models becomes paramount.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"Attention has been seen as a solution to increase performance, while providing some explanations.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":8,"sentence":"However, a debate has started to cast doubt on the explanatory power of attention in neural networks.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":8,"sentence":"Although the debate has created a vast literature thanks to contributions from various areas, the lack of communication is becoming more and more tangible.","offset":3,"pro":0.42857142857142855,"labels":"GAP"},{"idx":8,"sentence":"In this paper, we provide a clear overview of the insights on the debate by critically confronting works from these different areas.","offset":4,"pro":0.5714285714285714,"labels":"PUR"},{"idx":8,"sentence":"This holistic vision can be of great interest for future works in all the communities concerned by this debate.","offset":5,"pro":0.7142857142857143,"labels":"IMP"},{"idx":8,"sentence":"We sum up the main challenges spotted in these areas, and we conclude by discussing the most promising future avenues on attention as an explanation.","offset":6,"pro":0.8571428571428571,"labels":"MTD"},{"idx":9,"sentence":"Knowledge-grounded conversation (KGC) shows great potential in building an engaging and knowledgeable chatbot, and knowledge selection is a key ingredient in it.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"However, previous methods for knowledge selection only concentrate on the relevance between knowledge and dialogue context, ignoring the fact that age, hobby, education and life experience of an interlocutor have a major effect on his or her personal preference over external knowledge.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":9,"sentence":"We propose a variational method to model the underlying relationship between one's personal memory and his or her selection of knowledge, and devise a learning scheme in which the forward mapping from personal memory to knowledge and its inverse mapping is included in a closed loop so that they could teach each other.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":9,"sentence":"Experiment results show that our methods outperform existing KGC methods significantly on both automatic evaluation and human evaluation.","offset":3,"pro":0.6,"labels":"CLN"},{"idx":9,"sentence":"Without taking the personalization issue into account, it is difficult for existing dialogue systems to select the proper knowledge and generate persona-consistent responses.","offset":4,"pro":0.8,"labels":"GAP"},{"idx":10,"sentence":"In data-to-text (D2T) generation, training on in-domain data leads to overfitting to the data representation and repeating training data noise.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"We examine how to avoid finetuning pretrained language models (PLMs) on D2T generation datasets while still taking advantage of surface realization capabilities of PLMs.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":10,"sentence":"Inspired by pipeline approaches, we propose to generate text by transforming single-item descriptions with a sequence of modules trained on general-domain text-based operations: ordering, aggregation, and paragraph compression.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":10,"sentence":"We train PLMs for performing these operations on a synthetic corpus WikiFluent which we build from English Wikipedia.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":10,"sentence":"Our experiments on two major triple-to-text datasets-WebNLG and E2E-show that our approach enables D2T generation from RDF triples in zero-shot settings.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":11,"sentence":"Languages are classified as low-resource when they lack the quantity of data necessary for training statistical and machine learning tools and models.","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"Causes of resource scarcity vary but can include poor access to technology for developing these resources, a relatively small population of speakers, or a lack of urgency for collecting such resources in bilingual populations where the second language is high-resource.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":11,"sentence":"As a result, the languages described as low-resource in the literature are as different as Finnish on the one hand, with millions of speakers using it in every imaginable domain, and Seneca, with only a small-handful of fluent speakers using the language primarily in a restricted domain.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":11,"sentence":"While issues stemming from the lack of resources necessary to train models unite this disparate group of languages, many other issues cut across the divide between widely-spoken low-resource languages and endangered languages.","offset":3,"pro":0.42857142857142855,"labels":"GAP"},{"idx":11,"sentence":"In this position paper, we discuss the unique technological, cultural, practical, and ethical challenges that researchers and indigenous speech community members face when working together to develop language technology to support endangered language documentation and revitalization.","offset":4,"pro":0.5714285714285714,"labels":"PUR"},{"idx":11,"sentence":"We report the perspectives of language teachers, Master Speakers and elders from indigenous communities, as well as the point of view of academics.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":11,"sentence":"We describe an ongoing fruitful collaboration and make recommendations for future partnerships between academic researchers and language community stakeholders.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":12,"sentence":"Bragging is a speech act employed with the goal of constructing a favorable self-image through positive statements about oneself.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"It is widespread in daily communication and especially popular in social media, where users aim to build a positive image of their persona directly or indirectly.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":12,"sentence":"In this paper, we present the first large scale study of bragging in computational linguistics, building on previous research in linguistics and pragmatics.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":12,"sentence":"To facilitate this, we introduce a new publicly available data set of tweets annotated for bragging and their types.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":12,"sentence":"We empirically evaluate different transformer-based models injected with linguistic information in (a) binary bragging classification, i.e., if tweets contain bragging statements or not; and (b) multi-class bragging type prediction including not bragging.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":12,"sentence":"Our results show that our models can predict bragging with macro F1 up to 72.42 and 35.95 in the binary and multi-class classification tasks respectively.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":12,"sentence":"Finally, we present an extensive linguistic and error analysis of bragging prediction to guide future research on this topic.","offset":6,"pro":0.8571428571428571,"labels":"IMP"},{"idx":13,"sentence":"Document-level information extraction (IE) tasks have recently begun to be revisited in earnest using the end-to-end neural network techniques that have been successful on their sentence-level IE counterparts.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"Evaluation of the approaches, however, has been limited in a number of dimensions.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":13,"sentence":"In particular, the precision/recall/F1 scores typically reported provide few insights on the range of errors the models make.","offset":2,"pro":0.4,"labels":"GAP"},{"idx":13,"sentence":"We build on the work of Kummerfeld and Klein (2013) to propose a transformation-based framework for automating error analysis in document-level event and (N-ary) relation extraction.","offset":3,"pro":0.6,"labels":"PUR"},{"idx":13,"sentence":"We employ our framework to compare two state-of-the-art document-level template-filling approaches on datasets from three domains; and then, to gauge progress in IE since its inception 30 years ago, vs. four systems from the MUC-4 (1992) evaluation.","offset":4,"pro":0.8,"labels":"MTD"},{"idx":14,"sentence":"Functional Distributional Semantics is a recently proposed framework for learning distributional semantics that provides linguistic interpretability.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"It models the meaning of a word as a binary classifier rather than a numerical vector.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":14,"sentence":"In this work, we propose a method to train a Functional Distributional Semantics model with grounded visual data.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":14,"sentence":"We train it on the Visual Genome dataset, which is closer to the kind of data encountered in human language acquisition than a large text corpus.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":14,"sentence":"On four external evaluation datasets, our model outperforms previous work on learning semantics from Visual Genome.","offset":4,"pro":0.8,"labels":"RST"},{"idx":15,"sentence":"While large language models have shown exciting progress on several NLP benchmarks, evaluating their ability for complex analogical reasoning remains under-explored.","offset":0,"pro":0,"labels":"GAP"},{"idx":15,"sentence":"Here, we introduce a high-quality crowdsourced dataset of narratives for employing proverbs in context as a benchmark for abstract language understanding.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":15,"sentence":"The dataset provides fine-grained annotation of aligned spans between proverbs and narratives, and contains minimal lexical overlaps between narratives and proverbs, ensuring that models need to go beyond surface-level reasoning to succeed.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":15,"sentence":"We explore three tasks: (1) proverb recommendation and alignment prediction, (2) narrative generation for a given proverb and topic, and (3) identifying narratives with similar motifs.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":15,"sentence":"Our experiments show that neural language models struggle on these tasks compared to humans, and these tasks pose multiple learning challenges.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":16,"sentence":"Charts are commonly used for exploring data and communicating insights.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"Generating natural language summaries from charts can be very helpful for people in inferring key insights that would otherwise require a lot of cognitive and perceptual efforts.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":16,"sentence":"We present Chart-to-text, a large-scale benchmark with two datasets and a total of 44,096 charts covering a wide range of topics and chart types.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":16,"sentence":"We explain the dataset construction process and analyze the datasets.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":16,"sentence":"We also introduce a number of state-of-the-art neural models as baselines that utilize image captioning and data-to-text generation techniques to tackle two problem variations: one assumes the underlying data table of the chart is available while the other needs to extract data from chart images.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":16,"sentence":"Our analysis with automatic and human evaluation shows that while our best models usually generate fluent summaries and yield reasonable BLEU scores, they also suffer from hallucinations and factual errors as well as difficulties in correctly explaining complex patterns and trends in charts.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":17,"sentence":"Idioms are unlike most phrases in two important ways.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"First, words in an idiom have non-canonical meanings.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":17,"sentence":"Second, the non-canonical meanings of words in an idiom are contingent on the presence of other words in the idiom.","offset":2,"pro":0.3333333333333333,"labels":"BAC"},{"idx":17,"sentence":"Linguistic theories differ on whether these properties depend on one another, as well as whether special theoretical machinery is needed to accommodate idioms.","offset":3,"pro":0.5,"labels":"BAC"},{"idx":17,"sentence":"We define two measures that correspond to the properties above, and we show that idioms fall at the expected intersection of the two dimensions, but that the dimensions themselves are not correlated.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":17,"sentence":"Our results suggest that introducing special machinery to handle idioms may not be warranted.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":18,"sentence":"Graph neural networks have triggered a resurgence of graph-based text classification methods, defining today's state of the art.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"We show that a wide multi-layer perceptron (MLP) using a Bag-of-Words (BoW) outperforms the recent graph-based models TextGCN and HeteGCN in an inductive text classification setting and is comparable with HyperGAT.","offset":1,"pro":0.16666666666666666,"labels":"CLN"},{"idx":18,"sentence":"Moreover, we fine-tune a sequence-based BERT and a lightweight DistilBERT model, which both outperform all state-of-the-art models.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":18,"sentence":"These results question the importance of synthetic graphs used in modern text classifiers.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":18,"sentence":"In terms of efficiency, DistilBERT is still twice as large as our BoW-based wide MLP, while graph-based models like TextGCN require setting up an mathcal{O(N^2) graph, where N is the vocabulary plus corpus size.","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":18,"sentence":"Finally, since Transformers need to compute mathcal{O(L^2) attention weights with sequence length L, the MLP models show higher training and inference speeds on datasets with long sequences.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":19,"sentence":"We introduce ParaBLEU, a paraphrase representation learning model and evaluation metric for text generation.","offset":0,"pro":0,"labels":"PUR"},{"idx":19,"sentence":"We show that our model is robust to data scarcity, exceeding previous state-of-the-art performance using only 50% of the available training data and surpassing BLEU, ROUGE and METEOR with only 40 labelled examples.","offset":1,"pro":0.2,"labels":"CLN"},{"idx":19,"sentence":"Finally, we demonstrate that ParaBLEU can be used to conditionally generate novel paraphrases from a single demonstration, which we use to confirm our hypothesis that it learns abstract, generalized paraphrase representations.","offset":2,"pro":0.4,"labels":"CLN"},{"idx":19,"sentence":"Unlike previous approaches, ParaBLEU learns to understand paraphrasis using generative conditioning as a pretraining objective.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":19,"sentence":"ParaBLEU correlates more strongly with human judgements than existing metrics, obtaining new state-of-the-art results on the 2017 WMT Metrics Shared Task.","offset":4,"pro":0.8,"labels":"MTD"},{"idx":0,"sentence":"Research in stance detection has so far focused on models which leverage purely textual input.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"In this paper, we investigate the integration of textual and financial signals for stance detection in the financial domain.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":0,"sentence":"Specifically, we propose a robust multi-task neural architecture that combines textual input with high-frequency intra-day time series from stock market prices.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":0,"sentence":"Moreover, we extend wt-wt, an existing stance detection dataset which collects tweets discussing Mergers and Acquisitions operations, with the relevant financial signal.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":0,"sentence":"Importantly, the obtained dataset aligns with Stander, an existing news stance detection dataset, thus resulting in a unique multimodal, multi-genre stance detection resource.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":0,"sentence":"We show experimentally and through detailed result analysis that our stance detection system benefits from financial information, and achieves state-of-the-art results on the wt-wt dataset: this demonstrates that the combination of multiple input signals is effective for cross-target stance detection, and opens interesting research directions for future work.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":1,"sentence":"Multilingual neural machine translation models are trained to maximize the likelihood of a mix of examples drawn from multiple language pairs.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"The dominant inductive bias applied to these models is a shared vocabulary and a shared set of parameters across languages; the inputs and labels corresponding to examples drawn from different language pairs might still reside in distinct sub-spaces.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":1,"sentence":"In this paper, we introduce multilingual crossover encoder-decoder (mXEncDec) to fuse language pairs at an instance level.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":1,"sentence":"Our approach interpolates instances from different language pairs into joint crossover examples' in order to encourage sharing input and output spaces across languages.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":1,"sentence":"Experiments on a large-scale WMT multilingual dataset demonstrate that our approach significantly improves quality on English-to-Many, Many-to-English and zero-shot translation tasks (from +0.5 BLEU up to +5.5 BLEU points).","offset":4,"pro":0.5,"labels":"RST"},{"idx":1,"sentence":"Results on code-switching sets demonstrate the capability of our approach to improve model generalization to out-of-distribution multilingual examples.","offset":5,"pro":0.625,"labels":"CLN"},{"idx":1,"sentence":"We also conduct qualitative and quantitative representation comparisons to analyze the advantages of our approach at the representation level.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":1,"sentence":"To ensure better fusion of examples in multilingual settings, we propose several techniques to improve example interpolation across dissimilar languages under heavy data imbalance.","offset":7,"pro":0.875,"labels":"MTD"},{"idx":2,"sentence":"Word identification from continuous input is typically viewed as a segmentation task.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"Experiments with human adults suggest that familiarity with syntactic structures in their native language also influences word identification in artificial languages; however, the relation between syntactic processing and word identification is yet unclear.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":2,"sentence":"This work takes one step forward by exploring a radically different approach of word identification, in which segmentation of a continuous input is viewed as a process isomorphic to unsupervised constituency parsing.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":2,"sentence":"Results show that this model can reproduce human behavior in word identification experiments, suggesting that this is a viable approach to study word identification and its relation to syntactic processing.","offset":3,"pro":0.6,"labels":"CLN"},{"idx":2,"sentence":"Besides formalizing the approach, this study reports simulations of human experiments with DIORA (Drozdov et al., 2020), a neural unsupervised constituency parser.","offset":4,"pro":0.8,"labels":"MTD"},{"idx":3,"sentence":"The social impact of natural language processing and its applications has received increasing attention.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"In this position paper, we focus on the problem of safety for end-to-end conversational AI.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":3,"sentence":"We survey the problem landscape therein, introducing a taxonomy of three observed phenomena: the Instigator, Yea-Sayer, and Impostor effects.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":3,"sentence":"We then empirically assess the extent to which current tools can measure these effects and current systems display them.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":3,"sentence":"We release these tools as part of a first aid kit (SafetyKit) to quickly assess apparent safety concerns.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":3,"sentence":"Our results show that, while current tools are able to provide an estimate of the relative safety of systems in various settings, they still have several shortcomings.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":3,"sentence":"We suggest several future directions and discuss ethical considerations.","offset":6,"pro":0.8571428571428571,"labels":"IMP"},{"idx":4,"sentence":"Recent work in cross-lingual semantic parsing has successfully applied machine translation to localize parsers to new languages.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"However, these advances assume access to high-quality machine translation systems and word alignment tools.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":4,"sentence":"We propose a multi-task encoder-decoder model to transfer parsing knowledge to additional languages using only English-logical form paired data and in-domain natural language corpora in each new language.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":4,"sentence":"Our model encourages language-agnostic encodings by jointly optimizing for logical-form generation with auxiliary objectives designed for cross-lingual latent representation alignment.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":4,"sentence":"Our parser performs significantly above translation-based baselines and, in some cases, competes with the supervised upper-bound.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":4,"sentence":"We remove these assumptions and study cross-lingual semantic parsing as a zero-shot problem, without parallel data (i.e., utterance-logical form pairs) for new languages.","offset":5,"pro":0.8333333333333334,"labels":"PUR"},{"idx":5,"sentence":"Obtaining human-like performance in NLP is often argued to require compositional generalisation.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"Whether neural networks exhibit this ability is usually studied by training models on highly compositional synthetic data.","offset":1,"pro":0.25,"labels":"BAC"},{"idx":5,"sentence":"However, compositionality in natural language is much more complex than the rigid, arithmetic-like version such data adheres to, and artificial compositionality tests thus do not allow us to determine how neural models deal with more realistic forms of compositionality.","offset":2,"pro":0.5,"labels":"GAP"},{"idx":5,"sentence":"In this work, we re-instantiate three compositionality tests from the literature and reformulate them for neural machine translation (NMT).","offset":3,"pro":0.75,"labels":"PUR"},{"idx":6,"sentence":"Document-level neural machine translation (DocNMT) achieves coherent translations by incorporating cross-sentence context.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"However, for most language pairs there's a shortage of parallel documents, although parallel sentences are readily available.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":6,"sentence":"In this paper, we study whether and how contextual modeling in DocNMT is transferable via multilingual modeling.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":6,"sentence":"We focus on the scenario of zero-shot transfer from teacher languages with document level data to student languages with no documents but sentence level data, and for the first time treat document-level translation as a transfer learning problem.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":6,"sentence":"Using simple concatenation-based DocNMT, we explore the effect of 3 factors on the transfer: the number of teacher languages with document level data, the balance between document and sentence level data at training, and the data condition of parallel documents (genuine vs. back-translated).","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":6,"sentence":"We observe that more teacher languages and adequate data balance both contribute to better transfer quality.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":6,"sentence":"Our experiments on Europarl-7 and IWSLT-10 show the feasibility of multilingual transfer for DocNMT, particularly on document-specific metrics.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":7,"sentence":"Cross-lingual retrieval aims to retrieve relevant text across languages.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"Current methods typically achieve cross-lingual retrieval by learning language-agnostic text representations in word or sentence level.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":7,"sentence":"However, how to learn phrase representations for cross-lingual phrase retrieval is still an open problem.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":7,"sentence":"In this paper, we propose , a cross-lingual phrase retriever that extracts phrase representations from unlabeled example sentences.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":7,"sentence":"Moreover, we create a large-scale cross-lingual phrase retrieval dataset, which contains 65K bilingual phrase pairs and 4.2M example sentences in 8 English-centric language pairs.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":7,"sentence":"Experimental results show that outperforms state-of-the-art baselines which utilize word-level or sentence-level representations. also shows impressive zero-shot transferability that enables the model to perform retrieval in an unseen language pair during training.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":7,"sentence":"Our dataset, code, and trained models are publicly available at github.com/cwszz/XPR/.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":8,"sentence":"Data-to-text generation focuses on generating fluent natural language responses from structured meaning representations (MRs).","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"Such representations are compositional and it is costly to collect responses for all possible combinations of atomic meaning schemata, thereby necessitating few-shot generalization to novel MRs.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":8,"sentence":"In this work, we systematically study the compositional generalization of the state-of-the-art T5 models in few-shot data-to-text tasks.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":8,"sentence":"On the commonly-used SGD and Weather benchmarks, the proposed self-training approach improves tree accuracy by 46%+ and reduces the slot error rates by 73%+ over the strong T5 baselines in few-shot settings.","offset":3,"pro":0.5,"labels":"RST"},{"idx":8,"sentence":"We show that T5 models fail to generalize to unseen MRs, and we propose a template-based input representation that considerably improves the model's generalization capability.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":8,"sentence":"To further improve the model's performance, we propose an approach based on self-training using fine-tuned BLEURT for pseudo-response selection.","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":9,"sentence":"The rapid development of conversational assistants accelerates the study on conversational question answering (QA).","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"However, the existing conversational QA systems usually answer users' questions with a single knowledge source, e.g., paragraphs or a knowledge graph, but overlook the important visual cues, let alone multiple knowledge sources of different modalities.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":9,"sentence":"In this paper, we hence define a novel research task, i.e., multimodal conversational question answering (MMCoQA), aiming to answer users' questions with multimodal knowledge sources via multi-turn conversations.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":9,"sentence":"This new task brings a series of research challenges, including but not limited to priority, consistency, and complementarity of multimodal knowledge.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":9,"sentence":"Questions are fully annotated with not only natural language answers but also the corresponding evidence and valuable decontextualized self-contained questions.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":9,"sentence":"Meanwhile, we introduce an end-to-end baseline model, which divides this complex research task into question understanding, multi-modal evidence retrieval, and answer extraction.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":9,"sentence":"Moreover, we report a set of benchmarking results, and the results indicate that there is ample room for improvement.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":10,"sentence":"The state-of-the-art model for structured sentiment analysis casts the task as a dependency parsing problem, which has some limitations: (1) The label proportions for span prediction and span relation prediction are imbalanced.","offset":0,"pro":0,"labels":"GAP"},{"idx":10,"sentence":"(2) The span lengths of sentiment tuple components may be very large in this task, which will further exacerbates the imbalance problem.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":10,"sentence":"(3) Two nodes in a dependency graph cannot have multiple arcs, therefore some overlapped sentiment tuples cannot be recognized.","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":10,"sentence":"In this work, we propose nichetargeting solutions for these issues.","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":10,"sentence":"First, we introduce a novel labeling strategy, which contains two sets of token pair labels, namely essential label set and whole label set.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":10,"sentence":"The essential label set consists of the basic labels for this task, which are relatively balanced and applied in the prediction layer.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":10,"sentence":"The whole label set includes rich labels to help our model capture various token relations, which are applied in the hidden layer to softly influence our model.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":10,"sentence":"Moreover, we also propose an effective model to well collaborate with our labeling strategy, which is equipped with the graph attention networks to iteratively refine token representations, and the adaptive multi-label classifier to dynamically predict multiple relations between token pairs.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":10,"sentence":"We perform extensive experiments on 5 benchmark datasets in four languages.","offset":8,"pro":0.8888888888888888,"labels":"MTD"},{"idx":11,"sentence":"This paper focuses on the Data Augmentation for low-resource Natural Language Understanding (NLU) tasks.","offset":0,"pro":0,"labels":"PUR"},{"idx":11,"sentence":"We propose Prompt-based Data Augmentation model (PromDA) which only trains small-scale Soft Prompt (i.e., a set of trainable vectors) in the frozen Pre-trained Language Models (PLMs).","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":11,"sentence":"In addition, PromDA generates synthetic data via two different views and filters out the low-quality data using NLU models.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":11,"sentence":"Experiments on four benchmarks show that synthetic data produced by PromDA successfully boost up the performance of NLU models which consistently outperform several competitive baseline models, including a state-of-the-art semi-supervised model using unlabeled in-domain data.","offset":3,"pro":0.5,"labels":"CLN"},{"idx":11,"sentence":"The NLU models can be further improved when they are combined for training.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":11,"sentence":"The synthetic data from PromDA are also complementary with unlabeled in-domain data.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":12,"sentence":"There is mounting evidence that existing neural network models, in particular the very popular sequence-to-sequence architecture, struggle to systematically generalize to unseen compositions of seen components.","offset":0,"pro":0,"labels":"GAP"},{"idx":12,"sentence":"Specifically, we condition the source representations on the newly decoded target context which makes it easier for the encoder to exploit specialized information for each prediction rather than capturing it all in a single forward pass.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":12,"sentence":"Experimental results on semantic parsing and machine translation empirically show that our proposal delivers more disentangled representations and better generalization.","offset":2,"pro":0.4,"labels":"CLN"},{"idx":12,"sentence":"We demonstrate that one of the reasons hindering compositional generalization relates to representations being entangled.","offset":3,"pro":0.6,"labels":"PUR"},{"idx":12,"sentence":"We propose an extension to sequence-to-sequence models which encourage disentanglement by adaptively re-encoding (at each time step) the source input.","offset":4,"pro":0.8,"labels":"MTD"},{"idx":13,"sentence":"Pre-trained language models (PLMs) have shown great potentials in natural language processing (NLP) including rhetorical structure theory (RST) discourse parsing.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"To this end, we propose a second-stage EDU-level pre-training approach in this work, which presents two novel tasks to learn effective EDU representations continually based on well pre-trained language models.","offset":1,"pro":0.3333333333333333,"labels":"PUR"},{"idx":13,"sentence":"We take a state-of-the-art transition-based neural parser as baseline, and adopt it with a light bi-gram EDU modification to effectively explore the EDU-level pre-trained EDU representation.Experimental results on a benckmark dataset show that our method is highly effective,leading a 2.1-point improvement in F1-score.","offset":2,"pro":0.6666666666666666,"labels":"MTD"},{"idx":14,"sentence":"Knowledge graph completion (KGC) aims to reason over known facts and infer the missing links.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"Text-based methods such as KGBERT (Yao et al., 2019) learn entity representations from natural language descriptions, and have the potential for inductive KGC.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":14,"sentence":"However, the performance of text-based methods still largely lag behind graph embedding-based methods like TransE (Bordes et al., 2013) and RotatE (Sun et al., 2019b).","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":14,"sentence":"In this paper, we identify that the key issue is efficient contrastive learning.","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":14,"sentence":"Combined with InfoNCE loss, our proposed model SimKGC can substantially outperform embedding-based methods on several benchmark datasets.","offset":4,"pro":0.4444444444444444,"labels":"RST"},{"idx":14,"sentence":"In terms of mean reciprocal rank (MRR), we advance the state-of-the-art by +19% on WN18RR, +6.8% on the Wikidata5M transductive setting, and +22% on the Wikidata5M inductive setting.","offset":5,"pro":0.5555555555555556,"labels":"RST"},{"idx":14,"sentence":"Thorough analyses are conducted to gain insights into each component.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":14,"sentence":"Our code is available at https://github.com/intfloat/SimKGC .","offset":7,"pro":0.7777777777777778,"labels":"CTN"},{"idx":14,"sentence":"To improve the learning efficiency, we introduce three types of negatives: in-batch negatives, pre-batch negatives, and self-negatives which act as a simple form of hard negatives.","offset":8,"pro":0.8888888888888888,"labels":"MTD"},{"idx":15,"sentence":"Learned self-attention functions in state-of-the-art NLP models often correlate with human attention.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"We investigate whether self-attention in large-scale pre-trained language models is as predictive of human eye fixation patterns during task-reading as classical cognitive models of human attention.","offset":1,"pro":0.25,"labels":"PUR"},{"idx":15,"sentence":"We compare attention functions across two task-specific reading datasets for sentiment analysis and relation extraction.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":15,"sentence":"We find the predictiveness of large-scale pre-trained self-attention for human attention depends on what is in the tail', e.g., the syntactic nature of rare contexts.","offset":3,"pro":0.75,"labels":"RST"},{"idx":16,"sentence":"Laws and their interpretations, legal arguments and agreements are typically expressed in writing, leading to the production of vast corpora of legal text.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"Their analysis, which is at the center of legal practice, becomes increasingly elaborate as these collections grow in size.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":16,"sentence":"Natural language understanding (NLU) technologies can be a valuable tool to support legal practitioners in these endeavors.","offset":2,"pro":0.3333333333333333,"labels":"BAC"},{"idx":16,"sentence":"Their usefulness, however, largely depends on whether current state-of-the-art models can generalize across various tasks in the legal domain.","offset":3,"pro":0.5,"labels":"GAP"},{"idx":16,"sentence":"To answer this currently open question, we introduce the Legal General Language Understanding Evaluation (LexGLUE) benchmark, a collection of datasets for evaluating model performance across a diverse set of legal NLU tasks in a standardized way.","offset":4,"pro":0.6666666666666666,"labels":"PUR"},{"idx":16,"sentence":"We also provide an evaluation and analysis of several generic and legal-oriented models demonstrating that the latter consistently offer performance improvements across multiple tasks.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":17,"sentence":"Lexical ambiguity poses one of the greatest challenges in the field of Machine Translation.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"Over the last few decades, multiple efforts have been undertaken to investigate incorrect translations caused by the polysemous nature of words.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":17,"sentence":"In this paper, we present DiBiMT, the first entirely manually-curated evaluation benchmark which enables an extensive study of semantic biases in Machine Translation of nominal and verbal words in five different language combinations, namely, English and one or other of the following languages: Chinese, German, Italian, Russian and Spanish.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":17,"sentence":"Furthermore, we test state-of-the-art Machine Translation systems, both commercial and non-commercial ones, against our new test bed and provide a thorough statistical and linguistic analysis of the results.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":17,"sentence":"We release DiBiMT at https://nlp.uniroma1.it/dibimt as a closed benchmark with a public leaderboard.","offset":4,"pro":0.6666666666666666,"labels":"CTN"},{"idx":17,"sentence":"Within this body of research, some studies have posited that models pick up semantic biases existing in the training data, thus producing translation errors.","offset":5,"pro":0.8333333333333334,"labels":"GAP"},{"idx":18,"sentence":"Word translation or bilingual lexicon induction (BLI) is a key cross-lingual task, aiming to bridge the lexical gap between different languages.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"In this work, we propose a robust and effective two-stage contrastive learning framework for the BLI task.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":18,"sentence":"At Stage C1, we propose to refine standard cross-lingual linear maps between static word embeddings (WEs) via a contrastive learning objective; we also show how to integrate it into the self-learning procedure for even more refined cross-lingual maps.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":18,"sentence":"In Stage C2, we conduct BLI-oriented contrastive fine-tuning of mBERT, unlocking its word translation capability.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":18,"sentence":"We also show that static WEs induced from the C2-tuned' mBERT complement static WEs from Stage C1.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":18,"sentence":"Comprehensive experiments on standard BLI datasets for diverse languages and different experimental setups demonstrate substantial gains achieved by our framework.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":19,"sentence":"Neural Chat Translation (NCT) aims to translate conversational text into different languages.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"Existing methods mainly focus on modeling the bilingual dialogue characteristics (e.g., coherence) to improve chat translation via multi-task learning on small-scale chat translation data.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":19,"sentence":"Although the NCT models have achieved impressive success, it is still far from satisfactory due to insufficient chat translation data and simple joint training manners.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":19,"sentence":"To address the above issues, we propose a scheduled multi-task learning framework for NCT.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":19,"sentence":"Specifically, we devise a three-stage training framework to incorporate the large-scale in-domain chat translation data into training by adding a second pre-training stage between the original pre-training and fine-tuning stages.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":19,"sentence":"Further, we investigate where and how to schedule the dialogue-related auxiliary tasks in multiple training stages to effectively enhance the main chat translation task.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":19,"sentence":"Extensive experiments on four language directions (English-Chinese and English-German) verify the effectiveness and superiority of the proposed approach.","offset":6,"pro":0.75,"labels":"CLN"},{"idx":19,"sentence":"Additionally, we will make the large-scale in-domain paired bilingual dialogue dataset publicly available for the research community.","offset":7,"pro":0.875,"labels":"CTN"},{"idx":0,"sentence":"We present a benchmark suite of four datasets for evaluating the fairness of pre-trained language models and the techniques used to fine-tune them for downstream tasks.","offset":0,"pro":0,"labels":"PUR"},{"idx":0,"sentence":"Our benchmarks cover four jurisdictions (European Council, USA, Switzerland, and China), five languages (English, German, French, Italian and Chinese) and fairness across five attributes (gender, age, region, language, and legal area).","offset":1,"pro":0.25,"labels":"MTD"},{"idx":0,"sentence":"In our experiments, we evaluate pre-trained language models using several group-robust fine-tuning techniques and show that performance group disparities are vibrant in many cases, while none of these techniques guarantee fairness, nor consistently mitigate group disparities.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":0,"sentence":"Furthermore, we provide a quantitative and qualitative analysis of our results, highlighting open challenges in the development of robustness methods in legal NLP.","offset":3,"pro":0.75,"labels":"RST"},{"idx":1,"sentence":"Podcasts have shown a recent rise in popularity.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"Summarization of podcasts is of practical benefit to both content providers and consumers.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":1,"sentence":"It helps people quickly decide whether they will listen to a podcast and/or reduces the cognitive load of content providers to write summaries.","offset":2,"pro":0.2222222222222222,"labels":"BAC"},{"idx":1,"sentence":"Nevertheless, podcast summarization faces significant challenges including factual inconsistencies of summaries with respect to the inputs.","offset":3,"pro":0.3333333333333333,"labels":"GAP"},{"idx":1,"sentence":"The problem is exacerbated by speech disfluencies and recognition errors in transcripts of spoken language.","offset":4,"pro":0.4444444444444444,"labels":"GAP"},{"idx":1,"sentence":"In this paper, we explore a novel abstractive summarization method to alleviate these issues.","offset":5,"pro":0.5555555555555556,"labels":"PUR"},{"idx":1,"sentence":"Our approach learns to produce an abstractive summary while grounding summary segments in specific regions of the transcript to allow for full inspection of summary details.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":1,"sentence":"We conduct a series of analyses of the proposed approach on a large podcast dataset and show that the approach can achieve promising results.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":1,"sentence":"Grounded summaries bring clear benefits in locating the summary and transcript segments that contain inconsistent information, and hence improve summarization quality in terms of automatic and human evaluation.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":2,"sentence":"Publicly traded companies are required to submit periodic reports with eXtensive Business Reporting Language (XBRL) word-level tags.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"Manually tagging the reports is tedious and costly.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":2,"sentence":"We, therefore, introduce XBRL tagging as a new entity extraction task for the financial domain and release FiNER-139, a dataset of 1.1M sentences with gold XBRL tags.","offset":2,"pro":0.2222222222222222,"labels":"PUR"},{"idx":2,"sentence":"Unlike typical entity extraction datasets, FiNER-139 uses a much larger label set of 139 entity types.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":2,"sentence":"Most annotated tokens are numeric, with the correct tag per token depending mostly on context, rather than the token itself.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":2,"sentence":"We show that subword fragmentation of numeric expressions harms BERT's performance, allowing word-level BILSTMs to perform better.","offset":5,"pro":0.5555555555555556,"labels":"RST"},{"idx":2,"sentence":"To improve BERT's performance, we propose two simple and effective solutions that replace numeric expressions with pseudo-tokens reflecting original token shapes and numeric magnitudes.","offset":6,"pro":0.6666666666666666,"labels":"PUR"},{"idx":2,"sentence":"We also experiment with FIN-BERT, an existing BERT model for the financial domain, and release our own BERT (SEC-BERT), pre-trained on financial filings, which performs best.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":2,"sentence":"Through data and error analysis, we finally identify possible limitations to inspire future work on XBRL tagging.","offset":8,"pro":0.8888888888888888,"labels":"RST"},{"idx":3,"sentence":"Contrastive learning has achieved impressive success in generation tasks to militate the exposure bias problem and discriminatively exploit the different quality of references.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"Existing works mostly focus on contrastive learning on the instance-level without discriminating the contribution of each word, while keywords are the gist of the text and dominant the constrained mapping relationships.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":3,"sentence":"Hence, in this work, we propose a hierarchical contrastive learning mechanism, which can unify hybrid granularities semantic meaning in the input text.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":3,"sentence":"Concretely, we first propose a keyword graph via contrastive correlations of positive-negative pairs to iteratively polish the keyword representations.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":3,"sentence":"Then, we construct intra-contrasts within instance-level and keyword-level, where we assume words are sampled nodes from a sentence distribution.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":3,"sentence":"Finally, to bridge the gap between independent contrast levels and tackle the common contrast vanishing problem, we propose an inter-contrast mechanism that measures the discrepancy between contrastive keyword nodes respectively to the instance distribution.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":3,"sentence":"Experiments demonstrate that our model outperforms competitive baselines on paraphrasing, dialogue generation, and storytelling tasks.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":4,"sentence":"In this paper, we propose a neural model EPT-X (Expression-Pointer Transformer with Explanations), which utilizes natural language explanations to solve an algebraic word problem.","offset":0,"pro":0,"labels":"PUR"},{"idx":4,"sentence":"To enhance the explainability of the encoding process of a neural model, EPT-X adopts the concepts of plausibility and faithfulness which are drawn from math word problem solving strategies by humans.","offset":1,"pro":0.125,"labels":"MTD"},{"idx":4,"sentence":"A plausible explanation is one that includes contextual information for the numbers and variables that appear in a given math word problem.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":4,"sentence":"A faithful explanation is one that accurately represents the reasoning process behind the model's solution equation.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":4,"sentence":"The EPT-X model yields an average baseline performance of 69.59% on our PEN dataset and produces explanations with quality that is comparable to human output.","offset":4,"pro":0.5,"labels":"RST"},{"idx":4,"sentence":"The contribution of this work is two-fold.","offset":5,"pro":0.625,"labels":"CTN"},{"idx":4,"sentence":"(1) EPT-X model: An explainable neural model that sets a baseline for algebraic word problem solving task, in terms of model's correctness, plausibility, and faithfulness.","offset":6,"pro":0.75,"labels":"CTN"},{"idx":4,"sentence":"(2) New dataset: We release a novel dataset PEN (Problems with Explanations for Numbers), which expands the existing datasets by attaching explanations to each number/variable.","offset":7,"pro":0.875,"labels":"CTN"},{"idx":5,"sentence":"This paper studies the (often implicit) human values behind natural language arguments, such as to have freedom of thought or to be broadminded.","offset":0,"pro":0,"labels":"PUR"},{"idx":5,"sentence":"Values are commonly accepted answers to why some option is desirable in the ethical sense and are thus essential both in real-world argumentation and theoretical argumentation frameworks.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":5,"sentence":"However, their large variety has been a major obstacle to modeling them in argument mining.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":5,"sentence":"To overcome this obstacle, we contribute an operationalization of human values, namely a multi-level taxonomy with 54 values that is in line with psychological research.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":5,"sentence":"Moreover, we provide a dataset of 5270 arguments from four geographical cultures, manually annotated for human values.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":5,"sentence":"First experiments with the automatic classification of human values are promising, with F_1-scores up to 0.81 and 0.25 on average.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":6,"sentence":"Intrinsic evaluations of OIE systems are carried out either manually-with human evaluators judging the correctness of extractions-or automatically, on standardized benchmarks.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"The latter, while much more cost-effective, is less reliable, primarily because of the incompleteness of the existing OIE benchmarks: the ground truth extractions do not include all acceptable variants of the same fact, leading to unreliable assessment of the models' performance.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":6,"sentence":"Moreover, the existing OIE benchmarks are available for English only.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":6,"sentence":"In this work, we introduce BenchIE: a benchmark and evaluation framework for comprehensive evaluation of OIE systems for English, Chinese, and German.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":6,"sentence":"In contrast to existing OIE benchmarks, BenchIE is fact-based, i.e., it takes into account informational equivalence of extractions: our gold standard consists of fact synsets, clusters in which we exhaustively list all acceptable surface forms of the same fact.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":6,"sentence":"Moreover, having in mind common downstream applications for OIE, we make BenchIE multi-faceted; i.e., we create benchmark variants that focus on different facets of OIE evaluation, e.g., compactness or minimality of extractions.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":6,"sentence":"We benchmark several state-of-the-art OIE systems using BenchIE and demonstrate that these systems are significantly less effective than indicated by existing OIE benchmarks.","offset":6,"pro":0.75,"labels":"RST"},{"idx":6,"sentence":"We make BenchIE (data and evaluation code) publicly available.","offset":7,"pro":0.875,"labels":"CTN"},{"idx":7,"sentence":"Training Transformer-based models demands a large amount of data, while obtaining aligned and labelled data in multimodality is rather cost-demanding, especially for audio-visual speech recognition (AVSR).","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"Thus it makes a lot of sense to make use of unlabelled unimodal data.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":7,"sentence":"On the other side, although the effectiveness of large-scale self-supervised learning is well established in both audio and visual modalities, how to integrate those pre-trained models into a multimodal scenario remains underexplored.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":7,"sentence":"In this work, we successfully leverage unimodal self-supervised learning to promote the multimodal AVSR.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":7,"sentence":"In particular, audio and visual front-ends are trained on large-scale unimodal datasets, then we integrate components of both front-ends into a larger multimodal framework which learns to recognize parallel audio-visual data into characters through a combination of CTC and seq2seq decoding.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":7,"sentence":"We show that both components inherited from unimodal self-supervised learning cooperate well, resulting in that the multimodal framework yields competitive results through fine-tuning.","offset":5,"pro":0.625,"labels":"RST"},{"idx":7,"sentence":"Our model is experimentally validated on both word-level and sentence-level tasks.","offset":6,"pro":0.75,"labels":"RST"},{"idx":7,"sentence":"Especially, even without an external language model, our proposed model raises the state-of-the-art performances on the widely accepted Lip Reading Sentences 2 (LRS2) dataset by a large margin, with a relative improvement of 30%.","offset":7,"pro":0.875,"labels":"RST"},{"idx":8,"sentence":"Sequence-to-sequence neural networks have recently achieved great success in abstractive summarization, especially through fine-tuning large pre-trained language models on the downstream dataset.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"These models are typically decoded with beam search to generate a unique summary.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":8,"sentence":"However, the search space is very large, and with the exposure bias, such decoding is not optimal.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":8,"sentence":"In this paper, we show that it is possible to directly train a second-stage model performing re-ranking on a set of summary candidates.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":8,"sentence":"Our mixture-of-experts SummaReranker learns to select a better candidate and consistently improves the performance of the base model.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":8,"sentence":"With a base PEGASUS, we push ROUGE scores by 5.44% on CNN- DailyMail (47.16 ROUGE-1), 1.31% on XSum (48.12 ROUGE-1) and 9.34% on Reddit TIFU (29.83 ROUGE-1), reaching a new state-of-the-art.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":8,"sentence":"Our code and checkpoints will be available at https://github.com/ntunlp/SummaReranker.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":9,"sentence":"The ability to sequence unordered events is evidence of comprehension and reasoning about real world tasks/procedures.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"It is essential for applications such as task planning and multi-source instruction summarization.It often requires thorough understanding of temporal common sense and multimodal information, since these procedures are often conveyed by a combination of texts and images","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":9,"sentence":"While humans are capable of reasoning about and sequencing unordered procedural instructions, the extent to which the current machine learning methods possess such capability is still an open question","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":9,"sentence":"In this work, we benchmark models' capability of reasoning over and sequencing unordered multimodal instructions by curating datasets from online instructional manuals and collecting comprehensive human annotations","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":9,"sentence":"We find current state-of-the-art models not only perform significantly worse than humans but also seem incapable of efficiently utilizing multimodal information","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":9,"sentence":"To improve machines' performance on multimodal event sequencing, we propose sequence-aware pretraining techniques exploiting the sequential alignment properties of both texts and images","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":9,"sentence":"resulting in greater 5% improvements on perfect match ratio","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":10,"sentence":"Fake news detection is crucial for preventing the dissemination of misinformation on social media.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"To differentiate fake news from real ones, existing methods observe the language patterns of the news post and zoom in to verify its content with knowledge sources or check its readers' replies.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":10,"sentence":"However, these methods neglect the information in the external news environment where a fake news post is created and disseminated.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":10,"sentence":"The news environment represents recent mainstream media opinion and public attention, which is an important inspiration of fake news fabrication because fake news is often designed to ride the wave of popular events and catch public attention with unexpected novel content for greater exposure and spread.","offset":3,"pro":0.375,"labels":"GAP"},{"idx":10,"sentence":"To capture the environmental signals of news posts, we zoom out to observe the news environment and propose the News Environment Perception Framework (NEP).","offset":4,"pro":0.5,"labels":"PUR"},{"idx":10,"sentence":"For each post, we construct its macro and micro news environment from recent mainstream news.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":10,"sentence":"Then we design a popularity-oriented and a novelty-oriented module to perceive useful signals and further assist final prediction.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":10,"sentence":"Experiments on our newly built datasets show that the NEP can efficiently improve the performance of basic fake news detectors.","offset":7,"pro":0.875,"labels":"RST"},{"idx":11,"sentence":"Multi-encoder models are a broad family of context-aware neural machine translation systems that aim to improve translation quality by encoding document-level contextual information alongside the current sentence.","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"The context encoding is undertaken by contextual parameters, trained on document-level data.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":11,"sentence":"In this work, we discuss the difficulty of training these parameters effectively, due to the sparsity of the words in need of context (i.e., the training signal), and their relevant context.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":11,"sentence":"We propose to pre-train the contextual parameters over split sentence pairs, which makes an efficient use of the available data for two reasons.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":11,"sentence":"Firstly, it increases the contextual training signal by breaking intra-sentential syntactic relations, and thus pushing the model to search the context for disambiguating clues more frequently.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":11,"sentence":"Secondly, it eases the retrieval of relevant context, since context segments become shorter.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":11,"sentence":"We propose four different splitting methods, and evaluate our approach with BLEU and contrastive test sets.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":11,"sentence":"Results show that it consistently improves learning of contextual parameters, both in low and high resource settings.","offset":7,"pro":0.875,"labels":"RST"},{"idx":12,"sentence":"Event detection (ED) is a critical subtask of event extraction that seeks to identify event triggers of certain types in texts","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"Despite significant advances in ED, existing methods typically follow a one model fits all types approach, which sees no differences between event types and often results in a quite skewed performance.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":12,"sentence":"This research examines the issue in depth and presents a new concept termed trigger salience attribution, which can explicitly quantify the underlying patterns of events","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":12,"sentence":"On this foundation, we develop a new training mechanism for ED, which can distinguish between trigger-dependent and context-dependent types and achieve promising performance on two benchmarks","offset":3,"pro":0.5,"labels":"MTD"},{"idx":12,"sentence":"Finally, by highlighting many distinct characteristics of trigger-dependent and context-dependent types","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":12,"sentence":"our work may promote more research into this problem.","offset":5,"pro":0.8333333333333334,"labels":"CTN"},{"idx":13,"sentence":"In the field of sentiment analysis, several studies have highlighted that a single sentence may express multiple, sometimes contrasting, sentiments and emotions, each with its own experiencer, target and/or cause.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"To this end, over the past few years researchers have started to collect and annotate data manually, in order to investigate the capabilities of automatic systems not only to distinguish between emotions, but also to capture their semantic constituents.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":13,"sentence":"However, currently available gold datasets are heterogeneous in size, domain, format, splits, emotion categories and role labels, making comparisons across different works difficult and hampering progress in the area.","offset":2,"pro":0.4,"labels":"GAP"},{"idx":13,"sentence":"In this paper, we tackle this issue and present a unified evaluation framework focused on Semantic Role Labeling for Emotions (SRL4E), in which we unify several datasets tagged with emotions and semantic roles by using a common labeling scheme.","offset":3,"pro":0.6,"labels":"PUR"},{"idx":13,"sentence":"We use SRL4E as a benchmark to evaluate how modern pretrained language models perform and analyze where we currently stand in this task, hoping to provide the tools to facilitate studies in this complex area.","offset":4,"pro":0.8,"labels":"MTD"},{"idx":14,"sentence":"In linguistics, there are two main perspectives on negation: a semantic and a pragmatic view.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"So far, research in NLP on negation has almost exclusively adhered to the semantic view.","offset":1,"pro":0.25,"labels":"GAP"},{"idx":14,"sentence":"In this article, we adopt the pragmatic paradigm to conduct a study of negation understanding focusing on transformer-based PLMs.","offset":2,"pro":0.5,"labels":"PUR"},{"idx":14,"sentence":"Our results differ from previous, semantics-based studies and therefore help to contribute a more comprehensive - and, given the results, much more optimistic - picture of the PLMs' negation understanding.","offset":3,"pro":0.75,"labels":"CLN"},{"idx":15,"sentence":"Thanks to the effectiveness and wide availability of modern pretrained language models (PLMs), recently proposed approaches have achieved remarkable results in dependency- and span-based, multilingual and cross-lingual Semantic Role Labeling (SRL).","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"These results have prompted researchers to investigate the inner workings of modern PLMs with the aim of understanding how, where, and to what extent they encode information about SRL.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":15,"sentence":"In this paper, we follow this line of research and probe for predicate argument structures in PLMs.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":15,"sentence":"Our study shows that PLMs do encode semantic structures directly into the contextualized representation of a predicate, and also provides insights into the correlation between predicate senses and their structures, the degree of transferability between nominal and verbal structures, and how such structures are encoded across languages.","offset":3,"pro":0.6,"labels":"RST"},{"idx":15,"sentence":"Finally, we look at the practical implications of such insights and demonstrate the benefits of embedding predicate argument structure information into an SRL model.","offset":4,"pro":0.8,"labels":"RST"},{"idx":16,"sentence":"We present a study on leveraging multilingual pre-trained generative language models for zero-shot cross-lingual event argument extraction (EAE).","offset":0,"pro":0,"labels":"PUR"},{"idx":16,"sentence":"By formulating EAE as a language generation task, our method effectively encodes event structures and captures the dependencies between arguments.","offset":1,"pro":0.14285714285714285,"labels":"MTD"},{"idx":16,"sentence":"We design language-agnostic templates to represent the event argument structures, which are compatible with any language, hence facilitating the cross-lingual transfer.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":16,"sentence":"Our proposed model finetunes multilingual pre-trained generative language models to generate sentences that fill in the language-agnostic template with arguments extracted from the input passage.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":16,"sentence":"The model is trained on source languages and is then directly applied to target languages for event argument extraction.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":16,"sentence":"Experiments demonstrate that the proposed model outperforms the current state-of-the-art models on zero-shot cross-lingual EAE.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":16,"sentence":"Comprehensive studies and error analyses are presented to better understand the advantages and the current limitations of using generative language models for zero-shot cross-lingual transfer EAE.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":17,"sentence":"Identifying changes in individuals' behaviour and mood, as observed via content shared on online platforms, is increasingly gaining importance.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"Most research to-date on this topic focuses on either: (a) identifying individuals at risk or with a certain mental health condition given a batch of posts or (b) providing equivalent labels at the post level.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":17,"sentence":"A disadvantage of such work is the lack of a strong temporal component and the inability to make longitudinal assessments following an individual's trajectory and allowing timely interventions.","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":17,"sentence":"Here we define a new task, that of identifying moments of change in individuals on the basis of their shared content online.","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":17,"sentence":"The changes we consider are sudden shifts in mood (switches) or gradual mood progression (escalations).","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":17,"sentence":"We have created detailed guidelines for capturing moments of change and a corpus of 500 manually annotated user timelines (18.7K posts).","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":17,"sentence":"We also introduce new metrics for capturing rare events in temporal windows.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":17,"sentence":"We have developed a variety of baseline models drawing inspiration from related tasks","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":17,"sentence":"and show that the best performance is obtained through context aware sequential modelling","offset":8,"pro":0.8888888888888888,"labels":"RST"},{"idx":18,"sentence":"Pre-trained language models have been recently shown to benefit task-oriented dialogue (TOD) systems.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"Despite their success, existing methods often formulate this task as a cascaded generation problem which can lead to error accumulation across different sub-tasks and greater data annotation overhead.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":18,"sentence":"In this study, we present PPTOD, a unified plug-and-play model for task-oriented dialogue.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":18,"sentence":"In addition, we introduce a new dialogue multi-task pre-training strategy that allows the model to learn the primary TOD task completion skills from heterogeneous dialog corpora.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":18,"sentence":"We extensively test our model on three benchmark TOD tasks, including end-to-end dialogue modelling, dialogue state tracking, and intent classification.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":18,"sentence":"Experimental results show that PPTOD achieves new state of the art on all evaluated tasks in both high-resource and low-resource scenarios.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":18,"sentence":"Furthermore, comparisons against previous SOTA methods show that the responses generated by PPTOD are more factually correct and semantically coherent as judged by human annotators.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":19,"sentence":"The impression section of a radiology report summarizes the most prominent observation from the findings section and is the most important section for radiologists to communicate to physicians.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"Summarizing findings is time-consuming and can be prone to error for inexperienced radiologists, and thus automatic impression generation has attracted substantial attention.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":19,"sentence":"With the encoder-decoder framework, most previous studies explore incorporating extra knowledge (e.g., static pre-defined clinical ontologies or extra background information).","offset":2,"pro":0.2222222222222222,"labels":"BAC"},{"idx":19,"sentence":"Yet, they encode such knowledge by a separate encoder to treat it as an extra input to their models, which is limited in leveraging their relations with the original findings.","offset":3,"pro":0.3333333333333333,"labels":"GAP"},{"idx":19,"sentence":"To address the limitation, we propose a unified framework for exploiting both extra knowledge and the original findings in an integrated way so that the critical information (i.e., key words and their relations) can be extracted in an appropriate way to facilitate impression generation.","offset":4,"pro":0.4444444444444444,"labels":"PUR"},{"idx":19,"sentence":"In detail, for each input findings, it is encoded by a text encoder and a graph is constructed through its entities and dependency tree.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":19,"sentence":"Then, a graph encoder (e.g., graph neural networks (GNNs)) is adopted to model relation information in the constructed graph.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":19,"sentence":"Finally, to emphasize the key words in the findings, contrastive learning is introduced to map positive samples (constructed by masking non-key words) closer and push apart negative ones (constructed by masking key words).","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":19,"sentence":"The experimental results on two datasets, OpenI and MIMIC-CXR, confirm the effectiveness of our proposed method, where the state-of-the-art results are achieved.","offset":8,"pro":0.8888888888888888,"labels":"RST"},{"idx":0,"sentence":"Formality style transfer (FST) is a task that involves paraphrasing an informal sentence into a formal one without altering its meaning.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"To address the data-scarcity problem of existing parallel datasets, previous studies tend to adopt a cycle-reconstruction scheme to utilize additional unlabeled data, where the FST model mainly benefits from target-side unlabeled sentences.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":0,"sentence":"In this work, we propose a simple yet effective semi-supervised framework to better utilize source-side unlabeled sentences based on consistency training.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":0,"sentence":"Specifically, our approach augments pseudo-parallel data obtained from a source-side informal sentence by enforcing the model to generate similar outputs for its perturbed version.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":0,"sentence":"Moreover, we empirically examined the effects of various data perturbation methods and propose effective data filtering strategies to improve our framework.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":0,"sentence":"Experimental results on the GYAFC benchmark demonstrate that our approach can achieve state-of-the-art results, even with less than 40% of the parallel data.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":1,"sentence":"Multilingual pre-trained language models, such as mBERT and XLM-R, have shown impressive cross-lingual ability.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"Despite the encouraging results, we still lack a clear understanding of why cross-lingual ability could emerge from multilingual MLM.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":1,"sentence":"In our work, we argue that cross-language ability comes from the commonality between languages.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":1,"sentence":"Specifically, we study three language properties: constituent order, composition and word co-occurrence.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":1,"sentence":"First, we create an artificial language by modifying property in source language.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":1,"sentence":"Then we study the contribution of modified property through the change of cross-language transfer results on target language.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":1,"sentence":"We conduct experiments on six languages and two cross-lingual NLP tasks (textual entailment, sentence retrieval).","offset":6,"pro":0.75,"labels":"MTD"},{"idx":1,"sentence":"Surprisingly, both of them use multilingual masked language model (MLM) without any cross-lingual supervision or aligned data.","offset":7,"pro":0.875,"labels":"BAC"},{"idx":2,"sentence":"Word sense disambiguation (WSD) is a crucial problem in the natural language processing (NLP) community.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"Current methods achieve decent performance by utilizing supervised learning and large pre-trained language models.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":2,"sentence":"However, the imbalanced training dataset leads to poor performance on rare senses and zero-shot senses.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":2,"sentence":"Based on the relation, we propose a Z-reweighting method on the word level to adjust the training on the imbalanced dataset.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":2,"sentence":"The experiments show that the Z-reweighting strategy achieves performance gain on the standard English all words WSD benchmark.","offset":4,"pro":0.5714285714285714,"labels":"CLN"},{"idx":2,"sentence":"Moreover, the strategy can help models generalize better on rare and zero-shot senses.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":2,"sentence":"There are more training instances and senses for words with top frequency ranks than those with low frequency ranks in the training dataset. ","offset":6,"pro":0.8571428571428571,"labels":"GAP"},{"idx":3,"sentence":"With state-of-the-art systems having finally attained estimated human performance, Word Sense Disambiguation (WSD) has now joined the array of Natural Language Processing tasks that have seemingly been solved, thanks to the vast amounts of knowledge encoded into Transformer-based pre-trained language models.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"And yet, if we look below the surface of raw figures, it is easy to realize that current approaches still make trivial mistakes that a human would never make.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":3,"sentence":"In this work, we provide evidence showing why the F1 score metric should not simply be taken at face value and present an exhaustive analysis of the errors that seven of the most representative state-of-the-art systems for English all-words WSD make on traditional evaluation benchmarks.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":3,"sentence":"In addition, we produce and release a collection of test sets featuring (a) an amended version of the standard evaluation benchmark that fixes its lexical and semantic inaccuracies, (b) 42D, a challenge set devised to assess the resilience of systems with respect to least frequent word senses and senses not seen at training time, and (c) hardEN, a challenge set made up solely of instances which none of the investigated state-of-the-art systems can solve.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":3,"sentence":"We make all of the test sets and model predictions available to the research community at https://github.com/SapienzaNLP/wsd-hard-benchmark.","offset":4,"pro":0.8,"labels":"CTN"},{"idx":4,"sentence":"Evaluation on English Wikipedia that was sense-tagged using our method shows that both the induced senses, and the per-instance sense assignment, are of high quality even compared to WSD methods, such as Babelfy.","offset":0,"pro":0,"labels":"CLN"},{"idx":4,"sentence":"Furthermore, by training a static word embeddings algorithm on the sense-tagged corpus, we obtain high-quality static senseful embeddings.","offset":1,"pro":0.2,"labels":"RST"},{"idx":4,"sentence":"These outperform existing senseful embeddings methods on the WiC dataset and on a new outlier detection dataset we developed.","offset":2,"pro":0.4,"labels":"RST"},{"idx":4,"sentence":"The data driven nature of the algorithm allows to induce corpora-specific senses, which may not appear in standard sense inventories, as we demonstrate using a case study on the scientific domain.","offset":3,"pro":0.6,"labels":"CLN"},{"idx":4,"sentence":"We present a word-sense induction method based on pre-trained masked language models (MLMs), which can cheaply scale to large vocabularies and large corpora.","offset":4,"pro":0.8,"labels":"PUR"},{"idx":5,"sentence":"Synthetic translations have been used for a wide range of NLP tasks primarily as a means of data augmentation.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"This work explores, instead, how synthetic translations can be used to revise potentially imperfect reference translations in mined bitext.","offset":1,"pro":0.25,"labels":"PUR"},{"idx":5,"sentence":"We find that synthetic samples can improve bitext quality without any additional bilingual supervision when they replace the originals based on a semantic equivalence classifier that helps mitigate NMT noise.","offset":2,"pro":0.5,"labels":"RST"},{"idx":5,"sentence":"The improved quality of the revised bitext is confirmed intrinsically via human evaluation and extrinsically through bilingual induction and MT tasks.","offset":3,"pro":0.75,"labels":"MTD"},{"idx":6,"sentence":"Recent work has identified properties of pretrained self-attention models that mirror those of dependency parse structures.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"Inspired by these developments, we propose a new competitive mechanism that encourages these attention heads to model different dependency relations.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":6,"sentence":"We introduce a new model, the Unsupervised Dependency Graph Network (UDGN), that can induce dependency structures from raw corpora and the masked language modeling task.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":6,"sentence":"Experiment results show that UDGN achieves very strong unsupervised dependency parsing performance without gold POS tags and any other external information.","offset":3,"pro":0.5,"labels":"RST"},{"idx":6,"sentence":"In particular, some self-attention heads correspond well to individual dependency types.","offset":4,"pro":0.6666666666666666,"labels":"BAC"},{"idx":6,"sentence":"The competitive gated heads show a strong correlation with human-annotated dependency types. ","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":7,"sentence":"Multimodal Entity Linking (MEL) which aims at linking mentions with multimodal contexts to the referent entities from a knowledge base (e.g., Wikipedia), is an essential task for many multimodal applications.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"Although much attention has been paid to MEL, the shortcomings of existing MEL datasets including limited contextual topics and entity types, simplified mention ambiguity, and restricted availability, have caused great obstacles to the research and application of MEL.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":7,"sentence":"In this paper, we present WikiDiverse, a high-quality human-annotated MEL dataset with diversified contextual topics and entity types from Wikinews, which uses Wikipedia as the corresponding knowledge base.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":7,"sentence":"A well-tailored annotation procedure is adopted to ensure the quality of the dataset.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":7,"sentence":"Based on WikiDiverse, a sequence of well-designed MEL models with intra-modality and inter-modality attentions are implemented, which utilize the visual information of images more adequately than existing MEL models do.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":7,"sentence":"Extensive experimental analyses are conducted to investigate the contributions of different modalities in terms of MEL, facilitating the future research on this task.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":8,"sentence":"Knowledge probing is crucial for understanding the knowledge transfer mechanism behind the pre-trained language models (PLMs).","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"To facilitate this, we release a well-curated biomedical knowledge probing benchmark, MedLAMA, constructed based on the Unified Medical Language System (UMLS) Metathesaurus.","offset":1,"pro":0.125,"labels":"PUR"},{"idx":8,"sentence":"We test a wide spectrum of state-of-the-art PLMs and probing approaches on our benchmark, reaching at most 3% of acc@10.","offset":2,"pro":0.25,"labels":"RST"},{"idx":8,"sentence":"While highlighting various sources of domain-specific challenges that amount to this underwhelming performance, we illustrate that the underlying PLMs have a higher potential for probing tasks.","offset":3,"pro":0.375,"labels":"CLN"},{"idx":8,"sentence":"While Contrastive-Probe pushes the acc@10 to 28%, the performance gap still remains notable.","offset":4,"pro":0.5,"labels":"RST"},{"idx":8,"sentence":"Our human expert evaluation suggests that the probing performance of our Contrastive-Probe is still under-estimated as UMLS still does not include the full spectrum of factual knowledge.","offset":5,"pro":0.625,"labels":"CLN"},{"idx":8,"sentence":"We hope MedLAMA and Contrastive-Probe facilitate further developments of more suited probing techniques for this domain.","offset":6,"pro":0.75,"labels":"IMP"},{"idx":8,"sentence":"Our code and dataset are publicly available at https://github.com/cambridgeltl/medlama.","offset":7,"pro":0.875,"labels":"CTN"},{"idx":9,"sentence":"Transformer-based pre-trained models, such as BERT, have shown extraordinary success in achieving state-of-the-art results in many natural language processing applications.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"However, deploying these models can be prohibitively costly, as the standard self-attention mechanism of the Transformer suffers from quadratic computational cost in the input sequence length.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":9,"sentence":"To confront this, we propose FCA, a fine- and coarse-granularity hybrid self-attention that reduces the computation cost through progressively shortening the computational sequence length in self-attention.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":9,"sentence":"Specifically, FCA conducts an attention-based scoring strategy to determine the informativeness of tokens at each layer.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":9,"sentence":"Then, the informative tokens serve as the fine-granularity computing units in self-attention and the uninformative tokens are replaced with one or several clusters as the coarse-granularity computing units in self-attention.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":9,"sentence":"Experiments on the standard GLUE benchmark show that BERT with FCA achieves 2x reduction in FLOPs over original BERT with less1% loss in accuracy.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":9,"sentence":"We show that FCA offers a significantly better trade-off between accuracy and FLOPs compared to prior methods.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":10,"sentence":"The increasing size of generative Pre-trained Language Models (PLMs) have greatly increased the demand for model compression.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"Despite various methods to compress BERT or its variants, there are few attempts to compress generative PLMs, and the underlying difficulty remains unclear.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":10,"sentence":"In this paper, we compress generative PLMs by quantization.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":10,"sentence":"Correspondingly, we propose a token-level contrastive distillation to learn distinguishable word embeddings, and a module-wise dynamic scaling to make quantizers adaptive to different modules.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":10,"sentence":"Empirical results on various tasks show that our proposed method outperforms the state-of-the-art compression methods on generative PLMs by a clear margin.","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":10,"sentence":"With comparable performance with the full-precision models, we achieve 14.4x and 13.4x compression rate on GPT-2 and BART, respectively.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":11,"sentence":"Vision-language navigation (VLN) is a challenging task due to its large searching space in the environment.","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"To address this problem, previous works have proposed some methods of fine-tuning a large model that pretrained on large-scale datasets.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":11,"sentence":"However, the conventional fine-tuning methods require extra human-labeled navigation data and lack self-exploration capabilities in environments, which hinders their generalization of unseen scenes.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":11,"sentence":"To improve the ability of fast cross-domain adaptation, we propose Prompt-based Environmental Self-exploration (ProbES), which can self-explore the environments by sampling trajectories and automatically generates structured instructions via a large-scale cross-modal pretrained model (CLIP).","offset":3,"pro":0.375,"labels":"PUR"},{"idx":11,"sentence":"Our method fully utilizes the knowledge learned from CLIP to build an in-domain dataset by self-exploration without human labeling.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":11,"sentence":"Unlike the conventional approach of fine-tuning, we introduce prompt tuning to achieve fast adaptation for language embeddings, which substantially improves the learning efficiency by leveraging prior knowledge.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":11,"sentence":"By automatically synthesizing trajectory-instruction pairs in any environment without human supervision and instruction prompt tuning, our model can adapt to diverse vision-language navigation tasks, including VLN and REVERIE.","offset":6,"pro":0.75,"labels":"RST"},{"idx":11,"sentence":"Both qualitative and quantitative results show that our ProbES significantly improves the generalization ability of the navigation model.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":12,"sentence":"Dialog response generation in open domain is an important research topic where the main challenge is to generate relevant and diverse responses.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"In this paper, we propose a new dialog pre-training framework called DialogVED, which introduces continuous latent variables into the enhanced encoder-decoder pre-training framework to increase the relevance and diversity of responses.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":12,"sentence":"With the help of a large dialog corpus (Reddit), we pre-train the model using the following 4 tasks, used in training language models (LMs) and Variational Autoencoders (VAEs) literature: 1) masked language model; 2) response generation; 3) bag-of-words prediction; and 4) KL divergence reduction.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":12,"sentence":"We also add additional parameters to model the turn structure in dialogs to improve the performance of the pre-trained model.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":12,"sentence":"We conduct experiments on PersonaChat, DailyDialog, and DSTC7-AVSD benchmarks for response generation.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":12,"sentence":"Experimental results show that our model achieves the new state-of-the-art results on all these datasets.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":13,"sentence":"The problem is equally important with fine-grained response selection, but is less explored in existing literature.","offset":0,"pro":0,"labels":"GAP"},{"idx":13,"sentence":"In this paper, we propose a Contextual Fine-to-Coarse (CFC) distilled model for coarse-grained response selection in open-domain conversations.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":13,"sentence":"In our CFC model, dense representations of query, candidate contexts and responses is learned based on the multi-tower architecture using contextual matching, and richer knowledge learned from the one-tower architecture (fine-grained) is distilled into the multi-tower architecture (coarse-grained) to enhance the performance of the retriever.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":13,"sentence":"To evaluate the performance of the proposed model, we construct two new datasets based on the Reddit comments dump and Twitter corpus.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":13,"sentence":"Extensive experimental results on the two datasets show that the proposed method achieves huge improvement over all evaluation metrics compared with traditional baseline methods.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":14,"sentence":"Summarizing biomedical discovery from genomics data using natural languages is an essential step in biomedical research but is mostly done manually.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"Here, we introduce Textomics, a novel dataset of genomics data description, which contains 22,273 pairs of genomics data matrices and their summaries.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":14,"sentence":"Each summary is written by the researchers who generated the data and associated with a scientific paper.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":14,"sentence":"Based on this dataset, we study two novel tasks: generating textual summary from a genomics data matrix and vice versa.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":14,"sentence":"Inspired by the successful applications of k nearest neighbors in modeling genomics data, we propose a kNN-Vec2Text model to address these tasks and observe substantial improvement on our dataset.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":14,"sentence":"We further illustrate how Textomics can be used to advance other applications, including evaluating scientific paper embeddings and generating masked templates for scientific paper understanding.","offset":5,"pro":0.7142857142857143,"labels":"CTN"},{"idx":14,"sentence":"Textomics serves as the first benchmark for generating textual summaries for genomics data ","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":15,"sentence":"Learning high-quality sentence representations is a fundamental problem of natural language processing which could benefit a wide range of downstream tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"Though the BERT-like pre-trained language models have achieved great success, using their sentence representations directly often results in poor performance on the semantic textual similarity task.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":15,"sentence":"Recently, several contrastive learning methods have been proposed for learning sentence representations and have shown promising results.","offset":2,"pro":0.4,"labels":"BAC"},{"idx":15,"sentence":"However, most of them focus on the constitution of positive and negative representation pairs and pay little attention to the training objective like NT-Xent, which is not sufficient enough to acquire the discriminating power and is unable to model the partial order of semantics between sentences.","offset":3,"pro":0.6,"labels":"GAP"},{"idx":15,"sentence":"So in this paper, we propose a new method ArcCSE, with training objectives designed to enhance the pairwise discriminative power and model the entailment relation of triplet sentences.","offset":4,"pro":0.8,"labels":"PUR"},{"idx":16,"sentence":"Recent entity and relation extraction works focus on investigating how to obtain a better span representation from the pre-trained encoder.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"However, a major limitation of existing works is that they ignore the interrelation between spans (pairs).","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":16,"sentence":"In this work, we propose a novel span representation approach, named Packed Levitated Markers (PL-Marker), to consider the interrelation between the spans (pairs) by strategically packing the markers in the encoder.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":16,"sentence":"In particular, we propose a neighborhood-oriented packing strategy, which considers the neighbor spans integrally to better model the entity boundary information.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":16,"sentence":"Furthermore, for those more complicated span pair classification tasks, we design a subject-oriented packing strategy, which packs each subject and all its objects to model the interrelation between the same-subject span pairs.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":16,"sentence":"The experimental results show that, with the enhanced marker feature, our model advances baselines on six NER benchmarks, and obtains a 4.1%-4.3% strict relation F1 improvement with higher speed over previous state-of-the-art models on ACE04 and ACE05.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":16,"sentence":"Our code and models are publicly available at https://github.com/thunlp/PL-Marker","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":17,"sentence":"We study the interpretability issue of task-oriented dialogue systems in this paper.","offset":0,"pro":0,"labels":"PUR"},{"idx":17,"sentence":"Previously, most neural-based task-oriented dialogue systems employ an implicit reasoning strategy that makes the model predictions uninterpretable to humans.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":17,"sentence":"Since deriving reasoning chains requires multi-hop reasoning for task-oriented dialogues, existing neuro-symbolic approaches would induce error propagation due to the one-phase design.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":17,"sentence":"We first obtain multiple hypotheses, i.e., potential operations to perform the desired task, through the hypothesis generator.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":17,"sentence":"Each hypothesis is then verified by the reasoner, and the valid one is selected to conduct the final prediction.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":17,"sentence":"The whole system is trained by exploiting raw textual dialogues without using any reasoning chain annotations.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":17,"sentence":"Experimental studies on two public benchmark datasets demonstrate that the proposed approach not only achieves better results, but also introduces an interpretable decision process.","offset":6,"pro":0.75,"labels":"CLN"},{"idx":17,"sentence":"To obtain a transparent reasoning process, we introduce neuro-symbolic to perform explicit reasoning that justifies model decisions by reasoning chains.","offset":7,"pro":0.875,"labels":"MTD"},{"idx":18,"sentence":"There has been a growing interest in developing machine learning (ML) models for code summarization tasks, e.g., comment generation and method naming.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"Despite substantial increase in the effectiveness of ML models, the evaluation methodologies, i.e., the way people split datasets into training, validation, and test sets, were not well studied.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":18,"sentence":"Specifically, no prior work on code summarization considered the timestamps of code and comments during evaluation.","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":18,"sentence":"This may lead to evaluations that are inconsistent with the intended use cases.","offset":3,"pro":0.3333333333333333,"labels":"GAP"},{"idx":18,"sentence":"In this paper, we introduce the time-segmented evaluation methodology, which is novel to the code summarization research community, and compare it with the mixed-project and cross-project methodologies that have been commonly used.","offset":4,"pro":0.4444444444444444,"labels":"PUR"},{"idx":18,"sentence":"Each methodology can be mapped to some use cases, and the time-segmented methodology should be adopted in the evaluation of ML models for code summarization.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":18,"sentence":"To assess the impact of methodologies, we collect a dataset of (code, comment) pairs with timestamps to train and evaluate several recent ML models for code summarization.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":18,"sentence":"Our experiments show that different methodologies lead to conflicting evaluation results.","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":18,"sentence":"We invite the community to expand the set of methodologies used in evaluations.","offset":8,"pro":0.8888888888888888,"labels":"PUR"},{"idx":19,"sentence":"Current Open-Domain Question Answering (ODQA) models typically include a retrieving module and a reading module, where the retriever selects potentially relevant passages from open-source documents for a given question, and the reader produces an answer based on the retrieved passages.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"The recently proposed Fusion-in-Decoder (FiD) framework is a representative example, which is built on top of a dense passage retriever and a generative reader, achieving the state-of-the-art performance.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":19,"sentence":"In this paper we further improve the FiD approach by introducing a knowledge-enhanced version, namely KG-FiD.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":19,"sentence":"Our new model uses a knowledge graph to establish the structural relationship among the retrieved passages, and a graph neural network (GNN) to re-rank the passages and select only a top few for further processing.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":19,"sentence":"Our experiments on common ODQA benchmark datasets (Natural Questions and TriviaQA) demonstrate that KG-FiD can achieve comparable or better performance in answer prediction than FiD, with less than 40% of the computation cost.","offset":4,"pro":0.8,"labels":"RST"},{"idx":0,"sentence":"Social media is a breeding ground for threat narratives and related conspiracy theories.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"In these, an outside group threatens the integrity of an inside group, leading to the emergence of sharply defined group identities: Insiders - agents with whom the authors identify and Outsiders - agents who threaten the insiders.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":0,"sentence":"Inferring the members of these groups constitutes a challenging new NLP task: (i) Information is distributed over many poorly-constructed posts; (ii) Threats and threat agents are highly contextual, with the same post potentially having multiple agents assigned to membership in either group; (iii) An agent's identity is often implicit and transitive; and (iv) Phrases used to imply Outsider status often do not follow common negative sentiment patterns.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":0,"sentence":"To address these challenges, we define a novel Insider-Outsider classification task.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":0,"sentence":"Because we are not aware of any appropriate existing datasets or attendant models, we introduce a labeled dataset (CT5K) and design a model (NP2IO) to address this task.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":0,"sentence":"NP2IO leverages pretrained language modeling to classify Insiders and Outsiders.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":0,"sentence":"NP2IO is shown to be robust, generalizing to noun phrases not seen during training, and exceeding the performance of non-trivial baseline models by 20%.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":1,"sentence":"Most low resource language technology development is premised on the need to collect data for training statistical models.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"When we follow the typical process of recording and transcribing text for small Indigenous languages, we hit up against the so-called transcription bottleneck.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":1,"sentence":"Therefore it is worth exploring new ways of engaging with speakers which generate data while avoiding the transcription bottleneck.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":1,"sentence":"We have deployed a prototype app for speakers to use for confirming system guesses in an approach to transcription based on word spotting.","offset":3,"pro":0.42857142857142855,"labels":"GAP"},{"idx":1,"sentence":"However, in the process of testing the app we encountered many new problems for engagement with speakers.","offset":4,"pro":0.5714285714285714,"labels":"GAP"},{"idx":1,"sentence":"This paper presents a close-up study of the process of deploying data capture technology on the ground in an Australian Aboriginal community.","offset":5,"pro":0.7142857142857143,"labels":"PUR"},{"idx":1,"sentence":"We reflect on our interactions with participants and draw lessons that apply to anyone seeking to develop methods for language data collection in an Indigenous community.","offset":6,"pro":0.8571428571428571,"labels":"MTD"},{"idx":2,"sentence":"Multi-hop reading comprehension requires an ability to reason across multiple documents.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"On the one hand, deep learning approaches only implicitly encode query-related information into distributed embeddings which fail to uncover the discrete relational reasoning process to infer the correct answer.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":2,"sentence":"On the other hand, logic-based approaches provide interpretable rules to infer the target answer, but mostly work on structured data where entities and relations are well-defined.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":2,"sentence":"In this paper, we propose a deep-learning based inductive logic reasoning method that firstly extracts query-related (candidate-related) information, and then conducts logic reasoning among the filtered information by inducing feasible rules that entail the target relation.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":2,"sentence":"The reasoning process is accomplished via attentive memories with novel differentiable logic operators.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":2,"sentence":"To demonstrate the effectiveness of our model, we evaluate it on two reading comprehension datasets, namely WikiHop and MedHop.","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":3,"sentence":"This paper addresses the problem of dialogue reasoning with contextualized commonsense inference.","offset":0,"pro":0,"labels":"PUR"},{"idx":3,"sentence":"We curate CICERO, a dataset of dyadic conversations with five types of utterance-level reasoning-based inferences: cause, subsequent event, prerequisite, motivation, and emotional reaction.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":3,"sentence":"The dataset contains 53,105 of such inferences from 5,672 dialogues.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":3,"sentence":"We use this dataset to solve relevant generative and discriminative tasks: generation of cause and subsequent event; generation of prerequisite, motivation, and listener's emotional reaction; and selection of plausible alternatives.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":3,"sentence":"Our results ascertain the value of such dialogue-centric commonsense knowledge datasets.","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":3,"sentence":"It is our hope that CICERO will open new research avenues into commonsense-based dialogue reasoning.","offset":5,"pro":0.8333333333333334,"labels":"CTN"},{"idx":4,"sentence":"Interpretable methods to reveal the internal reasoning processes behind machine learning models have attracted increasing attention in recent years.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"To quantify the extent to which the identified interpretations truly reflect the intrinsic decision-making mechanisms, various faithfulness evaluation metrics have been proposed.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":4,"sentence":"However, we find that different faithfulness metrics show conflicting preferences when comparing different interpretations.","offset":2,"pro":0.2857142857142857,"labels":"RST"},{"idx":4,"sentence":"Motivated by this observation, we aim to conduct a comprehensive and comparative study of the widely adopted faithfulness metrics.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":4,"sentence":"In particular, we introduce two assessment dimensions, namely diagnosticity and complexity.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":4,"sentence":"Diagnosticity refers to the degree to which the faithfulness metric favors relatively faithful interpretations over randomly generated ones, and complexity is measured by the average number of model forward passes.","offset":5,"pro":0.7142857142857143,"labels":"BAC"},{"idx":4,"sentence":"According to the experimental results, we find that sufficiency and comprehensiveness metrics have higher diagnosticity and lower complexity than the other faithfulness metrics.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":5,"sentence":"There has been growing interest in parameter-efficient methods to apply pre-trained language models to downstream tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"Building on the Prompt Tuning approach of Lester et al. (2021), which learns task-specific soft prompts to condition a frozen pre-trained model to perform different tasks, we propose a novel prompt-based transfer learning approach called SPoT: Soft Prompt Transfer.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":5,"sentence":"SPoT first learns a prompt on one or more source tasks and then uses it to initialize the prompt for a target task.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":5,"sentence":"We show that SPoT significantly boosts the performance of Prompt Tuning across many tasks.","offset":3,"pro":0.42857142857142855,"labels":"RST"},{"idx":5,"sentence":"More remarkably, across all model sizes, SPoT matches or outperforms standard Model Tuning (which fine-tunes all model parameters) on the SuperGLUE benchmark, while using up to 27,000{mbox{times fewer task-specific parameters.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":5,"sentence":"To understand where SPoT is most effective, we conduct a large-scale study on task transferability with 26 NLP tasks in 160 combinations, and demonstrate that many tasks can benefit each other via prompt transfer.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":5,"sentence":"Finally, we propose an efficient retrieval approach that interprets task prompts as task embeddings to identify similar tasks and predict the most transferable source tasks for a novel target task.","offset":6,"pro":0.8571428571428571,"labels":"MTD"},{"idx":6,"sentence":"Selecting an appropriate pre-trained model (PTM) for a specific downstream task typically requires significant efforts of fine-tuning.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"To accelerate this process, researchers propose feature-based model selection (FMS) methods, which assess PTMs' transferability to a specific task in a fast way without fine-tuning.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":6,"sentence":"In this work, we argue that current FMS methods are vulnerable, as the assessment mainly relies on the static features extracted from PTMs.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":6,"sentence":"However, such features are derived without training PTMs on downstream tasks, and are not necessarily reliable indicators for the PTM's transferability.","offset":3,"pro":0.375,"labels":"GAP"},{"idx":6,"sentence":"To validate our viewpoints, we design two methods to evaluate the robustness of FMS: (1) model disguise attack, which post-trains an inferior PTM with a contrastive objective, and (2) evaluation data selection, which selects a subset of the data points for FMS evaluation based on K-means clustering.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":6,"sentence":"Experimental results prove that both methods can successfully make FMS mistakenly judge the transferability of PTMs.","offset":5,"pro":0.625,"labels":"CLN"},{"idx":6,"sentence":"To the best of our knowledge, this is the first work to demonstrate the defects of current FMS algorithms and evaluate their potential security risks.","offset":6,"pro":0.75,"labels":"CTN"},{"idx":6,"sentence":"By identifying previously unseen risks of FMS, our study indicates new directions for improving the robustness of FMS.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":7,"sentence":"Generating educational questions of fairytales or storybooks is vital for improving children's literacy ability.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"However, it is challenging to generate questions that capture the interesting aspects of a fairytale story with educational meaningfulness.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":7,"sentence":"In this paper, we propose a novel question generation method that first learns the question type distribution of an input story paragraph, and then summarizes salient events which can be used to generate high-cognitive-demand questions.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":7,"sentence":"To train the event-centric summarizer, we finetune a pre-trained transformer-based sequence-to-sequence model using silver samples composed by educational question-answer pairs.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":7,"sentence":"On a newly proposed educational question-answering dataset FairytaleQA, we show good performance of our method on both automatic and human evaluation metrics.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":7,"sentence":"Our work indicates the necessity of decomposing question type distribution learning and event-centric summary generation for educational question generation.","offset":5,"pro":0.8333333333333334,"labels":"CTN"},{"idx":8,"sentence":"Recently, various response generation models for two-party conversations have achieved impressive improvements, but less effort has been paid to multi-party conversations (MPCs) which are more practical and complicated.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"Compared with a two-party conversation where a dialogue context is a sequence of utterances, building a response generation model for MPCs is more challenging, since there exist complicated context structures and the generated responses heavily rely on both interlocutors (i.e., speaker and addressee) and history utterances.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":8,"sentence":"To address these challenges, we present HeterMPC, a heterogeneous graph-based neural network for response generation in MPCs which models the semantics of utterances and interlocutors simultaneously with two types of nodes in a graph.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":8,"sentence":"Besides, we also design six types of meta relations with node-edge-type-dependent parameters to characterize the heterogeneous interactions within the graph.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":8,"sentence":"Through multi-hop updating, HeterMPC can adequately utilize the structural knowledge of conversations for response generation.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":8,"sentence":"Experimental results on the Ubuntu Internet Relay Chat (IRC) channel benchmark show that HeterMPC outperforms various baseline models for response generation in MPCs.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":9,"sentence":"Although multi-document summarisation (MDS) of the biomedical literature is a highly valuable task that has recently attracted substantial interest, evaluation of the quality of biomedical summaries lacks consistency and transparency.","offset":0,"pro":0,"labels":"GAP"},{"idx":9,"sentence":"In this paper, we examine the summaries generated by two current models in order to understand the deficiencies of existing evaluation approaches in the context of the challenges that arise in the MDS task.","offset":1,"pro":0.3333333333333333,"labels":"PUR"},{"idx":9,"sentence":"Based on this analysis, we propose a new approach to human evaluation and identify several challenges that must be overcome to develop effective biomedical MDS systems.","offset":2,"pro":0.6666666666666666,"labels":"MTD"},{"idx":10,"sentence":"Multi-document summarization (MDS) has made significant progress in recent years, in part facilitated by the availability of new, dedicated datasets and capacious language models.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"However, a standing limitation of these models is that they are trained against limited references and with plain maximum-likelihood objectives.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":10,"sentence":"As for many other generative tasks, reinforcement learning (RL) offers the potential to improve the training of MDS models; yet, it requires a carefully-designed reward that can ensure appropriate leverage of both the reference summaries and the input documents.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":10,"sentence":"For this reason, in this paper we propose fine-tuning an MDS baseline with a reward that balances a reference-based metric such as ROUGE with coverage of the input documents.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":10,"sentence":"To implement the approach, we utilize RELAX (Grathwohl et al., 2018), a contemporary gradient estimator which is both low-variance and unbiased, and we fine-tune the baseline in a few-shot style for both stability and computational efficiency.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":10,"sentence":"Experimental results over the Multi-News and WCEP MDS datasets show significant improvements of up to +0.95 pp average ROUGE score and +3.17 pp METEOR score over the baseline, and competitive results with the literature.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":10,"sentence":"In addition, they show that the coverage of the input documents is increased, and evenly across all documents.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":11,"sentence":"The Out-of-Domain (OOD) intent classification is a basic and challenging task for dialogue systems.","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"Previous methods commonly restrict the region (in feature space) of In-domain (IND) intent features to be compact or simply-connected implicitly, which assumes no OOD intents reside, to learn discriminative semantic features.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":11,"sentence":"Then the distribution of the IND intent features is often assumed to obey a hypothetical distribution (Gaussian mostly) and samples outside this distribution are regarded as OOD samples.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":11,"sentence":"In this paper, we start from the nature of OOD intent classification and explore its optimization objective.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":11,"sentence":"We further propose a simple yet effective method, named KNN-contrastive learning.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":11,"sentence":"Our approach utilizes k-nearest neighbors (KNN) of IND intents to learn discriminative semantic features that are more conducive to OOD detection.Notably, the density-based novelty detection algorithm is so well-grounded in the essence of our method that it is reasonable to use it as the OOD detection algorithm without making any requirements for the feature distribution.Extensive experiments on four public datasets show that our approach can not only enhance the OOD detection performance substantially but also improve the IND intent classification while requiring no restrictions on feature distribution.","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":12,"sentence":"Program understanding is a fundamental task in program language processing.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"Despite the success, existing works fail to take human behaviors as reference in understanding programs.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":12,"sentence":"In this paper, we consider human behaviors and propose the PGNN-EK model that consists of two main components.","offset":2,"pro":0.2222222222222222,"labels":"PUR"},{"idx":12,"sentence":"On the one hand, inspired by the divide-and-conquer reading behaviors of humans, we present a partitioning-based graph neural network model PGNN on the upgraded AST of codes.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":12,"sentence":"On the other hand, to characterize human behaviors of resorting to other resources to help code comprehension, we transform raw codes with external knowledge and apply pre-training techniques for information extraction.","offset":4,"pro":0.4444444444444444,"labels":"PUR"},{"idx":12,"sentence":"Finally, we combine the two embeddings generated from the two components to output code embeddings.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":12,"sentence":"We conduct extensive experiments to show the superior performance of PGNN-EK on the code summarization and code clone detection tasks.","offset":6,"pro":0.6666666666666666,"labels":"RST"},{"idx":12,"sentence":"In particular, to show the generalization ability of our model, we release a new dataset that is more challenging for code clone detection and could advance the development of the community.","offset":7,"pro":0.7777777777777778,"labels":"PUR"},{"idx":12,"sentence":"Our codes and data are publicly available at https://github.com/RecklessRonan/PGNN-EK.","offset":8,"pro":0.8888888888888888,"labels":"CTN"},{"idx":13,"sentence":"Despite significant interest in developing general purpose fact checking models, it is challenging to construct a large-scale fact verification dataset with realistic real-world claims.","offset":0,"pro":0,"labels":"GAP"},{"idx":13,"sentence":"Existing claims are either authored by crowdworkers, thereby introducing subtle biases thatare difficult to control for, or manually verified by professional fact checkers, causing them to be expensive and limited in scale.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":13,"sentence":"In this paper, we construct a large-scale challenging fact verification dataset called FAVIQ, consisting of 188k claims derived from an existing corpus of ambiguous information-seeking questions.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":13,"sentence":"The ambiguities in the questions enable automatically constructing true and false claims that reflect user confusions (e.g., the year of the movie being filmed vs. being released).","offset":3,"pro":0.375,"labels":"MTD"},{"idx":13,"sentence":"Claims in FAVIQ are verified to be natural, contain little lexical bias, and require a complete understanding of the evidence for verification.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":13,"sentence":"Our experiments show that the state-of-the-art models are far from solving our new task.","offset":5,"pro":0.625,"labels":"RST"},{"idx":13,"sentence":"Moreover, training on our data helps in professional fact-checking, outperforming models trained on the widely used dataset FEVER or in-domain data by up to 17% absolute.","offset":6,"pro":0.75,"labels":"RST"},{"idx":13,"sentence":"Altogether, our data will serve as a challenging benchmark for natural language understanding and support future progress in professional fact checking.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":14,"sentence":"We study learning from user feedback for extractive question answering by simulating feedback using supervised data.","offset":0,"pro":0,"labels":"MTD"},{"idx":14,"sentence":"We cast the problem as contextual bandit learning, and analyze the characteristics of several learning scenarios with focus on reducing data annotation.","offset":1,"pro":0.3333333333333333,"labels":"MTD"},{"idx":14,"sentence":"We show that systems initially trained on few examples can dramatically improve given feedback from users on model-predicted answers, and that one can use existing datasets to deploy systems in new domains without any annotation effort, but instead improving the system on-the-fly via user feedback.","offset":2,"pro":0.6666666666666666,"labels":"RST"},{"idx":15,"sentence":"Despite recent improvements in open-domain dialogue models, state of the art models are trained and evaluated on short conversations with little context.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"In contrast, the long-term conversation setting has hardly been studied.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":15,"sentence":"In this work we collect and release a human-human dataset consisting of multiple chat sessions whereby the speaking partners learn about each other's interests and discuss the things they have learnt from past sessions.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":15,"sentence":"We show how existing models trained on existing datasets perform poorly in this long-term conversation setting in both automatic and human evaluations, and we study long-context models that can perform much better.","offset":3,"pro":0.6,"labels":"RST"},{"idx":15,"sentence":"In particular, we find retrieval-augmented methods and methods with an ability to summarize and recall previous conversations outperform the standard encoder-decoder architectures currently considered state of the art.","offset":4,"pro":0.8,"labels":"RST"},{"idx":16,"sentence":"Training a referring expression comprehension (ReC) model for a new visual domain requires collecting referring expressions, and potentially corresponding bounding boxes, for images in the domain.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"While large-scale pre-trained models are useful for image classification across domains, it remains unclear if they can be applied in a zero-shot manner to more complex tasks like ReC.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":16,"sentence":"We present ReCLIP, a simple but strong zero-shot baseline that repurposes CLIP, a state-of-the-art large-scale model, for ReC.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":16,"sentence":"Motivated by the close connection between ReC and CLIP's contrastive pre-training objective, the first component of ReCLIP is a region-scoring method that isolates object proposals via cropping and blurring, and passes them to CLIP.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":16,"sentence":"However, through controlled experiments on a synthetic dataset, we find that CLIP is largely incapable of performing spatial reasoning off-the-shelf.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":16,"sentence":"We reduce the gap between zero-shot baselines from prior work and supervised models by as much as 29% on RefCOCOg, and on RefGTA (video game imagery), ReCLIP's relative improvement over supervised ReC models trained on real images is 8%.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":17,"sentence":"We consider event extraction in a generative manner with template-based conditional generation.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"Although there is a rising trend of casting the task of event extraction as a sequence generation problem with prompts, these generation-based methods have two significant challenges, including using suboptimal prompts and static event type information.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":17,"sentence":"In this paper, we propose a generative template-based event extraction method with dynamic prefix (GTEE-DynPref) by integrating context information with type-specific prefixes to learn a context-specific prefix for each context.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":17,"sentence":"Experimental results show that our model achieves competitive results with the state-of-the-art classification-based model OneIE on ACE 2005 and achieves the best performances on ERE.","offset":3,"pro":0.6,"labels":"CLN"},{"idx":17,"sentence":"Additionally, our model is proven to be portable to new types of events effectively.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":18,"sentence":"Building huge and highly capable language models has been a trend in the past years.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"Despite their great performance, they incur high computational cost.","offset":1,"pro":0.08333333333333333,"labels":"GAP"},{"idx":18,"sentence":"A common solution is to apply model compression or choose light-weight architectures, which often need a separate fixed-size model for each desirable computational budget, and may lose performance in case of heavy compression.","offset":2,"pro":0.16666666666666666,"labels":"GAP"},{"idx":18,"sentence":"This paper proposes an effective dynamic inference approach, called E-LANG, which distributes the inference between large accurate Super-models and light-weight Swift models.","offset":3,"pro":0.25,"labels":"PUR"},{"idx":18,"sentence":"To this end, a decision making module routes the inputs to Super or Swift models based on the energy characteristics of the representations in the latent space.","offset":4,"pro":0.3333333333333333,"labels":"MTD"},{"idx":18,"sentence":"This method is easily adoptable and architecture agnostic.","offset":5,"pro":0.4166666666666667,"labels":"MTD"},{"idx":18,"sentence":"As such, it can be applied to black-box pre-trained models without a need for architectural manipulations, reassembling of modules, or re-training.","offset":6,"pro":0.5,"labels":"MTD"},{"idx":18,"sentence":"Unlike existing methods that are only applicable to encoder-only backbones and classification tasks, our method also works for encoder-decoder structures and sequence-to-sequence tasks such as translation.","offset":7,"pro":0.5833333333333334,"labels":"MTD"},{"idx":18,"sentence":"The E-LANG performance is verified through a set of experiments with T5 and BERT backbones on GLUE, SuperGLUE, and WMT.","offset":8,"pro":0.6666666666666666,"labels":"MTD"},{"idx":18,"sentence":"In particular, we outperform T5-11B with an average computations speed-up of 3.3X on GLUE and 2.9X on SuperGLUE.","offset":9,"pro":0.75,"labels":"RST"},{"idx":18,"sentence":"We also achieve BERT-based SOTA on GLUE with 3.2X less computations.","offset":10,"pro":0.8333333333333334,"labels":"RST"},{"idx":18,"sentence":"Code and demo are available in supplementary materials.","offset":11,"pro":0.9166666666666666,"labels":"CTN"},{"idx":19,"sentence":"We introduce PRIMERA, a pre-trained model for multi-document representation with a focus on summarization that reduces the need for dataset-specific architectures and large amounts of fine-tuning labeled data.","offset":0,"pro":0,"labels":"PUR"},{"idx":19,"sentence":"PRIMERA uses our newly proposed pre-training objective designed to teach the model to connect and aggregate information across documents.","offset":1,"pro":0.25,"labels":"MTD"},{"idx":19,"sentence":"It also uses efficient encoder-decoder transformers to simplify the processing of concatenated input documents.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":19,"sentence":"With extensive experiments on 6 multi-document summarization datasets from 3 different domains on zero-shot, few-shot and full-supervised settings, PRIMERA outperforms current state-of-the-art dataset-specific and pre-trained models on most of these settings with large margins.","offset":3,"pro":0.75,"labels":"RST"},{"idx":0,"sentence":"Extracting informative arguments of events from news articles is a challenging problem in information extraction, which requires a global contextual understanding of each document.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"While recent work on document-level extraction has gone beyond single-sentence and increased the cross-sentence inference capability of end-to-end models, they are still restricted by certain input sequence length constraints and usually ignore the global context between events.","offset":1,"pro":0.25,"labels":"GAP"},{"idx":0,"sentence":"To tackle this issue, we introduce a new global neural generation-based framework for document-level event argument extraction by constructing a document memory store to record the contextual event information and leveraging it to implicitly and explicitly help with decoding of arguments for later events.","offset":2,"pro":0.5,"labels":"PUR"},{"idx":0,"sentence":"Empirical results show that our framework outperforms prior methods substantially and it is more robust to adversarially annotated examples with our constrained decoding design.","offset":3,"pro":0.75,"labels":"CLN"},{"idx":1,"sentence":"There is a growing interest in the combined use of NLP and machine learning methods to predict gaze patterns during naturalistic reading.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"While promising results have been obtained through the use of transformer-based language models, little work has been undertaken to relate the performance of such models to general text characteristics.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":1,"sentence":"In this paper we report on experiments with two eye-tracking corpora of naturalistic reading and two language models (BERT and GPT-2).","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":1,"sentence":"In all experiments, we test effects of a broad spectrum of features for predicting human reading behavior that fall into five categories (syntactic complexity, lexical richness, register-based multiword combinations, readability and psycholinguistic word properties).","offset":3,"pro":0.5,"labels":"MTD"},{"idx":1,"sentence":"Our experiments show that both the features included and the architecture of the transformer-based language models play a role in predicting multiple eye-tracking measures during naturalistic reading.","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":1,"sentence":"We also report the results of experiments aimed at determining the relative importance of features from different groups using SP-LIME.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":2,"sentence":"Recent work in multilingual machine translation (MMT) has focused on the potential of positive transfer between languages, particularly cases where higher-resourced languages can benefit lower-resourced ones.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"While training an MMT model, the supervision signals learned from one language pair can be transferred to the other via the tokens shared by multiple source languages.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":2,"sentence":"However, the transfer is inhibited when the token overlap among source languages is small, which manifests naturally when languages use different writing systems.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":2,"sentence":"In this paper, we tackle inhibited transfer by augmenting the training data with alternative signals that unify different writing systems, such as phonetic, romanized, and transliterated input.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":2,"sentence":"We test these signals on Indic and Turkic languages, two language families where the writing systems differ but languages still share common features.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":2,"sentence":"Our results indicate that a straightforward multi-source self-ensemble - training a model on a mixture of various signals and ensembling the outputs of the same model fed with different signals during inference, outperforms strong ensemble baselines by 1.3 BLEU points on both language families.","offset":5,"pro":0.625,"labels":"RST"},{"idx":2,"sentence":"Further, we find that incorporating alternative inputs via self-ensemble can be particularly effective when training set is small, leading to +5 BLEU when only 5% of the total training data is accessible.","offset":6,"pro":0.75,"labels":"RST"},{"idx":2,"sentence":"Finally, our analysis demonstrates that including alternative signals yields more consistency and translates named entities more accurately, which is crucial for increased factuality of automated systems.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":3,"sentence":"Multi-modal techniques offer significant untapped potential to unlock improved NLP technology for local languages.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"However, many advances in language model pre-training are focused on text, a fact that only increases systematic inequalities in the performance of NLP tasks across the world's languages.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":3,"sentence":"In this work, we propose a multi-modal approach to train language models using whatever text and/or audio data might be available in a language.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":3,"sentence":"Preprocessing and training code will be uploaded to https://github.com/sil-ai/phone-it-in.","offset":3,"pro":0.5,"labels":"CTN"},{"idx":3,"sentence":"Initial experiments using Swahili and Kinyarwanda data suggest the viability of the approach for downstream Named Entity Recognition (NER) tasks,","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":3,"sentence":"with models pre-trained on phone data showing an improvement of up to 6% F1-score above models that are trained from scrat","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":4,"sentence":"We introduce a noisy channel approach for language model prompting in few-shot text classification.","offset":0,"pro":0,"labels":"MTD"},{"idx":4,"sentence":"Instead of computing the likelihood of the label given the input (referred as direct models), channel models compute the conditional probability of the input given the label, and are thereby required to explain every word in the input.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":4,"sentence":"We use channel models for recently proposed few-shot learning methods with no or very limited updates to the language model parameters, via either in-context demonstration or prompt tuning.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":4,"sentence":"Our experiments show that, for both methods, channel models significantly outperform their direct counterparts, which we attribute to their stability, i.e., lower variance and higher worst-case accuracy.","offset":3,"pro":0.6,"labels":"RST"},{"idx":4,"sentence":"We also present extensive ablations that provide recommendations for when to use channel prompt tuning instead of other competitive models (e.g., direct head tuning): channel prompt tuning is preferred when the number of training examples is small, labels in the training data are imbalanced, or generalization to unseen labels is required.","offset":4,"pro":0.8,"labels":"CTN"},{"idx":5,"sentence":"Further, we show that this transfer can be achieved by training over a collection of low-resource languages that are typologically similar (but phylogenetically unrelated) to the target language.","offset":0,"pro":0,"labels":"CLN"},{"idx":5,"sentence":"In our experiments, we transfer from a collection of 10 Indigenous American languages (AmericasNLP, Mager et al., 2021) to K'iche', a Mayan language.","offset":1,"pro":0.14285714285714285,"labels":"MTD"},{"idx":5,"sentence":"We compare our multilingual model to a monolingual (from-scratch) baseline, as well as a model pre-trained on Quechua only.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":5,"sentence":"We show that the multilingual pre-trained approach yields consistent segmentation quality across target dataset sizes, exceeding the monolingual baseline in 6/10 experimental settings.","offset":3,"pro":0.42857142857142855,"labels":"RST"},{"idx":5,"sentence":"Our model yields especially strong results at small target sizes, including a zero-shot performance of 20.6 F1.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":5,"sentence":"These results have promising implications for low-resource NLP pipelines involving human-like linguistic units, such as the sparse transcription framework proposed by Bird (2020).","offset":5,"pro":0.7142857142857143,"labels":"IMP"},{"idx":5,"sentence":"We show that unsupervised sequence-segmentation performance can be transferred to extremely low-resource languages ","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":6,"sentence":"Pre-trained language models such as BERT have been successful at tackling many natural language processing tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"However, the unsupervised sub-word tokenization methods commonly used in these models (e.g., byte-pair encoding - BPE) are sub-optimal at handling morphologically rich languages.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":6,"sentence":"Even given a morphological analyzer, naive sequencing of morphemes into a standard BERT architecture is inefficient at capturing morphological compositionality and expressing word-relative syntactic regularities.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":6,"sentence":"We evaluate our proposed method on the low-resource morphologically rich Kinyarwanda language, naming the proposed model architecture KinyaBERT.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":6,"sentence":"A robust set of experimental results reveal that KinyaBERT outperforms solid baselines by 2% in F1 score on a named entity recognition task and by 4.3% in average score of a machine-translated GLUE benchmark.","offset":4,"pro":0.5,"labels":"RST"},{"idx":6,"sentence":"KinyaBERT fine-tuning has better convergence and achieves more robust results on multiple tasks even in the presence of translation noise.","offset":5,"pro":0.625,"labels":"CLN"},{"idx":6,"sentence":"We address these challenges by proposing a simple yet effective two-tier BERT architecture that leverages a morphological analyzer and explicitly represents morphological compositionalit","offset":6,"pro":0.75,"labels":"MTD"},{"idx":6,"sentence":"Despite the success of BERT, most of its evaluations have been conducted on high-resource languages, obscuring its applicability on low-resource language","offset":7,"pro":0.875,"labels":"GAP"},{"idx":7,"sentence":"A well-calibrated neural model produces confidence (probability outputs) closely approximated by the expected accuracy.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"While prior studies have shown that mixup training as a data augmentation technique can improve model calibration on image classification tasks, little is known about using mixup for model calibration on natural language understanding (NLU) tasks.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":7,"sentence":"In this paper, we explore mixup for model calibration on several NLU tasks and propose a novel mixup strategy for pre-trained language models that improves model calibration further.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":7,"sentence":"Our proposed mixup is guided by both the Area Under the Margin (AUM) statistic (Pleiss et al., 2020) and the saliency map of each sample (Simonyan et al., 2013).","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":7,"sentence":"Moreover, we combine our mixup strategy with model miscalibration correction techniques (i.e., label smoothing and temperature scaling) and provide detailed analyses of their impact on our proposed mixup.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":7,"sentence":"We focus on systematically designing experiments on three NLU tasks: natural language inference, paraphrase detection, and commonsense reasoning.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":7,"sentence":"Our method achieves the lowest expected calibration error compared to strong baselines on both in-domain and out-of-domain test samples while maintaining competitive accuracy.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":8,"sentence":"Natural language inference (NLI) has been widely used as a task to train and evaluate models for language understanding.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"However, the ability of NLI models to perform inferences requiring understanding of figurative language such as idioms and metaphors remains understudied.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":8,"sentence":"We introduce the IMPLI (Idiomatic and Metaphoric Paired Language Inference) dataset, an English dataset consisting of paired sentences spanning idioms and metaphors.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":8,"sentence":"We develop novel methods to generate 24k semiautomatic pairs as well as manually creating 1.8k gold pairs.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":8,"sentence":"We use IMPLI to evaluate NLI models based on RoBERTa fine-tuned on the widely used MNLI dataset.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":8,"sentence":"We then show that while they can reliably detect entailment relationship between figurative phrases with their literal counterparts, they perform poorly on similarly structured examples where pairs are designed to be non-entailing.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":8,"sentence":"This suggests the limits of current NLI models with regard to understanding figurative language and this dataset serves as a benchmark for future improvements in this direction.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":9,"sentence":"This paper introduces QAConv, a new question answering (QA) dataset that uses conversations as a knowledge source.","offset":0,"pro":0,"labels":"PUR"},{"idx":9,"sentence":"We focus on informative conversations, including business emails, panel discussions, and work channels.","offset":1,"pro":0.125,"labels":"MTD"},{"idx":9,"sentence":"Unlike open-domain and task-oriented dialogues, these conversations are usually long, complex, asynchronous, and involve strong domain knowledge.","offset":2,"pro":0.25,"labels":"BAC"},{"idx":9,"sentence":"In total, we collect 34,608 QA pairs from 10,259 selected conversations with both human-written and machine-generated questions.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":9,"sentence":"We use a question generator and a dialogue summarizer as auxiliary tools to collect and recommend questions.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":9,"sentence":"The dataset has two testing scenarios: chunk mode and full mode, depending on whether the grounded partial conversation is provided or retrieved.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":9,"sentence":"Experimental results show that state-of-the-art pretrained QA systems have limited zero-shot performance and tend to predict our questions as unanswerable.","offset":6,"pro":0.75,"labels":"RST"},{"idx":9,"sentence":"Our dataset provides a new training and evaluation testbed to facilitate QA on conversations research.","offset":7,"pro":0.875,"labels":"CTN"},{"idx":10,"sentence":"Knowledge bases (KBs) contain plenty of structured world and commonsense knowledge.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"As such, they often complement distributional text-based information and facilitate various downstream tasks.","offset":1,"pro":0.09090909090909091,"labels":"BAC"},{"idx":10,"sentence":"Since their manual construction is resource- and time-intensive, recent efforts have tried leveraging large pretrained language models (PLMs) to generate additional monolingual knowledge facts for KBs.","offset":2,"pro":0.18181818181818182,"labels":"BAC"},{"idx":10,"sentence":"However, such methods have not been attempted for building and enriching multilingual KBs.","offset":3,"pro":0.2727272727272727,"labels":"GAP"},{"idx":10,"sentence":"Besides wider application, such multilingual KBs can provide richer combined knowledge than monolingual (e.g., English) KBs.","offset":4,"pro":0.36363636363636365,"labels":"BAC"},{"idx":10,"sentence":"Knowledge expressed in different languages may be complementary and unequally distributed: this implies that the knowledge available in high-resource languages can be transferred to low-resource ones.","offset":5,"pro":0.45454545454545453,"labels":"BAC"},{"idx":10,"sentence":"To achieve this, it is crucial to represent multilingual knowledge in a shared/unified space.","offset":6,"pro":0.5454545454545454,"labels":"MTD"},{"idx":10,"sentence":"To this end, we propose a unified representation model, Prix-LM, for multilingual KB construction and completion.","offset":7,"pro":0.6363636363636364,"labels":"PUR"},{"idx":10,"sentence":"We leverage two types of knowledge, monolingual triples and cross-lingual links, extracted from existing multilingual KBs, and tune a multilingual language encoder XLM-R via a causal language modeling objective.","offset":8,"pro":0.7272727272727273,"labels":"MTD"},{"idx":10,"sentence":"Prix-LM integrates useful multilingual and KB-based factual knowledge into a single model.","offset":9,"pro":0.8181818181818182,"labels":"MTD"},{"idx":10,"sentence":"Experiments on standard entity-related tasks, such as link prediction in multiple languages, cross-lingual entity linking and bilingual lexicon induction, demonstrate its effectiveness, with gains reported over strong task-specialised baselines.","offset":10,"pro":0.9090909090909091,"labels":"CLN"},{"idx":11,"sentence":"We introduce a data-driven approach to generating derivation trees from meaning representation graphs with probabilistic synchronous hyperedge replacement grammar (PSHRG).","offset":0,"pro":0,"labels":"PUR"},{"idx":11,"sentence":"SHRG has been used to produce meaning representation graphs from texts and syntax trees, but little is known about its viability on the reverse.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":11,"sentence":"In particular, we experiment on Dependency Minimal Recursion Semantics (DMRS) and adapt PSHRG as a formalism that approximates the semantic composition of DMRS graphs and simultaneously recovers the derivations that license the DMRS graphs.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":11,"sentence":"Consistent results are obtained as evaluated on a collection of annotated corpora.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":11,"sentence":"This work reveals the ability of PSHRG in formalizing a syntax-semantics interface, modelling compositional graph-to-tree translations, and channelling explainability to surface realization.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":12,"sentence":"AI systems embodied in the physical world face a fundamental challenge of partial observability; operating with only a limited view and knowledge of the environment.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"This creates challenges when AI systems try to reason about language and its relationship with the environment: objects referred to through language (e.g. giving many instructions) are not immediately visible.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":12,"sentence":"Actions by the AI system may be required to bring these objects in view.","offset":2,"pro":0.2222222222222222,"labels":"MTD"},{"idx":12,"sentence":"A good benchmark to study this challenge is Dynamic Referring Expression Recognition (dRER) task, where the goal is to find a target location by dynamically adjusting the field of view (FoV) in a partially observed 360 scenes.","offset":3,"pro":0.3333333333333333,"labels":"BAC"},{"idx":12,"sentence":"In this paper, we introduce HOLM, Hallucinating Objects with Language Models, to address the challenge of partial observability.","offset":4,"pro":0.4444444444444444,"labels":"PUR"},{"idx":12,"sentence":"HOLM uses large pre-trained language models (LMs) to infer object hallucinations for the unobserved part of the environment.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":12,"sentence":"Our core intuition is that if a pair of objects co-appear in an environment frequently, our usage of language should reflect this fact about the world.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":12,"sentence":"Based on this intuition, we prompt language models to extract knowledge about object affinities which gives us a proxy for spatial relationships of objects.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":12,"sentence":"Our experiments show that HOLM performs better than the state-of-the-art approaches on two datasets for dRER; allowing to study generalization for both indoor and outdoor settings.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":13,"sentence":"Massively Multilingual Transformer based Language Models have been observed to be surprisingly effective on zero-shot transfer across languages, though the performance varies from language to language depending on the pivot language(s) used for fine-tuning.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"We jointly train predictive models for different tasks which helps us build more accurate predictors for tasks where we have test data in very few languages to measure the actual performance of the model.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":13,"sentence":"Our approach also lends us the ability to perform a much more robust feature selection, and identify a common set of features that influence zero-shot performance across a variety of tasks.","offset":2,"pro":0.4,"labels":"CTN"},{"idx":13,"sentence":"In this work, we build upon some of the existing techniques for predicting the zero-shot performance on a task","offset":3,"pro":0.6,"labels":"PUR"},{"idx":13,"sentence":"y modeling it as a multi-task learning probl","offset":4,"pro":0.8,"labels":"MTD"},{"idx":14,"sentence":"Transformers are unable to model long-term memories effectively, since the amount of computation they need to perform grows with the context length.","offset":0,"pro":0,"labels":"GAP"},{"idx":14,"sentence":"While variations of efficient transformers have been proposed, they all have a finite memory capacity and are forced to drop old information.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":14,"sentence":"In this paper, we propose the infty-former, which extends the vanilla transformer with an unbounded long-term memory.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":14,"sentence":"By making use of a continuous-space attention mechanism to attend over the long-term memo","offset":3,"pro":0.375,"labels":"MTD"},{"idx":14,"sentence":"the infty-former's attention complexity becomes independent of the context length, trading off memory length with precisio","offset":4,"pro":0.5,"labels":"RST"},{"idx":14,"sentence":"n order to control where precision is more importan","offset":5,"pro":0.625,"labels":"PUR"},{"idx":14,"sentence":"fty-former maintains sticky memories, being able to model arbitrarily long contexts while keeping the computation budget fixe","offset":6,"pro":0.75,"labels":"MTD"},{"idx":14,"sentence":"Experiments on a synthetic sorting task, language modeling, and document grounded dialogue generation demonstrate the infty-former's ability to retain information from long sequences.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":15,"sentence":"Natural language processing (NLP) systems have become a central technology in communication, education, medicine, artificial intelligence, and many other domains of research and development.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"While the performance of NLP methods has grown enormously over the last decade, this progress has been restricted to a minuscule subset of the world's approx6,500 languages.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":15,"sentence":"We introduce a framework for estimating the global utility of language technologies as revealed in a comprehensive snapshot of recent publications in NLP.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":15,"sentence":"Our analyses involve the field at large, but also more in-depth studies on both user-facing technologies (machine translation, language understanding, question answering, text-to-speech synthesis) as well as foundational NLP tasks (dependency parsing, morphological inflection).","offset":3,"pro":0.5,"labels":"MTD"},{"idx":15,"sentence":"In the process, we (1) quantify disparities in the current state of NLP research, (2) explore some of its associated societal and academic factors, and (3) produce tailored recommendations for evidence-based policy making aimed at promoting more global and equitable language technologies.","offset":4,"pro":0.6666666666666666,"labels":"PUR"},{"idx":15,"sentence":"Data and code to reproduce the findings discussed in this paper areavailable on GitHub (https://github.com/neubig/globalutility).","offset":5,"pro":0.8333333333333334,"labels":"CTN"},{"idx":16,"sentence":"We introduce CaMEL (Case Marker Extraction without Labels), a novel and challenging task in computational morphology that is especially relevant for low-resource languages.","offset":0,"pro":0,"labels":"PUR"},{"idx":16,"sentence":"We propose a first model for CaMEL that uses a massively multilingual corpus to extract case markers in 83 languages based only on a noun phrase chunker and an alignment system.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":16,"sentence":"The case markers extracted by our model can be used to detect and visualise similarities and differences between the case systems of different languages as well as to annotate fine-grained deep cases in languages in which they are not overtly marked.","offset":2,"pro":0.4,"labels":"CTN"},{"idx":16,"sentence":"To evaluate CaME","offset":3,"pro":0.6,"labels":"PUR"},{"idx":16,"sentence":"we automatically construct a silver standard from UniMorp","offset":4,"pro":0.8,"labels":"MTD"},{"idx":17,"sentence":"Robustness of machine learning models on ever-changing real-world data is critical, especially for applications affecting human well-being such as content moderation.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"New kinds of abusive language continually emerge in online discussions in response to current events (e.g., COVID-19), and the deployed abuse detection systems should be updated regularly to remain accurate.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":17,"sentence":"In this paper, we show that general abusive language classifiers tend to be fairly reliable in detecting out-of-domain explicitly abusive utterances but fail to detect new types of more subtle, implicit abuse.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":17,"sentence":"Next, we propose an interpretability technique, based on the Testing Concept Activation Vector (TCAV) method from computer vision, to quantify the sensitivity of a trained model to the human-defined concepts of explicit and implicit abusive language, and use that to explain the generalizability of the model on new data, in this case, COVID-related anti-Asian hate speech.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":17,"sentence":"Extending this technique, we introduce a novel metric, Degree of Explicitness, for a single instance and show that the new metric is beneficial in suggesting out-of-domain unlabeled examples to effectively enrich the training data with informative, implicitly abusive texts.","offset":4,"pro":0.8,"labels":"MTD"},{"idx":18,"sentence":"Reports of personal experiences or stories can play a crucial role in argumentation, as they represent an immediate and (often) relatable way to back up one's position with respect to a given topic.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"They are easy to understand and increase empathy: this makes them powerful in argumentation.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":18,"sentence":"The impact of personal reports and stories in argumentation has been studied in the Social Sciences, but it is still largely underexplored in NLP.","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":18,"sentence":"Our work is the first step towards filling this gap: our goal is to develop robust classifiers to identify documents containing personal experiences and reports.","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":18,"sentence":"The main challenge is the scarcity of annotated data: our solution is to leverage existing annotations to be able to scale-up the analysis.","offset":4,"pro":0.4444444444444444,"labels":"GAP"},{"idx":18,"sentence":"Our contribution is two-fold.","offset":5,"pro":0.5555555555555556,"labels":"RST"},{"idx":18,"sentence":"First, we conduct a set of in-domain and cross-domain experiments involving three datasets (two from Argument Mining, one from the Social Sciences), modeling architectures, training setups and fine-tuning options tailored to the involved domains.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":18,"sentence":"We show that despite the differences among datasets and annotations, robust cross-domain classification is possible.","offset":7,"pro":0.7777777777777778,"labels":"CLN"},{"idx":18,"sentence":"Second, we employ linear regression for performance mining, identifying performance trends both for overall classification performance and individual classifier predictions.","offset":8,"pro":0.8888888888888888,"labels":"MTD"},{"idx":19,"sentence":"In recent years, neural models have often outperformed rule-based and classic Machine Learning approaches in NLG.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"These classic approaches are now often disregarded, for example when new neural models are evaluated.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":19,"sentence":"We argue that they should not be overlooked, since, for some tasks, well-designed non-neural approaches achieve better performance than neural ones.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":19,"sentence":"In this paper, the task of generating referring expressions in linguistic context is used as an example.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":19,"sentence":"We examined two very different English datasets (WEBNLG and WSJ), and evaluated each algorithm using both automatic and human evaluations.Overall, the results of these evaluations suggest that rule-based systems with simple rule sets achieve on-par or better performance on both datasets compared to state-of-the-art neural REG systems.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":19,"sentence":"In the case of the more realistic dataset, WSJ, a machine learning-based system with well-designed linguistic features performed best.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":19,"sentence":"We hope that our work can encourage researchers to consider non-neural models in future.","offset":6,"pro":0.8571428571428571,"labels":"IMP"},{"idx":0,"sentence":"Text-to-SQL parsers map natural language questions to programs that are executable over tables to generate answers, and are typically evaluated on large-scale datasets like Spider (Yu et al., 2018).","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"We argue that existing benchmarks fail to capture a certain out-of-domain generalization problem that is of significant practical importance: matching domain specific phrases to composite operation over columns.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":0,"sentence":"To study this problem, we first propose a synthetic dataset along with a re-purposed train/test split of the Squall dataset (Shi et al., 2020) as new benchmarks to quantify domain generalization over column operations, and find existing state-of-the-art parsers struggle in these benchmarks.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":0,"sentence":"We propose to address this problem by incorporating prior domain knowledge by preprocessing table schemas, and design a method that consists of two components: schema expansion and schema pruning.","offset":3,"pro":0.6,"labels":"PUR"},{"idx":0,"sentence":"This method can be easily applied to multiple existing base parsers, and we show that it significantly outperforms baseline parsers on this domain generalization problem, boosting the underlying parsers' overall performance by up to 13.8% relative accuracy gain (5.1% absolute) on the new Squall data split.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":1,"sentence":"Paraphrase identification involves identifying whether a pair of sentences express the same or similar meanings.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"While cross-encoders have achieved high performances across several benchmarks, bi-encoders such as SBERT have been widely applied to sentence pair tasks.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":1,"sentence":"They exhibit substantially lower computation complexity and are better suited to symmetric tasks.","offset":2,"pro":0.3333333333333333,"labels":"RST"},{"idx":1,"sentence":"In this work, we adopt a bi-encoder approach to the paraphrase identification task, and investigate the impact of explicitly incorporating predicate-argument information into SBERT through weighted aggregation.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":1,"sentence":"Experiments on six paraphrase identification datasets demonstrate that, with a minimal increase in parameters, the proposed model is able to outperform SBERT/SRoBERTa significantly.","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":1,"sentence":"Further, ablation studies reveal that the predicate-argument based component plays a significant role in the performance gain.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":2,"sentence":"NER model has achieved promising performance on standard NER benchmarks.","offset":0,"pro":0,"labels":"CLN"},{"idx":2,"sentence":"However, recent studies show that previous approaches may over-rely on entity mention information, resulting in poor performance on out-of-vocabulary(OOV) entity recognition.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":2,"sentence":"In this work, we propose MINER, a novel NER learning framework, to remedy this issue from an information-theoretic perspective.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":2,"sentence":"The proposed approach contains two mutual information based training objectives: i) generalizing information maximization, which enhances representation via deep understanding of context and entity surface forms; ii) superfluous information minimization, which discourages representation from rotate memorizing entity names or exploiting biased cues in data.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":2,"sentence":"Experiments on various settings and datasets demonstrate that it achieves better performance in predicting OOV entities.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":3,"sentence":"Detecting biased language is useful for a variety of applications, such as identifying hyperpartisan news sources or flagging one-sided rhetoric.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"In this work we introduce WikiEvolve, a dataset for document-level promotional tone detection.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":3,"sentence":"Unlike previously proposed datasets, WikiEvolve contains seven versions of the same article from Wikipedia, from different points in its revision history; one with promotional tone, and six without it.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":3,"sentence":"This allows for obtaining more precise training signal for learning models from promotional tone detection.","offset":3,"pro":0.5,"labels":"CTN"},{"idx":3,"sentence":"We adapt the previously proposed gradient reversal layer framework to encode two article versions simultaneously and thus leverage this additional training signal.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":3,"sentence":"In our experiments, our proposed adaptation of gradient reversal improves the accuracy of four different architectures on both in-domain and out-of-domain evaluation.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":4,"sentence":"Informal social interaction is the primordial home of human language.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"Linguistically diverse conversational corpora are an important and largely untapped resource for computational linguistics and language technology.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":4,"sentence":"Through the efforts of a worldwide language documentation movement, such corpora are increasingly becoming available.","offset":2,"pro":0.4,"labels":"BAC"},{"idx":4,"sentence":"We show how interactional data from 63 languages (26 families) harbours insights about turn-taking, timing, sequential structure and social action, with implications for language technology, natural language understanding, and the design of conversational interfaces.","offset":3,"pro":0.6,"labels":"RST"},{"idx":4,"sentence":"Harnessing linguistically diverse conversational corpora will provide the empirical foundations for flexible, localizable, humane language technologies of the future.","offset":4,"pro":0.8,"labels":"BAC"},{"idx":5,"sentence":"Adversarial robustness has attracted much attention recently, and the mainstream solution is adversarial training.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"However, the tradition of generating adversarial perturbations for each input embedding (in the settings of NLP) scales up the training computational complexity by the number of gradient steps it takes to obtain the adversarial samples.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":5,"sentence":"To address this problem, we leverage Flooding method which primarily aims at better generalization and we find promising in defending adversarial attacks.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":5,"sentence":"We further propose an effective criterion to bring hyper-parameter-dependent flooding into effect with a narrowed-down search space by measuring how the gradient steps taken within one epoch affect the loss of each batch.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":5,"sentence":"Our approach requires zero adversarial sample for training, and its time consumption is equivalent to fine-tuning, which can be 2-15 times faster than standard adversarial training.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":5,"sentence":"We experimentally show that our method improves BERT's resistance to textual adversarial attacks by a large margin, and achieves state-of-the-art robust accuracy on various text classification and GLUE tasks.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":6,"sentence":"Evaluating Natural Language Generation (NLG) systems is a challenging task.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"Firstly, the metric should ensure that the generated hypothesis reflects the reference's semantics.","offset":1,"pro":0.1111111111111111,"labels":"MTD"},{"idx":6,"sentence":"Secondly, it should consider the grammatical quality of the generated sentence.","offset":2,"pro":0.2222222222222222,"labels":"MTD"},{"idx":6,"sentence":"Thirdly, it should be robust enough to handle various surface forms of the generated sentence.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":6,"sentence":"Thus, an effective evaluation metric has to be multifaceted.","offset":4,"pro":0.4444444444444444,"labels":"GAP"},{"idx":6,"sentence":"In this paper, we propose an automatic evaluation metric incorporating several core aspects of natural language understanding (language competence, syntactic and semantic variation).","offset":5,"pro":0.5555555555555556,"labels":"PUR"},{"idx":6,"sentence":"Our proposed metric, RoMe, is trained on language features such as semantic similarity combined with tree edit distance and grammatical acceptability, using a self-supervised neural network to assess the overall quality of the generated sentence.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":6,"sentence":"Moreover, we perform an extensive robustness analysis of the state-of-the-art methods and RoMe.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":6,"sentence":"Empirical results suggest that RoMe has a stronger correlation to human judgment over state-of-the-art metrics in evaluating system-generated sentences across several NLG tasks.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":7,"sentence":"In this work, we investigate the knowledge learned in the embeddings of multimodal-BERT models.","offset":0,"pro":0,"labels":"PUR"},{"idx":7,"sentence":"More specifically, we probe their capabilities of storing the grammatical structure of linguistic data and the structure learned over objects in visual data.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":7,"sentence":"To reach that goal, we first make the inherent structure of language and visuals explicit by a dependency parse of the sentences that describe the image and by the dependencies between the object regions in the image, respectively.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":7,"sentence":"We call this explicit visual structure the scene tree, that is based on the dependency tree of the language description.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":7,"sentence":"Extensive probing experiments show that the multimodal-BERT models do not encode these scene trees.","offset":4,"pro":0.8,"labels":"RST"},{"idx":8,"sentence":"Hyperbolic neural networks have shown great potential for modeling complex data.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"However, existing hyperbolic networks are not completely hyperbolic, as they encode features in the hyperbolic space yet formalize most of their operations in the tangent space (a Euclidean subspace) at the origin of the hyperbolic model.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":8,"sentence":"This hybrid method greatly limits the modeling ability of networks.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":8,"sentence":"In this paper, we propose a fully hyperbolic framework to build hyperbolic networks based on the Lorentz model by adapting the Lorentz transformations (including boost and rotation) to formalize essential operations of neural networks.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":8,"sentence":"Moreover, we also prove that linear transformation in tangent spaces used by existing hyperbolic networks is a relaxation of the Lorentz rotation and does not include the boost, implicitly limiting the capabilities of existing hyperbolic networks.","offset":4,"pro":0.5714285714285714,"labels":"CLN"},{"idx":8,"sentence":"The experimental results on four NLP tasks show that our method has better performance for building both shallow and deep networks.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":8,"sentence":"Our code will be released to facilitate follow-up research.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":9,"sentence":"Multimodal machine translation (MMT) aims to improve neural machine translation (NMT) with additional visual information, but most existing MMT methods require paired input of source sentence and image, which makes them suffer from shortage of sentence-image pairs.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"In this paper, we propose a phrase-level retrieval-based method for MMT to get visual information for the source input from existing sentence-image data sets so that MMT can break the limitation of paired sentence-image input.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":9,"sentence":"Our method performs retrieval at the phrase level and hence learns visual information from pairs of source phrase and grounded region, which can mitigate data sparsity.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":9,"sentence":"Furthermore, our method employs the conditional variational auto-encoder to learn visual representations which can filter redundant visual information and only retain visual information related to the phrase.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":9,"sentence":"Experiments show that the proposed method significantly outperforms strong baselines on multiple MMT datasets, especially when the textual context is limited.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":10,"sentence":"The emotional state of a speaker can be influenced by many different factors in dialogues, such as dialogue scene, dialogue topic, and interlocutor stimulus.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"The currently available data resources to support such multimodal affective analysis in dialogues are however limited in scale and diversity.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":10,"sentence":"In this work, we propose a Multi-modal Multi-scene Multi-label Emotional Dialogue dataset, M^3ED, which contains 990 dyadic emotional dialogues from 56 different TV series, a total of 9,082 turns and 24,449 utterances.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":10,"sentence":"M^3ED is annotated with 7 emotion categories (happy, surprise, sad, disgust, anger, fear, and neutral) at utterance level, and encompasses acoustic, visual, and textual modalities.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":10,"sentence":"To the best of our knowledge, M^3ED is the first multimodal emotional dialogue dataset in Chinese.It is valuable for cross-culture emotion analysis and recognition.","offset":4,"pro":0.5,"labels":"CTN"},{"idx":10,"sentence":"We apply several state-of-the-art methods on the M^3ED dataset to verify the validity and quality of the dataset.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":10,"sentence":"We also propose a general Multimodal Dialogue-aware Interaction framework, MDI, to model the dialogue context for emotion recognition, which achieves comparable performance to the state-of-the-art methods on the M^3ED.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":10,"sentence":"The full dataset and codes are available.","offset":7,"pro":0.875,"labels":"CTN"},{"idx":11,"sentence":"Few-shot NER needs to effectively capture information from limited instances and transfer useful knowledge from external resources.","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"In this paper, we propose a self-describing mechanism for few-shot NER, which can effectively leverage illustrative instances and precisely transfer knowledge from external resources by describing both entity types and mentions using a universal concept set.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":11,"sentence":"Specifically, we design Self-describing Networks (SDNet), a Seq2Seq generation model which can universally describe mentions using concepts, automatically map novel entity types to concepts, and adaptively recognize entities on-demand.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":11,"sentence":"We pre-train SDNet with large-scale corpus, and conduct experiments on 8 benchmarks from different domains.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":11,"sentence":"Experiments show that SDNet achieves competitive performances on all benchmarks and achieves the new state-of-the-art on 6 benchmarks, which demonstrates its effectiveness and robustness.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":12,"sentence":"Motivated by the success of T5 (Text-To-Text Transfer Transformer) in pre-trained natural language processing models, we propose a unified-modal SpeechT5 framework that explores the encoder-decoder pre-training for self-supervised speech/text representation learning.","offset":0,"pro":0,"labels":"MTD"},{"idx":12,"sentence":"The SpeechT5 framework consists of a shared encoder-decoder network and six modal-specific (speech/text) pre/post-nets.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":12,"sentence":"After preprocessing the input speech/text through the pre-nets, the shared encoder-decoder network models the sequence-to-sequence transformation, and then the post-nets generate the output in the speech/text modality based on the output of the decoder.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":12,"sentence":"Leveraging large-scale unlabeled speech and text data, we pre-train SpeechT5 to learn a unified-modal representation, hoping to improve the modeling capability for both speech and text.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":12,"sentence":"To align the textual and speech information into this unified semantic space, we propose a cross-modal vector quantization approach that randomly mixes up speech/text states with latent units as the interface between encoder and decoder.","offset":4,"pro":0.6666666666666666,"labels":"PUR"},{"idx":12,"sentence":"Extensive evaluations show the superiority of the proposed SpeechT5 framework on a wide variety of spoken language processing tasks, including automatic speech recognition, speech synthesis, speech translation, voice conversion, speech enhancement, and speaker identification.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":13,"sentence":"In recent years, machine learning models have rapidly become better at generating clinical consultation notes; yet, there is little work on how to properly evaluate the generated consultation notes to understand the impact they may have on both the clinician using them and the patient's clinical safety.","offset":0,"pro":0,"labels":"GAP"},{"idx":13,"sentence":"To address this we present an extensive human evaluation study of consultation notes where 5 clinicians (i) listen to 57 mock consultations, (ii) write their own notes, (iii) post-edit a number of automatically generated notes, and (iv) extract all the errors, both quantitative and qualitative.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":13,"sentence":"We then carry out a correlation study with 18 automatic quality metrics and the human judgements.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":13,"sentence":"We find that a simple, character-based Levenshtein distance metric performs on par if not better than common model-based metrics like BertScore.","offset":3,"pro":0.6,"labels":"RST"},{"idx":13,"sentence":"All our findings and annotations are open-sourced.","offset":4,"pro":0.8,"labels":"CTN"},{"idx":14,"sentence":"Information extraction suffers from its varying targets, heterogeneous structures, and demand-specific schemas.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"In this paper, we propose a unified text-to-structure generation framework, namely UIE, which can universally model different IE tasks, adaptively generate targeted structures, and collaboratively learn general IE abilities from different knowledge sources.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":14,"sentence":"Specifically, UIE uniformly encodes different extraction structures via a structured extraction language, adaptively generates target extractions via a schema-based prompt mechanism - structural schema instructor, and captures the common IE abilities via a large-scale pretrained text-to-structure model.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":14,"sentence":"Experiments show that UIE achieved the state-of-the-art performance on 4 IE tasks, 13 datasets, and on all supervised, low-resource, and few-shot settings for a wide range of entity, relation, event and sentiment extraction tasks and their unification.","offset":3,"pro":0.6,"labels":"RST"},{"idx":14,"sentence":"These results verified the effectiveness, universality, and transferability of UIE.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":15,"sentence":"Recent works on knowledge base question answering (KBQA) retrieve subgraphs for easier reasoning.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"The desired subgraph is crucial as a small one may exclude the answer but a large one might introduce more noises.","offset":1,"pro":0.14285714285714285,"labels":"MTD"},{"idx":15,"sentence":"However, the existing retrieval is either heuristic or interwoven with the reasoning, causing reasoning on the partial subgraphs, which increases the reasoning bias when the intermediate supervision is missing.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":15,"sentence":"This paper proposes a trainable subgraph retriever (SR) decoupled from the subsequent reasoning process, which enables a plug-and-play framework to enhance any subgraph-oriented KBQA model.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":15,"sentence":"Extensive experiments demonstrate SR achieves significantly better retrieval and QA performance than existing retrieval methods.","offset":4,"pro":0.5714285714285714,"labels":"CLN"},{"idx":15,"sentence":"Via weakly supervised pre-training as well as the end-to-end fine-tuning, SR achieves new state-of-the-art performance when combined with NSM (He et al., 2021), a subgraph-oriented reasoner, for embedding-based KBQA methods.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":15,"sentence":"Codes and datasets are available online (https://github.com/RUCKBReasoning/SubgraphRetrievalKBQA)","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":16,"sentence":"Low-shot relation extraction (RE) aims to recognize novel relations with very few or even no samples, which is critical in real scenario application.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"Few-shot and zero-shot RE are two representative low-shot RE tasks, which seem to be with similar target but require totally different underlying abilities.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":16,"sentence":"In this paper, we propose Multi-Choice Matching Networks to unify low-shot relation extraction.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":16,"sentence":"To fill in the gap between zero-shot and few-shot RE, we propose the triplet-paraphrase meta-training, which leverages triplet paraphrase to pre-train zero-shot label matching ability and uses meta-learning paradigm to learn few-shot instance summarizing ability.","offset":3,"pro":0.6,"labels":"PUR"},{"idx":16,"sentence":"Experimental results on three different low-shot RE tasks show that the proposed method outperforms strong baselines by a large margin, and achieve the best performance on few-shot RE leaderboard.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":17,"sentence":"Prompt-based probing has been widely used in evaluating the abilities of pretrained language models (PLMs).","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"Unfortunately, recent studies have discovered such an evaluation may be inaccurate, inconsistent and unreliable.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":17,"sentence":"Furthermore, the lack of understanding its inner workings, combined with its wide applicability, has the potential to lead to unforeseen risks for evaluating and applying PLMs in real-world applications.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":17,"sentence":"To discover, understand and quantify the risks, this paper investigates the prompt-based probing from a causal view, highlights three critical biases which could induce biased results and conclusions, and proposes to conduct debiasing via causal intervention.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":17,"sentence":"This paper provides valuable insights for the design of unbiased datasets, better probing frameworks and more reliable evaluations of pretrained language models.","offset":4,"pro":0.6666666666666666,"labels":"CTN"},{"idx":17,"sentence":"Furthermore, our conclusions also echo that we need to rethink the criteria for identifying better pretrained language models.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":18,"sentence":"Several natural language processing (NLP) tasks are defined as a classification problem in its most complex form: Multi-label Hierarchical Extreme classification, in which items may be associated with multiple classes from a set of thousands of possible classes organized in a hierarchy and with a highly unbalanced distribution both in terms of class frequency and the number of labels per item.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"We analyze the state of the art of evaluation metrics based on a set of formal properties and we define an information theoretic based metric inspired by the Information Contrast Model (ICM).","offset":1,"pro":0.3333333333333333,"labels":"MTD"},{"idx":18,"sentence":"Experiments on synthetic data and a case study on real data show the suitability of the ICM for such scenarios.","offset":2,"pro":0.6666666666666666,"labels":"CLN"},{"idx":19,"sentence":"We present a complete pipeline to extract characters in a novel and link them to their direct-speech utterances.","offset":0,"pro":0,"labels":"MTD"},{"idx":19,"sentence":"Our model is divided into three independent components: extracting direct-speech, compiling a list of characters, and attributing those characters to their utterances.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":19,"sentence":"Although we find that existing systems can perform the first two tasks accurately, attributing characters to direct speech is a challenging problem due to the narrator's lack of explicit character mentions, and the frequent use of nominal and pronominal coreference when such explicit mentions are made.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":19,"sentence":"We adapt the progress made on Dialogue State Tracking to tackle a new problem: attributing speakers to dialogues.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":19,"sentence":"This is the first application of deep learning to speaker attribution, and it shows that is possible to overcome the need for the hand-crafted features and rules used in the past.","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":19,"sentence":"Our full pipeline improves the performance of state-of-the-art models by a relative 50% in F1-score.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":0,"sentence":"With the rapid growth in language processing applications, fairness has emerged as an important consideration in data-driven solutions.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"Although various fairness definitions have been explored in the recent literature, there is lack of consensus on which metrics most accurately reflect the fairness of a system.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":0,"sentence":"In this work, we propose a new formulation - accumulated prediction sensitivity, which measures fairness in machine learning models based on the model's prediction sensitivity to perturbations in input features.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":0,"sentence":"We show that the metric can be theoretically linked with a specific notion of group fairness (statistical parity) and individual fairness.","offset":3,"pro":0.42857142857142855,"labels":"CLN"},{"idx":0,"sentence":"We conduct experiments on two text classification datasets - Jigsaw Toxicity, and Bias in Bios, and evaluate the correlations between metrics and manual annotations on whether the model produced a fair outcome.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":0,"sentence":"We observe that the proposed fairness metric based on prediction sensitivity is statistically significantly more correlated with human annotation than the existing counterfactual fairness metric.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":0,"sentence":"The metric attempts to quantify the extent to which a single prediction depends on a protected attribute, where the protected attribute encodes the membership status of an individual in a protected group.","offset":6,"pro":0.8571428571428571,"labels":"PUR"},{"idx":1,"sentence":"Temporal factors are tied to the growth of facts in realistic applications, such as the progress of diseases and the development of political situation, therefore, research on Temporal Knowledge Graph (TKG) attracks much attention.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"However, existing methods can hardly model temporal relation patterns, nor can capture the intrinsic connections between relations when evolving over time, lacking of interpretability.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":1,"sentence":"In this paper, we propose a novel temporal modeling method which represents temporal entities as Rotations in Quaternion Vector Space (RotateQVS) and relations as complex vectors in Hamilton's quaternion space.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":1,"sentence":"We demonstrate our method can model key patterns of relations in TKG, such as symmetry, asymmetry, inverse, and can capture time-evolved relations by theory.","offset":3,"pro":0.6,"labels":"CLN"},{"idx":1,"sentence":"In TKG, relation patterns inherent with temporality are required to be studied for representation learning and reasoning across temporal facts.","offset":4,"pro":0.8,"labels":"BAC"},{"idx":2,"sentence":"Machine Reading Comprehension (MRC) reveals the ability to understand a given text passage and answer questions based on it.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"Existing research works in MRC rely heavily on large-size models and corpus to improve the performance evaluated by metrics such as Exact Match (EM) and F_1.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":2,"sentence":"However, such a paradigm lacks sufficient interpretation to model capability and can not efficiently train a model with a large corpus.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":2,"sentence":"In this paper, we argue that a deep understanding of model capabilities and data properties can help us feed a model with appropriate training data based on its learning status.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":2,"sentence":"Specifically, we design an MRC capability assessment framework that assesses model capabilities in an explainable and multi-dimensional manner.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":2,"sentence":"Based on it, we further uncover and disentangle the connections between various data properties and model performance.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":2,"sentence":"Finally, to verify the effectiveness of the proposed MRC capability assessment framework, we incorporate it into a curriculum learning pipeline and devise a Capability Boundary Breakthrough Curriculum (CBBC) strategy, which performs a model capability-based training to maximize the data value and improve training efficiency.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":2,"sentence":"Extensive experiments demonstrate that our approach significantly improves performance, achieving up to an 11.22% / 8.71% improvement of EM / F_1 on MRC tasks.","offset":7,"pro":0.875,"labels":"RST"},{"idx":3,"sentence":"Simile interpretation (SI) and simile generation (SG) are challenging tasks for NLP because models require adequate world knowledge to produce predictions.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"Previous works have employed many hand-crafted resources to bring knowledge-related into models, which is time-consuming and labor-intensive.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":3,"sentence":"In recent years, pre-trained language models (PLMs) based approaches have become the de-facto standard in NLP since they learn generic knowledge from a large corpus.","offset":2,"pro":0.2,"labels":"BAC"},{"idx":3,"sentence":"The knowledge embedded in PLMs may be useful for SI and SG tasks.","offset":3,"pro":0.3,"labels":"BAC"},{"idx":3,"sentence":"Nevertheless, there are few works to explore it.","offset":4,"pro":0.4,"labels":"GAP"},{"idx":3,"sentence":"In this paper, we probe simile knowledge from PLMs to solve the SI and SG tasks in the unified framework of simile triple completion for the first time.","offset":5,"pro":0.5,"labels":"PUR"},{"idx":3,"sentence":"The backbone of our framework is to construct masked sentences with manual patterns and then predict the candidate words in the masked position.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":3,"sentence":"In this framework, we adopt a secondary training process (Adjective-Noun mask Training) with the masked language model (MLM) loss to enhance the prediction diversity of candidate words in the masked position.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":3,"sentence":"Moreover, pattern ensemble (PE) and pattern search (PS) are applied to improve the quality of predicted words.","offset":8,"pro":0.8,"labels":"MTD"},{"idx":3,"sentence":"Finally, automatic and human evaluations demonstrate the effectiveness of our framework in both SI and SG tasks.","offset":9,"pro":0.9,"labels":"CLN"},{"idx":4,"sentence":"Entity alignment (EA) aims to discover the equivalent entity pairs between KGs, which is a crucial step for integrating multi-source KGs.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"Specifically, we derive two sets of isomorphism equations: (1) Adjacency tensor isomorphism equations and (2) Gramian tensor isomorphism equations.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":4,"sentence":"Extensive experiments on public datasets indicate that our decoding algorithm can deliver significant performance improvements even on the most advanced EA methods, while the extra required time is less than 3 seconds.","offset":2,"pro":0.4,"labels":"CLN"},{"idx":4,"sentence":"For a long time, most researchers have regarded EA as a pure graph representation learning task and focused on improving graph encoders while paying little attention to the decoding process.","offset":3,"pro":0.6,"labels":"BAC"},{"idx":4,"sentence":"By combining these equations,","offset":4,"pro":0.8,"labels":"MTD"},{"idx":5,"sentence":"Typed entailment graphs try to learn the entailment relations between predicates from text and model them as edges between predicate nodes.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"EGT2 learns the local entailment relations by recognizing the textual entailment between template sentences formed by typed CCG-parsed predicates.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":5,"sentence":"Based on the generated local graph, EGT2 then uses three novel soft transitivity constraints to consider the logical transitivity in entailment structures.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":5,"sentence":"Experiments on benchmark datasets show that EGT2 can well model the transitivity in entailment graph to alleviate the sparsity, and leads to signifcant improvement over current state-of-the-art methods.","offset":3,"pro":0.5,"labels":"RST"},{"idx":5,"sentence":"The construction of entailment graphs usually suffers from severe sparsity and unreliability of distributional similarity.","offset":4,"pro":0.6666666666666666,"labels":"GAP"},{"idx":5,"sentence":"We propose a two-stage method, Entailment Graph with Textual Entailment and Transitivity (EGT2).","offset":5,"pro":0.8333333333333334,"labels":"PUR"},{"idx":6,"sentence":"Modern deep learning models are notoriously opaque, which has motivated the development of methods for interpreting how deep models predict.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"This goal is usually approached with attribution method, which assesses the influence of features on model predictions.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":6,"sentence":"However, some crucial logic traps in these evaluation methods are ignored in most works, causing inaccurate evaluation and unfair comparison.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":6,"sentence":"This paper systematically reviews existing methods for evaluating attribution scores and summarizes the logic traps in these methods.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":6,"sentence":"As an explanation method, the evaluation criteria of attribution methods is how accurately it reflects the actual reasoning process of the model (faithfulness).","offset":4,"pro":0.5714285714285714,"labels":"BAC"},{"idx":6,"sentence":"Meanwhile, since the reasoning process of deep models is inaccessible, researchers design various evaluation methods to demonstrate their arguments.","offset":5,"pro":0.7142857142857143,"labels":"BAC"},{"idx":6,"sentence":"We further conduct experiments to demonstrate the existence of each logic trap.","offset":6,"pro":0.8571428571428571,"labels":"MTD"},{"idx":7,"sentence":"In this paper, we study how to continually pre-train language models for improving the understanding of math problems.","offset":0,"pro":0,"labels":"PUR"},{"idx":7,"sentence":"Specifically, we focus on solving a fundamental challenge in modeling math problems, how to fuse the semantics of textual description and formulas, which are highly different in essence.","offset":1,"pro":0.125,"labels":"MTD"},{"idx":7,"sentence":"In this approach, we first construct the math syntax graph to model the structural semantic information, by combining the parsing trees of the text and formulas, and then design the syntax-aware memory networks to deeply fuse the features from the graph and text.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":7,"sentence":"With the help of syntax relations, we can model the interaction between the token from the text and its semantic-related nodes within the formulas, which is helpful to capture fine-grained semantic correlations between texts and formulas.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":7,"sentence":"Besides, we devise three continual pre-training tasks to further align and fuse the representations of the text and math syntax graph.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":7,"sentence":"Experimental results on four tasks in the math domain demonstrate the effectiveness of our approach.","offset":5,"pro":0.625,"labels":"CLN"},{"idx":7,"sentence":"Our code and data are publicly available at the link: blueurl{https://github.com/RUCAIBox/COMUS.","offset":6,"pro":0.75,"labels":"CTN"},{"idx":7,"sentence":"To address this issue, we propose a new approach called COMUS to continually pre-train language models for math problem understanding with syntax-aware memory network.","offset":7,"pro":0.875,"labels":"MTD"},{"idx":8,"sentence":"The definition generation task can help language learners by providing explanations for unfamiliar words.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"This task has attracted much attention in recent years.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":8,"sentence":"We propose a novel task of Simple Definition Generation (SDG) to help language learners and low literacy readers.","offset":2,"pro":0.2,"labels":"PUR"},{"idx":8,"sentence":"We disentangle the complexity factors from the text by carefully designing a parameter sharing scheme between two decoders.","offset":3,"pro":0.3,"labels":"MTD"},{"idx":8,"sentence":"We demonstrate that the framework can generate relevant, simple definitions for the target words through automatic and manual evaluations on English and Chinese datasets.","offset":4,"pro":0.4,"labels":"CLN"},{"idx":8,"sentence":"Our method outperforms the baseline model by a 1.77 SARI score on the English dataset, and raises the proportion of the low level (HSK level 1-3) words in Chinese definitions by 3.87%.","offset":5,"pro":0.5,"labels":"RST"},{"idx":8,"sentence":"A significant challenge of this task is the lack of learner's dictionaries in many languages, and therefore the lack of data for supervised training.","offset":6,"pro":0.6,"labels":"PUR"},{"idx":8,"sentence":"We explore this task and propose a multitasking framework SimpDefiner that only requires a standard dictionary with complex definitions and a corpus containing arbitrary simple texts.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":8,"sentence":"By jointly training these components,","offset":8,"pro":0.8,"labels":"MTD"},{"idx":8,"sentence":"the framework can generate both complex and simple definitions simultaneously.","offset":9,"pro":0.9,"labels":"RST"},{"idx":9,"sentence":"Solving math word problems requires deductive reasoning over the quantities in the text.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"Various recent research efforts mostly relied on sequence-to-sequence or sequence-to-tree models to generate mathematical expressions without explicitly performing relational reasoning between quantities in the given context.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":9,"sentence":"While empirically effective, such approaches typically do not provide explanations for the generated expressions.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":9,"sentence":"In this work, we view the task as a complex relation extraction problem, proposing a novel approach that presents explainable deductive reasoning steps to iteratively construct target expressions, where each step involves a primitive operation over two quantities defining their relation.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":9,"sentence":"Through extensive experiments on four benchmark datasets, we show that the proposed model significantly outperforms existing strong baselines.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":9,"sentence":"We further demonstrate that the deductive procedure not only presents more explainable steps but also enables us to make more accurate predictions on questions that require more complex reasoning.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":10,"sentence":"Indirect speech such as sarcasm achieves a constellation of discourse goals in human communication.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"While the indirectness of figurative language warrants speakers to achieve certain pragmatic goals, it is challenging for AI agents to comprehend such idiosyncrasies of human communication.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":10,"sentence":"Though sarcasm identification has been a well-explored topic in dialogue analysis, for conversational systems to truly grasp a conversation's innate meaning and generate appropriate responses, simply detecting sarcasm is not enough; it is vital to explain its underlying sarcastic connotation to capture its true essence.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":10,"sentence":"In this work, we study the discourse structure of sarcastic conversations and propose a novel task - Sarcasm Explanation in Dialogue (SED).","offset":3,"pro":0.375,"labels":"PUR"},{"idx":10,"sentence":"Set in a multimodal and code-mixed setting, the task aims to generate natural language explanations of satirical conversations.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":10,"sentence":"The proposed attention module surpasses the traditional multimodal fusion baselines and reports the best performance on almost all metrics.","offset":5,"pro":0.625,"labels":"RST"},{"idx":10,"sentence":"Lastly, we carry out detailed analysis both quantitatively and qualitatively.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":10,"sentence":"To this end, we curate WITS, a new dataset to support our task.","offset":7,"pro":0.875,"labels":"MTD"},{"idx":11,"sentence":"Recently, finetuning a pretrained language model to capture the similarity between sentence embeddings has shown the state-of-the-art performance on the semantic textual similarity (STS) task.","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"However, the absence of an interpretation method for the sentence similarity makes it difficult to explain the model output.","offset":1,"pro":0.25,"labels":"GAP"},{"idx":11,"sentence":"In this work, we explicitly describe the sentence distance as the weighted sum of contextualized token distances on the basis of a transportation problem, and then present the optimal transport-based distance measure, named RCMD; it identifies and leverages semantically-aligned token pairs.","offset":2,"pro":0.5,"labels":"PUR"},{"idx":11,"sentence":"Extensive experiments demonstrate that our learning framework outperforms other baselines on both STS and interpretable-STS benchmarks, indicating that it computes effective sentence similarity and also provides interpretation consistent with human judgement.","offset":3,"pro":0.75,"labels":"CLN"},{"idx":12,"sentence":"Recent years have witnessed growing interests in incorporating external knowledge such as pre-trained word embeddings (PWEs) or pre-trained language models (PLMs) into neural topic modeling.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"In this paper, we propose a novel strategy to incorporate external knowledge into neural topic modeling where the neural topic model is pre-trained on a large corpus and then fine-tuned on the target dataset.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":12,"sentence":"Moreover, further study shows that the proposed approach greatly reduces the need for the huge size of training data.","offset":2,"pro":0.4,"labels":"CLN"},{"idx":12,"sentence":"Experiments have been conducted on three datasets","offset":3,"pro":0.6,"labels":"MTD"},{"idx":12,"sentence":"and results show that the proposed approach significantly outperforms both current state-of-the-art neural topic models and some topic modeling approaches enhanced with PWEs or PLMs.","offset":4,"pro":0.8,"labels":"RST"},{"idx":13,"sentence":"Dense retrieval has achieved impressive advances in first-stage retrieval from a large-scale document collection, which is built on bi-encoder architecture to produce single vector representation of query and document.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"However, a document can usually answer multiple potential queries from different views.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":13,"sentence":"So the single vector representation of a document is hard to match with multi-view queries, and faces a semantic mismatch problem.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":13,"sentence":"This paper proposes a multi-view document representation learning framework, aiming to produce multi-view embeddings to represent documents and enforce them to align with different queries.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":13,"sentence":"First, we propose a simple yet effective method of generating multiple embeddings through viewers.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":13,"sentence":"Experiments show our method outperforms recent works and achieves state-of-the-art results.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":13,"sentence":"Second, to prevent multi-view embeddings from collapsing to the same one, we further propose a global-local loss with annealed temperature to encourage the multiple viewers to better align with different potential queries.","offset":6,"pro":0.8571428571428571,"labels":"MTD"},{"idx":14,"sentence":"Abstract meaning representation (AMR) highlights the core semantic information of text in a graph structure.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"Recently, pre-trained language models (PLMs) have advanced tasks of AMR parsing and AMR-to-text generation, respectively.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":14,"sentence":"However, PLMs are typically pre-trained on textual data, thus are sub-optimal for modeling structural knowledge.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":14,"sentence":"To this end, we investigate graph self-supervised training to improve the structure awareness of PLMs over AMR graphs.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":14,"sentence":"In particular, we introduce two graph auto-encoding strategies for graph-to-graph pre-training and four tasks to integrate text and graph information during pre-training.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":14,"sentence":"We further design a unified framework to bridge the gap between pre-training and fine-tuning tasks.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":14,"sentence":"Experiments on both AMR parsing and AMR-to-text generation show the superiority of our model.","offset":6,"pro":0.75,"labels":"CLN"},{"idx":14,"sentence":"To our knowledge, we are the first to consider pre-training on semantic graphs.","offset":7,"pro":0.875,"labels":"CTN"},{"idx":15,"sentence":"In this work, we propose to leverage semi-structured tables, and automatically generate at scale question-paragraph pairs, where answering the question requires reasoning over multiple facts in the paragraph.","offset":0,"pro":0,"labels":"PUR"},{"idx":15,"sentence":"We add a pre-training step over this synthetic data, which includes examples that require 16 different reasoning skills such as number comparison, conjunction, and fact composition.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":15,"sentence":"Moreover, sampling examples based on model errors leads to faster training and higher performance.","offset":2,"pro":0.3333333333333333,"labels":"RST"},{"idx":15,"sentence":"Models pre-trained with a language modeling objective possess ample world knowledge and language skills,","offset":3,"pro":0.5,"labels":"BAC"},{"idx":15,"sentence":"To improve data efficiency, we sample examples from reasoning skills where the model currently errs.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":15,"sentence":"We evaluate our approach on three reasoning-focused reading comprehension datasets,","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":16,"sentence":"Existing KBQA approaches, despite achieving strong performance on i.i.d. test data, often struggle in generalizing to questions involving unseen KB schema items.","offset":0,"pro":0,"labels":"GAP"},{"idx":16,"sentence":"Prior ranking-based approaches have shown some success in generalization, but suffer from the coverage issue.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":16,"sentence":"Our approach first uses a contrastive ranker to rank a set of candidate logical forms obtained by searching over the knowledge graph.","offset":2,"pro":0.2222222222222222,"labels":"MTD"},{"idx":16,"sentence":"It then introduces a tailored generation model conditioned on the question and the top-ranked candidates to compose the final logical form.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":16,"sentence":"We achieve new state-of-the-art results on GrailQA and WebQSP datasets.","offset":4,"pro":0.4444444444444444,"labels":"RST"},{"idx":16,"sentence":"In particular, our method surpasses the prior state-of-the-art by a large margin on the GrailQA leaderboard.","offset":5,"pro":0.5555555555555556,"labels":"RST"},{"idx":16,"sentence":"In addition, RnG-KBQA outperforms all prior approaches on the popular WebQSP benchmark, even including the ones that use the oracle entity linking.","offset":6,"pro":0.6666666666666666,"labels":"RST"},{"idx":16,"sentence":"The experimental results demonstrate the effectiveness of the interplay between ranking and generation, which leads to the superior performance of our proposed approach across all settings with especially strong improvements in zero-shot generalization.","offset":7,"pro":0.7777777777777778,"labels":"CLN"},{"idx":16,"sentence":"We present RnG-KBQA, a Rank-and-Generate approach for KBQA, which remedies the coverage issue with a generation model while preserving a strong generalization capability.","offset":8,"pro":0.8888888888888888,"labels":"PUR"},{"idx":17,"sentence":"Prior work in neural coherence modeling has primarily focused on devising new architectures for solving the permuted document task.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"Given the claims of improved text generation quality across various pre-trained neural models, we consider the coherence evaluation of machine generated text to be one of the principal applications of coherence models that needs to be investigated.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":17,"sentence":"We instead use a basic model architecture","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":17,"sentence":"and show significant improvements over state of the art within the same training regime.","offset":3,"pro":0.42857142857142855,"labels":"RST"},{"idx":17,"sentence":"We then design a harder self-supervision objective by increasing the ratio of negative samples within a contrastive learning setup,","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":17,"sentence":"and enhance the model further through automatic hard negative mining coupled with a large global negative queue encoded by a momentum encoder. ","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":17,"sentence":"We evaluate the coherence model on task-independent test sets that resemble real-world applications","offset":6,"pro":0.8571428571428571,"labels":"MTD"},{"idx":18,"sentence":"Word and sentence embeddings are useful feature representations in natural language processing.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"Word and sentence similarity tasks have become the de facto evaluation method.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":18,"sentence":"It leads models to overfit to such evaluations, negatively impacting embedding models' development.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":18,"sentence":"This paper first points out the problems using semantic similarity as the gold standard for word and sentence embedding evaluations.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":18,"sentence":"Further, we propose a new intrinsic evaluation method called EvalRank, which shows a much stronger correlation with downstream tasks.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":18,"sentence":"Extensive experiments are conducted based on 60+ models and popular datasets to certify our judgments.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":18,"sentence":"Finally, the practical evaluation toolkit is released for future benchmarking purposes.","offset":6,"pro":0.75,"labels":"CTN"},{"idx":18,"sentence":"However, intrinsic evaluation for embeddings lags far behind, and there has been no significant update since the past decade.","offset":7,"pro":0.875,"labels":"BAC"},{"idx":19,"sentence":"Multimodal pre-training with text, layout, and image has made significant progress for Visually Rich Document Understanding (VRDU), especially the fixed-layout documents such as scanned document images.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"While, there are still a large number of digital documents where the layout information is not fixed and needs to be interactively and dynamically rendered for visualization, making existing layout-based pre-training approaches not easy to apply.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":19,"sentence":"In this paper, we propose MarkupLM for document understanding tasks with markup languages as the backbone, such as HTML/XML-based documents, where text and markup information is jointly pre-trained.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":19,"sentence":"Experiment results show that the pre-trained MarkupLM significantly outperforms the existing strong baseline models on several document understanding tasks.","offset":3,"pro":0.6,"labels":"RST"},{"idx":19,"sentence":"The pre-trained model and code will be publicly available at https://aka.ms/markuplm.","offset":4,"pro":0.8,"labels":"CTN"},{"idx":0,"sentence":"CLIP has shown a remarkable zero-shot capability on a wide range of vision tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"Previously, CLIP is only regarded as a powerful visual encoder.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":0,"sentence":"However, after being pre-trained by language supervision from a large amount of image-caption pairs, CLIP itself should also have acquired some few-shot abilities for vision-language tasks.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":0,"sentence":"In this work, we empirically show that CLIP can be a strong vision-language few-shot learner by leveraging the power of language.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":0,"sentence":"We first evaluate CLIP's zero-shot performance on a typical visual question answering task and demonstrate a zero-shot cross-modality transfer capability of CLIP on the visual entailment task.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":0,"sentence":"Then we propose a parameter-efficient fine-tuning strategy to boost the few-shot performance on the vqa task.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":0,"sentence":"We achieve competitive zero/few-shot results on the visual question answering and visual entailment tasks without introducing any additional pre-training procedure.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":1,"sentence":"To this end, we introduce KQA Pro, a dataset for Complex KBQA including around 120K diverse natural language questions.","offset":0,"pro":0,"labels":"PUR"},{"idx":1,"sentence":"We introduce a compositional and interpretable programming language KoPL to represent the reasoning process of complex questions.","offset":1,"pro":0.125,"labels":"MTD"},{"idx":1,"sentence":"For each question, we provide the corresponding KoPL program and SPARQL query, so that KQA Pro can serve for both KBQA and semantic parsing tasks.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":1,"sentence":"Experimental results show that state-of-the-art KBQA methods cannot achieve promising results on KQA Pro as on current datasets, which suggests that KQA Pro is challenging and Complex KBQA requires further research efforts.","offset":3,"pro":0.375,"labels":"RST"},{"idx":1,"sentence":"We also treat KQA Pro as a diagnostic dataset for testing multiple reasoning skills, conduct a thorough evaluation of existing models and discuss further directions for Complex KBQA.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":1,"sentence":"Our codes and datasets can be obtained from https://github.com/shijx12/KQAPro{_Baselines.","offset":5,"pro":0.625,"labels":"CTN"},{"idx":1,"sentence":"Complex question answering over knowledge base (Complex KBQA) is challenging because it requires various compositional reasoning capabilities, such as multi-hop inference, attribute comparison, set operation, etc","offset":6,"pro":0.75,"labels":"BAC"},{"idx":1,"sentence":"Existing benchmarks have some shortcomings that limit the development of Complex KBQA: 1) they only provide QA pairs without explicit reasoning processes; 2) questions are poor in diversity or scale","offset":7,"pro":0.875,"labels":"GAP"},{"idx":2,"sentence":"Recently, contrastive learning has been shown to be effective in improving pre-trained language models (PLM) to derive high-quality sentence representations.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"It aims to pull close positive examples to enhance the alignment while push apart irrelevant negatives for the uniformity of the whole representation space.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":2,"sentence":"However, previous works mostly adopt in-batch negatives or sample from training data at random.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":2,"sentence":"Such a way may cause the sampling bias that improper negatives (false negatives and anisotropy representations) are used to learn sentence representations, which will hurt the uniformity of the representation space.","offset":3,"pro":0.42857142857142855,"labels":"GAP"},{"idx":2,"sentence":"To address it, we present a new framework DCLR (Debiased Contrastive Learning of unsupervised sentence Representations) to alleviate the influence of these improper negatives.","offset":4,"pro":0.5714285714285714,"labels":"PUR"},{"idx":2,"sentence":"In DCLR, we design an instance weighting method to punish false negatives and generate noise-based negatives to guarantee the uniformity of the representation space.Experiments on seven semantic textual similarity tasks show that our approach is more effective than competitive baselines.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":2,"sentence":"Our code and data are publicly available at the link: blueurl{https://github.com/RUCAIBox/DCLR.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":3,"sentence":"Prompting has recently been shown as a promising approach for applying pre-trained language models to perform downstream tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"We present Multi-Stage Prompting, a simple and automatic approach for leveraging pre-trained language models to translation tasks.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":3,"sentence":"To better mitigate the discrepancy between pre-training and translation, MSP divides the translation process via pre-trained language models into three separate stages: the encoding stage, the re-encoding stage, and the decoding stage.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":3,"sentence":"During each stage, we independently apply different continuous prompts for allowing pre-trained language models better shift to translation tasks.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":3,"sentence":"We conduct extensive experiments on three translation tasks.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":3,"sentence":"Experiments show that our method can significantly improve the translation performance of pre-trained language models.","offset":5,"pro":0.8333333333333334,"labels":"CTN"},{"idx":4,"sentence":"Dialogue systems are usually categorized into two types, open-domain and task-oriented.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"The first one focuses on chatting with users and making them engage in the conversations, where selecting a proper topic to fit the dialogue context is essential for a successful dialogue.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":4,"sentence":"The other one focuses on a specific task instead of casual talks, e.g., finding a movie on Friday night, playing a song.","offset":2,"pro":0.2,"labels":"BAC"},{"idx":4,"sentence":"These two directions have been studied separately due to their different purposes.","offset":3,"pro":0.3,"labels":"BAC"},{"idx":4,"sentence":"However, how to smoothly transition from social chatting to task-oriented dialogues is important for triggering the business opportunities, and there is no any public data focusing on such scenarios.","offset":4,"pro":0.4,"labels":"GAP"},{"idx":4,"sentence":"Hence, this paper focuses on investigating the conversations starting from open-domain social chatting and then gradually transitioning to task-oriented purposes, and releases a large-scale dataset with detailed annotations for encouraging this research direction.","offset":5,"pro":0.5,"labels":"PUR"},{"idx":4,"sentence":"To achieve this goal, this paper proposes a framework to automatically generate many dialogues without human involvement, in which any powerful open-domain dialogue generation model can be easily leveraged.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":4,"sentence":"Furthermore, the released models allow researchers to automatically generate unlimited dialogues in the target scenarios, which can greatly benefit semi-supervised and unsupervised approaches.","offset":7,"pro":0.7,"labels":"CTN"},{"idx":4,"sentence":"The human evaluation shows that our generated dialogue data has a natural flow at a reasonable quality","offset":8,"pro":0.8,"labels":"RST"},{"idx":4,"sentence":"showing that our released data has a great potential of guiding future research directions and commercial activities","offset":9,"pro":0.9,"labels":"CTN"},{"idx":5,"sentence":"High-quality phrase representations are essential to finding topics and related terms in documents (a.k.a. topic mining).","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"Existing phrase representation learning methods either simply combine unigram representations in a context-free manner or rely on extensive annotations to learn context-aware knowledge.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":5,"sentence":"In this paper, we propose UCTopic, a novel unsupervised contrastive learning framework for context-aware phrase representations and topic mining.","offset":2,"pro":0.2222222222222222,"labels":"PUR"},{"idx":5,"sentence":"UCTopic is pretrained in a large scale to distinguish if the contexts of two phrase mentions have the same semantics.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":5,"sentence":"The key to the pretraining is positive pair construction from our phrase-oriented assumptions.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":5,"sentence":"However, we find traditional in-batch negatives cause performance decay when finetuning on a dataset with small topic numbers.","offset":5,"pro":0.5555555555555556,"labels":"RST"},{"idx":5,"sentence":"Hence, we propose cluster-assisted contrastive learning (CCL) which largely reduces noisy negatives by selecting negatives from clusters and further improves phrase representations for topics accordingly.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":5,"sentence":"UCTopic outperforms the state-of-the-art phrase representation model by 38.2% NMI in average on four entity clustering tasks.","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":5,"sentence":"Comprehensive evaluation on topic mining shows that UCTopic can extract coherent and diverse topical phrases.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":6,"sentence":"In this paper, we introduce ELECTRA-style tasks to cross-lingual language model pre-training.","offset":0,"pro":0,"labels":"PUR"},{"idx":6,"sentence":"Specifically, we present two pre-training tasks, namely multilingual replaced token detection, and translation replaced token detection.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":6,"sentence":"Besides, we pretrain the model, named as XLM-E, on both multilingual and parallel corpora.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":6,"sentence":"Our model outperforms the baseline models on various cross-lingual understanding tasks with much less computation cost.","offset":3,"pro":0.6,"labels":"RST"},{"idx":6,"sentence":"Moreover, analysis shows that XLM-E tends to obtain better cross-lingual transferability.","offset":4,"pro":0.8,"labels":"CTN"},{"idx":7,"sentence":"Nested named entity recognition (NER) has been receiving increasing attention.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"Recently, Fu et al. (2020) adapt a span-based constituency parser to tackle nested NER.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":7,"sentence":"They treat nested entities as partially-observed constituency trees and propose the masked inside algorithm for partial marginalization.","offset":2,"pro":0.2222222222222222,"labels":"BAC"},{"idx":7,"sentence":"However, their method cannot leverage entity heads, which have been shown useful in entity mention detection and entity typing.","offset":3,"pro":0.3333333333333333,"labels":"GAP"},{"idx":7,"sentence":"In this work, we resort to more expressive structures, lexicalized constituency trees in which constituents are annotated by headwords, to model nested entities.","offset":4,"pro":0.4444444444444444,"labels":"PUR"},{"idx":7,"sentence":"We leverage the Eisner-Satta algorithm to perform partial marginalization and inference efficiently.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":7,"sentence":"In addition, we propose to use (1) a two-stage strategy (2) a head regularization loss and (3) a head-aware labeling loss in order to enhance the performance.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":7,"sentence":"We make a thorough ablation study to investigate the functionality of each component.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":7,"sentence":"Experimentally, our method achieves the state-of-the-art performance on ACE2004, ACE2005 and NNE, and competitive performance on GENIA, and meanwhile has a fast inference speed.","offset":8,"pro":0.8888888888888888,"labels":"RST"},{"idx":8,"sentence":"NLP practitioners often want to take existing trained models and apply them to data from new domains.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"While fine-tuning or few-shot learning can be used to adapt a base model, there is no single recipe for making these techniques work; moreover, one may not have access to the original model weights if it is deployed as a black box.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":8,"sentence":"We study how to improve a black box model's performance on a new domain by leveraging explanations of the model's behavior.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":8,"sentence":"Our approach first extracts a set of features combining human intuition about the task with model attributions generated by black box interpretation techniques, then uses a simple calibrator, in the form of a classifier, to predict whether the base model was correct or not.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":8,"sentence":"We experiment with our method on two tasks, extractive question answering and natural language inference, covering adaptation from several pairs of domains with limited target-domain data.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":8,"sentence":"The experimental results across all the domain pairs show that explanations are useful for calibrating these models, boosting accuracy when predictions do not have to be returned on every example.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":8,"sentence":"We further show that the calibration model transfers to some extent between tasks.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":9,"sentence":"Different Open Information Extraction (OIE) tasks require different types of information, so the OIE field requires strong adaptability of OIE algorithms to meet different task requirements.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"This paper discusses the adaptability problem in existing OIE systems and designs a new adaptable and efficient OIE system - OIE@OIA as a solution.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":9,"sentence":"OIE@OIA follows the methodology of Open Information eXpression (OIX): parsing a sentence to an Open Information Annotation (OIA) Graph and then adapting the OIA graph to different OIE tasks with simple rules.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":9,"sentence":"As the core of our OIE@OIA system, we implement an end-to-end OIA generator by annotating a dataset (we make it open available) and designing an efficient learning algorithm for the complex OIA graph.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":9,"sentence":"We easily adapt the OIE@OIA system to accomplish three popular OIE tasks.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":9,"sentence":"The experimental show that our OIE@OIA achieves new SOTA performances on these tasks, showing the great adaptability of our OIE@OIA system.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":9,"sentence":"Furthermore, compared to other end-to-end OIE baselines that need millions of samples for training, our OIE@OIA needs much fewer training samples (12K), showing a significant advantage in terms of efficiency.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":10,"sentence":"Code completion, which aims to predict the following code token(s) according to the code context, can improve the productivity of software development.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"Recent work has proved that statistical language modeling with transformers can greatly improve the performance in the code completion task via learning from large-scale source code datasets.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":10,"sentence":"However, current approaches focus only on code context within the file or project, i.e. internal context.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":10,"sentence":"Our distinction is utilizing external context, inspired by human behaviors of copying from the related code snippets when writing code.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":10,"sentence":"Specifically, we propose a retrieval-augmented code completion framework, leveraging both lexical copying and referring to code with similar semantics by retrieval.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":10,"sentence":"We adopt a stage-wise training approach that combines a source code retriever and an auto-regressive language model for programming language.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":10,"sentence":"We evaluate our approach in the code completion task in Python and Java programming languages","offset":6,"pro":0.75,"labels":"MTD"},{"idx":10,"sentence":"achieving a state-of-the-art performance on CodeXGLUE benchmark","offset":7,"pro":0.875,"labels":"RST"},{"idx":11,"sentence":"DocRED is a widely used dataset for document-level relation extraction.","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"In the large-scale annotation, a recommend-revise scheme is adopted to reduce the workload.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":11,"sentence":"Within this scheme, annotators are provided with candidate relation instances from distant supervision, and they then manually supplement and remove relational facts based on the recommendations.","offset":2,"pro":0.2222222222222222,"labels":"BAC"},{"idx":11,"sentence":"Furthermore, we observe that the models trained on DocRED have low recall on our relabeled dataset and inherit the same bias in the training data.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":11,"sentence":"Through the analysis of annotators' behaviors, we figure out the underlying reason for the problems above: the scheme actually discourages annotators from supplementing adequate instances in the revision phase.","offset":4,"pro":0.4444444444444444,"labels":"RST"},{"idx":11,"sentence":"We appeal to future research to take into consideration the issues with the recommend-revise scheme when designing new models and annotation schemes.","offset":5,"pro":0.5555555555555556,"labels":"IMP"},{"idx":11,"sentence":"The relabeled dataset is released at https://github.com/AndrewZhe/Revisit-DocRED, to serve as a more reliable test set of document RE models.","offset":6,"pro":0.6666666666666666,"labels":"CTN"},{"idx":11,"sentence":"However, when comparing DocRED with a subset relabeled from scratch,","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":11,"sentence":"we find that this scheme results in a considerable amount of false negative samples and an obvious bias towards popular entities and relations","offset":8,"pro":0.8888888888888888,"labels":"RST"},{"idx":12,"sentence":"Recent parameter-efficient language model tuning (PELT) methods manage to match the performance of fine-tuning with much fewer trainable parameters and perform especially well when training data is limited.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"However, different PELT methods may perform rather differently on the same task, making it nontrivial to select the most appropriate method for a specific task, especially considering the fast-growing number of new PELT methods and tasks.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":12,"sentence":"In light of model diversity and the difficulty of model selection, we propose a unified framework, UniPELT, which incorporates different PELT methods as submodules and learns to activate the ones that best suit the current data or task setup via gating mechanism.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":12,"sentence":"On the GLUE benchmark, UniPELT consistently achieves 1 4% gains compared to the best individual PELT method that it incorporates and even outperforms fine-tuning under different setups.","offset":3,"pro":0.6,"labels":"RST"},{"idx":12,"sentence":"Moreover, UniPELT generally surpasses the upper bound that takes the best performance of all its submodules used individually on each task, indicating that a mixture of multiple PELT methods may be inherently more effective than single methods.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":13,"sentence":"A recent study by Feldman (2020) proposed a long-tail theory to explain the memorization behavior of deep learning models.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"However, memorization has not been empirically verified in the context of NLP, a gap addressed by this work.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":13,"sentence":"In this paper, we use three different NLP tasks to check if the long-tail theory holds.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":13,"sentence":"Our experiments demonstrate that top-ranked memorized training instances are likely atypical, and removing the top-memorized training instances leads to a more serious drop in test accuracy compared with removing training instances randomly.","offset":3,"pro":0.5,"labels":"RST"},{"idx":13,"sentence":"Furthermore, we develop an attribution method to better understand why a training instance is memorized.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":13,"sentence":"We empirically show that our memorization attribution method is faithful, and share our interesting finding that the top-memorized parts of a training instance tend to be features negatively correlated with the class label.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":14,"sentence":"Pretrained multilingual models are able to perform cross-lingual transfer in a zero-shot setting, even for languages unseen during pretraining.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"However, prior work evaluating performance on unseen languages has largely been limited to low-level, syntactic tasks, and it remains unclear if zero-shot learning of high-level, semantic tasks is possible for unseen languages.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":14,"sentence":"To explore this question, we present AmericasNLI, an extension of XNLI (Conneau et al., 2018) to 10 Indigenous languages of the Americas.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":14,"sentence":"We conduct experiments with XLM-R, testing multiple zero-shot and translation-based approaches.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":14,"sentence":"Additionally, we explore model adaptation via continued pretraining and provide an analysis of the dataset by considering hypothesis-only models.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":14,"sentence":"We find that XLM-R's zero-shot performance is poor for all 10 languages, with an average performance of 38.48%.","offset":5,"pro":0.625,"labels":"RST"},{"idx":14,"sentence":"Continued pretraining offers improvements, with an average accuracy of 43.85%.","offset":6,"pro":0.75,"labels":"RST"},{"idx":14,"sentence":"Surprisingly, training on poorly translated data by far outperforms all other methods with an accuracy of 49.12%.","offset":7,"pro":0.875,"labels":"RST"},{"idx":15,"sentence":"Understanding the functional (dis)-similarity of source code is significant for code modeling tasks such as software vulnerability and code clone detection.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"We present DISCO (DIS-similarity of COde), a novel self-supervised model focusing on identifying (dis)similar functionalities of source code.","offset":1,"pro":0.125,"labels":"PUR"},{"idx":15,"sentence":"Different from existing works, our approach does not require a huge amount of randomly collected datasets.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":15,"sentence":"Rather, we design structure-guided code transformation algorithms to generate synthetic code clones and inject real-world security bugs, augmenting the collected datasets in a targeted way.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":15,"sentence":"We propose to pre-train the Transformer model with such automatically generated program contrasts to better identify similar code in the wild and differentiate vulnerable programs from benign ones.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":15,"sentence":"To better capture the structural features of source code, we propose a new cloze objective to encode the local tree-based context (e.g., parents or sibling nodes).","offset":5,"pro":0.625,"labels":"MTD"},{"idx":15,"sentence":"We pre-train our model with a much smaller dataset, the size of which is only 5% of the state-of-the-art models' training datasets, to illustrate the effectiveness of our data augmentation and the pre-training approach.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":15,"sentence":"The evaluation shows that, even with much less data, DISCO can still outperform the state-of-the-art models in vulnerability and code clone detection tasks.","offset":7,"pro":0.875,"labels":"RST"},{"idx":16,"sentence":"Most works on financial forecasting use information directly associated with individual companies (e.g., stock prices, news on the company) to predict stock returns for trading.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"We refer to such company-specific information as local information.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":16,"sentence":"Stock returns may also be influenced by global information (e.g., news on the economy in general), and inter-company relationships.","offset":2,"pro":0.2857142857142857,"labels":"BAC"},{"idx":16,"sentence":"Capturing such diverse information is challenging due to the low signal-to-noise ratios, different time-scales, sparsity and distributions of global and local information from different modalities.","offset":3,"pro":0.42857142857142855,"labels":"GAP"},{"idx":16,"sentence":"In this paper, we propose a model that captures both global and local multimodal information for investment and risk management-related forecasting tasks.","offset":4,"pro":0.5714285714285714,"labels":"PUR"},{"idx":16,"sentence":"Our proposed Guided Attention Multimodal Multitask Network (GAME) model addresses these challenges by using novel attention modules to guide learning with global and local information from different modalities and dynamic inter-company relationship networks.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":16,"sentence":"Our extensive experiments show that GAME outperforms other state-of-the-art models in several forecasting tasks and important real-world application case studies.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":17,"sentence":"Previous work on multimodal machine translation (MMT) has focused on the way of incorporating vision features into translation but little attention is on the quality of vision models.","offset":0,"pro":0,"labels":"GAP"},{"idx":17,"sentence":"In this work, we investigate the impact of vision models on MMT.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":17,"sentence":"Given the fact that Transformer is becoming popular in computer vision, we experiment with various strong models (such as Vision Transformer) and enhanced features (such as object-detection and image captioning).","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":17,"sentence":"We develop a selective attention model to study the patch-level contribution of an image in MMT.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":17,"sentence":"On detailed probing tasks, we find that stronger vision models are helpful for learning translation from the visual modality.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":17,"sentence":"Our results also suggest the need of carefully examining MMT models, especially when current benchmarks are small-scale and biased.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":18,"sentence":"Named Entity Recognition (NER) in Few-Shot setting is imperative for entity tagging in low resource domains.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"Existing approaches only learn class-specific semantic features and intermediate representations from source domains.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":18,"sentence":"This affects generalizability to unseen target domains, resulting in suboptimal performances.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":18,"sentence":"To this end, we present CONTaiNER, a novel contrastive learning technique that optimizes the inter-token distribution distance for Few-Shot NER.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":18,"sentence":"Instead of optimizing class-specific attributes, CONTaiNER optimizes a generalized objective of differentiating between token categories based on their Gaussian-distributed embeddings.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":18,"sentence":"This effectively alleviates overfitting issues originating from training domains.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":18,"sentence":"Our experiments in several traditional test domains (OntoNotes, CoNLL'03, WNUT 17, GUM) and a new large scale Few-Shot NER dataset (Few-NERD) demonstrate that on average, CONTaiNER outperforms previous methods by 3%-13% absolute F1 points while showing consistent performance trends, even in challenging scenarios where previous approaches could not achieve appreciable performance.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":19,"sentence":"Plains Cree is an Indigenous language that is spoken in Canada and the USA.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"It is the most widely spoken dialect of Cree and a morphologically complex language that is polysynthetic, highly inflective, and agglutinative.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":19,"sentence":"It is an extremely low resource language, with no existing corpus that is both available and prepared for supporting the development of language technologies.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":19,"sentence":"To support revitalization and preservation, we developed a corpus covering diverse genres, time periods, and texts for a variety of intended audiences.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":19,"sentence":"The data has been verified and cleaned; it is ready for use in developing language technologies.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":19,"sentence":"The corpus includes the corresponding English phrases or audio files where available.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":19,"sentence":"We demonstrate the utility of the corpus through its community use and its use to build language technologies that can provide the types of support that community members have expressed are desirable.","offset":6,"pro":0.75,"labels":"CLN"},{"idx":19,"sentence":"The corpus is available for public use.","offset":7,"pro":0.875,"labels":"CTN"},{"idx":0,"sentence":"Visual storytelling (VIST) is a typical vision and language task that has seen extensive development in the natural language generation research domain.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"However, it remains unclear whether conventional automatic evaluation metrics for text generation are applicable on VIST.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":0,"sentence":"In this paper, we present the VHED (VIST Human Evaluation Data) dataset, which first re-purposes human evaluation results for automatic evaluation; hence we develop Vrank (VIST Ranker), a novel reference-free VIST metric for story evaluation.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":0,"sentence":"We first show that the results from commonly adopted automatic metrics for text generation have little correlation with those obtained from human evaluation, which motivates us to directly utilize human evaluation results to learn the automatic evaluation model.","offset":3,"pro":0.375,"labels":"RST"},{"idx":0,"sentence":"In the experiments, we evaluate the generated texts to predict story ranks using our model as well as other reference-based and reference-free metrics.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":0,"sentence":"Results show that Vrank prediction is significantly more aligned to human evaluation than other metrics with almost 30% higher accuracy when ranking story pairs.","offset":5,"pro":0.625,"labels":"RST"},{"idx":0,"sentence":"Moreover, we demonstrate that only Vrank shows human-like behavior in its strong ability to find better stories when the quality gap between two stories is high.","offset":6,"pro":0.75,"labels":"CLN"},{"idx":0,"sentence":"Finally, we show the superiority of Vrank by its generalizability to pure textual stories, and conclude that this reuse of human evaluation results puts Vrank in a strong position for continued future advances.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":1,"sentence":"Pre-trained sequence-to-sequence models have significantly improved Neural Machine Translation (NMT).","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"Specifically, we propose CeMAT, a conditional masked language model pre-trained on large-scale bilingual and monolingual corpora in many languages.","offset":1,"pro":0.14285714285714285,"labels":"MTD"},{"idx":1,"sentence":"We also introduce two simple but effective methods to enhance the CeMAT, aligned code-switching & masking and dynamic dual-masking.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":1,"sentence":"For Non-autoregressive NMT, we demonstrate it can also produce consistent performance gains, i.e., up to +5.3 BLEU.","offset":3,"pro":0.42857142857142855,"labels":"RST"},{"idx":1,"sentence":"Different from prior works where pre-trained models usually adopt an unidirectional decoder, this paper demonstrates that pre-training a sequence-to-sequence model but with a bidirectional decoder can produce notable performance gains for both Autoregressive and Non-autoregressive NMT.","offset":4,"pro":0.5714285714285714,"labels":"PUR"},{"idx":1,"sentence":"We conduct extensive experiments","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":1,"sentence":"To the best of our knowledge, this is the first work to pre-train a unified model for fine-tuning on both NMT tasks.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":2,"sentence":"In contrast to existing VQA test sets, CARETS features balanced question generation to create pairs of instances to test models, with each pair focusing on a specific capability such as rephrasing, logical symmetry or image obfuscation. ","offset":0,"pro":0,"labels":"MTD"},{"idx":2,"sentence":"Interestingly, even the most sophisticated models are sensitive to aspects such as swapping the order of terms in a conjunction or varying the number of answer choices mentioned in the question.","offset":1,"pro":0.5,"labels":"CLN"},{"idx":3,"sentence":"Recent studies have achieved inspiring success in unsupervised grammar induction using masked language modeling (MLM) as the proxy task.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"Despite their high accuracy in identifying low-level structures, prior arts tend to struggle in capturing high-level structures like clauses, since the MLM task usually only requires information from local context.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":3,"sentence":"In this work, we revisit LM-based constituency parsing from a phrase-centered perspective.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":3,"sentence":"Inspired by the natural reading process of human, we propose to regularize the parser with phrases extracted by an unsupervised phrase tagger to help the LM model quickly manage low-level structures.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":3,"sentence":"We show that the initial phrase regularization serves as an effective bootstrap, and phrase-guided masking improves the identification of high-level structures.","offset":4,"pro":0.5714285714285714,"labels":"CLN"},{"idx":3,"sentence":"Experiments on the public benchmark with two different backbone models demonstrate the effectiveness and generality of our method.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":3,"sentence":"For a better understanding of high-level structures, we propose a phrase-guided masking strategy for LM to emphasize more on reconstructing non-phrase words.","offset":6,"pro":0.8571428571428571,"labels":"MTD"},{"idx":4,"sentence":"Evaluation of open-domain dialogue systems is highly challenging and development of better techniques is highlighted time and again as desperately needed.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"Despite substantial efforts to carry out reliable live evaluation of systems in recent competitions, annotations have been abandoned and reported as too unreliable to yield sensible results.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":4,"sentence":"This is a serious problem since automatic metrics are not known to provide a good indication of what may or may not be a high-quality conversation.","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":4,"sentence":"Answering the distress call of competitions that have emphasized the urgent need for better evaluation techniques in dialogue, we present the successful development of human evaluation that is highly reliable while still remaining feasible and low cost.","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":4,"sentence":"Self-replication experiments reveal almost perfectly repeatable results with a correlation of r=0.969.","offset":4,"pro":0.4444444444444444,"labels":"RST"},{"idx":4,"sentence":"Since we have developed a highly reliable evaluation method, new insights into system performance can be revealed.","offset":5,"pro":0.5555555555555556,"labels":"CTN"},{"idx":4,"sentence":"We therefore include a comparison of state-of-the-art models (i) with and without personas, to measure the contribution of personas to conversation quality, as well as (ii) prescribed versus freely chosen topics.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":4,"sentence":"Interestingly with respect to personas, results indicate that personas do not positively contribute to conversation quality as expected.","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":4,"sentence":"Furthermore, due to the lack of appropriate methods of statistical significance testing, the likelihood of potential improvements to systems occurring due to chance is rarely taken into account in dialogue evaluation, and the evaluation we propose facilitates application of standard tests.","offset":8,"pro":0.8888888888888888,"labels":"RST"},{"idx":5,"sentence":"We propose the task of updated headline generation, in which a system generates a headline for an updated article, considering both the previous article and headline.","offset":0,"pro":0,"labels":"PUR"},{"idx":5,"sentence":"The system must identify the novel information in the article update, and modify the existing headline accordingly.","offset":1,"pro":0.25,"labels":"MTD"},{"idx":5,"sentence":"We create data for this task using the NewsEdits corpus by automatically identifying contiguous article versions that are likely to require a substantive headline update.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":5,"sentence":"We find that models conditioned on the prior headline and body revisions produce headlines judged by humans to be as factual as gold headlines while making fewer unnecessary edits compared to a standard headline generation model.","offset":3,"pro":0.75,"labels":"RST"},{"idx":6,"sentence":"Current open-domain conversational models can easily be made to talk in inadequate ways.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"Online learning from conversational feedback given by the conversation partner is a promising avenue for a model to improve and adapt, so as to generate fewer of these safety failures.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":6,"sentence":"However, current state-of-the-art models tend to react to feedback with defensive or oblivious responses.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":6,"sentence":"This makes for an unpleasant experience and may discourage conversation partners from giving feedback in the future.","offset":3,"pro":0.42857142857142855,"labels":"GAP"},{"idx":6,"sentence":"This work proposes SaFeRDialogues, a task and dataset of graceful responses to conversational feedback about safety failures.","offset":4,"pro":0.5714285714285714,"labels":"PUR"},{"idx":6,"sentence":"We collect a dataset of 8k dialogues demonstrating safety failures, feedback signaling them, and a response acknowledging the feedback.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":6,"sentence":"We show how fine-tuning on this dataset results in conversations that human raters deem considerably more likely to lead to a civil conversation, without sacrificing engagingness or general conversational ability.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":7,"sentence":"Compositionality- the ability to combine familiar units like words into novel phrases and sentences- has been the focus of intense interest in artificial intelligence in recent years.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"Dependency parsing, however, lacks a compositional generalization benchmark.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":7,"sentence":"In this work, we introduce a gold-standard set of dependency parses for CFQ, and use this to analyze the behaviour of a state-of-the art dependency parser (Qi et al., 2020) on the CFQ dataset.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":7,"sentence":"We find that increasing compound divergence degrades dependency parsing performance, although not as dramatically as semantic parsing performance.","offset":3,"pro":0.5,"labels":"RST"},{"idx":7,"sentence":"Additionally, we find the performance of the dependency parser does not uniformly degrade relative to compound divergence, and the parser performs differently on different splits with the same compound divergence.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":7,"sentence":"To test compositional generalization in semantic parsing, Keysers et al. (2020) introduced Compositional Freebase Queries (CFQ). ","offset":5,"pro":0.8333333333333334,"labels":"BAC"},{"idx":8,"sentence":"Generic summaries try to cover an entire document and query-based summaries try to answer document-specific questions.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"But real users' needs often fall in between these extremes and correspond to aspects, high-level topics discussed among similar types of documents.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":8,"sentence":"In this paper, we collect a dataset of realistic aspect-oriented summaries, AspectNews, which covers different subtopics about articles in news sub-domains.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":8,"sentence":"We annotate data across two domains of articles, earthquakes and fraud investigations, where each article is annotated with two distinct summaries focusing on different aspects for each domain.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":8,"sentence":"Our focus in evaluation is how well existing techniques can generalize to these domains without seeing in-domain training data, so we turn to techniques to construct synthetic training data that have been used in query-focused summarization work.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":8,"sentence":"We compare several training schemes that differ in how strongly keywords are used and how oracle summaries are extracted.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":8,"sentence":"Our evaluation shows that our final approach yields (a) focused summaries, better than those from a generic summarization system or from keyword matching; (b) a system sensitive to the choice of keywords.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":9,"sentence":"When MemSum iteratively selects sentences into the summary, it considers a broad information set that would intuitively also be used by humans in this task: 1) the text content of the sentence, 2) the global text context of the rest of the document, and 3) the extraction history consisting of the set of sentences that have already been extracted.","offset":0,"pro":0,"labels":"MTD"},{"idx":9,"sentence":"With a lightweight architecture, MemSum obtains state-of-the-art test-set performance (ROUGE) in summarizing long documents taken from PubMed, arXiv, and GovReport.","offset":1,"pro":0.25,"labels":"RST"},{"idx":9,"sentence":"A human evaluation confirms the high quality and low redundancy of the generated summaries, stemming from MemSum's awareness of extraction history.","offset":2,"pro":0.5,"labels":"CLN"},{"idx":9,"sentence":"We introduce MemSum (Multi-step Episodic Markov decision process extractive SUMmarizer), a reinforcement-learning-based extractive summarizer enriched at each step with information on the current extraction history.","offset":3,"pro":0.75,"labels":"PUR"},{"idx":10,"sentence":"Supervised learning has traditionally focused on inductive learning by observing labeled examples of a task.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"In contrast, a hallmark of human intelligence is the ability to learn new concepts purely from language.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":10,"sentence":"Here, we explore training zero-shot classifiers for structured data purely from language.","offset":2,"pro":0.2222222222222222,"labels":"PUR"},{"idx":10,"sentence":"For this, we introduce CLUES, a benchmark for Classifier Learning Using natural language ExplanationS, consisting of a range of classification tasks over structured data along with natural language supervision in the form of explanations.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":10,"sentence":"CLUES consists of 36 real-world and 144 synthetic classification tasks.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":10,"sentence":"It contains crowdsourced explanations describing real-world tasks from multiple teachers and programmatically generated explanations for the synthetic tasks.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":10,"sentence":"ExEnt generalizes up to 18% better (relative) on novel tasks than a baseline that does not use explanations.","offset":6,"pro":0.6666666666666666,"labels":"RST"},{"idx":10,"sentence":"We delineate key challenges for automated learning from explanations, addressing which can lead to progress on CLUES in the future.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":10,"sentence":"Code and datasets are available at: https://clues-benchmark.github.io.","offset":8,"pro":0.8888888888888888,"labels":"CTN"},{"idx":11,"sentence":"Models for the target domain can then be trained, using the projected distributions as soft silver labels.","offset":0,"pro":0,"labels":"MTD"},{"idx":11,"sentence":"We evaluate SubDP on zero shot cross-lingual dependency parsing, taking dependency arcs as substructures: we project the predicted dependency arc distributions in the source language(s) to target language(s), and train a target language parser on the resulting distributions.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":11,"sentence":"Given an English tree bank as the only source of human supervision, SubDP achieves better unlabeled attachment score than all prior work on the Universal Dependencies v2.2 (Nivre et al., 2020) test set across eight diverse target languages, as well as the best labeled attachment score on six languages.","offset":2,"pro":0.4,"labels":"RST"},{"idx":11,"sentence":"In addition, SubDP improves zero shot cross-lingual dependency parsing with very few (e.g., 50) supervised bitext pairs, across a broader range of target languages.","offset":3,"pro":0.6,"labels":"RST"},{"idx":11,"sentence":"We present substructure distribution projection (SubDP), a technique that projects a distribution over structures in one domain to another, by projecting substructure distributions separately.","offset":4,"pro":0.8,"labels":"PUR"},{"idx":12,"sentence":"Detecting disclosures of individuals' employment status on social media can provide valuable information to match job seekers with suitable vacancies, offer social protection, or measure labor market flows.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"However, identifying such personal disclosures is a challenging task due to their rarity in a sea of social media content and the variety of linguistic forms used to describe them.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":12,"sentence":"Here, we examine three Active Learning (AL) strategies in real-world settings of extreme class imbalance, and identify five types of disclosures about individuals' employment status (e.g. job loss) in three languages using BERT-based classification models.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":12,"sentence":"Our findings show that, even under extreme imbalance settings, a small number of AL iterations is sufficient to obtain large and significant gains in precision, recall, and diversity of results compared to a supervised baseline with the same number of labels.","offset":3,"pro":0.5,"labels":"CLN"},{"idx":12,"sentence":"We also find that no AL strategy consistently outperforms the rest.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":12,"sentence":"Qualitative analysis suggests that AL helps focus the attention mechanism of BERT on core terms and adjust the boundaries of semantic expansion, highlighting the importance of interpretable models to provide greater control and visibility into this dynamic learning process.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":13,"sentence":"Numerical reasoning over hybrid data containing both textual and tabular content (e.g., financial reports) has recently attracted much attention in the NLP community.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"However, existing question answering (QA) benchmarks over hybrid data only include a single flat table in each document and thus lack examples of multi-step numerical reasoning across multiple hierarchical tables.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":13,"sentence":"To facilitate data analytical progress, we construct a new large-scale benchmark, MultiHiertt, with QA pairs over Multi Hierarchical Tabular and Textual data.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":13,"sentence":"MultiHiertt is built from a wealth of financial reports and has the following unique characteristics: 1) each document contain multiple tables and longer unstructured texts; 2) most of tables contained are hierarchical; 3) the reasoning process required for each question is more complex and challenging than existing benchmarks; and 4) fine-grained annotations of reasoning processes and supporting facts are provided to reveal complex numerical reasoning.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":13,"sentence":"We further introduce a novel QA model termed MT2Net, which first applies facts retrieving to extract relevant supporting facts from both tables and text and then uses a reasoning module to perform symbolic reasoning over retrieved facts.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":13,"sentence":"We conduct comprehensive experiments on various baselines.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":13,"sentence":"The experimental results show that MultiHiertt presents a strong challenge for existing baselines whose results lag far behind the performance of human experts.","offset":6,"pro":0.75,"labels":"RST"},{"idx":13,"sentence":"The dataset and code are publicly available at https://github.com/psunlpgroup/MultiHiertt.","offset":7,"pro":0.875,"labels":"CTN"},{"idx":14,"sentence":"Representation of linguistic phenomena in computational language models is typically assessed against the predictions of existing linguistic theories of these phenomena.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"We probe polarity via so-called negative polarity items' (in particular, English any') in two pre-trained Transformer-based models (BERT and GPT-2).","offset":1,"pro":0.25,"labels":"MTD"},{"idx":14,"sentence":"We show that - at least for polarity - metrics derived from language models are more consistent with data from psycholinguistic experiments than linguistic theory predictions.","offset":2,"pro":0.5,"labels":"CLN"},{"idx":14,"sentence":"This work contributes to establishing closer ties between psycholinguistic experiments and experiments with language models.","offset":3,"pro":0.75,"labels":"CTN"},{"idx":15,"sentence":"Back-translation is a critical component of Unsupervised Neural Machine Translation (UNMT), which generates pseudo parallel data from target monolingual data.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"The source discrepancy between training and inference hinders the translation performance of UNMT models.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":15,"sentence":"By carefully designing experiments, we identify two representative characteristics of the data gap in source: (1) style gap (i.e., translated vs natural text style) that leads to poor generalization capability; (2) content gap that induces the model to produce hallucination content biased towards the target language.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":15,"sentence":"To narrow the data gap, we propose an online self-training approach, which simultaneously uses the pseudo parallel data natural source, translated target to mimic the inference scenario.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":15,"sentence":"Experimental results on several widely-used language pairs show that our approach outperforms two strong baselines (XLM and MASS) by remedying the style and content gaps.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":15,"sentence":"A UNMT model is trained on the pseudo parallel data with translated source, and translates bf natural source sentences in inference.","offset":5,"pro":0.8333333333333334,"labels":"BAC"},{"idx":16,"sentence":"BERT based ranking models have achieved superior performance on various information retrieval tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"However, the large number of parameters and complex self-attention operations come at a significant latency overhead.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":16,"sentence":"To remedy this, recent works propose late-interaction architectures, which allow pre-computation of intermediate document representations, thus reducing latency.","offset":2,"pro":0.2222222222222222,"labels":"BAC"},{"idx":16,"sentence":"Nonetheless, having solved the immediate latency issue, these methods now introduce storage costs and network fetching latency, which limit their adoption in real-life production systems.","offset":3,"pro":0.3333333333333333,"labels":"GAP"},{"idx":16,"sentence":"In this work, we propose the Succinct Document Representation (SDR) scheme that computes highly compressed intermediate document representations, mitigating the storage/network issue.","offset":4,"pro":0.4444444444444444,"labels":"PUR"},{"idx":16,"sentence":"Our approach first reduces the dimension of token representations by encoding them using a novel autoencoder architecture that uses the document's textual content in both the encoding and decoding phases.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":16,"sentence":"After this token encoding step, we further reduce the size of the document representations using modern quantization techniques.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":16,"sentence":"Evaluation on MSMARCO's passage re-reranking task show that compared to existing approaches using compressed document representations, our method is highly efficient, achieving 4x-11.6x higher compression rates for the same ranking quality.","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":16,"sentence":"Similarly, on the TREC CAR dataset, we achieve 7.7x higher compression rate for the same ranking quality.","offset":8,"pro":0.8888888888888888,"labels":"RST"},{"idx":17,"sentence":"Task-oriented dialogue systems are increasingly prevalent in healthcare settings, and have been characterized by a diverse range of architectures and objectives.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"Although these systems have been surveyed in the medical community from a non-technical perspective, a systematic review from a rigorous computational perspective has to date remained noticeably absent.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":17,"sentence":"As a result, many important implementation details of healthcare-oriented dialogue systems remain limited or underspecified, slowing the pace of innovation in this area.","offset":2,"pro":0.4,"labels":"GAP"},{"idx":17,"sentence":"To fill this gap, we investigated an initial pool of 4070 papers from well-known computer science, natural language processing, and artificial intelligence venues, identifying 70 papers discussing the system-level implementation of task-oriented dialogue systems for healthcare applications.","offset":3,"pro":0.6,"labels":"PUR"},{"idx":17,"sentence":"We conducted a comprehensive technical review of these papers, and present our key findings including identified gaps and corresponding recommendations.","offset":4,"pro":0.8,"labels":"MTD"},{"idx":18,"sentence":"Even though several methods have proposed to defend textual neural network (NN) models against black-box adversarial attacks, they often defend against a specific text perturbation strategy and/or require re-training the models from scratch.","offset":0,"pro":0,"labels":"GAP"},{"idx":18,"sentence":"This leads to a lack of generalization in practice and redundant computation.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":18,"sentence":"In particular, the state-of-the-art transformer models (e.g., BERT, RoBERTa) require great time and computation resources.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":18,"sentence":"By borrowing an idea from software engineering, in order to address these limitations, we propose a novel algorithm, SHIELD, which modifies and re-trains only the last layer of a textual NN, and thus it patches and transforms the NN into a stochastic weighted ensemble of multi-expert prediction heads.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":18,"sentence":"In other words, SHIELD breaks a fundamental assumption of the attack, which is a victim NN model remains constant during an attack.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":18,"sentence":"By conducting comprehensive experiments, we demonstrate that all of CNN, RNN, BERT, and RoBERTa-based textual NNs, once patched by SHIELD, exhibit a relative enhancement of 15%-70% in accuracy on average against 14 different black-box attacks, outperforming 6 defensive baselines across 3 public datasets.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":18,"sentence":"All codes are to be released.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":19,"sentence":"Online alignment in machine translation refers to the task of aligning a target word to a source word when the target sequence has only been partially decoded.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"Good online alignments facilitate important applications such as lexically constrained translation where user-defined dictionaries are used to inject lexical constraints into the translation model.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":19,"sentence":"Our proposed inference technique jointly considers alignment and token probabilities in a principled manner and can be seamlessly integrated within existing constrained beam-search decoding algorithms.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":19,"sentence":"On five language pairs, including two distant language pairs, we achieve consistent drop in alignment error rates.","offset":3,"pro":0.5,"labels":"RST"},{"idx":19,"sentence":"When deployed on seven lexically constrained translation tasks, we achieve significant improvements in BLEU specifically around the constrained positions.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":19,"sentence":"We propose a novel posterior alignment technique that is truly online in its execution and superior in terms of alignment error rates compared to existing methods.","offset":5,"pro":0.8333333333333334,"labels":"PUR"},{"idx":0,"sentence":"Identifying sections is one of the critical components of understanding medical information from unstructured clinical notes and developing assistive technologies for clinical note-writing tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"Most state-of-the-art text classification systems require thousands of in-domain text data to achieve high performance.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":0,"sentence":"However, collecting in-domain and recent clinical note data with section labels is challenging given the high level of privacy and sensitivity.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":0,"sentence":"The present paper proposes an algorithmic way to improve the task transferability of meta-learning-based text classification in order to address the issue of low-resource target data.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":0,"sentence":"Specifically, we explore how to make the best use of the source dataset and propose a unique task transferability measure named Normalized Negative Conditional Entropy (NNCE).","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":0,"sentence":"Leveraging the NNCE, we develop strategies for selecting clinical categories and sections from source task data to boost cross-domain meta-learning accuracy.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":0,"sentence":"Experimental results show that our task selection strategies improve section classification accuracy significantly compared to meta-learning algorithms.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":1,"sentence":"As large Pre-trained Language Models (PLMs) trained on large amounts of data in an unsupervised manner become more ubiquitous, identifying various types of bias in the text has come into sharp focus.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"Existing Stereotype Detection' datasets mainly adopt a diagnostic approach toward large PLMs.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":1,"sentence":"Blodgett(2021) show that there are significant reliability issues with the existing benchmark datasets.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":1,"sentence":"Annotating a reliable dataset requires a precise understanding of the subtle nuances of how stereotypes manifest in text.","offset":3,"pro":0.375,"labels":"GAP"},{"idx":1,"sentence":"In this paper, we annotate a focused evaluation set for Stereotype Detection' that addresses those pitfalls by de-constructing various ways in which stereotypes manifest in text.","offset":4,"pro":0.5,"labels":"PUR"},{"idx":1,"sentence":"Further, we present a multi-task model that leverages the abundance of data-rich neighboring tasks such as hate speech detection, offensive language detection, misogyny detection, etc., to improve the empirical performance on Stereotype Detection.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":1,"sentence":"We then propose a reinforcement-learning agent that guides the multi-task learning model by learning to identify the training examples from the neighboring tasks that help the target task the most.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":1,"sentence":"We show that the proposed models achieve significant empirical gains over existing baselines on all the tasks.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":2,"sentence":"While a great deal of work has been done on NLP approaches to lexical semantic change detection, other aspects of language change have received less attention from the NLP community.","offset":0,"pro":0,"labels":"GAP"},{"idx":2,"sentence":"In this paper, we address the detection of sound change through historical spelling.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":2,"sentence":"We propose that a sound change can be captured by comparing the relative distance through time between the distributions of the characters involved before and after the change has taken place.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":2,"sentence":"We model these distributions using PPMI character embeddings.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":2,"sentence":"We verify this hypothesis in synthetic data and then test the method's ability to trace the well-known historical change of lenition of plosives in Danish historical sources.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":2,"sentence":"We show that the models are able to identify several of the changes under consideration and to uncover meaningful contexts in which they appeared.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":2,"sentence":"The methodology has the potential to contribute to the study of open questions such as the relative chronology of sound shifts and their geographical distribution.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":3,"sentence":"Large pretrained generative models like GPT-3 often suffer from hallucinating non-existent or incorrect content, which undermines their potential merits in real applications.","offset":0,"pro":0,"labels":"GAP"},{"idx":3,"sentence":"Existing work usually attempts to detect these hallucinations based on a corresponding oracle reference at a sentence or document level.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":3,"sentence":"However ground-truth references may not be readily available for many free-form text generation applications, and sentence- or document-level detection may fail to provide the fine-grained signals that would prevent fallacious content in real time.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":3,"sentence":"As a first step to addressing these issues, we propose a novel token-level, reference-free hallucination detection task and an associated annotated dataset named HaDeS (HAllucination DEtection dataSet).","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":3,"sentence":"To create this dataset, we first perturb a large number of text segments extracted from English language Wikipedia, and then verify these with crowd-sourced annotations.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":3,"sentence":"To mitigate label imbalance during annotation, we utilize an iterative model-in-loop strategy.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":3,"sentence":"We conduct comprehensive data analyses and create multiple baseline models.","offset":6,"pro":0.8571428571428571,"labels":"MTD"},{"idx":4,"sentence":"Classifiers in natural language processing (NLP) often have a large number of output classes.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"For example, neural language models (LMs) and machine translation (MT) models both predict tokens from a vocabulary of thousands.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":4,"sentence":"The Softmax output layer of these models typically receives as input a dense feature representation, which has much lower dimensionality than the output.","offset":2,"pro":0.25,"labels":"BAC"},{"idx":4,"sentence":"In theory, the result is some words may be impossible to be predicted via argmax, irrespective of input features, and empirically, there is evidence this happens in small language models (Demeter et al., 2020).","offset":3,"pro":0.375,"labels":"BAC"},{"idx":4,"sentence":"In this paper we ask whether it can happen in practical large language models and translation models.","offset":4,"pro":0.5,"labels":"PUR"},{"idx":4,"sentence":"To do so, we develop algorithms to detect such unargmaxable tokens in public models.","offset":5,"pro":0.625,"labels":"PUR"},{"idx":4,"sentence":"We find that 13 out of 150 models do indeed have such tokens; however, they are very infrequent and unlikely to impact model quality.","offset":6,"pro":0.75,"labels":"RST"},{"idx":4,"sentence":"We release our algorithms and code to the public.","offset":7,"pro":0.875,"labels":"CTN"},{"idx":5,"sentence":"In this paper, we propose an effective yet efficient model PAIE for both sentence-level and document-level Event Argument Extraction (EAE), which also generalizes well when there is a lack of training data.","offset":0,"pro":0,"labels":"PUR"},{"idx":5,"sentence":"On the one hand, PAIE utilizes prompt tuning for extractive objectives to take the best advantages of Pre-trained Language Models (PLMs).","offset":1,"pro":0.1111111111111111,"labels":"MTD"},{"idx":5,"sentence":"It introduces two span selectors based on the prompt to select start/end tokens among input texts for each role.","offset":2,"pro":0.2222222222222222,"labels":"MTD"},{"idx":5,"sentence":"On the other hand, it captures argument interactions via multi-role prompts and conducts joint optimization with optimal span assignments via a bipartite matching loss.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":5,"sentence":"Also, with a flexible prompt design, PAIE can extract multiple arguments with the same role instead of conventional heuristic threshold tuning.","offset":4,"pro":0.4444444444444444,"labels":"RST"},{"idx":5,"sentence":"We have conducted extensive experiments on three benchmarks, including both sentence- and document-level EAE.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":5,"sentence":"The results present promising improvements from PAIE (3.5% and 2.3% F1 gains in average on three benchmarks, for PAIE-base and PAIE-large respectively).","offset":6,"pro":0.6666666666666666,"labels":"RST"},{"idx":5,"sentence":"Further analysis demonstrates the efficiency, generalization to few-shot settings, and effectiveness of different extractive prompt tuning strategies.","offset":7,"pro":0.7777777777777778,"labels":"CLN"},{"idx":5,"sentence":"Our code is available at https://github.com/mayubo2333/PAIE.","offset":8,"pro":0.8888888888888888,"labels":"CTN"},{"idx":6,"sentence":"Simultaneous machine translation (SiMT) starts translating while receiving the streaming source inputs, and hence the source sentence is always incomplete during translating.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"Different from the full-sentence MT using the conventional seq-to-seq architecture, SiMT often applies prefix-to-prefix architecture, which forces each target word to only align with a partial source prefix to adapt to the incomplete source in streaming inputs.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":6,"sentence":"However, the source words in the front positions are always illusoryly considered more important since they appear in more prefixes, resulting in position bias, which makes the model pay more attention on the front source positions in testing.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":6,"sentence":"In this paper, we first analyze the phenomenon of position bias in SiMT, and develop a Length-Aware Framework to reduce the position bias by bridging the structural gap between SiMT and full-sentence MT.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":6,"sentence":"Specifically, given the streaming inputs, we first predict the full-sentence length and then fill the future source position with positional encoding, thereby turning the streaming inputs into a pseudo full-sentence.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":6,"sentence":"The proposed framework can be integrated into most existing SiMT methods to further improve performance.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":6,"sentence":"Experiments on two representative SiMT methods, including the state-of-the-art adaptive policy, show that our method successfully reduces the position bias and thereby achieves better SiMT performance.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":7,"sentence":"Statutory article retrieval is the task of automatically retrieving law articles relevant to a legal question.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"While recent advances in natural language processing have sparked considerable interest in many legal tasks, statutory article retrieval remains primarily untouched due to the scarcity of large-scale and high-quality annotated datasets.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":7,"sentence":"To address this bottleneck, we introduce the Belgian Statutory Article Retrieval Dataset (BSARD), which consists of 1,100+ French native legal questions labeled by experienced jurists with relevant articles from a corpus of 22,600+ Belgian law articles.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":7,"sentence":"Using BSARD, we benchmark several state-of-the-art retrieval approaches, including lexical and dense architectures, both in zero-shot and supervised setups.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":7,"sentence":"We find that fine-tuned dense retrieval models significantly outperform other systems.","offset":4,"pro":0.5,"labels":"RST"},{"idx":7,"sentence":"Our best performing baseline achieves 74.8% R@100, which is promising for the feasibility of the task and indicates there is still room for improvement.","offset":5,"pro":0.625,"labels":"RST"},{"idx":7,"sentence":"By the specificity of the domain and addressed task, BSARD presents a unique challenge problem for future research on legal information retrieval.","offset":6,"pro":0.75,"labels":"CLN"},{"idx":7,"sentence":"Our dataset and source code are publicly available.","offset":7,"pro":0.875,"labels":"CTN"},{"idx":8,"sentence":"We present a novel pipeline for the collection of parallel data for the detoxification task.","offset":0,"pro":0,"labels":"PUR"},{"idx":8,"sentence":"We collect non-toxic paraphrases for over 10,000 English toxic sentences.","offset":1,"pro":0.1,"labels":"MTD"},{"idx":8,"sentence":"We also show that this pipeline can be used to distill a large existing corpus of paraphrases to get toxic-neutral sentence pairs.","offset":2,"pro":0.2,"labels":"RST"},{"idx":8,"sentence":"We release two parallel corpora which can be used for the training of detoxification models.","offset":3,"pro":0.3,"labels":"MTD"},{"idx":8,"sentence":"To the best of our knowledge, these are the first parallel datasets for this task.","offset":4,"pro":0.4,"labels":"CTN"},{"idx":8,"sentence":"We describe our pipeline in detail to make it fast to set up for a new language or domain, thus contributing to faster and easier development of new parallel resources.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":8,"sentence":"We train several detoxification models on the collected data and compare them with several baselines and state-of-the-art unsupervised approaches.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":8,"sentence":"We conduct both automatic and manual evaluations.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":8,"sentence":"All models trained on parallel data outperform the state-of-the-art unsupervised models by a large margin.","offset":8,"pro":0.8,"labels":"RST"},{"idx":8,"sentence":"This suggests that our novel datasets can boost the performance of detoxification systems.","offset":9,"pro":0.9,"labels":"CLN"},{"idx":9,"sentence":"Character-level information is included in many NLP models, but evaluating the information encoded in character representations is an open issue.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"We leverage perceptual representations in the form of shape, sound, and color embeddings and perform a representational similarity analysis to evaluate their correlation with textual representations in five languages.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":9,"sentence":"This cross-lingual analysis shows that textual character representations correlate strongly with sound representations for languages using an alphabetic script, while shape correlates with featural scripts.","offset":2,"pro":0.4,"labels":"CLN"},{"idx":9,"sentence":"We further develop a set of probing classifiers to intrinsically evaluate what phonological information is encoded in character embeddings.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":9,"sentence":"Our results suggest that information on features such as voicing are embedded in both LSTM and transformer-based representations.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":10,"sentence":"The introduction of immensely large Causal Language Models (CLMs) has rejuvenated the interest in open-ended text generation.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"However, controlling the generative process for these Transformer-based models is at large an unsolved problem.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":10,"sentence":"Earlier work has explored either plug-and-play decoding strategies, or more powerful but blunt approaches such as prompting.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":10,"sentence":"There hence currently exists a trade-off between fine-grained control, and the capability for more expressive high-level instructions.","offset":3,"pro":0.42857142857142855,"labels":"GAP"},{"idx":10,"sentence":"To alleviate this trade-off, we propose an encoder-decoder architecture that enables intermediate text prompts at arbitrary time steps.","offset":4,"pro":0.5714285714285714,"labels":"PUR"},{"idx":10,"sentence":"We propose a resource-efficient method for converting a pre-trained CLM into this architecture, and demonstrate its potential on various experiments, including the novel task of contextualized word inclusion.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":10,"sentence":"Our method provides strong results on multiple experimental settings, proving itself to be both expressive and versatile.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":11,"sentence":"While neural text-to-speech systems perform remarkably well in high-resource scenarios, they cannot be applied to the majority of the over 6,000 spoken languages in the world due to a lack of appropriate training data.","offset":0,"pro":0,"labels":"GAP"},{"idx":11,"sentence":"In this work, we use embeddings derived from articulatory vectors rather than embeddings derived from phoneme identities to learn phoneme representations that hold across languages.","offset":1,"pro":0.3333333333333333,"labels":"PUR"},{"idx":11,"sentence":"In conjunction with language agnostic meta learning, this enables us to fine-tune a high-quality text-to-speech model on just 30 minutes of data in a previously unseen language spoken by a previously unseen speaker.","offset":2,"pro":0.6666666666666666,"labels":"MTD"},{"idx":12,"sentence":"Modern Irish is a minority language lacking sufficient computational resources for the task of accurate automatic syntactic parsing of user-generated content such as tweets.","offset":0,"pro":0,"labels":"GAP"},{"idx":12,"sentence":"Although language technology for the Irish language has been developing in recent years, these tools tend to perform poorly on user-generated content.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":12,"sentence":"As with other languages, the linguistic style observed in Irish tweets differs, in terms of orthography, lexicon, and syntax, from that of standard texts more commonly used for the development of language models and parsers.","offset":2,"pro":0.3333333333333333,"labels":"BAC"},{"idx":12,"sentence":"We release the first Universal Dependencies treebank of Irish tweets, facilitating natural language processing of user-generated content in Irish.","offset":3,"pro":0.5,"labels":"BAC"},{"idx":12,"sentence":"In this paper, we explore the differences between Irish tweets and standard Irish text, and the challenges associated with dependency parsing of Irish tweets.","offset":4,"pro":0.6666666666666666,"labels":"PUR"},{"idx":12,"sentence":"We describe our bootstrapping method of treebank development and report on preliminary parsing experiments.","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":13,"sentence":"Previous length-controllable summarization models mostly control lengths at the decoding stage, whereas the encoding or the selection of information from the source document is not sensitive to the designed length.","offset":0,"pro":0,"labels":"GAP"},{"idx":13,"sentence":"They also tend to generate summaries as long as those in the training data.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":13,"sentence":"In this paper, we propose a length-aware attention mechanism (LAAM) to adapt the encoding of the source based on the desired length.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":13,"sentence":"Our approach works by training LAAM on a summary length balanced dataset built from the original training data, and then fine-tuning as usual.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":13,"sentence":"Results show that this approach is effective in generating high-quality summaries with desired lengths and even those short lengths never seen in the original training set.","offset":4,"pro":0.8,"labels":"RST"},{"idx":14,"sentence":"Multi-hop question generation focuses on generating complex questions that require reasoning over multiple pieces of information of the input passage.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"Current models with state-of-the-art performance have been able to generate the correct questions corresponding to the answers.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":14,"sentence":"However, most models can not ensure the complexity of generated questions, so they may generate shallow questions that can be answered without multi-hop reasoning.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":14,"sentence":"To address this challenge, we propose the CQG, which is a simple and effective controlled framework.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":14,"sentence":"CQG employs a simple method to generate the multi-hop questions that contain key entities in multi-hop reasoning chains, which ensure the complexity and quality of the questions.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":14,"sentence":"In addition, we introduce a novel controlled Transformer-based decoder to guarantee that key entities appear in the questions.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":14,"sentence":"Experiment results show that our model greatly improves performance, which also outperforms the state-of-the-art model about 25% by 5 BLEU points on HotpotQA.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":15,"sentence":"Recent studies have shown that language models pretrained and/or fine-tuned on randomly permuted sentences exhibit competitive performance on GLUE, putting into question the importance of word order information.","offset":0,"pro":0,"labels":"GAP"},{"idx":15,"sentence":"Somewhat counter-intuitively, some of these studies also report that position embeddings appear to be crucial for models' good performance with shuffled text.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":15,"sentence":"We probe these language models for word order information and investigate what position embeddings learned from shuffled text encode, showing that these models retain a notion of word order information.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":15,"sentence":"We show this is in part due to a subtlety in how shuffling is implemented in previous work - before rather than after subword segmentation.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":15,"sentence":"Surprisingly, we find even Language models trained on text shuffled after subword segmentation retain some semblance of information about word order because of the statistical dependencies between sentence length and unigram probabilities.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":15,"sentence":"Finally, we show that beyond GLUE, a variety of language understanding tasks do require word order information, often to an extent that cannot be learned through fine-tuning.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":16,"sentence":"Recent work in Natural Language Processing has focused on developing approaches that extract faithful explanations, either via identifying the most important tokens in the input (i.e. post-hoc explanations) or by designing inherently faithful models that first select the most important tokens and then use them to predict the correct label (i.e. select-then-predict models).","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"Currently, these approaches are largely evaluated on in-domain settings.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":16,"sentence":"Yet, little is known about how post-hoc explanations and inherently faithful models perform in out-of-domain settings.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":16,"sentence":"In this paper, we conduct an extensive empirical study that examines: (1) the out-of-domain faithfulness of post-hoc explanations, generated by five feature attribution methods; and (2) the out-of-domain performance of two inherently faithful models over six datasets.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":16,"sentence":"Contrary to our expectations, results show that in many cases out-of-domain post-hoc explanation faithfulness measured by sufficiency and comprehensiveness is higher compared to in-domain.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":16,"sentence":"We find this misleading and suggest using a random baseline as a yardstick for evaluating post-hoc explanation faithfulness.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":16,"sentence":"Our findings also show that select-then predict models demonstrate comparable predictive performance in out-of-domain settings to full-text trained models.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":17,"sentence":"Open Information Extraction (OpenIE) is the task of extracting (subject, predicate, object) triples from natural language sentences.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"Current OpenIE systems extract all triple slots independently.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":17,"sentence":"In contrast, we explore the hypothesis that it may be beneficial to extract triple slots iteratively: first extract easy slots, followed by the difficult ones by conditioning on the easy slots, and therefore achieve a better overall extraction.Based on this hypothesis, we propose a neural OpenIE system, MILIE, that operates in an iterative fashion.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":17,"sentence":"Due to the iterative nature, the system is also modularit is possible to seamlessly integrate rule based extraction systems with a neural end-to-end system, thereby allowing rule based systems to supply extraction slots which MILIE can leverage for extracting the remaining slots.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":17,"sentence":"We confirm our hypothesis empirically: MILIE outperforms SOTA systems on multiple languages ranging from Chinese to Arabic.","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":17,"sentence":"Additionally, we are the first to provide an OpenIE test dataset for Arabic and Galician.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":18,"sentence":"For a natural language understanding benchmark to be useful in research, it has to consist of examples that are diverse and difficult enough to discriminate among current and near-future state-of-the-art systems.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"However, we do not yet know how best to select text sources to collect a variety of challenging examples.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":18,"sentence":"In this study, we crowdsource multiple-choice reading comprehension questions for passages taken from seven qualitatively distinct sources, analyzing what attributes of passages contribute to the difficulty and question types of the collected examples.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":18,"sentence":"To our surprise, we find that passage source, length, and readability measures do not significantly affect question difficulty.","offset":3,"pro":0.5,"labels":"RST"},{"idx":18,"sentence":"Through our manual annotation of seven reasoning types, we observe several trends between passage sources and reasoning types, e.g., logical reasoning is more often required in questions written for technical passages.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":18,"sentence":"These results suggest that when creating a new benchmark dataset, selecting a diverse set of passages can help ensure a diverse range of question types, but that passage difficulty need not be a priority.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":19,"sentence":"Simultaneous machine translation has recently gained traction thanks to significant quality improvements and the advent of streaming applications.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"Simultaneous translation systems need to find a trade-off between translation quality and response time, and with this purpose multiple latency measures have been proposed.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":19,"sentence":"However, latency evaluations for simultaneous translation are estimated at the sentence level, not taking into account the sequential nature of a streaming scenario.","offset":2,"pro":0.4,"labels":"GAP"},{"idx":19,"sentence":"Indeed, these sentence-level latency measures are not well suited for continuous stream translation, resulting in figures that are not coherent with the simultaneous translation policy of the system being assessed.","offset":3,"pro":0.6,"labels":"GAP"},{"idx":19,"sentence":"This work proposes a stream-level adaptation of the current latency measures based on a re-segmentation approach applied to the output translation, that is successfully evaluated on streaming conditions for a reference IWSLT task.","offset":4,"pro":0.8,"labels":"PUR"},{"idx":0,"sentence":"By using static semi-factual generation and dynamic human-intervened correction, RDL, acting like a sensible inductive bias, exploits rationales (phrases that cause the prediction), human interventions and semi-factual augmentations to decouple spurious associations and bias models towards generally applicable underlying distributions, which enables fast and accurate generalisation.","offset":0,"pro":0,"labels":"MTD"},{"idx":0,"sentence":"Experimental results show that RDL leads to significant prediction benefits on both in-distribution and out-of-distribution tests, especially for few-shot learning scenarios, compared to many state-of-the-art benchmarks.","offset":1,"pro":0.2,"labels":"RST"},{"idx":0,"sentence":"We also perform extensive ablation studies to support in-depth analyses of each component in our framework.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":0,"sentence":"We present a novel rational-centric framework ","offset":3,"pro":0.6,"labels":"PUR"},{"idx":0,"sentence":"to boost model out-of-distribution performance in few-shot learning scenarios","offset":4,"pro":0.8,"labels":"PUR"},{"idx":1,"sentence":"Various efforts in the Natural Language Processing (NLP) community have been made to accommodate linguistic diversity and serve speakers of many different languages.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"However, it is important to acknowledge that speakers and the content they produce and require, vary not just by language, but also by culture.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":1,"sentence":"Although language and culture are tightly linked, there are important differences.","offset":2,"pro":0.4,"labels":"GAP"},{"idx":1,"sentence":"Analogous to cross-lingual and multilingual NLP, cross-cultural and multicultural NLP considers these differences in order to better serve users of NLP systems.","offset":3,"pro":0.6,"labels":"BAC"},{"idx":1,"sentence":"We propose a principled framework to frame these efforts, and survey existing and potential strategies.","offset":4,"pro":0.8,"labels":"PUR"},{"idx":2,"sentence":"Prompt-based tuning for pre-trained language models (PLMs) has shown its effectiveness in few-shot learning.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"Typically, prompt-based tuning wraps the input text into a cloze question.","offset":1,"pro":0.08333333333333333,"labels":"BAC"},{"idx":2,"sentence":"However, manual verbalizers heavily depend on domain-specific prior knowledge and human efforts, while finding appropriate label words automatically still remains challenging.","offset":2,"pro":0.16666666666666666,"labels":"GAP"},{"idx":2,"sentence":"In this work, we propose the prototypical verbalizer (ProtoVerb) which is built directly from training data.","offset":3,"pro":0.25,"labels":"PUR"},{"idx":2,"sentence":"Specifically, ProtoVerb learns prototype vectors as verbalizers by contrastive learning.","offset":4,"pro":0.3333333333333333,"labels":"MTD"},{"idx":2,"sentence":"In this way, the prototypes summarize training instances and are able to enclose rich class-level semantics.","offset":5,"pro":0.4166666666666667,"labels":"MTD"},{"idx":2,"sentence":"More surprisingly, ProtoVerb consistently boosts prompt-based tuning even on untuned PLMs, indicating an elegant non-tuning way to utilize PLMs.","offset":6,"pro":0.5,"labels":"CLN"},{"idx":2,"sentence":"Our codes are avaliable at https://github.com/thunlp/OpenPrompt.","offset":7,"pro":0.5833333333333334,"labels":"CTN"},{"idx":2,"sentence":"To make predictions","offset":8,"pro":0.6666666666666666,"labels":"PUR"},{"idx":2,"sentence":" the model maps the output words to labels via a verbalizer, which is either manually designed or automatically built","offset":9,"pro":0.75,"labels":"MTD"},{"idx":2,"sentence":"We conduct experiments on both topic classification and entity typing tasks,","offset":10,"pro":0.8333333333333334,"labels":"MTD"},{"idx":2,"sentence":"and the results demonstrate that ProtoVerb significantly outperforms current automatic verbalizers, especially when training data is extremely scarce.","offset":11,"pro":0.9166666666666666,"labels":"RST"},{"idx":3,"sentence":"We introduce and study the task of clickbait spoiling: generating a short text that satisfies the curiosity induced by a clickbait post.","offset":0,"pro":0,"labels":"PUR"},{"idx":3,"sentence":"Clickbait links to a web page and advertises its contents by arousing curiosity instead of providing an informative summary.","offset":1,"pro":0.3333333333333333,"labels":"MTD"},{"idx":3,"sentence":"Our contributions are approaches to classify the type of spoiler needed (i.e., a phrase or a passage), and to generate appropriate spoilers.","offset":2,"pro":0.6666666666666666,"labels":"CTN"},{"idx":4,"sentence":"We present Knowledge Distillation with Meta Learning (MetaDistil), a simple yet effective alternative to traditional knowledge distillation (KD) methods where the teacher model is fixed during training.","offset":0,"pro":0,"labels":"PUR"},{"idx":4,"sentence":"We show the teacher network can learn to better transfer knowledge to the student network (i.e., learning to teach) with the feedback from the performance of the distilled student network in a meta learning framework.","offset":1,"pro":0.25,"labels":"RST"},{"idx":4,"sentence":"Moreover, we introduce a pilot update mechanism to improve the alignment between the inner-learner and meta-learner in meta learning algorithms that focus on an improved inner-learner.","offset":2,"pro":0.5,"labels":"PUR"},{"idx":4,"sentence":"Experiments on various benchmarks show that MetaDistil can yield significant improvements compared with traditional KD algorithms and is less sensitive to the choice of different student capacity and hyperparameters, facilitating the use of KD on different tasks and models.","offset":3,"pro":0.75,"labels":"CLN"},{"idx":5,"sentence":"How to learn a better speech representation for end-to-end speech-to-text translation (ST) with limited labeled data?","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"Existing techniques often attempt to transfer powerful machine translation (MT) capabilities to ST, but neglect the representation discrepancy across modalities.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":5,"sentence":"In this paper, we propose the Speech-TExt Manifold Mixup (STEMM) method to calibrate such discrepancy.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":5,"sentence":"Specifically, we mix up the representation sequences of different modalities, and take both unimodal speech sequences and multimodal mixed sequences as input to the translation model in parallel, and regularize their output predictions with a self-learning framework.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":5,"sentence":"Experiments on MuST-C speech translation benchmark and further analysis show that our method effectively alleviates the cross-modal representation discrepancy, and achieves significant improvements over a strong baseline on eight translation directions.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":6,"sentence":"Lexically constrained neural machine translation (NMT), which controls the generation of NMT models with pre-specified constraints, is important in many practical scenarios.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"Due to the representation gap between discrete constraints and continuous vectors in NMT models, most existing works choose to construct synthetic data or modify the decoding algorithm to impose lexical constraints, treating the NMT model as a black box.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":6,"sentence":"In this work, we propose to open this black box by directly integrating the constraints into NMT models.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":6,"sentence":"Specifically, we vectorize source and target constraints into continuous keys and values, which can be utilized by the attention modules of NMT models.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":6,"sentence":"The proposed integration method is based on the assumption that the correspondence between keys and values in attention modules is naturally suitable for modeling constraint pairs.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":6,"sentence":"Experimental results show that our method consistently outperforms several representative baselines on four language pairs, demonstrating the superiority of integrating vectorized lexical constraints.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":7,"sentence":"In order to better understand the rationale behind model behavior, recent works have exploited providing interpretation to support the inference prediction.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"However, existing methods tend to provide human-unfriendly interpretation, and are prone to sub-optimal performance due to one-side promotion, i.e. either inference promotion with interpretation or vice versa.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":7,"sentence":"In this paper, we propose a multi-level Mutual Promotion mechanism for self-evolved Inference and sentence-level Interpretation (MPII).","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":7,"sentence":"Specifically, from the model-level, we propose a Step-wise Integration Mechanism to jointly perform and deeply integrate inference and interpretation in an autoregressive manner.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":7,"sentence":"From the optimization-level, we propose an Adversarial Fidelity Regularization to improve the fidelity between inference and interpretation with the Adversarial Mutual Information training strategy.","offset":4,"pro":0.6666666666666666,"labels":"PUR"},{"idx":7,"sentence":"Extensive experiments on NLI and CQA tasks reveal that the proposed MPII approach can significantly outperform baseline models for both the inference performance and the interpretation quality.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":8,"sentence":"The Mixture-of-Experts (MoE) technique can scale up the model size of Transformers with an affordable computational overhead.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"We point out that existing learning-to-route MoE methods suffer from the routing fluctuation issue, i.e., the target expert of the same input may change along with training, but only one expert will be activated for the input during inference.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":8,"sentence":"The routing fluctuation tends to harm sample efficiency because the same input updates different experts but only one is finally used.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":8,"sentence":"In this paper, we propose StableMoE with two training stages to address the routing fluctuation problem.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":8,"sentence":"In the first training stage, we learn a balanced and cohesive routing strategy and distill it into a lightweight router decoupled from the backbone model.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":8,"sentence":"In the second training stage, we utilize the distilled router to determine the token-to-expert assignment and freeze it for a stable routing strategy.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":8,"sentence":"We validate our method on language modeling and multilingual machine translation.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":8,"sentence":"The results show that StableMoE outperforms existing MoE methods in terms of both convergence speed and performance.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":9,"sentence":"Neural named entity recognition (NER) models may easily encounter the over-confidence issue, which degrades the performance and calibration.","offset":0,"pro":0,"labels":"GAP"},{"idx":9,"sentence":"It re-assigns entity probabilities from annotated spans to the surrounding ones.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":9,"sentence":"Built on a simple but strong baseline, our model achieves results better than or competitive with previous state-of-the-art systems on eight well-known NER benchmarks.","offset":2,"pro":0.4,"labels":"RST"},{"idx":9,"sentence":"Further empirical analysis suggests that boundary smoothing effectively mitigates over-confidence, improves model calibration, and brings flatter neural minima and more smoothed loss landscapes.","offset":3,"pro":0.6,"labels":"CLN"},{"idx":9,"sentence":"we propose boundary smoothing as a regularization technique for span-based neural NER models","offset":4,"pro":0.8,"labels":"PUR"},{"idx":10,"sentence":"Hierarchical text classification is a challenging subtask of multi-label classification due to its complex label hierarchy.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"Existing methods encode text and label hierarchy separately and mix their representations for classification, where the hierarchy remains unchanged for all input text.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":10,"sentence":"Instead of modeling them separately, in this work, we propose Hierarchy-guided Contrastive Learning (HGCLR) to directly embed the hierarchy into a text encoder.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":10,"sentence":"During training, HGCLR constructs positive samples for input text under the guidance of the label hierarchy.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":10,"sentence":"By pulling together the input text and its positive sample, the text encoder can learn to generate the hierarchy-aware text representation independently.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":10,"sentence":"Therefore, after training, the HGCLR enhanced text encoder can dispense with the redundant hierarchy.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":10,"sentence":"Extensive experiments on three benchmark datasets verify the effectiveness of HGCLR.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":11,"sentence":"Natural language processing models learn word representations based on the distributional hypothesis, which asserts that word context (e.g., co-occurrence) correlates with meaning.","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"We propose that n-grams composed of random character sequences, or garble, provide a novel context for studying word meaning both within and beyond extant language.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":11,"sentence":"In particular, randomly generated character n-grams lack meaning but contain primitive information based on the distribution of characters they contain.","offset":2,"pro":0.3333333333333333,"labels":"BAC"},{"idx":11,"sentence":"By studying the embeddings of a large corpus of garble, extant language, and pseudowords using CharacterBERT, we identify an axis in the model's high-dimensional embedding space that separates these classes of n-grams.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":11,"sentence":"Furthermore, we show that this axis relates to structure within extant language, including word part-of-speech, morphology, and concept concreteness.","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":11,"sentence":"Thus, in contrast to studies that are mainly limited to extant language, our work reveals that meaning and primitive information are intrinsically linked.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":12,"sentence":"To alleviate the data scarcity problem in training question answering systems, recent works propose additional intermediate pre-training for dense passage retrieval (DPR).","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"However, there still remains a large discrepancy between the provided upstream signals and the downstream question-passage relevance, which leads to less improvement.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":12,"sentence":"To bridge this gap, we propose the HyperLink-induced Pre-training (HLP), a method to pre-train the dense retriever with the text relevance induced by hyperlink-based topology within Web documents.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":12,"sentence":"We demonstrate that the hyperlink-based structures of dual-link and co-mention can provide effective relevance signals for large-scale pre-training that better facilitate downstream passage retrieval.","offset":3,"pro":0.42857142857142855,"labels":"CLN"},{"idx":12,"sentence":"We investigate the effectiveness of our approach across a wide range of open-domain QA datasets under zero-shot, few-shot, multi-hop, and out-of-domain scenarios.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":12,"sentence":"The experiments show our HLP outperforms the BM25 by up to 7 points as well as other pre-training methods by more than 10 points in terms of top-20 retrieval accuracy under the zero-shot scenario.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":12,"sentence":"Furthermore, HLP significantly outperforms other pre-training methods under the other scenarios.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":13,"sentence":"Recent machine reading comprehension datasets such as ReClor and LogiQA require performing logical reasoning over text.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"Conventional neural models are insufficient for logical reasoning, while symbolic reasoners cannot directly apply to text.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":13,"sentence":"To meet the challenge, we present a neural-symbolic approach which, to predict an answer, passes messages over a graph representing logical relations between text units.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":13,"sentence":"It incorporates an adaptive logic graph network (AdaLoGN) which adaptively infers logical relations to extend the graph and, essentially, realizes mutual and iterative reinforcement between neural and symbolic reasoning.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":13,"sentence":"We also implement a novel subgraph-to-node message passing mechanism to enhance context-option interaction for answering multiple-choice questions.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":13,"sentence":"Our approach shows promising results on ReClor and LogiQA.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":14,"sentence":"Model ensemble is a popular approach to produce a low-variance and well-generalized model.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"However, it induces large memory and inference costs, which is often not affordable for real-world deployment.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":14,"sentence":"Existing work has resorted to sharing weights among models.","offset":2,"pro":0.2222222222222222,"labels":"BAC"},{"idx":14,"sentence":"However, when increasing the proportion of the shared weights, the resulting models tend to be similar, and the benefits of using model ensemble diminish.","offset":3,"pro":0.3333333333333333,"labels":"GAP"},{"idx":14,"sentence":"To retain ensemble benefits while maintaining a low memory cost, we propose a consistency-regularized ensemble learning approach based on perturbed models, named CAMERO.","offset":4,"pro":0.4444444444444444,"labels":"PUR"},{"idx":14,"sentence":"Specifically, we share the weights of bottom layers across all models and apply different perturbations to the hidden representations for different models, which can effectively promote the model diversity.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":14,"sentence":"Meanwhile, we apply a prediction consistency regularizer across the perturbed models to control the variance due to the model diversity.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":14,"sentence":"Our experiments using large language models demonstrate that CAMERO significantly improves the generalization performance of the ensemble model.","offset":7,"pro":0.7777777777777778,"labels":"CLN"},{"idx":14,"sentence":"Specifically, CAMERO outperforms the standard ensemble of 8 BERT-base models on the GLUE benchmark by 0.7 with a significantly smaller model size (114.2M vs. 880.6M).","offset":8,"pro":0.8888888888888888,"labels":"RST"},{"idx":15,"sentence":"Grammatical Error Correction (GEC) should not focus only on high accuracy of corrections but also on interpretability for language learning.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"However, existing neural-based GEC models mainly aim at improving accuracy, and their interpretability has not been explored.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":15,"sentence":"A promising approach for improving interpretability is an example-based method, which uses similar retrieved examples to generate corrections.","offset":2,"pro":0.25,"labels":"BAC"},{"idx":15,"sentence":"In addition, examples are beneficial in language learning, helping learners understand the basis of grammatically incorrect/correct texts and improve their confidence in writing.Therefore, we hypothesize that incorporating an example-based method into GEC can improve interpretability as well as support language learners.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":15,"sentence":"In this study, we introduce an Example-Based GEC (EB-GEC) that presents examples to language learners as a basis for a correction result.","offset":4,"pro":0.5,"labels":"PUR"},{"idx":15,"sentence":"The examples consist of pairs of correct and incorrect sentences similar to a given input and its predicted correction.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":15,"sentence":"Experiments demonstrate that the examples presented by EB-GEC help language learners decide to accept or refuse suggestions from the GEC output.","offset":6,"pro":0.75,"labels":"CLN"},{"idx":15,"sentence":"Furthermore, the experiments also show that retrieved examples improve the accuracy of corrections.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":16,"sentence":"Negative sampling is highly effective in handling missing annotations for named entity recognition (NER).","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"One of our contributions is an analysis on how it makes sense through introducing two insightful concepts: missampling and uncertainty.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":16,"sentence":"Empirical studies show low missampling rate and high uncertainty are both essential for achieving promising performances with negative sampling.","offset":2,"pro":0.2857142857142857,"labels":"CLN"},{"idx":16,"sentence":"Based on the sparsity of named entities, we also theoretically derive a lower bound for the probability of zero missampling rate, which is only relevant to sentence length.","offset":3,"pro":0.42857142857142855,"labels":"RST"},{"idx":16,"sentence":"The other contribution is an adaptive and weighted sampling distribution that further improves negative sampling via our former analysis.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":16,"sentence":"Experiments on synthetic datasets and well-annotated datasets (e.g., CoNLL-2003) show that our proposed approach benefits negative sampling in terms of F1 score and loss convergence.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":16,"sentence":"Besides, models with improved negative sampling have achieved new state-of-the-art results on real-world datasets (e.g., EC).","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":17,"sentence":"In this paper, we study the named entity recognition (NER) problem under distant supervision.","offset":0,"pro":0,"labels":"PUR"},{"idx":17,"sentence":"Due to the incompleteness of the external dictionaries and/or knowledge bases, such distantly annotated training data usually suffer from a high false negative rate.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":17,"sentence":"To this end, we formulate the Distantly Supervised NER (DS-NER) problem via Multi-class Positive and Unlabeled (MPU) learning and propose a theoretically and practically novel CONFidence-based MPU (Conf-MPU) approach.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":17,"sentence":"To handle the incomplete annotations, Conf-MPU consists of two steps.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":17,"sentence":"First, a confidence score is estimated for each token of being an entity token.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":17,"sentence":"Then, the proposed Conf-MPU risk estimation is applied to train a multi-class classifier for the NER task.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":17,"sentence":"Thorough experiments on two benchmark datasets labeled by various external knowledge demonstrate the superiority of the proposed Conf-MPU over existing DS-NER methods.","offset":6,"pro":0.75,"labels":"CLN"},{"idx":17,"sentence":"Our code is available at Github.","offset":7,"pro":0.875,"labels":"CTN"},{"idx":18,"sentence":"Pre-trained models for programming languages have recently demonstrated great success on code intelligence.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"To support both code-related understanding and generation tasks, recent works attempt to pre-train unified encoder-decoder models.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":18,"sentence":"However, such encoder-decoder framework is sub-optimal for auto-regressive tasks, especially code completion that requires a decoder-only manner for efficient inference.","offset":2,"pro":0.2,"labels":"GAP"},{"idx":18,"sentence":"In this paper, we present UniXcoder, a unified cross-modal pre-trained model for programming language.","offset":3,"pro":0.3,"labels":"PUR"},{"idx":18,"sentence":"The model utilizes mask attention matrices with prefix adapters to control the behavior of the model and leverages cross-modal contents like AST and code comment to enhance code representation.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":18,"sentence":"To encode AST that is represented as a tree in parallel, we propose a one-to-one mapping method to transform AST in a sequence structure that retains all structural information from the tree.","offset":5,"pro":0.5,"labels":"PUR"},{"idx":18,"sentence":"Furthermore, we propose to utilize multi-modal contents to learn representation of code fragment with contrastive learning, and then align representations among programming languages using a cross-modal generation task.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":18,"sentence":"We evaluate UniXcoder on five code-related tasks over nine datasets.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":18,"sentence":"To further evaluate the performance of code fragment representation, we also construct a dataset for a new task, called zero-shot code-to-code search.","offset":8,"pro":0.8,"labels":"PUR"},{"idx":18,"sentence":"Results show that our model achieves state-of-the-art performance on most tasks and analysis reveals that comment and AST can both enhance UniXcoder.","offset":9,"pro":0.9,"labels":"CLN"},{"idx":19,"sentence":"NLP research is impeded by a lack of resources and awareness of the challenges presented by underrepresented languages and dialects.","offset":0,"pro":0,"labels":"GAP"},{"idx":19,"sentence":"Focusing on the languages spoken in Indonesia, the second most linguistically diverse and the fourth most populous nation of the world, we provide an overview of the current state of NLP research for Indonesia's 700+ languages.","offset":1,"pro":0.25,"labels":"PUR"},{"idx":19,"sentence":"We highlight challenges in Indonesian NLP and how these affect the performance of current NLP systems.","offset":2,"pro":0.5,"labels":"PUR"},{"idx":19,"sentence":"Finally, we provide general recommendations to help develop NLP technology not only for languages of Indonesia but also other underrepresented languages.","offset":3,"pro":0.75,"labels":"MTD"},{"idx":0,"sentence":"Modern neural language models can produce remarkably fluent and grammatical text.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"So much, in fact, that recent work by Clark et al. (2021) has reported that conventional crowdsourcing can no longer reliably distinguish between machine-authored (GPT-3) and human-authored writing.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":0,"sentence":"As errors in machine generations become ever subtler and harder to spot, it poses a new challenge to the research community for robust machine text evaluation.","offset":2,"pro":0.2,"labels":"GAP"},{"idx":0,"sentence":"We propose a new framework called Scarecrow for scrutinizing machine text via crowd annotation.","offset":3,"pro":0.3,"labels":"PUR"},{"idx":0,"sentence":"To support the broad range of real machine errors that can be identified by laypeople, the ten error categories of Scarecrow-such as redundancy, commonsense errors, and incoherence-are identified through several rounds of crowd annotation experiments without a predefined ontology.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":0,"sentence":"We then use Scarecrow to collect over 41k error spans in human-written and machine-generated paragraphs of English language news text.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":0,"sentence":"We isolate factors for detailed analysis, including parameter count, training data, and various decoding-time configurations.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":0,"sentence":"Our approach successfully quantifies measurable gaps between human authored text and generations from models of several sizes, including fourteen configurations of GPT-3.","offset":7,"pro":0.7,"labels":"CLN"},{"idx":0,"sentence":"In addition, our analysis unveils new insights, with detailed rationales provided by laypeople, e.g., that the commonsense capabilities have been improving with larger models while math capabilities have not, and that the choices of simple decoding hyperparameters can make remarkable differences on the perceived quality of machine text.","offset":8,"pro":0.8,"labels":"CLN"},{"idx":0,"sentence":"We release our training material, annotation toolkit and dataset at https://yao-dou.github.io/scarecrow/.","offset":9,"pro":0.9,"labels":"CTN"},{"idx":1,"sentence":"Transformer architecture has become the de-facto model for many machine learning tasks from natural language processing and computer vision.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"As such, improving its computational efficiency becomes paramount.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":1,"sentence":"One of the major computational inefficiency of Transformer based models is that they spend the identical amount of computation throughout all layers.","offset":2,"pro":0.2,"labels":"BAC"},{"idx":1,"sentence":"However, they suffer from not having effectual and end-to-end optimization of the discrete skimming predictor.","offset":3,"pro":0.3,"labels":"GAP"},{"idx":1,"sentence":"To address the above limitations, we propose the Transkimmer architecture, which learns to identify hidden state tokens that are not required by each layer.","offset":4,"pro":0.4,"labels":"PUR"},{"idx":1,"sentence":"The skimmed tokens are then forwarded directly to the final output, thus reducing the computation of the successive layers.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":1,"sentence":"The key idea in Transkimmer is to add a parameterized predictor before each layer that learns to make the skimming decision.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":1,"sentence":"We also propose to adopt reparameterization trick and add skim loss for the end-to-end training of Transkimmer.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":1,"sentence":"Transkimmer achieves 10.97x average speedup on GLUE benchmark compared with vanilla BERT-base baseline with less than 1% accuracy degradation.","offset":8,"pro":0.8,"labels":"RST"},{"idx":1,"sentence":"Prior works have proposed to augment the Transformer model with the capability of skimming tokens to improve its computational efficiency.","offset":9,"pro":0.9,"labels":"BAC"},{"idx":2,"sentence":"In this paper, we propose SkipBERT to accelerate BERT inference by skipping the computation of shallow layers.","offset":0,"pro":0,"labels":"PUR"},{"idx":2,"sentence":"To achieve this, our approach encodes small text chunks into independent representations, which are then materialized to approximate the shallow representation of BERT.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":2,"sentence":"Since the use of such approximation is inexpensive compared with transformer calculations, we leverage it to replace the shallow layers of BERT to skip their runtime overhead.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":2,"sentence":"With off-the-shelf early exit mechanisms, we also skip redundant computation from the highest few layers to further improve inference efficiency.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":2,"sentence":"Results on GLUE show that our approach can reduce latency by 65% without sacrificing performance.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":2,"sentence":"By using only two-layer transformer calculations, we can still maintain 95% accuracy of BERT.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":3,"sentence":"We investigate what kind of structural knowledge learned in neural network encoders is transferable to processing natural language.","offset":0,"pro":0,"labels":"PUR"},{"idx":3,"sentence":"We design artificial languages with structural properties that mimic natural language, pretrain encoders on the data, and see how much performance the encoder exhibits on downstream tasks in natural language.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":3,"sentence":"Our experimental results show that pretraining with an artificial language with a nesting dependency structure provides some knowledge transferable to natural language.","offset":2,"pro":0.4,"labels":"CLN"},{"idx":3,"sentence":"A follow-up probing analysis indicates that its success in the transfer is related to the amount of encoded contextual information and what is transferred is the knowledge of position-aware context dependence of language.","offset":3,"pro":0.6,"labels":"CLN"},{"idx":3,"sentence":"Our results provide insights into how neural network encoders process human languages and the source of cross-lingual transferability of recent multilingual language models.","offset":4,"pro":0.8,"labels":"CTN"},{"idx":4,"sentence":"Recent studies have shown that multilingual pretrained language models can be effectively improved with cross-lingual alignment information from Wikipedia entities.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"However, existing methods only exploit entity information in pretraining and do not explicitly use entities in downstream tasks.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":4,"sentence":"In this study, we explore the effectiveness of leveraging entity representations for downstream cross-lingual tasks.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":4,"sentence":"We train a multilingual language model with 24 languages with entity representations and showthe model consistently outperforms word-based pretrained models in various cross-lingual transfer tasks.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":4,"sentence":"We also analyze the model and the key insight is that incorporating entity representations into the input allows us to extract more language-agnostic features.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":4,"sentence":"We also evaluate the model with a multilingual cloze prompt task with the mLAMA dataset.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":4,"sentence":"We show that entity-based prompt elicits correct factual knowledge more likely than using only word representations.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":5,"sentence":"Automated simplification models aim to make input texts more readable.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"Such methods have the potential to make complex information accessible to a wider audience, e.g., providing access to recent medical literature which might otherwise be impenetrable for a lay reader.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":5,"sentence":"However, such models risk introducing errors into automatically simplified texts, for instance by inserting statements unsupported by the corresponding original text, or by omitting key information.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":5,"sentence":"Providing more readable but inaccurate versions of texts may in many cases be worse than providing no such access at all.","offset":3,"pro":0.42857142857142855,"labels":"GAP"},{"idx":5,"sentence":"The problem of factual accuracy (and the lack thereof) has received heightened attention in the context of summarization models, but the factuality of automatically simplified texts has not been investigated.","offset":4,"pro":0.5714285714285714,"labels":"GAP"},{"idx":5,"sentence":"We introduce a taxonomy of errors that we use to analyze both references drawn from standard simplification datasets and state-of-the-art model outputs.","offset":5,"pro":0.7142857142857143,"labels":"PUR"},{"idx":5,"sentence":"We find that errors often appear in both that are not captured by existing evaluation metrics,","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":6,"sentence":"This paper describes the motivation and development of speech synthesis systems for the purposes of language revitalization.","offset":0,"pro":0,"labels":"PUR"},{"idx":6,"sentence":"By building speech synthesis systems for three Indigenous languages spoken in Canada, we re-evaluate the question of how much data is required to build low-resource speech synthesis systems featuring state-of-the-art neural models.","offset":1,"pro":0.25,"labels":"MTD"},{"idx":6,"sentence":"For example, preliminary results with English data show that a FastSpeech2 model trained with 1 hour of training data can produce speech with comparable naturalness to a Tacotron2 model trained with 10 hours of data.","offset":2,"pro":0.5,"labels":"CLN"},{"idx":6,"sentence":"Finally, we motivate future research in evaluation and classroom integration in the field of speech synthesis for language revitalization.","offset":3,"pro":0.75,"labels":"IMP"},{"idx":7,"sentence":"The allure of superhuman-level capabilities has led to considerable interest in language models like GPT-3 and T5, wherein the research has, by and large, revolved around new model architectures, training tasks, and loss objectives, along with substantial engineering efforts to scale up model capacity and dataset size.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"Comparatively little work has been done to improve the generalization of these models through better optimization.","offset":1,"pro":0.25,"labels":"GAP"},{"idx":7,"sentence":"In this work, we show that Sharpness-Aware Minimization (SAM), a recently proposed optimization procedure that encourages convergence to flatter minima, can substantially improve the generalization of language models without much computational overhead.","offset":2,"pro":0.5,"labels":"PUR"},{"idx":7,"sentence":"We show that SAM is able to boost performance on SuperGLUE, GLUE, Web Questions, Natural Questions, Trivia QA, and TyDiQA, with particularly large gains when training data for these tasks is limited.","offset":3,"pro":0.75,"labels":"RST"},{"idx":8,"sentence":"Recent advances in natural language processing have enabled powerful privacy-invasive authorship attribution.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"To counter authorship attribution, researchers have proposed a variety of rule-based and learning-based text obfuscation approaches.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":8,"sentence":"However, existing authorship obfuscation approaches do not consider the adversarial threat model.","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":8,"sentence":"Specifically, they are not evaluated against adversarially trained authorship attributors that are aware of potential obfuscation.","offset":3,"pro":0.3333333333333333,"labels":"GAP"},{"idx":8,"sentence":"To fill this gap, we investigate the problem of adversarial authorship attribution for deobfuscation.","offset":4,"pro":0.4444444444444444,"labels":"PUR"},{"idx":8,"sentence":"We show that adversarially trained authorship attributors are able to degrade the effectiveness of existing obfuscators from 20-30% to 5-10%.","offset":5,"pro":0.5555555555555556,"labels":"RST"},{"idx":8,"sentence":"We also evaluate the effectiveness of adversarial training when the attributor makes incorrect assumptions about whether and which obfuscator was used.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":8,"sentence":"While there is a a clear degradation in attribution accuracy, it is noteworthy that this degradation is still at or above the attribution accuracy of the attributor that is not adversarially trained at all.","offset":7,"pro":0.7777777777777778,"labels":"CLN"},{"idx":8,"sentence":"Our results motivate the need to develop authorship obfuscation approaches that are resistant to deobfuscation.","offset":8,"pro":0.8888888888888888,"labels":"IMP"},{"idx":9,"sentence":"Word and morpheme segmentation are fundamental steps of language documentation as they allow to discover lexical units in a language for which the lexicon is unknown.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"However, in most language documentation scenarios, linguists do not start from a blank page: they may already have a pre-existing dictionary or have initiated manual segmentation of a small part of their data.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":9,"sentence":"This paper studies how such a weak supervision can be taken advantage of in Bayesian non-parametric models of segmentation.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":9,"sentence":"Our experiments on two very low resource languages (Mboshi and Japhug), whose documentation is still in progress, show that weak supervision can be beneficial to the segmentation quality.","offset":3,"pro":0.5,"labels":"CLN"},{"idx":9,"sentence":"In addition, we investigate an incremental learning scenario where manual segmentations are provided in a sequential manner.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":9,"sentence":"This work opens the way for interactive annotation tools for documentary linguists.","offset":5,"pro":0.8333333333333334,"labels":"CTN"},{"idx":10,"sentence":"Existing Natural Language Inference (NLI) datasets, while being instrumental in the advancement of Natural Language Understanding (NLU) research, are not related to scientific text.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"In this paper, we introduce SciNLI, a large dataset for NLI that captures the formality in scientific text and contains 107,412 sentence pairs extracted from scholarly papers on NLP and computational linguistics.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":10,"sentence":"Given that the text used in scientific literature differs vastly from the text used in everyday language both in terms of vocabulary and sentence structure, our dataset is well suited to serve as a benchmark for the evaluation of scientific NLU models.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":10,"sentence":"Our experiments show that SciNLI is harder to classify than the existing NLI datasets.","offset":3,"pro":0.6,"labels":"CLN"},{"idx":10,"sentence":"Our best performing model with XLNet achieves a Macro F1 score of only 78.18% and an accuracy of 78.23% showing that there is substantial room for improvement.","offset":4,"pro":0.8,"labels":"RST"},{"idx":11,"sentence":"In lexicalist linguistic theories, argument structure is assumed to be predictable from the meaning of verbs.","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"As a result, the verb is the primary determinant of the meaning of a clause.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":11,"sentence":"Two decades of psycholinguistic research have produced substantial empirical evidence in favor of the construction view.","offset":2,"pro":0.2222222222222222,"labels":"BAC"},{"idx":11,"sentence":"Here we adapt several psycholinguistic studies to probe for the existence of argument structure constructions (ASCs) in Transformer-based language models (LMs).","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":11,"sentence":"First, using a sentence sorting experiment, we find that sentences sharing the same construction are closer in embedding space than sentences sharing the same verb.","offset":4,"pro":0.4444444444444444,"labels":"CLN"},{"idx":11,"sentence":"Furthermore, LMs increasingly prefer grouping by construction with more input data, mirroring the behavior of non-native language learners.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":11,"sentence":"Second, in a Jabberwocky priming-based experiment, we find that LMs associate ASCs with meaning, even in semantically nonsensical sentences.","offset":6,"pro":0.6666666666666666,"labels":"CLN"},{"idx":11,"sentence":"Our work offers the first evidence for ASCs in LMs and highlights the potential to devise novel probing methods grounded in psycholinguistic research.","offset":7,"pro":0.7777777777777778,"labels":"CTN"},{"idx":11,"sentence":"In contrast, construction grammarians propose that argument structure is encoded in constructions (or form-meaning pairs) that are distinct from verbs.","offset":8,"pro":0.8888888888888888,"labels":"BAC"},{"idx":12,"sentence":"Social media platforms are deploying machine learning based offensive language classification systems to combat hateful, racist, and other forms of offensive speech at scale.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"However, despite their real-world deployment, we do not yet comprehensively understand the extent to which offensive language classifiers are robust against adversarial attacks.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":12,"sentence":"Prior work in this space is limited to studying robustness of offensive language classifiers against primitive attacks such as misspellings and extraneous spaces.","offset":2,"pro":0.4,"labels":"GAP"},{"idx":12,"sentence":"To address this gap, we systematically analyze the robustness of state-of-the-art offensive language classifiers against more crafty adversarial attacks that leverage greedy- and attention-based word selection and context-aware embeddings for word replacement.","offset":3,"pro":0.6,"labels":"PUR"},{"idx":12,"sentence":"Our results on multiple datasets show that these crafty adversarial attacks can degrade the accuracy of offensive language classifiers by more than 50% while also being able to preserve the readability and meaning of the modified text.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":13,"sentence":"Style transfer is the task of rewriting a sentence into a target style while approximately preserving content.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"While most prior literature assumes access to a large style-labelled corpus, recent work (Riley et al. 2021) has attempted few-shot style transfer using only 3-10 sentences at inference for style extraction.","offset":1,"pro":0.1,"labels":"GAP"},{"idx":13,"sentence":"In this work we study a relevant low-resource setting: style transfer for languages where no style-labelled corpora are available.","offset":2,"pro":0.2,"labels":"PUR"},{"idx":13,"sentence":"We notice that existing few-shot methods perform this task poorly, often copying inputs verbatim.","offset":3,"pro":0.3,"labels":"RST"},{"idx":13,"sentence":"We push the state-of-the-art for few-shot style transfer with a new method modeling the stylistic difference between paraphrases.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":13,"sentence":"When compared to prior work, our model achieves 2-3x better performance in formality transfer and code-mixing addition across seven languages.","offset":5,"pro":0.5,"labels":"CLN"},{"idx":13,"sentence":"Moreover, our method is better at controlling the style transfer magnitude using an input scalar knob.","offset":6,"pro":0.6,"labels":"CLN"},{"idx":13,"sentence":"We report promising qualitative results for several attribute transfer tasks (sentiment transfer, simplification, gender neutralization, text anonymization) all without retraining the model.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":13,"sentence":"Finally, we find model evaluation to be difficult due to the lack of datasets and metrics for many languages.","offset":8,"pro":0.8,"labels":"CLN"},{"idx":13,"sentence":"To facilitate future research we crowdsource formality annotations for 4000 sentence pairs in four Indic languages, and use this data to design our automatic evaluations.","offset":9,"pro":0.9,"labels":"MTD"},{"idx":14,"sentence":"Transformer architectures have achieved state- of-the-art results on a variety of natural language processing (NLP) tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"However, their attention mechanism comes with a quadratic complexity in sequence lengths, making the computational overhead prohibitive, especially for long sequences.","offset":1,"pro":0.09090909090909091,"labels":"GAP"},{"idx":14,"sentence":"Attention context can be seen as a random-access memory with each token taking a slot.","offset":2,"pro":0.18181818181818182,"labels":"BAC"},{"idx":14,"sentence":"Under this perspective, the memory size grows linearly with the sequence length, and so does the overhead of reading from it.","offset":3,"pro":0.2727272727272727,"labels":"BAC"},{"idx":14,"sentence":"One way to improve the efficiency is to bound the memory size.","offset":4,"pro":0.36363636363636365,"labels":"BAC"},{"idx":14,"sentence":"We show that disparate approaches can be subsumed into one abstraction, attention with bounded-memory control (ABC), and they vary in their organization of the memory.","offset":5,"pro":0.45454545454545453,"labels":"PUR"},{"idx":14,"sentence":"ABC reveals new, unexplored possibilities.","offset":6,"pro":0.5454545454545454,"labels":"RST"},{"idx":14,"sentence":"First, it connects several efficient attention variants that would otherwise seem apart.","offset":7,"pro":0.6363636363636364,"labels":"MTD"},{"idx":14,"sentence":"Second, this abstraction gives new insights-an established approach (Wang et al., 2020b) previously thought to not be applicable in causal attention, actually is.","offset":8,"pro":0.7272727272727273,"labels":"MTD"},{"idx":14,"sentence":"Last, we present a new instance of ABC, which draws inspiration from existing ABC approaches, but replaces their heuristic memory-organizing functions with a learned, contextualized one.","offset":9,"pro":0.8181818181818182,"labels":"MTD"},{"idx":14,"sentence":"Our experiments on language modeling, machine translation, and masked language model finetuning show that our approach outperforms previous efficient attention models; compared to the strong transformer baselines, it significantly improves the inference time and space efficiency with no or negligible accuracy loss.","offset":10,"pro":0.9090909090909091,"labels":"CLN"},{"idx":15,"sentence":"Researchers in NLP often frame and discuss research results in ways that serve to deemphasize the field's successes, often in response to the field's widespread hype.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"Though well-meaning, this has yielded many misleading or false claims about the limits of our best technology.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":15,"sentence":"This is a problem, and it may be more serious than it looks: It harms our credibility in ways that can make it harder to mitigate present-day harms, like those involving biased systems for content moderation or resume screening.","offset":2,"pro":0.4,"labels":"GAP"},{"idx":15,"sentence":"It also limits our ability to prepare for the potentially enormous impacts of more distant future advances.","offset":3,"pro":0.6,"labels":"GAP"},{"idx":15,"sentence":"This paper urges researchers to be careful about these claims and suggests some research directions and communication strategies that will make it easier to avoid or rebut them.","offset":4,"pro":0.8,"labels":"PUR"},{"idx":16,"sentence":"Humanities scholars commonly provide evidence for claims that they make about a work of literature (e.g., a novel) in the form of quotations from the work.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"We collect a large-scale dataset (RELiC) of 78K literary quotations and surrounding critical analysis and use it to formulate the novel task of literary evidence retrieval, in which models are given an excerpt of literary analysis surrounding a masked quotation and asked to retrieve the quoted passage from the set of all passages in the work.","offset":1,"pro":0.25,"labels":"MTD"},{"idx":16,"sentence":"Solving this retrieval task requires a deep understanding of complex literary and linguistic phenomena, which proves challenging to methods that overwhelmingly rely on lexical and semantic similarity matching.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":16,"sentence":"We implement a RoBERTa-based dense passage retriever for this task that outperforms existing pretrained information retrieval baselines;","offset":3,"pro":0.75,"labels":"MTD"},{"idx":17,"sentence":"Vision and language navigation (VLN) is a challenging visually-grounded language understanding task.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"Given a natural language navigation instruction, a visual agent interacts with a graph-based environment equipped with panorama images and tries to follow the described route.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":17,"sentence":"Most prior work has been conducted in indoor scenarios where best results were obtained for navigation on routes that are similar to the training routes, with sharp drops in performance when testing on unseen environments.","offset":2,"pro":0.4,"labels":"BAC"},{"idx":17,"sentence":"We focus on VLN in outdoor scenarios and find that in contrast to indoor VLN, most of the gain in outdoor VLN on unseen data is due to features like junction type embedding or heading delta that are specific to the respective environment graph, while image information plays a very minor role in generalizing VLN to unseen outdoor areas.","offset":3,"pro":0.6,"labels":"PUR"},{"idx":17,"sentence":"These findings show a bias to specifics of graph representations of urban environments, demanding that VLN tasks grow in scale and diversity of geographical environments.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":18,"sentence":"Neural coreference resolution models trained on one dataset may not transfer to new, low-resource domains.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"Active learning mitigates this problem by sampling a small subset of data for annotators to label.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":18,"sentence":"While active learning is well-defined for classification tasks, its application to coreference resolution is neither well-defined nor fully understood.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":18,"sentence":"This paper explores how to actively label coreference, examining sources of model uncertainty and document reading costs.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":18,"sentence":"We compare uncertainty sampling strategies and their advantages through thorough error analysis.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":18,"sentence":"In both synthetic and human experiments, labeling spans within the same document is more effective than annotating spans across documents.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":18,"sentence":"The findings contribute to a more realistic development of coreference resolution models.","offset":6,"pro":0.8571428571428571,"labels":"IMP"},{"idx":19,"sentence":"We propose a framework for training non-autoregressive sequence-to-sequence models for editing tasks, where the original input sequence is iteratively edited to produce the output.","offset":0,"pro":0,"labels":"PUR"},{"idx":19,"sentence":"We show that the imitation learning algorithms designed to train such models for machine translation introduces mismatches between training and inference that lead to undertraining and poor generalization in editing scenarios.","offset":1,"pro":0.2,"labels":"CLN"},{"idx":19,"sentence":"We address this issue with two complementary strategies: 1) a roll-in policy that exposes the model to intermediate training sequences that it is more likely to encounter during inference, 2) a curriculum that presents easy-to-learn edit operations first, gradually increasing the difficulty of training samples as the model becomes competent.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":19,"sentence":"We show the efficacy of these strategies on two challenging English editing tasks: controllable text simplification and abstractive summarization.","offset":3,"pro":0.6,"labels":"CLN"},{"idx":19,"sentence":"Our approach significantly improves output quality on both tasks and controls output complexity better on the simplification task.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":0,"sentence":"State-of-the-art pre-trained language models have been shown to memorise facts and perform well with limited amounts of training data.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"To gain a better understanding of how these models learn, we study their generalisation and memorisation capabilities in noisy and low-resource scenarios.","offset":1,"pro":0.25,"labels":"PUR"},{"idx":0,"sentence":"We find that the training of these models is almost unaffected by label noise and that it is possible to reach near-optimal results even on extremely noisy datasets.","offset":2,"pro":0.5,"labels":"RST"},{"idx":0,"sentence":"However, our experiments also show that they mainly learn from high-frequency patterns and largely fail when tested on low-resource tasks such as few-shot learning and rare entity recognition.","offset":3,"pro":0.75,"labels":"RST"},{"idx":1,"sentence":"Existing automatic evaluation systems of chatbots mostly rely on static chat scripts as ground truth, which is hard to obtain, and requires access to the models of the bots as a form of white-box testing.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"Interactive evaluation mitigates this problem but requires human involvement.","offset":1,"pro":0.3333333333333333,"labels":"BAC"},{"idx":1,"sentence":"In our work, we propose an interactive chatbot evaluation framework in which chatbots compete with each other like in a sports tournament, using flexible scoring metrics.","offset":2,"pro":0.6666666666666666,"labels":"PUR"},{"idx":2,"sentence":"Self-supervised models for speech processing form representational spaces without using any external labels.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"We examine the representational spaces of three kinds of state of the art self-supervised models: wav2vec, HuBERT and contrastive predictive coding (CPC), and compare them with the perceptual spaces of French-speaking and English-speaking human listeners, both globally and taking account of the behavioural differences between the two language groups.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":2,"sentence":"We show that the CPC model shows a small native language effect, but that wav2vec and HuBERT seem to develop a universal speech perception space which is not language specific.","offset":2,"pro":0.3333333333333333,"labels":"RST"},{"idx":2,"sentence":"A comparison against the predictions of supervised phone recognisers suggests that all three self-supervised models capture relatively fine-grained perceptual phenomena, while supervised models are better at capturing coarser, phone-level effects, and effects of listeners' native language, on perception.","offset":3,"pro":0.5,"labels":"CLN"},{"idx":2,"sentence":"Increasingly, they appear to be a feasible way of at least partially eliminating costly manual annotations, a problem of particular concern for low-resource languages.","offset":4,"pro":0.6666666666666666,"labels":"BAC"},{"idx":2,"sentence":"But what kind of representational spaces do these models construct?","offset":5,"pro":0.8333333333333334,"labels":"PUR"},{"idx":3,"sentence":"A long-term goal of AI research is to build intelligent agents that can communicate with humans in natural language, perceive the environment, and perform real-world tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"Vision-and-Language Navigation (VLN) is a fundamental and interdisciplinary research topic towards this goal, and receives increasing attention from natural language processing, computer vision, robotics, and machine learning communities.","offset":1,"pro":0.25,"labels":"BAC"},{"idx":3,"sentence":"In this paper, we review contemporary studies in the emerging field of VLN, covering tasks, evaluation metrics, methods, etc. Through structured analysis of current progress and challenges, we also highlight the limitations of current VLN and opportunities for future work.","offset":2,"pro":0.5,"labels":"PUR"},{"idx":3,"sentence":"This paper serves as a thorough reference for the VLN research community.","offset":3,"pro":0.75,"labels":"CTN"},{"idx":4,"sentence":"Table fact verification aims to check the correctness of textual statements based on given semi-structured data.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"Most existing methods are devoted to better comprehending logical operations and tables, but they hardly study generating latent programs from statements, with which we can not only retrieve evidences efficiently but also explain reasons behind verifications naturally.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":4,"sentence":"However, it is challenging to get correct programs with existing weakly supervised semantic parsers due to the huge search space with lots of spurious programs.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":4,"sentence":"In this paper, we address the challenge by leveraging both lexical features and structure features for program generation.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":4,"sentence":"Through analyzing the connection between the program tree and the dependency tree, we define a unified concept, operation-oriented tree, to mine structure features, and introduce Structure-Aware Semantic Parsing to integrate structure features into program generation.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":4,"sentence":"Moreover, we design a refined objective function with lexical features and violation punishments to further avoid spurious programs.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":4,"sentence":"Experimental results show that our proposed method generates programs more accurately than existing semantic parsers, and achieves comparable performance to the SOTA on the large-scale benchmark TABFACT.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":5,"sentence":"In real-world scenarios, a text classification task often begins with a cold start, when labeled data is scarce.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"In such cases, the common practice of fine-tuning pre-trained models, such as BERT, for a target classification task, is prone to produce poor performance.","offset":1,"pro":0.25,"labels":"GAP"},{"idx":5,"sentence":"We suggest a method to boost the performance of such models by adding an intermediate unsupervised classification task, between the pre-training and fine-tuning phases.","offset":2,"pro":0.5,"labels":"PUR"},{"idx":5,"sentence":"As such an intermediate task, we perform clustering and train the pre-trained model on predicting the cluster labels.","offset":3,"pro":0.75,"labels":"MTD"},{"idx":6,"sentence":"Second, we use layer normalization to bring the cross-entropy of both models arbitrarily close to zero.","offset":0,"pro":0,"labels":"MTD"},{"idx":6,"sentence":"We demonstrate three ways of overcoming the limitation implied by Hahn's lemma.","offset":1,"pro":0.125,"labels":"MTD"},{"idx":6,"sentence":"First, we settle an open question by constructing a transformer that recognizes PARITY with perfect accuracy, and similarly for FIRST.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":6,"sentence":"Third, when transformers need to focus on a single position, as for FIRST, we find that they can fail to generalize to longer strings;","offset":3,"pro":0.375,"labels":"MTD"},{"idx":6,"sentence":"we offer a simple remedy to this problem that also improves length generalization in machine translation.","offset":4,"pro":0.5,"labels":"CTN"},{"idx":6,"sentence":"Although transformers are remarkably effective for many tasks, there are some surprisingly easy-looking regular languages that they struggle with.","offset":5,"pro":0.625,"labels":"GAP"},{"idx":6,"sentence":"Hahn shows that for languages where acceptance depends on a single input symbol, a transformer's classification decisions get closer and closer to random guessing (that is, a cross-entropy of 1) as input strings get longer and longer.","offset":6,"pro":0.75,"labels":"GAP"},{"idx":6,"sentence":"We examine this limitation using two languages: PARITY, the language of bit strings with an odd number of 1s, and FIRST, the language of bit strings starting with a 1.","offset":7,"pro":0.875,"labels":"PUR"},{"idx":7,"sentence":"Regularization methods applying input perturbation have drawn considerable attention and have been frequently explored for NMT tasks in recent years.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"Despite their simplicity and effectiveness, we argue that these methods are limited by the under-fitting of training data.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":7,"sentence":"In this paper, we utilize prediction difference for ground-truth tokens to analyze the fitting of token-level samples and find that under-fitting is almost as common as over-fitting.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":7,"sentence":"We introduce prediction difference regularization (PD-R), a simple and effective method that can reduce over-fitting and under-fitting at the same time.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":7,"sentence":"For all token-level samples, PD-R minimizes the prediction difference between the original pass and the input-perturbed pass, making the model less sensitive to small input changes, thus more robust to both perturbations and under-fitted training data.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":7,"sentence":"Experiments on three widely used WMT translation tasks show that our approach can significantly improve over existing perturbation regularization methods.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":7,"sentence":"On WMT16 En-De task, our model achieves 1.80 SacreBLEU improvement over vanilla transformer.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":8,"sentence":"Cross-lingual transfer learning with large multilingual pre-trained models can be an effective approach for low-resource languages with no labeled training data.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"Existing evaluations of zero-shot cross-lingual generalisability of large pre-trained models use datasets with English training data, and test data in a selection of target languages.","offset":1,"pro":0.25,"labels":"BAC"},{"idx":8,"sentence":"We explore a more extensive transfer learning setup with 65 different source languages and 105 target languages for part-of-speech tagging.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":8,"sentence":"Through our analysis, we show that pre-training of both source and target language, as well as matching language families, writing systems, word order systems, and lexical-phonetic distance significantly impact cross-lingual performance.","offset":3,"pro":0.75,"labels":"RST"},{"idx":9,"sentence":"Previous sarcasm generation research has focused on how to generate text that people perceive as sarcastic to create more human-like interactions.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"Next, we use a theory-driven framework for generating sarcastic responses, which allows us to control the linguistic devices included during generation.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":9,"sentence":"In this paper, we argue that we should first turn our attention to the question of when sarcasm should be generated,","offset":2,"pro":0.4,"labels":"PUR"},{"idx":9,"sentence":"finding that humans consider sarcastic responses inappropriate to many input utterances.","offset":3,"pro":0.6,"labels":"RST"},{"idx":9,"sentence":"For each device, we investigate how much humans associate it with sarcasm,","offset":4,"pro":0.8,"labels":"MTD"},{"idx":10,"sentence":"With the rapid development of deep learning, Seq2Seq paradigm has become prevalent for end-to-end data-to-text generation, and the BLEU scores have been increasing in recent years.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"However, it is widely recognized that there is still a gap between the quality of the texts generated by models and the texts written by human.","offset":1,"pro":0.25,"labels":"GAP"},{"idx":10,"sentence":"In order to better understand the ability of Seq2Seq models, evaluate their performance and analyze the results, we choose to use Multidimensional Quality Metric(MQM) to evaluate several representative Seq2Seq models on end-to-end data-to-text generation.","offset":2,"pro":0.5,"labels":"PUR"},{"idx":10,"sentence":"We annotate the outputs of five models on four datasets with eight error types","offset":3,"pro":0.75,"labels":"MTD"},{"idx":11,"sentence":"Probing has become an important tool for analyzing representations in Natural Language Processing (NLP).","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"For graphical NLP tasks such as dependency parsing, linear probes are currently limited to extracting undirected or unlabeled parse trees which do not capture the full task.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":11,"sentence":"This work introduces DepProbe, a linear probe which can extract labeled and directed dependency parse trees from embeddings while using fewer parameters and compute than prior methods.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":11,"sentence":"Leveraging its full task coverage and lightweight parametrization, we investigate its predictive power for selecting the best transfer language for training a full biaffine attention parser.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":11,"sentence":"Across 13 languages, our proposed method identifies the best source treebank 94% of the time, outperforming competitive baselines and prior work.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":11,"sentence":"Finally, we analyze the informativeness of task-specific subspaces in contextual embeddings as well as which benefits a full parser's non-linear parametrization provides.","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":12,"sentence":"Natural language processing (NLP) algorithms have become very successful, but they still struggle when applied to out-of-distribution examples.","offset":0,"pro":0,"labels":"GAP"},{"idx":12,"sentence":"In this paper we propose a controllable generation approach in order to deal with this domain adaptation (DA) challenge.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":12,"sentence":"Given an input text example, our DoCoGen algorithm generates a domain-counterfactual textual example (D-con) - that is similar to the original in all aspects, including the task label, but its domain is changed to a desired one.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":12,"sentence":"Importantly, DoCoGen is trained using only unlabeled examples from multiple domains - no NLP task labels or parallel pairs of textual examples and their domain-counterfactuals are required.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":12,"sentence":"We show that DoCoGen can generate coherent counterfactuals consisting of multiple sentences.","offset":4,"pro":0.5714285714285714,"labels":"CLN"},{"idx":12,"sentence":"We use the D-cons generated by DoCoGen to augment a sentiment classifier and a multi-label intent classifier in 20 and 78 DA setups, respectively, where source-domain labeled data is scarce.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":12,"sentence":"Our model outperforms strong baselines and improves the accuracy of a state-of-the-art unsupervised DA algorithm.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":13,"sentence":"Structured document understanding has attracted considerable attention and made significant progress recently, owing to its crucial role in intelligent document processing.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"However, most existing related models can only deal with the document data of specific language(s) (typically English) included in the pre-training collection, which is extremely limited.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":13,"sentence":"To address this issue, we propose a simple yet effective Language-independent Layout Transformer (LiLT) for structured document understanding.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":13,"sentence":"LiLT can be pre-trained on the structured documents of a single language and then directly fine-tuned on other languages with the corresponding off-the-shelf monolingual/multilingual pre-trained textual models.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":13,"sentence":"Experimental results on eight languages have shown that LiLT can achieve competitive or even superior performance on diverse widely-used downstream benchmarks, which enables language-independent benefit from the pre-training of document layout structure.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":13,"sentence":"Code and model are publicly available at https://github.com/jpWang/LiLT.","offset":5,"pro":0.8333333333333334,"labels":"CTN"},{"idx":14,"sentence":"However, previous works have relied heavily on elaborate components for a specific language model, usually recurrent neural network (RNN), which makes themselves unwieldy in practice to fit into other neural language models, such as Transformer and GPT-2.","offset":0,"pro":0,"labels":"GAP"},{"idx":14,"sentence":"In this paper, we introduce the Dependency-based Mixture Language Models.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":14,"sentence":"In detail, we first train neural language models with a novel dependency modeling objective to learn the probability distribution of future dependent tokens given context.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":14,"sentence":"We then formulate the next-token probability by mixing the previous dependency modeling probability distributions with self-attention.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":14,"sentence":"Extensive experiments and human evaluations show that our method can be easily and effectively applied to different neural language models while improving neural text generation on various tasks.","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":14,"sentence":"Various models have been proposed to incorporate knowledge of syntactic structures into neural language models.","offset":5,"pro":0.8333333333333334,"labels":"BAC"},{"idx":15,"sentence":"Identifying argument components from unstructured texts and predicting the relationships expressed among them are two primary steps of argument mining.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"The intrinsic complexity of these tasks demands powerful learning models.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":15,"sentence":"While pretrained Transformer-based Language Models (LM) have been shown to provide state-of-the-art results over different NLP tasks, the scarcity of manually annotated data and the highly domain-dependent nature of argumentation restrict the capabilities of such models.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":15,"sentence":"In this work, we propose a novel transfer learning strategy to overcome these challenges.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":15,"sentence":"We utilize argumentation-rich social discussions from the ChangeMyView subreddit as a source of unsupervised, argumentative discourse-aware knowledge by finetuning pretrained LMs on a selectively masked language modeling task.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":15,"sentence":"Furthermore, we introduce a novel prompt-based strategy for inter-component relation prediction that compliments our proposed finetuning method while leveraging on the discourse context.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":15,"sentence":"Exhaustive experiments show the generalization capability of our method on these two tasks over within-domain as well as out-of-domain datasets, outperforming several existing and employed strong baselines.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":16,"sentence":"In this paper, we propose an entity-based neural local coherence model which is linguistically more sound than previously proposed neural coherence models.","offset":0,"pro":0,"labels":"PUR"},{"idx":16,"sentence":"Recent neural coherence models encode the input document using large-scale pretrained language models.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":16,"sentence":"Hence their basis for computing local coherence are words and even sub-words.","offset":2,"pro":0.2,"labels":"BAC"},{"idx":16,"sentence":"In contrast to these models, we compute coherence on the basis of entities by constraining the input to noun phrases and proper names.","offset":3,"pro":0.3,"labels":"MTD"},{"idx":16,"sentence":"Our approach is also in accord with a recent study (O'Connor and Andreas, 2021), which shows that most usable information is captured by nouns and verbs in transformer-based language models.","offset":4,"pro":0.4,"labels":"CLN"},{"idx":16,"sentence":"We evaluate our model on three downstream tasks showing that it is not only linguistically more sound than previous models but also that it outperforms them in end applications.","offset":5,"pro":0.5,"labels":"CLN"},{"idx":16,"sentence":"The analysis of their output shows that these models frequently compute coherence on the basis of connections between (sub-)words which, from a linguistic perspective, should not play a role.","offset":6,"pro":0.6,"labels":"BAC"},{"idx":16,"sentence":"This provides us with an explicit representation of the most important items in sentences leading to the notion of focus.","offset":7,"pro":0.7,"labels":"RST"},{"idx":16,"sentence":"This brings our model linguistically in line with pre-neural models of computing coherence.","offset":8,"pro":0.8,"labels":"RST"},{"idx":16,"sentence":"It also gives us better insight into the behaviour of the model thus leading to better explainability.","offset":9,"pro":0.9,"labels":"RST"},{"idx":17,"sentence":"Adversarial attacks are a major challenge faced by current machine learning research.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"Extensive research in computer vision has been carried to develop reliable defense strategies.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":17,"sentence":"However, the same issue remains less explored in natural language processing.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":17,"sentence":"Our work presents a model-agnostic detector of adversarial text examples.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":17,"sentence":"The approach identifies patterns in the logits of the target classifier when perturbing the input text.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":17,"sentence":"The proposed detector improves the current state-of-the-art performance in recognizing adversarial inputs and exhibits strong generalization capabilities across different NLP models, datasets, and word-level attacks.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":18,"sentence":"How can language technology address the diverse situations of the world's languages?","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"These are often subsumed under the label of under-resourced languages even though they have distinct functions and prospects.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":18,"sentence":"I explore this position and propose some ecologically-aware language technology agendas.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":18,"sentence":"In one view, languages exist on a resource continuum and the challenge is to scale existing solutions, bringing under-resourced languages into the high-resource world.","offset":3,"pro":0.6,"labels":"BAC"},{"idx":18,"sentence":"In another view, presented here, the world's language ecology includes standardised languages, local languages, and contact languages.","offset":4,"pro":0.8,"labels":"BAC"},{"idx":19,"sentence":"The evolution of language follows the rule of gradual change.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"Experiments on four corpora from different eras show that the performance of each corpus significantly improves.","offset":1,"pro":0.14285714285714285,"labels":"CLN"},{"idx":19,"sentence":"Further analyses also demonstrate that the SM can effectively integrate the knowledge of the eras into the neural network.","offset":2,"pro":0.2857142857142857,"labels":"CLN"},{"idx":19,"sentence":"As such, a considerable amount of texts are written in languages of different eras, which creates obstacles for natural language processing tasks, such as word segmentation and machine translation. ","offset":3,"pro":0.42857142857142855,"labels":"BAC"},{"idx":19,"sentence":"Grammar, vocabulary, and lexical semantic shifts take place over time, resulting in a diachronic linguistic gap.","offset":4,"pro":0.5714285714285714,"labels":"BAC"},{"idx":19,"sentence":"Therefore, we propose a cross-era learning framework for Chinese word segmentation (CWS), CROSSWISE,","offset":5,"pro":0.7142857142857143,"labels":"PUR"},{"idx":19,"sentence":"which uses the Switch-memory (SM) module to incorporate era-specific linguistic knowledge.","offset":6,"pro":0.8571428571428571,"labels":"MTD"},{"idx":0,"sentence":"Although much work in NLP has focused on measuring and mitigating stereotypical bias in semantic spaces, research addressing bias in computational argumentation is still in its infancy.","offset":0,"pro":0,"labels":"GAP"},{"idx":0,"sentence":"In this paper, we address this research gap and conduct a thorough investigation of bias in argumentative language models.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":0,"sentence":"To this end, we introduce ABBA, a novel resource for bias measurement specifically tailored to argumentation.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":0,"sentence":"We employ our resource to assess the effect of argumentative fine-tuning and debiasing on the intrinsic bias found in transformer-based language models using a lightweight adapter-based approach that is more sustainable and parameter-efficient than full fine-tuning.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":0,"sentence":"Finally, we analyze the potential impact of language model debiasing on the performance in argument quality prediction, a downstream task of computational argumentation.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":0,"sentence":"Our results show that we are able to successfully and sustainably remove bias in general and argumentative language models while preserving (and sometimes improving) model performance in downstream tasks.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":0,"sentence":"We make all experimental code and data available at https://github.com/umanlp/FairArgumentativeLM.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":1,"sentence":"End-to-end simultaneous speech-to-text translation aims to directly perform translation from streaming source speech to target text with high translation quality and low latency.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"A typical simultaneous translation (ST) system consists of a speech translation model and a policy module, which determines when to wait and when to translate.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":1,"sentence":"Thus the policy is crucial to balance translation quality and latency.","offset":2,"pro":0.25,"labels":"BAC"},{"idx":1,"sentence":"Conventional methods usually adopt fixed policies, e.g. segmenting the source speech with a fixed length and generating translation.","offset":3,"pro":0.375,"labels":"BAC"},{"idx":1,"sentence":"However, this method ignores contextual information and suffers from low translation quality.","offset":4,"pro":0.5,"labels":"GAP"},{"idx":1,"sentence":"This paper proposes an adaptive segmentation policy for end-to-end ST.","offset":5,"pro":0.625,"labels":"PUR"},{"idx":1,"sentence":"Inspired by human interpreters, the policy learns to segment the source streaming speech into meaningful units by considering both acoustic features and translation history, maintaining consistency between the segmentation and translation.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":1,"sentence":"Experimental results on English-German and Chinese-English show that our method achieves a good accuracy-latency trade-off over recently proposed state-of-the-art methods.","offset":7,"pro":0.875,"labels":"RST"},{"idx":2,"sentence":"Simile interpretation is a crucial task in natural language processing.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"Nowadays, pre-trained language models (PLMs) have achieved state-of-the-art performance on many tasks.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":2,"sentence":"However, it remains under-explored whether PLMs can interpret similes or not.","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":2,"sentence":"In this paper, we investigate the ability of PLMs in simile interpretation by designing a novel task named Simile Property Probing, i.e., to let the PLMs infer the shared properties of similes.","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":2,"sentence":"We construct our simile property probing datasets from both general textual corpora and human-designed questions, containing 1,633 examples covering seven main categories.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":2,"sentence":"Our empirical study based on the constructed datasets shows that PLMs can infer similes' shared properties while still underperforming humans.","offset":5,"pro":0.5555555555555556,"labels":"RST"},{"idx":2,"sentence":"To bridge the gap with human performance, we additionally design a knowledge-enhanced training objective by incorporating the simile knowledge into PLMs via knowledge embedding methods.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":2,"sentence":"Our method results in a gain of 8.58% in the probing task and 1.37% in the downstream task of sentiment classification.","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":2,"sentence":"The datasets and code are publicly available at https://github.com/Abbey4799/PLMs-Interpret-Simile.","offset":8,"pro":0.8888888888888888,"labels":"CTN"},{"idx":3,"sentence":"Artificial Intelligence (AI), along with the recent progress in biomedical language understanding, is gradually offering great promise for medical practice.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"With the development of biomedical language understanding benchmarks, AI applications are widely used in the medical field.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":3,"sentence":"However, most benchmarks are limited to English, which makes it challenging to replicate many of the successes in English for other languages.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":3,"sentence":"To facilitate research in this direction, we collect real-world biomedical data and present the first Chinese Biomedical Language Understanding Evaluation (CBLUE) benchmark: a collection of natural language understanding tasks including named entity recognition, information extraction, clinical diagnosis normalization, single-sentence/sentence-pair classification, and an associated online platform for model evaluation, comparison, and analysis.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":3,"sentence":"To establish evaluation on these tasks, we report empirical results with the current 11 pre-trained Chinese models","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":3,"sentence":" and experimental results show that state-of-the-art neural models perform by far worse than the human ceiling.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":4,"sentence":"Text summarization aims to generate a short summary for an input text.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"In this work, we propose a Non-Autoregressive Unsupervised Summarization (NAUS) approach, which does not require parallel data for training.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":4,"sentence":"Our NAUS first performs edit-based search towards a heuristically defined score, and generates a summary as pseudo-groundtruth.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":4,"sentence":"Then, we train an encoder-only non-autoregressive Transformer based on the search result.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":4,"sentence":"We also propose a dynamic programming approach for length-control decoding, which is important for the summarization task.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":4,"sentence":"Experiments on two datasets show that NAUS achieves state-of-the-art performance for unsupervised summarization, yet largely improving inference efficiency.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":4,"sentence":"Further, our algorithm is able to perform explicit length-transfer summary generation.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":5,"sentence":"The principal task in supervised neural machine translation (NMT) is to learn to generate target sentences conditioned on the source inputs from a set of parallel sentence pairs, and thus produce a model capable of generalizing to unseen instances.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"However, it is commonly observed that the generalization performance of the model is highly influenced by the amount of parallel data used in training.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":5,"sentence":"Although data augmentation is widely used to enrich the training data, conventional methods with discrete manipulations fail to generate diverse and faithful training samples.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":5,"sentence":"In this paper, we present a novel data augmentation paradigm termed Continuous Semantic Augmentation (CsaNMT), which augments each training instance with an adjacency semantic region that could cover adequate variants of literal expression under the same meaning.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":5,"sentence":"We conduct extensive experiments on both rich-resource and low-resource settings involving various language pairs, including WMT14 Englishrightarrow{German,French, NIST ChineserightarrowEnglish and multiple low-resource IWSLT translation tasks.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":5,"sentence":"The provided empirical evidences show that CsaNMT sets a new level of performance among existing augmentation techniques, improving on the state-of-the-art by a large margin.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":5,"sentence":"The core codes are contained in Appendix E.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":6,"sentence":"We propose knowledge internalization (KI), which aims to complement the lexical knowledge into neural dialog models.","offset":0,"pro":0,"labels":"PUR"},{"idx":6,"sentence":"Instead of further conditioning the knowledge-grounded dialog (KGD) models on externally retrieved knowledge, we seek to integrate knowledge about each input token internally into the model's parameters.","offset":1,"pro":0.25,"labels":"MTD"},{"idx":6,"sentence":"To tackle the challenge due to the large scale of lexical knowledge, we adopt the contrastive learning approach and create an effective token-level lexical knowledge retriever that requires only weak supervision mined from Wikipedia.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":6,"sentence":"We demonstrate the effectiveness and general applicability of our approach on various datasets and diversified model structures.","offset":3,"pro":0.75,"labels":"CLN"},{"idx":7,"sentence":"In this paper, we propose a mixture model-based end-to-end method to model the syntactic-semantic dependency correlation in Semantic Role Labeling (SRL).","offset":0,"pro":0,"labels":"PUR"},{"idx":7,"sentence":"Semantic dependencies in SRL are modeled as a distribution over semantic dependency labels conditioned on a predicate and an argument word","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":7,"sentence":"The semantic label distribution varies depending on Shortest Syntactic Dependency Path (SSDP) hop patterns","offset":2,"pro":0.2857142857142857,"labels":"BAC"},{"idx":7,"sentence":"We target the variation of semantic label distributions using a mixture model, separately estimating semantic label distributions for different hop patterns and probabilistically clustering hop patterns with similar semantic label distributions","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":7,"sentence":"Modeling the variation improves performance in predicting short distance semantic dependencies, in addition to the improvement on long distance semantic dependencies that previous syntax-aware methods have achieved","offset":4,"pro":0.5714285714285714,"labels":"CLN"},{"idx":7,"sentence":"Experiments show that the proposed method successfully learns a cluster assignment reflecting the variation of semantic label distributions","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":7,"sentence":"The proposed method achieves a small but statistically significant improvement over baseline methods in English, German, and Spanish and obtains competitive performance with state-of-the-art methods in English","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":8,"sentence":"We are interested in a novel task, singing voice beautification (SVB).","offset":0,"pro":0,"labels":"PUR"},{"idx":8,"sentence":"Given the singing voice of an amateur singer, SVB aims to improve the intonation and vocal tone of the voice, while keeping the content and vocal timbre.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":8,"sentence":"Current automatic pitch correction techniques are immature, and most of them are restricted to intonation but ignore the overall aesthetic quality.","offset":2,"pro":0.2,"labels":"GAP"},{"idx":8,"sentence":"Hence, we introduce Neural Singing Voice Beautifier (NSVB), the first generative model to solve the SVB task, which adopts a conditional variational autoencoder as the backbone and learns the latent representations of vocal tone.","offset":3,"pro":0.3,"labels":"PUR"},{"idx":8,"sentence":"In NSVB, we propose a novel time-warping approach for pitch correction: Shape-Aware Dynamic Time Warping (SADTW), which ameliorates the robustness of existing time-warping approaches, to synchronize the amateur recording with the template pitch curve.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":8,"sentence":"Furthermore, we propose a latent-mapping algorithm in the latent space to convert the amateur vocal tone to the professional one.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":8,"sentence":"To achieve this, we also propose a new dataset containing parallel singing recordings of both amateur and professional versions.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":8,"sentence":"Extensive experiments on both Chinese and English songs demonstrate the effectiveness of our methods in terms of both objective and subjective metrics.","offset":7,"pro":0.7,"labels":"RST"},{"idx":8,"sentence":"Audio samples are available at url{https://neuralsvb.github.io.","offset":8,"pro":0.8,"labels":"CTN"},{"idx":8,"sentence":"Codes: url{https://github.com/MoonInTheRiver/NeuralSVB.","offset":9,"pro":0.9,"labels":"CTN"},{"idx":9,"sentence":"Towards building intelligent dialogue agents, there has been a growing interest in introducing explicit personas in generation models.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"However, with limited persona-based dialogue data at hand, it may be difficult to train a dialogue generation model well.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":9,"sentence":"We point out that the data challenges of this generation task lie in two aspects: first, it is expensive to scale up current persona-based dialogue datasets; second, each data sample in this task is more complex to learn with than conventional dialogue data.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":9,"sentence":"To alleviate the above data issues, we propose a data manipulation method, which is model-agnostic to be packed with any persona-based dialogue generation model to improve their performance.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":9,"sentence":"The original training samples will first be distilled and thus expected to be fitted more easily.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":9,"sentence":"Next, we show various effective ways that can diversify such easier distilled data.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":9,"sentence":"A given base model will then be trained via the constructed data curricula, i.e. first on augmented distilled samples and then on original ones.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":9,"sentence":"Experiments illustrate the superiority of our method with two strong base dialogue models (Transformer encoder-decoder and GPT2).","offset":7,"pro":0.875,"labels":"RST"},{"idx":10,"sentence":"Language model (LM) pretraining captures various knowledge from text corpora, helping downstream tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"However, existing methods such as BERT model a single document, and do not capture dependencies or knowledge that span across documents.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":10,"sentence":"In this work, we propose LinkBERT, an LM pretraining method that leverages links between documents, e.g., hyperlinks.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":10,"sentence":"Given a text corpus, we view it as a graph of documents and create LM inputs by placing linked documents in the same context.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":10,"sentence":"We then pretrain the LM with two joint self-supervised objectives: masked language modeling and our new proposal, document relation prediction.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":10,"sentence":"We show that LinkBERT outperforms BERT on various downstream tasks across two domains: the general domain (pretrained on Wikipedia with hyperlinks) and biomedical domain (pretrained on PubMed with citation links).","offset":5,"pro":0.625,"labels":"RST"},{"idx":10,"sentence":"LinkBERT is especially effective for multi-hop reasoning and few-shot QA (+5% absolute improvement on HotpotQA and TriviaQA), and our biomedical LinkBERT sets new states of the art on various BioNLP tasks (+7% on BioASQ and USMLE).","offset":6,"pro":0.75,"labels":"RST"},{"idx":10,"sentence":"We release our pretrained models, LinkBERT and BioLinkBERT, as well as code and data.","offset":7,"pro":0.875,"labels":"CTN"},{"idx":11,"sentence":"Question answering over temporal knowledge graphs (KGs) efficiently uses facts contained in a temporal KG, which records entity relations and when they occur in time, to answer natural language questions (e.g., Who was the president of the US before Obama?).","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"These questions often involve three time-related challenges that previous work fail to adequately address: 1) questions often do not specify exact timestamps of interest (e.g., Obama instead of 2000); 2) subtle lexical differences in time relations (e.g., before vs after); 3) off-the-shelf temporal KG embeddings that previous work builds on ignore the temporal order of timestamps, which is crucial for answering temporal-order related questions.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":11,"sentence":"In this paper, we propose a time-sensitive question answering (TSQA) framework to tackle these problems.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":11,"sentence":"TSQA features a timestamp estimation module to infer the unwritten timestamp from the question.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":11,"sentence":"We also employ a time-sensitive KG encoder to inject ordering information into the temporal KG embeddings that TSQA is based on.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":11,"sentence":"With the help of techniques to reduce the search space for potential answers, TSQA significantly outperforms the previous state of the art on a new benchmark for question answering over temporal KGs, especially achieving a 32% (absolute) error reduction on complex questions that require multiple steps of reasoning over facts in the temporal KG.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":12,"sentence":"Phonemes are defined by their relationship to words: changing a phoneme changes the word.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"Learning a phoneme inventory with little supervision has been a longstanding challenge with important applications to under-resourced speech technology.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":12,"sentence":"In this paper, we bridge the gap between the linguistic and statistical definition of phonemes and propose a novel neural discrete representation learning model for self-supervised learning of phoneme inventory with raw speech and word labels.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":12,"sentence":"Under mild assumptions, we prove that the phoneme inventory learned by our approach converges to the true one with an exponentially low error rate.","offset":3,"pro":0.6,"labels":"RST"},{"idx":12,"sentence":"Moreover, in experiments on TIMIT and Mboshi benchmarks, our approach consistently learns a better phoneme-level representation and achieves a lower error rate in a zero-resource phoneme recognition task than previous state-of-the-art self-supervised representation learning algorithms.","offset":4,"pro":0.8,"labels":"RST"},{"idx":13,"sentence":"Neural language models (LMs) such as GPT-2 estimate the probability distribution over the next word by a softmax over the vocabulary.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"The softmax layer produces the distribution based on the dot products of a single hidden state and the embeddings of words in the vocabulary.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":13,"sentence":"However, we discover that this single hidden state cannot produce all probability distributions regardless of the LM size or training data size because the single hidden state embedding cannot be close to the embeddings of all the possible next words simultaneously when there are other interfering word embeddings between them.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":13,"sentence":"In this work, we demonstrate the importance of this limitation both theoretically and practically.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":13,"sentence":"Our work not only deepens our understanding of softmax bottleneck and mixture of softmax (MoS) but also inspires us to propose multi-facet softmax (MFS) to address the limitations of MoS.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":13,"sentence":"Extensive empirical analyses confirm our findings and show that against MoS, the proposed MFS achieves two-fold improvements in the perplexity of GPT-2 and BERT.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":14,"sentence":"Conversational question answering aims to provide natural-language answers to users in information-seeking conversations.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"Existing conversational QA benchmarks compare models with pre-collected human-human conversations, using ground-truth answers provided in conversational history.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":14,"sentence":"It remains unclear whether we can rely on this static evaluation for model development and whether current systems can well generalize to real-world human-machine conversations.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":14,"sentence":"In this work, we conduct the first large-scale human evaluation of state-of-the-art conversational QA systems, where human evaluators converse with models and judge the correctness of their answers.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":14,"sentence":"We find that the distribution of human machine conversations differs drastically from that of human-human conversations, and there is a disagreement between human and gold-history evaluation in terms of model ranking.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":14,"sentence":"We further investigate how to improve automatic evaluations, and propose a question rewriting mechanism based on predicted history, which better correlates with human judgments.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":14,"sentence":"Finally, we analyze the impact of various modeling strategies and discuss future directions towards building better conversational question answering systems.","offset":6,"pro":0.8571428571428571,"labels":"MTD"},{"idx":15,"sentence":"When primed with only a handful of training samples, very large, pretrained language models such as GPT-3 have shown competitive results when compared to fully-supervised, fine-tuned, large, pretrained language models.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"We demonstrate that the order in which the samples are provided can make the difference between near state-of-the-art and random guess performance: essentially some permutations are fantastic and some not.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":15,"sentence":"We analyse this phenomenon in detail, establishing that: it is present across model sizes (even for the largest current models), it is not related to a specific subset of samples, and that a given good permutation for one model is not transferable to another.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":15,"sentence":"While one could use a development set to determine which permutations are performant, this would deviate from the true few-shot setting as it requires additional annotated data.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":15,"sentence":"Instead, we use the generative nature of language models to construct an artificial development set and based on entropy statistics of the candidate permutations on this set, we identify performant prompts.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":15,"sentence":"Our method yields a 13% relative improvement for GPT-family models across eleven different established text classification tasks.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":16,"sentence":"We teach goal-driven agents to interactively act and speak in situated environments by training on generated curriculums.","offset":0,"pro":0,"labels":"PUR"},{"idx":16,"sentence":"Our agents operate in LIGHT (Urbanek et al. 2019)-a large-scale crowd-sourced fantasy text adventure game wherein an agent perceives and interacts with the world through textual natural language.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":16,"sentence":"Goals in this environment take the form of character-based quests, consisting of personas and motivations.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":16,"sentence":"We augment LIGHT by learning to procedurally generate additional novel textual worlds and quests to create a curriculum of steadily increasing difficulty for training agents to achieve such goals.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":16,"sentence":"In particular, we measure curriculum difficulty in terms of the rarity of the quest in the original training distribution-an easier environment is one that is more likely to have been found in the unaugmented dataset.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":16,"sentence":"An ablation study shows that this method of learning from the tail of a distribution results in significantly higher generalization abilities as measured by zero-shot performance on never-before-seen quests.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":17,"sentence":"Translation quality evaluation plays a crucial role in machine translation.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"According to the input format, it is mainly separated into three tasks, i.e., reference-only, source-only and source-reference-combined.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":17,"sentence":"Recent methods, despite their promising results, are specifically designed and optimized on one of them.","offset":2,"pro":0.2222222222222222,"labels":"BAC"},{"idx":17,"sentence":"This limits the convenience of these methods, and overlooks the commonalities among tasks.","offset":3,"pro":0.3333333333333333,"labels":"GAP"},{"idx":17,"sentence":"In this paper, we propose , which is the first unified framework engaged with abilities to handle all three evaluation tasks.","offset":4,"pro":0.4444444444444444,"labels":"PUR"},{"idx":17,"sentence":"Concretely, we propose monotonic regional attention to control the interaction among input segments, and unified pretraining to better adapt multi-task training.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":17,"sentence":"We testify our framework on WMT 2019 Metrics and WMT 2020 Quality Estimation benchmarks.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":17,"sentence":"Extensive analyses show that our single model can universally surpass various state-of-the-art or winner methods across tasks","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":17,"sentence":"Both source code and associated models are available at https://github.com/NLP2CT/UniTE","offset":8,"pro":0.8888888888888888,"labels":"CTN"},{"idx":18,"sentence":"Program induction for answering complex questions over knowledge bases (KBs) aims to decompose a question into a multi-step program, whose execution against the KB produces the final answer.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"Learning to induce programs relies on a large number of parallel question-program pairs for the given KB.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":18,"sentence":"However, for most KBs, the gold program annotations are usually lacking, making learning difficult.","offset":2,"pro":0.2,"labels":"GAP"},{"idx":18,"sentence":"In this paper, we propose the approach of program transfer, which aims to leverage the valuable program annotations on the rich-resourced KBs as external supervision signals to aid program induction for the low-resourced KBs that lack program annotations.","offset":3,"pro":0.3,"labels":"PUR"},{"idx":18,"sentence":"For program transfer, we design a novel two-stage parsing framework with an efficient ontology-guided pruning strategy.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":18,"sentence":"First, a sketch parser translates the question into a high-level program sketch, which is the composition of functions.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":18,"sentence":"Second, given the question and sketch, an argument parser searches the detailed arguments from the KB for functions.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":18,"sentence":"During the searching, we incorporate the KB ontology to prune the search space.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":18,"sentence":"The experiments on ComplexWebQuestions and WebQuestionSP show that our method outperforms SOTA methods significantly, demonstrating the effectiveness of program transfer and our framework.","offset":8,"pro":0.8,"labels":"RST"},{"idx":18,"sentence":"Our codes and datasets can be obtained from https://github.com/THU-KEG/ProgramTransfer.","offset":9,"pro":0.9,"labels":"CTN"},{"idx":19,"sentence":"Complete Multi-lingual Neural Machine Translation (C-MNMT) achieves superior performance against the conventional MNMT by constructing multi-way aligned corpus, i.e., aligning bilingual training examples from different language pairs when either their source or target sides are identical.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"However, since exactly identical sentences from different language pairs are scarce, the power of the multi-way aligned corpus is limited by its scale.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":19,"sentence":"To handle this problem, this paper proposes Extract and Generate (EAG), a two-step approach to construct large-scale and high-quality multi-way aligned corpus from bilingual data.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":19,"sentence":"Specifically, we first extract candidate aligned examples by pairing the bilingual examples from different language pairs with highly similar source or target sentences; and then generate the final aligned examples from the candidates with a well-trained generation model.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":19,"sentence":"With this two-step pipeline, EAG can construct a large-scale and multi-way aligned corpus whose diversity is almost identical to the original bilingual corpus.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":19,"sentence":"Experiments on two publicly available datasets i.e., WMT-5 and OPUS-100, show that the proposed method achieves significant improvements over strong baselines, with +1.1 and +1.4 BLEU points improvements on the two datasets respectively.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":0,"sentence":"Although contextualized embeddings generated from large-scale pre-trained models perform well in many tasks, traditional static embeddings (e.g., Skip-gram, Word2Vec) still play an important role in low-resource and lightweight settings due to their low computational cost, ease of deployment, and stability.","offset":0,"pro":0,"labels":"GAP"},{"idx":0,"sentence":"In this paper, we aim to improve word embeddings by 1) incorporating more contextual information from existing pre-trained models into the Skip-gram framework, which we call Context-to-Vec; 2) proposing a post-processing retrofitting method for static embeddings independent of training by employing priori synonym knowledge and weighted vector distribution.","offset":1,"pro":0.3333333333333333,"labels":"PUR"},{"idx":0,"sentence":"Through extrinsic and intrinsic tasks, our methods are well proven to outperform the baselines by a large margin.","offset":2,"pro":0.6666666666666666,"labels":"RST"},{"idx":1,"sentence":"Sarcasm is important to sentiment analysis on social media.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"Sarcasm Target Identification (STI) deserves further study to understand sarcasm in depth.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":1,"sentence":"However, text lacking context or missing sarcasm target makes target identification very difficult.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":1,"sentence":"In this paper, we introduce multimodality to STI and present Multimodal Sarcasm Target Identification (MSTI) task.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":1,"sentence":"In the model, we extract multi-scale visual features to enrich spatial information for different sized visual sarcasm targets.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":1,"sentence":"We design a set of convolution networks to unify multi-scale visual features with textual features for cross-modal attention learning, and correspondingly a set of transposed convolution networks to restore multi-scale visual information.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":1,"sentence":"The results show that visual clues can improve the performance of TSTI by a large margin, and VSTI achieves good accuracy.","offset":6,"pro":0.75,"labels":"RST"},{"idx":1,"sentence":"We propose a novel multi-scale cross-modality model that can simultaneously perform textual target labeling and visual target detection.","offset":7,"pro":0.875,"labels":"MTD"},{"idx":2,"sentence":"The dominant paradigm for high-performance models in novel NLP tasks today is direct specialization for the task via training from scratch or fine-tuning large pre-trained models.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"But does direct specialization capture how humans approach novel language tasks?","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":2,"sentence":"We conduct three types of evaluation: human judgments of completion quality, satisfaction of syntactic constraints imposed by the input fragment, and similarity to human behavior in the structural statistics of the completions.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":2,"sentence":"With no task-specific parameter tuning, GibbsComplete performs comparably to direct-specialization models in the first two evaluations, and outperforms all direct-specialization models in the third evaluation.","offset":3,"pro":0.5,"labels":"RST"},{"idx":2,"sentence":"These results support our hypothesis that human behavior in novel language tasks and environments may be better characterized by flexible composition of basic computational motifs rather than by direct specialization.","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":2,"sentence":"We hypothesize that human performance is better characterized by flexible inference through composition of basic computational motifs available to the human language user.","offset":5,"pro":0.8333333333333334,"labels":"PUR"},{"idx":3,"sentence":"Non-autoregressive text to speech (NAR-TTS) models have attracted much attention from both academia and industry due to their fast generation speed.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"One limitation of NAR-TTS models is that they ignore the correlation in time and frequency domains while generating speech mel-spectrograms, and thus cause blurry and over-smoothed results.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":3,"sentence":"In this work, we revisit this over-smoothing problem from a novel perspective: the degree of over-smoothness is determined by the gap between the complexity of data distributions and the capability of modeling methods.","offset":2,"pro":0.2222222222222222,"labels":"PUR"},{"idx":3,"sentence":"Accordingly, we first study methods reducing the complexity of data distributions.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":3,"sentence":"Then we conduct a comprehensive study on NAR-TTS models that use some advanced modeling methods.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":3,"sentence":"Based on these studies, we find that 1) methods that provide additional condition inputs reduce the complexity of data distributions to model, thus alleviating the over-smoothing problem and achieving better voice quality.","offset":5,"pro":0.5555555555555556,"labels":"RST"},{"idx":3,"sentence":"2) Among advanced modeling methods, Laplacian mixture loss performs well at modeling multimodal distributions and enjoys its simplicity, while GAN and Glow achieve the best voice quality while suffering from increased training or model complexity.","offset":6,"pro":0.6666666666666666,"labels":"RST"},{"idx":3,"sentence":"3) The two categories of methods can be combined to further alleviate the over-smoothness and improve the voice quality.","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":3,"sentence":"4) Our experiments on the multi-speaker dataset lead to similar conclusions as above and providing more variance information can reduce the difficulty of modeling the target data distribution and alleviate the requirements for model capacity.","offset":8,"pro":0.8888888888888888,"labels":"RST"},{"idx":4,"sentence":"It is also found that coherence boosting with state-of-the-art models for various zero-shot NLP tasks yields performance gains with no additional training.","offset":0,"pro":0,"labels":"RST"},{"idx":4,"sentence":"Long-range semantic coherence remains a challenge in automatic language generation and understanding.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":4,"sentence":"We demonstrate that large language models have insufficiently learned the effect of distant words on next-token prediction.","offset":2,"pro":0.4,"labels":"GAP"},{"idx":4,"sentence":"We present coherence boosting, an inference procedure that increases a LM's focus on a long context.","offset":3,"pro":0.6,"labels":"PUR"},{"idx":4,"sentence":"We show the benefits of coherence boosting with pretrained models by distributional analyses of generated ordinary text and dialog responses.","offset":4,"pro":0.8,"labels":"RST"},{"idx":5,"sentence":"Little attention has been paid to UE in natural language processing.","offset":0,"pro":0,"labels":"GAP"},{"idx":5,"sentence":"To fill this gap, we perform a vast empirical investigation of state-of-the-art UE methods for Transformer models on misclassification detection in named entity recognition and text classification tasks and propose two computationally efficient modifications, one of which approaches or even outperforms computationally intensive methods.","offset":1,"pro":0.3333333333333333,"labels":"PUR"},{"idx":5,"sentence":"Uncertainty estimation (UE) of model predictions is a crucial step for a variety of tasks such as active learning, misclassification detection, adversarial attack detection, out-of-distribution detection, etc.","offset":2,"pro":0.6666666666666666,"labels":"BAC"},{"idx":6,"sentence":"VALSE offers a suite of six tests covering various linguistic constructs.","offset":0,"pro":0,"labels":"MTD"},{"idx":6,"sentence":"We build VALSE using methods that support the construction of valid foils, and report results from evaluating five widely-used V{&L models.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":6,"sentence":"Our experiments suggest that current models have considerable difficulty addressing most phenomena.","offset":2,"pro":0.4,"labels":"CLN"},{"idx":6,"sentence":"Hence, we expect VALSE to serve as an important benchmark to measure future progress of pretrained V{&L models from a linguistic perspective, complementing the canonical task-centred V{&L evaluations.","offset":3,"pro":0.6,"labels":"IMP"},{"idx":6,"sentence":"We propose VALSE (Vision And Language Structured Evaluation), a novel benchmark designed for testing general-purpose pretrained vision and language (V{&L) models for their visio-linguistic grounding capabilities on specific linguistic phenomena.","offset":4,"pro":0.8,"labels":"PUR"},{"idx":7,"sentence":"The learning trajectories of linguistic phenomena in humans provide insight into linguistic representation, beyond what can be gleaned from inspecting the behavior of an adult speaker.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"In this paper, we show that NLMs with different initialization, architecture, and training data acquire linguistic phenomena in a similar order, despite their different end performance.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":7,"sentence":"These findings suggest that there is some mutual inductive bias that underlies these models' learning of linguistic phenomena.","offset":2,"pro":0.2857142857142857,"labels":"CLN"},{"idx":7,"sentence":"Results suggest that NLMs exhibit consistent developmental stages.","offset":3,"pro":0.42857142857142855,"labels":"CLN"},{"idx":7,"sentence":"Moreover, we find the learning trajectory to be approximately one-dimensional: given an NLM with a certain overall performance, it is possible to predict what linguistic generalizations it has already acquired.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":7,"sentence":"To apply a similar approach to analyze neural language models (NLM), it is first necessary to establish that different models are similar enough in the generalizations they make.","offset":5,"pro":0.7142857142857143,"labels":"BAC"},{"idx":7,"sentence":"Taking inspiration from psycholinguistics, we argue that studying this inductive bias is an opportunity to study the linguistic representation implicit in NLMs.","offset":6,"pro":0.8571428571428571,"labels":"PUR"},{"idx":8,"sentence":"Unfamiliar terminology and complex language can present barriers to understanding science.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"Natural language processing stands to help address these issues by automatically defining unfamiliar terms.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":8,"sentence":"We introduce a new task and dataset for defining scientific terms and controlling the complexity of generated definitions as a way of adapting to a specific reader's background knowledge.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":8,"sentence":"We then explore the version of the task in which definitions are generated at a target complexity level.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":8,"sentence":"We test four definition generation methods for this new task, ","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":8,"sentence":"We introduce a novel reranking approach","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":9,"sentence":"In text classification tasks, useful information is encoded in the label names.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"Label semantic aware systems have leveraged this information for improved text classification performance during fine-tuning and prediction.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":9,"sentence":"However, use of label-semantics during pre-training has not been extensively explored.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":9,"sentence":"LSAP incorporates label semantics into pre-trained generative models (T5 in our case) by performing secondary pre-training on labeled sentences from a variety of domains.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":9,"sentence":"As domain-general pre-training requires large amounts of data, we develop a filtering and labeling pipeline to automatically create sentence-label pairs from unlabeled text.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":9,"sentence":"LSAP obtains significant accuracy improvements over state-of-the-art models for few-shot text classification while maintaining performance comparable to state of the art in high-resource settings.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":9,"sentence":"We therefore propose Label Semantic Aware Pre-training (LSAP) to improve the generalization and data efficiency of text classification systems.","offset":6,"pro":0.8571428571428571,"labels":"PUR"},{"idx":10,"sentence":"Residual networks are an Euler discretization of solutions to Ordinary Differential Equations (ODE).","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"This paper explores a deeper relationship between Transformer and numerical ODE methods.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":10,"sentence":"We first show that a residual block of layers in Transformer can be described as a higher-order solution to ODE.","offset":2,"pro":0.2857142857142857,"labels":"RST"},{"idx":10,"sentence":"Inspired by this, we design a new architecture, ODE Transformer, which is analogous to the Runge-Kutta method that is well motivated in ODE.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":10,"sentence":"As a natural extension to Transformer, ODE Transformer is easy to implement and efficient to use.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":10,"sentence":"Experimental results on the large-scale machine translation, abstractive summarization, and grammar error correction tasks demonstrate the high genericity of ODE Transformer.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":10,"sentence":"It can gain large improvements in model performance over strong baselines (e.g., 30.77 and 44.11 BLEU scores on the WMT'14 English-German and English-French benchmarks) at a slight cost in inference efficiency.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":11,"sentence":"Then we systematically compare these different strategies across multiple tasks and domains.","offset":0,"pro":0,"labels":"MTD"},{"idx":11,"sentence":"We find that active learning yields consistent gains across all SemEval 2021 Task 10 tasks and domains, but though the shared task saw successful self-trained and data augmented models, our systematic comparison finds these strategies to be unreliable for source-free domain adaptation.","offset":1,"pro":0.2,"labels":"RST"},{"idx":11,"sentence":"Data sharing restrictions are common in NLP, especially in the clinical domain,","offset":2,"pro":0.4,"labels":"BAC"},{"idx":11,"sentence":"but there is limited research on adapting models to new domains without access to the original training data, a setting known as source-free domain adaptation.","offset":3,"pro":0.6,"labels":"GAP"},{"idx":11,"sentence":"We take algorithms that traditionally assume access to the source-domain training data-active learning, self-training, and data augmentation-and adapt them for source free domain adaptation.","offset":4,"pro":0.8,"labels":"MTD"},{"idx":12,"sentence":"Several high-profile events, such as the mass testing of emotion recognition systems on vulnerable sub-populations and using question answering systems to make moral judgments, have highlighted how technology will often lead to more adverse outcomes for those that are already marginalized.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"In this position paper, I make a case for thinking about ethical considerations not just at the level of individual models and datasets, but also at the level of AI tasks.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":12,"sentence":"I will also present a template for ethics sheets with 50 ethical considerations, using the task of emotion recognition as a running example.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":12,"sentence":"At issue here are not just individual systems and datasets, but also the AI tasks themselves.","offset":3,"pro":0.42857142857142855,"labels":"BAC"},{"idx":12,"sentence":"I will present a new form of such an effort, Ethics Sheets for AI Tasks, dedicated to fleshing out the assumptions and ethical considerations hidden in how a task is commonly framed and in the choices we make regarding the data, method, and evaluation.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":12,"sentence":"Similar to survey articles, a small number of carefully created ethics sheets can serve numerous researchers and developers.","offset":5,"pro":0.7142857142857143,"labels":"IMP"},{"idx":12,"sentence":"Ethics sheets are a mechanism to engage with and document ethical considerations before building datasets and systems.","offset":6,"pro":0.8571428571428571,"labels":"MTD"},{"idx":13,"sentence":"Negation and uncertainty modeling are long-standing tasks in natural language processing.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"Linguistic theory postulates that expressions of negation and uncertainty are semantically independent from each other and the content they modify.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":13,"sentence":"However, previous works on representation learning do not explicitly model this independence.","offset":2,"pro":0.4,"labels":"GAP"},{"idx":13,"sentence":"We find that simply supervising the latent representations results in good disentanglement, but auxiliary objectives based on adversarial learning and mutual information minimization can provide additional disentanglement gains.","offset":3,"pro":0.6,"labels":"RST"},{"idx":13,"sentence":"We therefore attempt to disentangle the representations of negation, uncertainty, and content using a Variational Autoencoder.","offset":4,"pro":0.8,"labels":"PUR"},{"idx":14,"sentence":"Recently, parallel text generation has received widespread attention due to its success in generation efficiency.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"Although many advanced techniques are proposed to improve its generation quality, they still need the help of an autoregressive model for training to overcome the one-to-many multi-modal phenomenon in the dataset, limiting their applications.","offset":1,"pro":0.25,"labels":"GAP"},{"idx":14,"sentence":"In this paper, we propose GLAT, which employs the discrete latent variables to capture word categorical information and invoke an advanced curriculum learning technique, alleviating the multi-modality problem.","offset":2,"pro":0.5,"labels":"PUR"},{"idx":14,"sentence":"Experiment results show that our method outperforms strong baselines without the help of an autoregressive model, which further broadens the application scenarios of the parallel decoding paradigm.","offset":3,"pro":0.75,"labels":"CLN"},{"idx":15,"sentence":"Prompts for pre-trained language models (PLMs) have shown remarkable performance by bridging the gap between pre-training tasks and various downstream tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"Among these methods, prompt tuning, which freezes PLMs and only tunes soft prompts, provides an efficient and effective solution for adapting large-scale PLMs to downstream tasks.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":15,"sentence":"However, prompt tuning is yet to be fully explored.","offset":2,"pro":0.2,"labels":"GAP"},{"idx":15,"sentence":"Therefore, in this work, we propose to pre-train prompts by adding soft prompts into the pre-training stage to obtain a better initialization.","offset":3,"pro":0.3,"labels":"PUR"},{"idx":15,"sentence":"We name this Pre-trained Prompt Tuning framework PPT.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":15,"sentence":"To ensure the generalization of PPT, we formulate similar classification tasks into a unified task form and pre-train soft prompts for this unified task.","offset":5,"pro":0.5,"labels":"PUR"},{"idx":15,"sentence":"Extensive experiments show that tuning pre-trained prompts for downstream tasks can reach or even outperform full-model fine-tuning under both full-data and few-shot settings.","offset":6,"pro":0.6,"labels":"CLN"},{"idx":15,"sentence":"Our approach is effective and efficient for using large-scale PLMs in practice.","offset":7,"pro":0.7,"labels":"CLN"},{"idx":15,"sentence":"In our pilot experiments, we find that prompt tuning performs comparably with conventional full-model tuning when downstream data are sufficient, whereas it is much worse under few-shot learning settings, which may hinder the application of prompt tuning.","offset":8,"pro":0.8,"labels":"GAP"},{"idx":15,"sentence":"We attribute this low performance to the manner of initializing soft prompts.","offset":9,"pro":0.9,"labels":"GAP"},{"idx":16,"sentence":"Deduplication allows us to train models that emit memorized text ten times less frequently and require fewer training steps to achieve the same or better accuracy.","offset":0,"pro":0,"labels":"MTD"},{"idx":16,"sentence":"We can also reduce train-test overlap, which affects over 4% of the validation set of standard datasets, thus allowing for more accurate evaluation.","offset":1,"pro":0.16666666666666666,"labels":"RST"},{"idx":16,"sentence":"Code for deduplication is released at https://github.com/google-research/deduplicate-text-datasets.","offset":2,"pro":0.3333333333333333,"labels":"CTN"},{"idx":16,"sentence":"We find that existing language modeling datasets contain many near-duplicate examples and long repetitive substrings.","offset":3,"pro":0.5,"labels":"GAP"},{"idx":16,"sentence":"As a result, over 1% of the unprompted output of language models trained on these datasets is copied verbatim from the training data.","offset":4,"pro":0.6666666666666666,"labels":"GAP"},{"idx":16,"sentence":"We develop two tools that allow us to deduplicate training datasets-for example removing from C4 a single 61 word English sentence that is repeated over 60,000 times.","offset":5,"pro":0.8333333333333334,"labels":"PUR"},{"idx":17,"sentence":"Automated methods have been widely used to identify and analyze mental health conditions (e.g., depression) from various sources of information, including social media.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"Yet, deployment of such models in real-world healthcare applications faces challenges including poor out-of-domain generalization and lack of trust in black box models.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":17,"sentence":"In this work, we propose approaches for depression detection that are constrained to different degrees by the presence of symptoms described in PHQ9, a questionnaire used by clinicians in the depression screening process.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":17,"sentence":"In dataset-transfer experiments on three social media datasets, we find that grounding the model in PHQ9's symptoms substantially improves its ability to generalize to out-of-distribution data compared to a standard BERT-based approach.","offset":3,"pro":0.5,"labels":"RST"},{"idx":17,"sentence":"Furthermore, this approach can still perform competitively on in-domain data.","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":17,"sentence":"These results and our qualitative analyses suggest that grounding model predictions in clinically-relevant symptoms can improve generalizability while producing a model that is easier to inspect.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":18,"sentence":"In this work we study giving access to this information to conversational agents.","offset":0,"pro":0,"labels":"PUR"},{"idx":18,"sentence":"In contrast, we propose an approach that learns to generate an internet search query based on the context, and then conditions on the search results to finally generate a response, a method that can employ up-to-the-minute relevant information.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":18,"sentence":"We train and evaluate such models on a newly collected dataset of human-human conversations whereby one of the speakers is given access to internet search during knowledgedriven discussions in order to ground their responses.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":18,"sentence":"We find that search-query based access of the internet in conversation provides superior performance compared to existing approaches that either use no augmentation or FAISS-based retrieval (Lewis et al., 2020b).","offset":3,"pro":0.5,"labels":"RST"},{"idx":18,"sentence":"The largest store of continually updating knowledge on our planet can be accessed via internet search.","offset":4,"pro":0.6666666666666666,"labels":"BAC"},{"idx":18,"sentence":"Large language models, even though they store an impressive amount of knowledge within their weights, are known to hallucinate facts when generating dialogue (Shuster et al., 2021); moreover, those facts are frozen in time at the point of model training.","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":19,"sentence":"Transfer learning has proven to be crucial in advancing the state of speech and natural language processing research in recent years.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"In speech, a model pre-trained by self-supervised learning transfers remarkably well on multiple tasks.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":19,"sentence":"However, the lack of a consistent evaluation methodology is limiting towards a holistic understanding of the efficacy of such models.","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":19,"sentence":"In this paper, we introduce SUPERB-SG, a new benchmark focusing on evaluating the semantic and generative capabilities of pre-trained models by increasing task diversity and difficulty over SUPERB.","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":19,"sentence":"We use a lightweight methodology to test the robustness of representations learned by pre-trained models under shifts in data domain and quality across different types of tasks.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":19,"sentence":"It entails freezing pre-trained model parameters, only using simple task-specific trainable heads.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":19,"sentence":"We also show that the task diversity of SUPERB-SG coupled with limited task supervision is an effective recipe for evaluating the generalizability of model representation.","offset":6,"pro":0.6666666666666666,"labels":"CLN"},{"idx":19,"sentence":"SUPERB was a step towards introducing a common benchmark to evaluate pre-trained models across various speech tasks.","offset":7,"pro":0.7777777777777778,"labels":"BAC"},{"idx":19,"sentence":"The goal is to be inclusive of all researchers, and encourage efficient use of computational resources.","offset":8,"pro":0.8888888888888888,"labels":"MTD"},{"idx":0,"sentence":"Large-scale pretrained language models are surprisingly good at recalling factual knowledge presented in the training corpus.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"In this paper, we present preliminary studies on how factual knowledge is stored in pretrained Transformers by introducing the concept of knowledge neurons.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":0,"sentence":"Specifically, we examine the fill-in-the-blank cloze task for BERT.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":0,"sentence":"Given a relational fact, we propose a knowledge attribution method to identify the neurons that express the fact.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":0,"sentence":"We find that the activation of such knowledge neurons is positively correlated to the expression of their corresponding facts.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":0,"sentence":"In our case studies, we attempt to leverage knowledge neurons to edit (such as update, and erase) specific factual knowledge without fine-tuning.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":0,"sentence":"Our results shed light on understanding the storage of knowledge within pretrained Transformers.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":1,"sentence":"Meta-learning, or learning to learn, is a technique that can help to overcome resource scarcity in cross-lingual NLP problems, by enabling fast adaptation to new tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"We apply model-agnostic meta-learning (MAML) to the task of cross-lingual dependency parsing.","offset":1,"pro":0.25,"labels":"MTD"},{"idx":1,"sentence":"We train our model on a diverse set of languages to learn a parameter initialization that can adapt quickly to new languages.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":1,"sentence":"We find that meta-learning with pre-training can significantly improve upon the performance of language transfer and standard supervised learning baselines for a variety of unseen, typologically diverse, and low-resource languages, in a few-shot learning setup.","offset":3,"pro":0.75,"labels":"RST"},{"idx":2,"sentence":"Warning: This paper contains explicit statements of offensive stereotypes which may be upsetting.","offset":0,"pro":0,"labels":"PUR"},{"idx":2,"sentence":"Much work on biases in natural language processing has addressed biases linked to the social and cultural experience of English speaking individuals in the United States.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":2,"sentence":"We seek to widen the scope of bias studies by creating material to measure social bias in language models (LMs) against specific demographic groups in France.","offset":2,"pro":0.2,"labels":"PUR"},{"idx":2,"sentence":"We build on the US-centered CrowS-pairs dataset to create a multilingual stereotypes dataset that allows for comparability across languages while also characterizing biases that are specific to each country and language.","offset":3,"pro":0.3,"labels":"MTD"},{"idx":2,"sentence":"We introduce 1,679 sentence pairs in French that cover stereotypes in ten types of bias like gender and age.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":2,"sentence":"1,467 sentence pairs are translated from CrowS-pairs and 212 are newly crowdsourced.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":2,"sentence":"The sentence pairs contrast stereotypes concerning underadvantaged groups with the same sentence concerning advantaged groups.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":2,"sentence":"We find that four widely used language models (three French, one multilingual) favor sentences that express stereotypes in most bias categories.","offset":7,"pro":0.7,"labels":"RST"},{"idx":2,"sentence":"We report on the translation process from English into French, which led to a characterization of stereotypes in CrowS-pairs including the identification of US-centric cultural traits.","offset":8,"pro":0.8,"labels":"IMP"},{"idx":2,"sentence":"We offer guidelines to further extend the dataset to other languages and cultural environments.","offset":9,"pro":0.9,"labels":"CTN"},{"idx":3,"sentence":"We study the problem of building text classifiers with little or no training data, commonly known as zero and few-shot text classification.","offset":0,"pro":0,"labels":"PUR"},{"idx":3,"sentence":"In recent years, an approach based on neural textual entailment models has been found to give strong results on a diverse range of tasks.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":3,"sentence":"In this work, we show that with proper pre-training, Siamese Networks that embed texts and labels offer a competitive alternative.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":3,"sentence":"These models allow for a large reduction in inference cost: constant in the number of labels rather than linear.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":3,"sentence":"Furthermore, we introduce label tuning, a simple and computationally efficient approach that allows to adapt the models in a few-shot setup by only changing the label embeddings.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":3,"sentence":"While giving lower performance than model fine-tuning, this approach has the architectural advantage that a single encoder can be shared by many different tasks.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":4,"sentence":"In classic instruction following, language like I'd like the JetBlue flight maps to actions (e.g., selecting that flight).","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"However, language also conveys information about a user's underlying reward function (e.g., a general preference for JetBlue), which can allow a model to carry out desirable actions in new contexts.","offset":1,"pro":0.25,"labels":"GAP"},{"idx":4,"sentence":"We present a model that infers rewards from language pragmatically: reasoning about how speakers choose utterances not only to elicit desired actions, but also to reveal information about their preferences.","offset":2,"pro":0.5,"labels":"PUR"},{"idx":4,"sentence":"On a new interactive flight-booking task with natural language, our model more accurately infers rewards and predicts optimal actions in unseen environments, in comparison to past work that first maps language to actions (instruction following) and then maps actions to rewards (inverse reinforcement learning).","offset":3,"pro":0.75,"labels":"MTD"},{"idx":5,"sentence":"Generating factual, long-form text such as Wikipedia articles raises three key challenges: how to gather relevant evidence, how to structure information into well-formed text, and how to ensure that the generated text is factually correct.","offset":0,"pro":0,"labels":"GAP"},{"idx":5,"sentence":"We address these by developing a model for English text that uses a retrieval mechanism to identify relevant supporting information on the web and a cache-based pre-trained encoder-decoder to generate long-form biographies section by section, including citation information.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":5,"sentence":"To assess the impact of available web evidence on the output text, we compare the performance of our approach when generating biographies about women (for which less information is available on the web) vs. biographies generally.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":5,"sentence":"To this end, we curate a dataset of 1,500 biographies about women.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":5,"sentence":"We analyze our generated text to understand how differences in available web evidence data affect generation.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":5,"sentence":"We evaluate the factuality, fluency, and quality of the generated texts using automatic metrics and human evaluation.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":5,"sentence":"We hope that these techniques can be used as a starting point for human writers, to aid in reducing the complexity inherent in the creation of long-form, factual text.","offset":6,"pro":0.8571428571428571,"labels":"IMP"},{"idx":6,"sentence":"Handing in a paper or exercise and merely receiving bad or incorrect as feedback is not very helpful when the goal is to improve.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"Unfortunately, this is currently the kind of feedback given by Automatic Short Answer Grading (ASAG) systems.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":6,"sentence":"One of the reasons for this is a lack of content-focused elaborated feedback datasets.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":6,"sentence":"To encourage research on explainable and understandable feedback systems, we present the Short Answer Feedback dataset (SAF).","offset":3,"pro":0.375,"labels":"PUR"},{"idx":6,"sentence":"Similar to other ASAG datasets, SAF contains learner responses and reference answers to German and English questions.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":6,"sentence":"However, instead of only assigning a label or score to the learners' answers, SAF also contains elaborated feedback explaining the given score.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":6,"sentence":"Thus, SAF enables supervised training of models that grade answers and explain where and why mistakes were made.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":6,"sentence":"This paper discusses the need for enhanced feedback models in real-world pedagogical scenarios, describes the dataset annotation process, gives a comprehensive analysis of SAF, and provides T5-based baselines for future comparison.","offset":7,"pro":0.875,"labels":"PUR"},{"idx":7,"sentence":"To effectively characterize the nature of paraphrase pairs without expert human annotation, we proposes two new metrics: word position deviation (WPD) and lexical deviation (LD).","offset":0,"pro":0,"labels":"PUR"},{"idx":7,"sentence":"WPD measures the degree of structural alteration, while LD measures the difference in vocabulary used.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":7,"sentence":"We apply these metrics to better understand the commonly-used MRPC dataset and study how it differs from PAWS, another paraphrase identification dataset.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":7,"sentence":"We also perform a detailed study on MRPC and propose improvements to the dataset, showing that it improves generalizability of models trained on the dataset.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":7,"sentence":"Lastly, we apply our metrics to filter the output of a paraphrase generation model and show how it can be used to generate specific forms of paraphrases for data augmentation or robustness testing of NLP models.","offset":4,"pro":0.8,"labels":"MTD"},{"idx":8,"sentence":"We introduce SummScreen, a summarization dataset comprised of pairs of TV series transcripts and human written recaps.","offset":0,"pro":0,"labels":"MTD"},{"idx":8,"sentence":"The dataset provides a challenging testbed for abstractive summarization for several reasons.","offset":1,"pro":0.09090909090909091,"labels":"GAP"},{"idx":8,"sentence":"Plot details are often expressed indirectly in character dialogues and may be scattered across the entirety of the transcript.","offset":2,"pro":0.18181818181818182,"labels":"BAC"},{"idx":8,"sentence":"These details must be found and integrated to form the succinct plot descriptions in the recaps.","offset":3,"pro":0.2727272727272727,"labels":"MTD"},{"idx":8,"sentence":"Also, TV scripts contain content that does not directly pertain to the central plot but rather serves to develop characters or provide comic relief.","offset":4,"pro":0.36363636363636365,"labels":"BAC"},{"idx":8,"sentence":"This information is rarely contained in recaps.","offset":5,"pro":0.45454545454545453,"labels":"GAP"},{"idx":8,"sentence":"Since characters are fundamental to TV series, we also propose two entity-centric evaluation metrics.","offset":6,"pro":0.5454545454545454,"labels":"MTD"},{"idx":8,"sentence":"Empirically, we characterize the dataset by evaluating several methods, including neural models and those based on nearest neighbors.","offset":7,"pro":0.6363636363636364,"labels":"MTD"},{"idx":8,"sentence":"An oracle extractive approach outperforms all benchmarked models according to automatic metrics, showing that the neural models are unable to fully exploit the input transcripts.","offset":8,"pro":0.7272727272727273,"labels":"RST"},{"idx":8,"sentence":"Human evaluation and qualitative analysis reveal that our non-oracle models are competitive with their oracle counterparts in terms of generating faithful plot events and can benefit from better content selectors.","offset":9,"pro":0.8181818181818182,"labels":"CLN"},{"idx":8,"sentence":"Both oracle and non-oracle models generate unfaithful facts, suggesting future research directions.","offset":10,"pro":0.9090909090909091,"labels":"IMP"},{"idx":9,"sentence":"We propose a novel method to sparsify attention in the Transformer model by learning to select the most-informative token representations during the training process, thus focusing on the task-specific parts of an input.","offset":0,"pro":0,"labels":"PUR"},{"idx":9,"sentence":"A reduction of quadratic time and memory complexity to sublinear was achieved due to a robust trainable top-k operator.","offset":1,"pro":0.3333333333333333,"labels":"RST"},{"idx":9,"sentence":"Our experiments on a challenging long document summarization task show that even our simple baseline performs comparably to the current SOTA, and with trainable pooling we can retain its top quality, while being 1.8times faster during training, 4.5times faster during inference, and up to 13times more computationally efficient in the decoder.","offset":2,"pro":0.6666666666666666,"labels":"RST"},{"idx":10,"sentence":"In many natural language processing (NLP) tasks the same input (e.g. source sentence) can have multiple possible outputs (e.g. translations).","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"To analyze how this ambiguity (also known as intrinsic uncertainty) shapes the distribution learned by neural sequence models we measure sentence-level uncertainty by computing the degree of overlap between references in multi-reference test sets from two different NLP tasks: machine translation (MT) and grammatical error correction (GEC).","offset":1,"pro":0.2,"labels":"PUR"},{"idx":10,"sentence":"At both the sentence- and the task-level, intrinsic uncertainty has major implications for various aspects of search such as the inductive biases in beam search and the complexity of exact search.","offset":2,"pro":0.4,"labels":"BAC"},{"idx":10,"sentence":"In particular, we show that well-known pathologies such as a high number of beam search errors, the inadequacy of the mode, and the drop in system performance with large beam sizes apply to tasks with high level of ambiguity such as MT but not to less uncertain tasks such as GEC.","offset":3,"pro":0.6,"labels":"RST"},{"idx":10,"sentence":"Furthermore, we propose a novel exact n-best search algorithm for neural sequence models, and show that intrinsic uncertainty affects model uncertainty as the model tends to overly spread out the probability mass for uncertain tasks and sentences.","offset":4,"pro":0.8,"labels":"RST"},{"idx":11,"sentence":"Most previous methods for text data augmentation are limited to simple tasks and weak baselines.","offset":0,"pro":0,"labels":"GAP"},{"idx":11,"sentence":"We explore data augmentation on hard tasks (i.e., few-shot natural language understanding) and strong baselines (i.e., pretrained models with over one billion parameters).","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":11,"sentence":"Under this setting, we reproduced a large number of previous augmentation methods and found that these methods bring marginal gains at best and sometimes degrade the performance much.","offset":2,"pro":0.3333333333333333,"labels":"RST"},{"idx":11,"sentence":"To address this challenge, we propose a novel data augmentation method FlipDA that jointly uses a generative model and a classifier to generate label-flipped data.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":11,"sentence":"Central to the idea of FlipDA is the discovery that generating label-flipped data is more crucial to the performance than generating label-preserved data.","offset":4,"pro":0.6666666666666666,"labels":"BAC"},{"idx":11,"sentence":"Experiments show that FlipDA achieves a good tradeoff between effectiveness and robustness-it substantially improves many tasks while not negatively affecting the others.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":12,"sentence":"Speech pre-training has primarily demonstrated efficacy on classification tasks, while its capability of generating novel speech, similar to how GPT-2 can generate coherent paragraphs, has barely been explored.","offset":0,"pro":0,"labels":"GAP"},{"idx":12,"sentence":"Generative Spoken Language Modeling (GSLM) (CITATION) is the only prior work addressing the generative aspect of speech pre-training, which builds a text-free language model using discovered units.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":12,"sentence":"Unfortunately, because the units used in GSLM discard most prosodic information, GSLM fails to leverage prosody for better comprehension and does not generate expressive speech.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":12,"sentence":"In this work, we present a prosody-aware generative spoken language model (pGSLM).","offset":3,"pro":0.375,"labels":"PUR"},{"idx":12,"sentence":"It is composed of a multi-stream transformer language model (MS-TLM) of speech, represented as discovered unit and prosodic feature streams, and an adapted HiFi-GAN model converting MS-TLM outputs to waveforms.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":12,"sentence":"Experimental results show that the pGSLM can utilize prosody to improve both prosody and content modeling, and also generate natural, meaningful, and coherent speech given a spoken prompt.","offset":5,"pro":0.625,"labels":"CLN"},{"idx":12,"sentence":"Audio samples can be found at https://speechbot.github.io/pgslm.","offset":6,"pro":0.75,"labels":"CTN"},{"idx":12,"sentence":"Codes and models are available at https://github.com/pytorch/fairseq/tree/main/examples/textless{_nlp/pgslm.","offset":7,"pro":0.875,"labels":"CTN"},{"idx":13,"sentence":"As a broad and major category in machine reading comprehension (MRC), the generalized goal of discriminative MRC is answer prediction from the given materials.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"However, the focuses of various discriminative MRC tasks may be diverse enough: multi-choice MRC requires model to highlight and integrate all potential critical evidence globally; while extractive MRC focuses on higher local boundary preciseness for answer extraction.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":13,"sentence":"Among previous works, there lacks a unified design with pertinence for the overall discriminative MRC tasks.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":13,"sentence":"To fill in above gap, we propose a lightweight POS-Enhanced Iterative Co-Attention Network (POI-Net) as the first attempt of unified modeling with pertinence, to handle diverse discriminative MRC tasks synchronously.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":13,"sentence":"Nearly without introducing more parameters, our lite unified design brings model significant improvement with both encoder and decoder components.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":13,"sentence":"The evaluation results on four discriminative MRC benchmarks consistently indicate the general effectiveness and applicability of our model, and the code is available at https://github.com/Yilin1111/poi-net.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":14,"sentence":"This work presents methods for learning cross-lingual sentence representations using paired or unpaired bilingual texts.","offset":0,"pro":0,"labels":"PUR"},{"idx":14,"sentence":"We hypothesize that the cross-lingual alignment strategy is transferable, and therefore a model trained to align only two languages can encode multilingually more aligned representations.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":14,"sentence":"We thus introduce dual-pivot transfer: training on one language pair and evaluating on other pairs.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":14,"sentence":"To study this theory, we design unsupervised models trained on unpaired sentences and single-pair supervised models trained on bitexts, both based on the unsupervised language model XLM-R with its parameters frozen.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":14,"sentence":"The experiments evaluate the models as universal sentence encoders on the task of unsupervised bitext mining on two datasets, where the unsupervised model reaches the state of the art of unsupervised retrieval, and the alternative single-pair supervised model approaches the performance of multilingually supervised models.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":14,"sentence":"The results suggest that bilingual training techniques as proposed can be applied to get sentence representations with multilingual alignment.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":15,"sentence":"Natural language spatial video grounding aims to detect the relevant objects in video frames with descriptive sentences as the query.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"In spite of the great advances, most existing methods rely on dense video frame annotations, which require a tremendous amount of human effort.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":15,"sentence":"To achieve effective grounding under a limited annotation budget, we investigate one-shot video grounding and learn to ground natural language in all video frames with solely one frame labeled, in an end-to-end manner.","offset":2,"pro":0.2222222222222222,"labels":"PUR"},{"idx":15,"sentence":"One major challenge of end-to-end one-shot video grounding is the existence of videos frames that are either irrelevant to the language query or the labeled frame.","offset":3,"pro":0.3333333333333333,"labels":"BAC"},{"idx":15,"sentence":"Another challenge relates to the limited supervision, which might result in ineffective representation learning.","offset":4,"pro":0.4444444444444444,"labels":"GAP"},{"idx":15,"sentence":"To address these challenges, we designed an end-to-end model via Information Tree for One-Shot video grounding (IT-OS).","offset":5,"pro":0.5555555555555556,"labels":"PUR"},{"idx":15,"sentence":"Its key module, the information tree, can eliminate the interference of irrelevant frames based on branch search and branch cropping techniques.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":15,"sentence":"In addition, several self-supervised tasks are proposed based on the information tree to improve the representation learning under insufficient labeling.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":15,"sentence":"Experiments on the benchmark dataset demonstrate the effectiveness of our model.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":16,"sentence":"A release note is a technical document that describes the latest changes to a software product and is crucial in open source software development.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"However, it still remains challenging to generate release notes automatically.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":16,"sentence":"In this paper, we present a new dataset called RNSum, which contains approximately 82,000 English release notes and the associated commit messages derived from the online repositories in GitHub.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":16,"sentence":"Then, we propose classwise extractive-then-abstractive/abstractive summarization approaches to this task, which can employ a modern transformer-based seq2seq network like BART and can be applied to various repositories without specific constraints.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":16,"sentence":"The experimental results on the RNSum dataset show that the proposed methods can generate less noisy release notes at higher coverage than the baselines.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":16,"sentence":"We also observe that there is a significant gap in the coverage of essential information when compared to human references.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":16,"sentence":"Our dataset and the code are publicly available.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":17,"sentence":"To perform well on a machine reading comprehension (MRC) task, machine readers usually require commonsense knowledge that is not explicitly mentioned in the given documents.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"This paper aims to extract a new kind of structured knowledge from scripts and use it to improve MRC.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":17,"sentence":"We focus on scripts as they contain rich verbal and nonverbal messages, and two relevant messages originally conveyed by different modalities during a short time period may serve as arguments of a piece of commonsense knowledge as they function together in daily communications.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":17,"sentence":"To save human efforts to name relations, we propose to represent relations implicitly by situating such an argument pair in a context and call it contextualized knowledge.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":17,"sentence":"To use the extracted knowledge to improve MRC, we compare several fine-tuning strategies to use the weakly-labeled MRC data constructed based on contextualized knowledge and further design a teacher-student paradigm with multiple teachers to facilitate the transfer of knowledge in weakly-labeled MRC data.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":17,"sentence":"Experimental results show that our paradigm outperforms other methods that use weakly-labeled data and improves a state-of-the-art baseline by 4.3% in accuracy on a Chinese multiple-choice MRC dataset C^3, wherein most of the questions require unstated prior knowledge.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":17,"sentence":"We also seek to transfer the knowledge to other tasks by simply adapting the resulting student reader, yielding a 2.9% improvement in F1 on a relation extraction dataset DialogRE, demonstrating the potential usefulness of the knowledge for non-MRC tasks that require document comprehension.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":18,"sentence":"We introduce an argumentation annotation approach to model the structure of argumentative discourse in student-written business model pitches.","offset":0,"pro":0,"labels":"MTD"},{"idx":18,"sentence":"Additionally, the annotation scheme captures a series of persuasiveness scores such as the specificity, strength, evidence, and relevance of the pitch and the individual components.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":18,"sentence":"Based on this scheme, we annotated a corpus of 200 business model pitches in German.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":18,"sentence":"Moreover, we trained predictive models to detect argumentative discourse structures and embedded them in an adaptive writing support system for students that provides them with individual argumentation feedback independent of an instructor, time, and location.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":18,"sentence":"We evaluated our tool in a real-world writing exercise and found promising results for the measured self-efficacy and perceived ease-of-use.","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":18,"sentence":"Finally, we present our freely available corpus of persuasive business model pitches with 3,207 annotated sentences in German language and our annotation guidelines.","offset":5,"pro":0.8333333333333334,"labels":"IMP"},{"idx":19,"sentence":"Recent studies have shown the advantages of evaluating NLG systems using pairwise comparisons as opposed to direct assessment.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"Given k systems, a naive approach for identifying the top-ranked system would be to uniformly obtain pairwise comparisons from all {k choose 2 pairs of systems.","offset":1,"pro":0.09090909090909091,"labels":"BAC"},{"idx":19,"sentence":"However, this can be very expensive as the number of human annotations required would grow quadratically with k.","offset":2,"pro":0.18181818181818182,"labels":"GAP"},{"idx":19,"sentence":"In this work, we introduce Active Evaluation, a framework to efficiently identify the top-ranked system by actively choosing system pairs for comparison using dueling bandit algorithms.","offset":3,"pro":0.2727272727272727,"labels":"PUR"},{"idx":19,"sentence":"We perform extensive experiments with 13 dueling bandits algorithms on 13 NLG evaluation datasets spanning 5 tasks and show that the number of human annotations can be reduced by 80%.","offset":4,"pro":0.36363636363636365,"labels":"MTD"},{"idx":19,"sentence":"To further reduce the number of human annotations, we propose model-based dueling bandit algorithms which combine automatic evaluation metrics with human evaluations.","offset":5,"pro":0.45454545454545453,"labels":"PUR"},{"idx":19,"sentence":"Specifically, we eliminate sub-optimal systems even before the human annotation process and perform human evaluations only on test examples where the automatic metric is highly uncertain.","offset":6,"pro":0.5454545454545454,"labels":"MTD"},{"idx":19,"sentence":"This reduces the number of human annotations required further by 89%.","offset":7,"pro":0.6363636363636364,"labels":"RST"},{"idx":19,"sentence":"In effect, we show that identifying the top-ranked system requires only a few hundred human annotations, which grow linearly with k.","offset":8,"pro":0.7272727272727273,"labels":"RST"},{"idx":19,"sentence":"Lastly, we provide practical recommendations and best practices to identify the top-ranked system efficiently.","offset":9,"pro":0.8181818181818182,"labels":"IMP"},{"idx":19,"sentence":"Our code has been made publicly available at https://github.com/akashkm99/duelnlg","offset":10,"pro":0.9090909090909091,"labels":"CTN"},{"idx":0,"sentence":"An audience's prior beliefs and morals are strong indicators of how likely they will be affected by a given argument.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"Utilizing such knowledge can help focus on shared values to bring disagreeing parties towards agreement.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":0,"sentence":"In argumentation technology, however, this is barely exploited so far.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":0,"sentence":"This paper studies the feasibility of automatically generating morally framed arguments as well as their effect on different audiences.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":0,"sentence":"Following the moral foundation theory, we propose a system that effectively generates arguments focusing on different morals.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":0,"sentence":"In an in-depth user study, we ask liberals and conservatives to evaluate the impact of these arguments.","offset":5,"pro":0.7142857142857143,"labels":"PUR"},{"idx":0,"sentence":"Our results suggest that, particularly when prior beliefs are challenged, an audience becomes more affected by morally framed arguments.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":1,"sentence":"Transformer-based language models such as BERT (CITATION) have achieved the state-of-the-art performance on various NLP tasks, but are computationally prohibitive.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"The core-set based token selection technique allows us to avoid expensive pre-training, gives a space-efficient fine tuning, and thus makes it suitable to handle longer sequence lengths.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":1,"sentence":"We provide extensive experiments establishing advantages of pyramid BERT over several baselines and existing works on the GLUE benchmarks and Long Range Arena (CITATION) datasets.","offset":2,"pro":0.4,"labels":"CLN"},{"idx":1,"sentence":"A recent line of works use various heuristics to successively shorten sequence length while transforming tokens through encoders, in tasks such as classification and ranking that require a single token embedding for prediction","offset":3,"pro":0.6,"labels":"BAC"},{"idx":1,"sentence":"We present a novel solution to this problem, called Pyramid-BERT where we replace previously used heuristics with a core-set based token selection method justified by theoretical results.","offset":4,"pro":0.8,"labels":"PUR"},{"idx":2,"sentence":"A central quest of probing is to uncover how pre-trained models encode a linguistic property within their representations.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"An encoding, however, might be spurious-i.e., the model might not rely on it when making predictions.","offset":1,"pro":0.1,"labels":"GAP"},{"idx":2,"sentence":"In this paper, we try to find an encoding that the model actually uses, introducing a usage-based probing setup.","offset":2,"pro":0.2,"labels":"PUR"},{"idx":2,"sentence":"We first choose a behavioral task which cannot be solved without using the linguistic property.","offset":3,"pro":0.3,"labels":"MTD"},{"idx":2,"sentence":"Then, we attempt to remove the property by intervening on the model's representations.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":2,"sentence":"As a case study, we focus on how BERT encodes grammatical number, and on how it uses this encoding to solve the number agreement task.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":2,"sentence":"Experimentally, we find that BERT relies on a linear encoding of grammatical number to produce the correct behavioral output.","offset":6,"pro":0.6,"labels":"RST"},{"idx":2,"sentence":"We also find that BERT uses a separate encoding of grammatical number for nouns and verbs.","offset":7,"pro":0.7,"labels":"RST"},{"idx":2,"sentence":"Finally, we identify in which layers information about grammatical number is transferred from a noun to its head verb.","offset":8,"pro":0.8,"labels":"MTD"},{"idx":2,"sentence":"We contend that, if an encoding is used by the model, its removal should harm the performance on the chosen behavioral task.","offset":9,"pro":0.9,"labels":"MTD"},{"idx":3,"sentence":"We propose DoTAT, a domain-oriented text annotation tool.","offset":0,"pro":0,"labels":"PUR"},{"idx":3,"sentence":"The tool designs and implements functions heavily in need in domain-oriented information extraction.","offset":1,"pro":0.1,"labels":"CTN"},{"idx":3,"sentence":"Firstly, the tool supports a multi-person collaborative process with automatically merging and review, which can greatly improve the annotation accuracy.","offset":2,"pro":0.2,"labels":"MTD"},{"idx":3,"sentence":"Secondly, the tool provides annotation of events, nested event and nested entity, which are frequently required in domain-related text structuring tasks.","offset":3,"pro":0.3,"labels":"MTD"},{"idx":3,"sentence":"Finally, DoTAT provides visual annotation specification definition, automatic batch annotation and iterative annotation to improve annotation efficiency.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":3,"sentence":"Experiments on the ACE2005 dataset show that DoTAT can reduce the event annotation time by 19.7% compared with existing annotation tools.","offset":5,"pro":0.5,"labels":"RST"},{"idx":3,"sentence":"The accuracy without review is 84.09%, 1.35% higher than Brat and 2.59% higher than Webanno.","offset":6,"pro":0.6,"labels":"RST"},{"idx":3,"sentence":"The accuracy of DoTAT even reaches 93.76% with review.","offset":7,"pro":0.7,"labels":"RST"},{"idx":3,"sentence":"The demonstration video can be accessed from https://ecust-nlp-docker.oss-cn-shanghai.aliyuncs.com/dotat{_demo.mp4.","offset":8,"pro":0.8,"labels":"CTN"},{"idx":3,"sentence":"A live demo website is available at https://github.com/FXLP/MarkTool.","offset":9,"pro":0.9,"labels":"CTN"},{"idx":4,"sentence":"Recent advances in NLP and information retrieval have given rise to a diverse set of question answering tasks that are of different formats (e.g., extractive, abstractive), require different model architectures (e.g., generative, discriminative), and setups (e.g., with or without retrieval).","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"Despite having a large number of powerful, specialized QA pipelines (which we refer to as Skills) that consider a single domain, model or setup, there exists no framework where users can easily explore and compare such pipelines and can extend them according to their needs.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":4,"sentence":"To address this issue, we present UKP-SQuARE, an extensible online QA platform for researchers which allows users to query and analyze a large collection of modern Skills via a user-friendly web interface and integrated behavioural tests.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":4,"sentence":"In addition, QA researchers can develop, manage, and share their custom Skills using our microservices that support a wide range of models (Transformers, Adapters, ONNX), datastores and retrieval techniques (e.g., sparse and dense).","offset":3,"pro":0.6,"labels":"MTD"},{"idx":4,"sentence":"UKP-SQuARE is available on https://square.ukp-lab.de","offset":4,"pro":0.8,"labels":"CTN"},{"idx":5,"sentence":"There is a growing need to model interactions between data modalities (e.g., vision, language) - both to improve AI predictions on existing tasks and to enable new applications.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"In the recent field of multimodal medical AI, integrating multiple modalities has gained widespread popularity as multimodal models have proven to improve performance, robustness, require less training samples and add complementary information.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":5,"sentence":"To improve technical reproducibility and transparency for multimodal medical tasks as well as speed up progress across medical AI, we present ViLMedic, a Vision-and-Language medical library.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":5,"sentence":"As of 2022, the library contains a dozen reference implementations replicating the state-of-the-art results for problems that range from medical visual question answering and radiology report generation to multimodal representation learning on widely adopted medical datasets.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":5,"sentence":"In addition, ViLMedic hosts a model-zoo with more than twenty pretrained models for the above tasks designed to be extensible by researchers but also simple for practitioners.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":5,"sentence":"Ultimately, we hope our reproducible pipelines can enable clinical translation and create real impact.","offset":5,"pro":0.7142857142857143,"labels":"CTN"},{"idx":5,"sentence":"The library is available at https://github.com/jbdel/vilmedic.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":6,"sentence":"Pre-trained language models have been prevailed in natural language processing and become the backbones of many NLP tasks, but the demands for computational resources have limited their applications.","offset":0,"pro":0,"labels":"GAP"},{"idx":6,"sentence":"In this paper, we introduce TextPruner, an open-source model pruning toolkit designed for pre-trained language models, targeting fast and easy model compression.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":6,"sentence":"TextPruner offers structured post-training pruning methods, including vocabulary pruning and transformer pruning, and can be applied to various models and tasks.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":6,"sentence":"We also propose a self-supervised pruning method that can be applied without the labeled data.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":6,"sentence":"Our experiments with several NLP tasks demonstrate the ability of TextPruner to reduce the model size without re-training the model.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":7,"sentence":"Open Information Extraction (OIE) is the task of extracting facts from sentences in the form of relations and their corresponding arguments in schema-free manner.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"Intrinsic performance of OIE systems is difficult to measure due to the incompleteness of existing OIE benchmarks: ground truth extractions do not group all acceptable surface realizations of the same fact that can be extracted from a sentence.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":7,"sentence":"To measure performance of OIE systems more realistically, it is necessary to manually annotate complete facts (i.e., clusters of all acceptable surface realizations of the same fact) from input sentences.","offset":2,"pro":0.25,"labels":"BAC"},{"idx":7,"sentence":"We propose AnnIE: an interactive annotation platform that facilitates such challenging annotation tasks and supports creation of complete fact-oriented OIE evaluation benchmarks.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":7,"sentence":"AnnIE is modular and flexible in order to support different use case scenarios (i.e., benchmarks covering different types of facts) and different languages.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":7,"sentence":"We use AnnIE to build two complete OIE benchmarks: one with verb-mediated facts and another with facts encompassing named entities.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":7,"sentence":"We evaluate several OIE systems on our complete benchmarks created with AnnIE.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":7,"sentence":"We publicly release AnnIE (and all gold datasets generated with it) under non-restrictive license.","offset":7,"pro":0.875,"labels":"MTD"},{"idx":8,"sentence":"The open-access dissemination of pretrained language models through online repositories has led to a democratization of state-of-the-art natural language processing (NLP) research.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"This also allows people outside of NLP to use such models and adapt them to specific use-cases.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":8,"sentence":"However, a certain amount of technical proficiency is still required which is an entry barrier for users who want to apply these models to a certain task but lack the necessary knowledge or resources.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":8,"sentence":"In this work, we aim to overcome this gap by providing a tool which allows researchers to leverage pretrained models without writing a single line of code.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":8,"sentence":"Built upon the parameter-efficient adapter modules for transfer learning, our AdapterHub Playground provides an intuitive interface, allowing the usage of adapters for prediction, training and analysis of textual data for a variety of NLP tasks.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":8,"sentence":"We present the tool's architecture and demonstrate its advantages with prototypical use-cases, where we show that predictive performance can easily be increased in a few-shot learning scenario.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":8,"sentence":"Finally, we evaluate its usability in a user study.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":8,"sentence":"We provide the code and a live interface at https://adapter-hub.github.io/playground.","offset":7,"pro":0.875,"labels":"CTN"},{"idx":9,"sentence":"Lyrics generation has been a very popular application of natural language generation.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"Previous works mainly focused on generating lyrics based on a couple of attributes or keywords, rendering very limited control over the content of the lyrics.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":9,"sentence":"In this paper, we demonstrate the QiuNiu, a Chinese lyrics generation system which is conditioned on passage-level text rather than a few attributes or keywords.","offset":2,"pro":0.2222222222222222,"labels":"PUR"},{"idx":9,"sentence":"By using the passage-level text as input, the content of generated lyrics is expected to reflect the nuances of users' needs.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":9,"sentence":"The QiuNiu system supports various forms of passage-level input, such as short stories, essays, poetry.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":9,"sentence":"The training of it is conducted under the framework of unsupervised machine translation, due to the lack of aligned passage-level text-to-lyrics corpus.","offset":5,"pro":0.5555555555555556,"labels":"GAP"},{"idx":9,"sentence":"We initialize the parameters of QiuNiu with a custom pretrained Chinese GPT-2 model and adopt a two-step process to finetune the model for better alignment between passage-level text and lyrics.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":9,"sentence":"Additionally, a postprocess module is used to filter and rerank the generated lyrics to select the ones of highest quality.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":9,"sentence":"The demo video of the system is available at https://youtu.be/OCQNzahqWgM.","offset":8,"pro":0.8888888888888888,"labels":"CTN"},{"idx":10,"sentence":"A multi-language dictionary is a fundamental tool for language learning, allowing the learner to look up unfamiliar words.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"Searching an unrecognized word in the dictionary does not usually require deep knowledge of the target language.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":10,"sentence":"However, this is not true for sign language, where gestural elements preclude this type of easy lookup.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":10,"sentence":"This paper introduces GlossFinder, an online tool supporting 2, 000 signs to assist language learners in determining the meaning of given signs.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":10,"sentence":"Unlike alternative systems of complex inputs, our system requires only that learners imitate the sign in front of a standard webcam.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":10,"sentence":"A user study conducted among sign language speakers of varying ability compared our system against existing alternatives and the interviews indicated a clear preference for our new system.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":10,"sentence":"This implies that GlossFinder can lower the barrier in sign language learning by addressing the common problem of sign finding and make it accessible to the wider community.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":11,"sentence":"PromptSource is a system for creating, sharing, and using natural language prompts.","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"Prompts are functions that map an example from a dataset to a natural language input and target output.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":11,"sentence":"Using prompts to train and query language models is an emerging area in NLP that requires new tools that let users develop and refine these prompts collaboratively.","offset":2,"pro":0.3333333333333333,"labels":"BAC"},{"idx":11,"sentence":"PromptSource addresses the emergent challenges in this new setting with (1) a templating language for defining data-linked prompts, (2) an interface that lets users quickly iterate on prompt development by observing outputs of their prompts on many examples, and (3) a community-driven set of guidelines for contributing new prompts to a common pool.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":11,"sentence":"Over 2,000 prompts for roughly 170 datasets are already available in PromptSource.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":11,"sentence":"PromptSource is available at https://github.com/bigscience-workshop/promptsource.","offset":5,"pro":0.8333333333333334,"labels":"CTN"},{"idx":12,"sentence":"Prompt-learning has become a new paradigm in modern natural language processing, which directly adapts pre-trained language models (PLMs) to cloze-style prediction, autoregressive modeling, or sequence to sequence generation, resulting in promising performances on various tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"However, no standard implementation framework of prompt-learning is proposed yet, and most existing prompt- learning codebases, often unregulated, only provide limited implementations for specific scenarios.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":12,"sentence":"Since there are many details such as templating strategy, initializing strategy, verbalizing strategy, etc., that need to be considered in prompt-learning, practitioners face impediments to quickly adapting the de-sired prompt learning methods to their applications.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":12,"sentence":"In this paper, we present Open- Prompt, a unified easy-to-use toolkit to conduct prompt-learning over PLMs.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":12,"sentence":"OpenPrompt is a research-friendly framework that is equipped with efficiency, modularity, and extendibility, and its combinability allows the freedom to combine different PLMs, task for- mats, and prompting modules in a unified paradigm.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":12,"sentence":"Users could expediently deploy prompt-learning frameworks and evaluate the generalization of them on different NLP tasks without constraints.","offset":5,"pro":0.8333333333333334,"labels":"BAC"},{"idx":13,"sentence":"Collecting data for conversational semantic parsing is a time-consuming and demanding process.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"In this paper we consider, given an incomplete dataset with only a small amount of data, how to build an AI-powered human-in-the-loop process to enable efficient data collection.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":13,"sentence":"A guided K-best selection process is proposed, which (i) generates a set of possible valid candidates; (ii) allows users to quickly traverse the set and filter incorrect parses; and (iii) asks users to select the correct parse, with minimal modification when necessary.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":13,"sentence":"We investigate how to best support users in efficiently traversing the candidate set and locating the correct parse, in terms of speed and accuracy.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":13,"sentence":"In our user study, consisting of five annotators labeling 300 instances each, we find that combining keyword searching, where keywords can be used to query relevant candidates, and keyword suggestion, where representative keywords are automatically generated, enables fast and accurate annotation.","offset":4,"pro":0.8,"labels":"MTD"},{"idx":14,"sentence":"Natural Language Processing (NLP) s applied nature makes it necessary to select the most effective and robust models.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"Producing slightly higher performance is insufficient; we want to know whether this advantage will carry over to other data sets.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":14,"sentence":"Bootstrapped significance tests can indicate that ability.","offset":2,"pro":0.3333333333333333,"labels":"CLN"},{"idx":14,"sentence":"So while necessary, computing the significance of models' performance differences has many levels of complexity.","offset":3,"pro":0.5,"labels":"GAP"},{"idx":14,"sentence":"It can be tedious, especially when the experimental design has many conditions to compare and several runs of experiments.","offset":4,"pro":0.6666666666666666,"labels":"BAC"},{"idx":14,"sentence":"We present BooStSa, a tool that makes it easy to compute significance levels with the BOOtSTrap SAmpling procedure to evaluate models that predict not only standard hard labels but soft-labels (i.e., probability distributions over different classes) as well.","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":15,"sentence":"To tackle the challenge of accurate and timely communication regarding the COVID-19 pandemic, we present a COVID-19 Claim Radar to automatically extract supporting and refuting claims on a daily basis.","offset":0,"pro":0,"labels":"PUR"},{"idx":15,"sentence":"We provide a comprehensive structured view of claims, including rich claim attributes (such as claimers and claimer affiliations) and associated knowledge elements as claim semantics (such as events, relations and entities), enabling users to explore equivalent, refuting, or supporting claims with structural evidence, such as shared claimers, similar centroid events and arguments.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":15,"sentence":"In order to consolidate claim structures at the corpus-level, we leverage Wikidata as the hub to merge coreferential knowledge elements.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":15,"sentence":"The system automatically provides users a comprehensive exposure to COVID-19 related claims, their importance, and their interconnections.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":15,"sentence":"The system is publicly available at GitHub and DockerHub, with complete documentation.","offset":4,"pro":0.8,"labels":"CTN"},{"idx":16,"sentence":"We introduce TS-ANNO, an open-source web application for manual creation and for evaluation of parallel corpora for text simplification.","offset":0,"pro":0,"labels":"PUR"},{"idx":16,"sentence":"TS-ANNO can be used for i) sentence-wise alignment, ii) rating alignment pairs (e.g., w.r.t. grammaticality, meaning preservation, ...), iii) annotating alignment pairs w.r.t. simplification transformations (e.g., lexical substitution, sentence splitting, ...), and iv) manual simplification of complex documents.","offset":1,"pro":0.3333333333333333,"labels":"MTD"},{"idx":16,"sentence":"For evaluation, TS-ANNO calculates inter-annotator agreement of alignments (i) and annotations (ii).","offset":2,"pro":0.6666666666666666,"labels":"MTD"},{"idx":17,"sentence":"The Universal Knowledge Core (UKC) is a large multilingual lexical database with a focus on language diversity and covering over two thousand languages.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"The aim of the database, as well as its tools and data catalogue, is to make the abstract notion of linguistic diversity visually understandable for humans and formally exploitable by machines.","offset":1,"pro":0.25,"labels":"PUR"},{"idx":17,"sentence":"The UKC website lets users explore millions of individual words and their meanings, but also phenomena of cross-lingual convergence and divergence, such as shared interlingual meanings, lexicon similarities, cognate clusters, or lexical gaps.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":17,"sentence":"The UKC LiveLanguage Catalogue, in turn, provides access to the underlying lexical data in a computer-processable form, ready to be reused in cross-lingual applications.","offset":3,"pro":0.75,"labels":"MTD"},{"idx":18,"sentence":"In this paper, we propose CogKGE, a knowledge graph embedding (KGE) toolkit, which aims to represent multi-source and heterogeneous knowledge.","offset":0,"pro":0,"labels":"PUR"},{"idx":18,"sentence":"For multi-source knowledge, unlike existing methods that mainly focus on entity-centric knowledge, CogKGE also supports the representations of event-centric, commonsense and linguistic knowledge.","offset":1,"pro":0.1111111111111111,"labels":"MTD"},{"idx":18,"sentence":"For heterogeneous knowledge, besides structured triple facts, CogKGE leverages additional unstructured information, such as text descriptions, node types and temporal information, to enhance the meaning of embeddings.","offset":2,"pro":0.2222222222222222,"labels":"MTD"},{"idx":18,"sentence":"Designing CogKGE aims to provide a unified programming framework for KGE tasks and a series of knowledge representations for downstream tasks.","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":18,"sentence":"As a research framework, CogKGE consists of five parts, including core, data, model, knowledge and adapter module.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":18,"sentence":"As a knowledge discovery toolkit, CogKGE provides pre-trained embedders to discover new facts, cluster entities and check facts.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":18,"sentence":"Furthermore, we construct two benchmark datasets for further research on multi-source heterogeneous KGE tasks: EventKG240K and CogNet360K.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":18,"sentence":"We also release an online system to discover knowledge visually.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":18,"sentence":"Source code, datasets and pre-trained embeddings are publicly available at GitHub, with a short instruction video.","offset":8,"pro":0.8888888888888888,"labels":"CTN"},{"idx":19,"sentence":"We introduce Dynatask: an open source system for setting up custom NLP tasks that aims to greatly lower the technical knowledge and effort required for hosting and evaluating state-of-the-art NLP models, as well as for conducting model in the loop data collection with crowdworkers.","offset":0,"pro":0,"labels":"PUR"},{"idx":19,"sentence":"Dynatask is integrated with Dynabench, a research platform for rethinking benchmarking in AI that facilitates human and model in the loop data collection and evaluation.","offset":1,"pro":0.25,"labels":"MTD"},{"idx":19,"sentence":"To create a task, users only need to write a short task configuration file from which the relevant web interfaces and model hosting infrastructure are automatically generated.","offset":2,"pro":0.5,"labels":"BAC"},{"idx":19,"sentence":"The system is available at https://dynabench.org/ and the full library can be found at https://github.com/facebookresearch/dynabench.","offset":3,"pro":0.75,"labels":"CTN"},{"idx":0,"sentence":"Despite data's crucial role in machine learning, most existing tools and research tend to focus on systems on top of existing data rather than how to interpret and manipulate data.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"In this paper, we propose DataLab, a unified data-oriented platform that not only allows users to interactively analyze the characteristics of data but also provides a standardized interface so that many data processing operations can be provided within a unified interface.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":0,"sentence":"Additionally, in view of the ongoing surge in the proliferation of datasets, DataLab has features for dataset recommendation and global vision analysis that help researchers form a better view of the data ecosystem.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":0,"sentence":"So far, DataLab covers 1,300 datasets and 3,583 of its transformed version, where 313 datasets support different types of analysis (e.g., with respect to gender bias) with the help of 119M samples annotated by 318 feature functions.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":0,"sentence":"DataLab is under active development and will be supported going forward.","offset":4,"pro":0.6666666666666666,"labels":"CTN"},{"idx":0,"sentence":"We have released a web platform, web API, Python SDK, and PyPI published package, which hopefully, can meet the diverse needs of researchers.","offset":5,"pro":0.8333333333333334,"labels":"CTN"},{"idx":1,"sentence":"Intelligent conversational assistants have become an integral part of our lives for performing simple tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"However, such agents, for example, Google bots, Alexa and others are yet to have any social impact on minority population, for example, for people with neurological disorders and people with speech, language and social communication disorders, sometimes with locked-in states where speaking or typing is a challenge.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":1,"sentence":"Language model technologies can be very powerful tools in enabling these users to carry out daily communication and social interactions.","offset":2,"pro":0.3333333333333333,"labels":"BAC"},{"idx":1,"sentence":"In this work, we present a system that users with varied levels of disabilties can use to interact with the world, supported by eye-tracking, mouse controls and an intelligent agent Cue-bot, that can represent the user in a conversation.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":1,"sentence":"The agent provides relevant controllable cues' to generate desirable responses quickly for an ongoing dialog context.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":1,"sentence":"In the context of usage of such systems for people with degenerative disorders, we present automatic and human evaluation of our cue/keyword predictor and the controllable dialog system and show that our models perform significantly better than models without control and can also reduce user effort (fewer keystrokes) and speed up communication (typing time) significantly.","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":2,"sentence":"M-SENA is an open-sourced platform for Multimodal Sentiment Analysis.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"It aims to facilitate advanced research by providing flexible toolkits, reliable benchmarks, and intuitive demonstrations.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":2,"sentence":"The platform features a fully modular video sentiment analysis framework consisting of data management, feature extraction, model training, and result analysis modules.","offset":2,"pro":0.2857142857142857,"labels":"BAC"},{"idx":2,"sentence":"In this paper, we first illustrate the overall architecture of the M-SENA platform and then introduce features of the core modules.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":2,"sentence":"Reliable baseline results of different modality features and MSA benchmarks are also reported.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":2,"sentence":"Moreover, we use model evaluation and analysis tools provided by M-SENA to present intermediate representation visualization, on-the-fly instance test, and generalization ability test results.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":2,"sentence":"The source code of the platform is publicly available at https://github.com/thuiar/M-SENA.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":3,"sentence":"We investigate the usage of entity linking (EL)in downstream tasks and present the first modularized EL toolkit for easy task adaptation.","offset":0,"pro":0,"labels":"PUR"},{"idx":3,"sentence":"Different from the existing EL methods that dealwith all the features simultaneously, we modularize the whole model into separate parts witheach feature.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":3,"sentence":"This decoupled design enablesflexibly adding new features without retraining the whole model as well as flow visualization with better interpretability of the ELresult.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":3,"sentence":"We release the corresponding toolkit,HOSMEL, for Chinese, with three flexible usage modes, a live demo, and a demonstrationvideo.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":3,"sentence":"Experiments on two benchmarks forthe question answering task demonstrate thatHOSMEL achieves much less time and spaceconsumption as well as significantly better accuracy performance compared with existingSOTA EL methods.","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":3,"sentence":"We hope the release ofHOSMEL will call for more attention to studyEL for downstream tasks in non-English languages.","offset":5,"pro":0.8333333333333334,"labels":"CTN"},{"idx":4,"sentence":"In recent years, large-scale pre-trained language models (PLMs) containing billions of parameters have achieved promising results on various NLP tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"Although we can pre-train these big models by stacking computing clusters at any cost, it is impractical to use such huge computing resources to apply big models for each downstream task.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":4,"sentence":"To address the computation bottleneck encountered in deploying big models in real-world scenarios, we introduce an open-source toolkit for big model inference and tuning (BMInf), which can support big model inference and tuning at extremely low computation cost.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":4,"sentence":"More specifically, at the algorithm level, we introduce model quantization and parameter-efficient tuning for efficient model inference and tuning.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":4,"sentence":"At the implementation level, we apply model offloading, model checkpointing, and CPU-GPU scheduling optimization to further reduce the computation and memory cost of big models.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":4,"sentence":"Based on above efforts, we can efficiently perform big model inference and tuning with a single GPU (even a consumer-level GPU like GTX 1060) instead of computing clusters, which is difficult for existing distributed learning toolkits for PLMs.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":4,"sentence":"BMInf is publicly released at https://github.com/OpenBMB/BMInf.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":5,"sentence":"Events are fundamental building blocks of real-world happenings.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"In this paper, we present a large-scale, multi-modal event knowledge graph named MMEKG.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":5,"sentence":"MMEKG unifies different modalities of knowledge via events, which complement and disambiguate each other.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":5,"sentence":"Specifically, MMEKG incorporates (i) over 990 thousand concept events with 644 relation types to cover most types of happenings, and (ii) over 863 million instance events connected through 934 million relations, which provide rich contextual information in texts and/or images.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":5,"sentence":"To collect billion-scale instance events and relations among them, we additionally develop an efficient yet effective pipeline for textual/visual knowledge extraction system.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":5,"sentence":"We also develop an induction strategy to create million-scale concept events and a schema organizing all events and relations in MMEKG.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":5,"sentence":"To this end, we also provide a pipeline enabling our system to seamlessly parse texts/images to event graphs and to retrieve multi-modal knowledge at both concept- and instance-levels.","offset":6,"pro":0.8571428571428571,"labels":"MTD"},{"idx":6,"sentence":"SOCIOFILLMORE is a multilingual tool which helps to bring to the fore the focus or the perspective that a text expresses in depicting an event.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"Our tool, whose rationale we also support through a large collection of human judgements, is theoretically grounded on frame semantics and cognitive linguistics, and implemented using the LOME frame semantic parser.","offset":1,"pro":0.3333333333333333,"labels":"MTD"},{"idx":6,"sentence":"We describe SOCIOFILLMORE's development and functionalities, ","offset":2,"pro":0.6666666666666666,"labels":"MTD"},{"idx":7,"sentence":"Despite its importance, the time variable has been largely neglected in the NLP and language model literature.","offset":0,"pro":0,"labels":"GAP"},{"idx":7,"sentence":"In this paper, we present TimeLMs, a set of language models specialized on diachronic Twitter data.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":7,"sentence":"We show that a continual learning strategy contributes to enhancing Twitter-based language models' capacity to deal with future and out-of-distribution tweets, while making them competitive with standardized and more monolithic benchmarks.","offset":2,"pro":0.4,"labels":"CLN"},{"idx":7,"sentence":"We also perform a number of qualitative analyses showing how they cope with trends and peaks in activity involving specific named entities or concept drift.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":7,"sentence":"TimeLMs is available at github.com/cardiffnlp/timelms.","offset":4,"pro":0.8,"labels":"CTN"},{"idx":8,"sentence":"This paper introduces Adaptor library, which transposes traditional model-centric approach composed of pre-training + fine-tuning steps to objective-centric approach, composing the training process by applications of selected objectives.","offset":0,"pro":0,"labels":"PUR"},{"idx":8,"sentence":"We survey research directions that can benefit from enhanced objective-centric experimentation in multitask training, custom objectives development, dynamic training curricula, or domain adaptation.","offset":1,"pro":0.25,"labels":"MTD"},{"idx":8,"sentence":"Adaptor aims to ease reproducibility of these research directions in practice.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":8,"sentence":"Finally, we demonstrate the practical applicability of Adaptor in selected unsupervised domain adaptation scenarios.","offset":3,"pro":0.75,"labels":"CLN"},{"idx":9,"sentence":"Acquiring high-quality annotated corpora for complex multi-task information extraction (MT-IE) is an arduous and costly process for human-annotators.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"Adoption of unsupervised techniques for automated annotation have thus become popular.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":9,"sentence":"However, these techniques rely heavily on dictionaries, gazetteers, and knowledge bases.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":9,"sentence":"While such resources are abundant for general domains, they are scarce for specialised technical domains.","offset":3,"pro":0.5,"labels":"GAP"},{"idx":9,"sentence":"In this paper, we discuss these key features and qualitatively compare QuickGraph to existing annotation tools.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":9,"sentence":"To tackle this challenge, we present QuickGraph, the first collaborative MT-IE annotation tool built with indirect weak supervision and clustering to maximise annotator productivity.","offset":5,"pro":0.8333333333333334,"labels":"PUR"},{"idx":10,"sentence":"Offensive language detection (OLD) has received increasing attention due to its societal impact.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"Recent work shows that bidirectional transformer based methods obtain impressive performance on OLD.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":10,"sentence":"However, such methods usually rely on large-scale well-labeled OLD datasets for model training.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":10,"sentence":"To address the issue of data/label scarcity in OLD, in this paper, we propose a simple yet effective domain adaptation approach to train bidirectional transformers.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":10,"sentence":"Our approach introduces domain adaptation (DA) training procedures to ALBERT, such that it can effectively exploit auxiliary data from source domains to improve the OLD performance in a target domain.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":10,"sentence":"Experimental results on benchmark datasets show that our approach, ALBERT (DA), obtains the state-of-the-art performance in most cases.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":10,"sentence":"Particularly, our approach significantly benefits underrepresented and under-performing classes, with a significant improvement over ALBERT.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":11,"sentence":"Hate speech and profanity detection suffer from data sparsity, especially for languages other than English, due to the subjective nature of the tasks and the resulting annotation incompatibility of existing corpora.","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"In this study, we identify profane subspaces in word and sentence representations and explore their generalization capability on a variety of similar and distant target tasks in a zero-shot setting.","offset":1,"pro":0.25,"labels":"PUR"},{"idx":11,"sentence":"This is done monolingually (German) and cross-lingually to closely-related (English), distantly-related (French) and non-related (Arabic) tasks.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":11,"sentence":"We observe that, on both similar and distant target tasks and across all languages, the subspace-based representations transfer more effectively than standard BERT representations in the zero-shot setting, with improvements between F1 +10.9 and F1 +42.9 over the baselines across all tested monolingual and cross-lingual scenarios.","offset":3,"pro":0.75,"labels":"CLN"},{"idx":12,"sentence":"We introduce HateBERT, a re-trained BERT model for abusive language detection in English.","offset":0,"pro":0,"labels":"PUR"},{"idx":12,"sentence":"The model was trained on RAL-E, a large-scale dataset of Reddit comments in English from communities banned for being offensive, abusive, or hateful that we have curated and made available to the public.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":12,"sentence":"We present the results of a detailed comparison between a general pre-trained language model and the retrained version on three English datasets for offensive, abusive language and hate speech detection tasks.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":12,"sentence":"In all datasets, HateBERT outperforms the corresponding general BERT model.","offset":3,"pro":0.6,"labels":"RST"},{"idx":12,"sentence":"We also discuss a battery of experiments comparing the portability of the fine-tuned models across the datasets, suggesting that portability is affected by compatibility of the annotated phenomena.","offset":4,"pro":0.8,"labels":"MTD"},{"idx":13,"sentence":"Hateful memes pose a unique challenge for current machine learning systems because their message is derived from both text- and visual-modalities.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"To this effect, Facebook released the Hateful Memes Challenge, a dataset of memes with pre-extracted text captions, but it is unclear whether these synthetic examples generalize to memes in the wild'.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":13,"sentence":"In this paper, we collect hateful and non-hateful memes from Pinterest to evaluate out-of-sample performance on models pre-trained on the Facebook dataset.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":13,"sentence":"We find that memes in the wild' differ in two key aspects: 1) Captions must be extracted via OCR, injecting noise and diminishing performance of multimodal models, and 2) Memes are more diverse than traditional memes', including screenshots of conversations or text on a plain background.","offset":3,"pro":0.6,"labels":"CLN"},{"idx":13,"sentence":"This paper thus serves as a reality-check for the current benchmark of hateful meme detection and its applicability for detecting real world hate.","offset":4,"pro":0.8,"labels":"CTN"},{"idx":14,"sentence":"Content moderation is often performed by a collaboration between humans and machine learning models.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"However, it is not well understood how to design the collaborative process so as to maximize the combined moderator-model system performance.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":14,"sentence":"This work presents a rigorous study of this problem, focusing on an approach that incorporates model uncertainty into the collaborative process.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":14,"sentence":"First, we introduce principled metrics to describe the performance of the collaborative system under capacity constraints on the human moderator, quantifying how efficiently the combined system utilizes human decisions.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":14,"sentence":"Using these metrics, we conduct a large benchmark study evaluating the performance of state-of-the-art uncertainty models under different collaborative review strategies.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":14,"sentence":"We find that an uncertainty-based strategy consistently outperforms the widely used strategy based on toxicity scores, and moreover that the choice of review strategy drastically changes the overall system performance.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":14,"sentence":"Our results demonstrate the importance of rigorous metrics for understanding and developing effective moderator-model systems for content moderation, as well as the utility of uncertainty estimation in this domain.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":15,"sentence":"As socially unacceptable language become pervasive in social media platforms, the need for automatic content moderation become more pressing.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"This contribution introduces the Dutch Abusive Language Corpus (DALC v1.0), a new dataset with tweets manually an- notated for abusive language.","offset":1,"pro":0.25,"labels":"PUR"},{"idx":15,"sentence":"The resource ad- dress a gap in language resources for Dutch and adopts a multi-layer annotation scheme modeling the explicitness and the target of the abusive messages.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":15,"sentence":"Baselines experiments on all annotation layers have been conducted, achieving a macro F1 score of 0.748 for binary classification of the explicitness layer and 0.489 for target classification.","offset":3,"pro":0.75,"labels":"RST"},{"idx":16,"sentence":"Social media texts such as blog posts, comments, and tweets often contain offensive languages including racial hate speech comments, personal attacks, and sexual harassment.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"Detecting inappropriate use of language is, therefore, of utmost importance for the safety of the users as well as for suppressing hateful conduct and aggression.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":16,"sentence":"Existing approaches to this problem are mostly available for resource-rich languages such as English and German.","offset":2,"pro":0.3333333333333333,"labels":"BAC"},{"idx":16,"sentence":"In this paper, we characterize the offensive language in Nepali, a low-resource language, highlighting the challenges that need to be addressed for processing Nepali social media text.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":16,"sentence":"We also present experiments for detecting offensive language using supervised machine learning.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":16,"sentence":"Besides contributing the first baseline approaches of detecting offensive language in Nepali, we also release human annotated data sets to encourage future research on this crucial topic.","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":17,"sentence":"Hate speech-related lexicons have been proved to be useful for many tasks such as data collection and classification.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"However, existing Portuguese lexicons do not distinguish between European and Brazilian Portuguese, and do not include neutral terms that are potentially useful to detect a broader spectrum of content referring to minorities.","offset":1,"pro":0.25,"labels":"GAP"},{"idx":17,"sentence":"In this work, we present MIN_PT, a new European Portuguese Lexicon for Minorities-Related Terms specifically designed to tackle the limitations of existing resources.","offset":2,"pro":0.5,"labels":"PUR"},{"idx":17,"sentence":"We describe the data collection and annotation process, discuss the limitation and ethical concerns, and prove the utility of the resource by applying it to a use case for the Portuguese 2021 presidential elections.","offset":3,"pro":0.75,"labels":"MTD"},{"idx":18,"sentence":"Current abusive language detection systems have demonstrated unintended bias towards sensitive features such as nationality or gender.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"This is a crucial issue, which may harm minorities and underrepresented groups if such systems were integrated in real-world applications.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":18,"sentence":"In this paper, we create ad hoc tests through the CheckList tool (Ribeiro et al., 2020) to detect biases within abusive language classifiers for English.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":18,"sentence":"We compare the behaviour of two BERT-based models, one trained on a generic hate speech dataset and the other on a dataset for misogyny detection.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":18,"sentence":"Our evaluation shows that, although BERT-based classifiers achieve high accuracy levels on a variety of natural language processing tasks, they perform very poorly as regards fairness and bias, in particular on samples involving implicit stereotypes, expressions of hate towards minorities and protected attributes such as race or sexual orientation.","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":18,"sentence":"We release both the notebooks implemented to extend the Fairness tests and the synthetic datasets usable to evaluate systems bias independently of CheckList.","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":19,"sentence":"Bias mitigation approaches reduce models' dependence on sensitive features of data, such as social group tokens (SGTs), resulting in equal predictions across the sensitive features.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"In hate speech detection, however, equalizing model predictions may ignore important differences among targeted social groups, as hate speech can contain stereotypical language specific to each SGT.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":19,"sentence":"Here, to take the specific language about each SGT into account, we rely on counterfactual fairness and equalize predictions among counterfactuals, generated by changing the SGTs.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":19,"sentence":"Our method evaluates the similarity in sentence likelihoods (via pre-trained language models) among counterfactuals, to treat SGTs equally only within interchangeable contexts.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":19,"sentence":"By applying logit pairing to equalize outcomes on the restricted set of counterfactuals for each instance, we improve fairness metrics while preserving model performance on hate speech detection.","offset":4,"pro":0.8,"labels":"MTD"},{"idx":0,"sentence":"There have been several attempts to create an accurate and thorough emotion lexicon in English, which identifies the emotional content of words.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"Of the several commonly used resources, the NRC emotion lexicon (Mohammad and Turney, 2013b) has received the most attention due to its availability, size, and its choice of Plutchik's expressive 8-class emotion model.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":0,"sentence":"In this paper we identify a large number of troubling entries in the NRC lexicon, where words that should in most contexts be emotionally neutral, with no affect (e.g., lesbian', stone', mountain'), are associated with emotional labels that are inaccurate, nonsensical, pejorative, or, at best, highly contingent and context-dependent (e.g., lesbian' labeled as Disgust and Sadness, stone' as Anger, or mountain' as Anticipation).","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":0,"sentence":"We describe a procedure for semi-automatically correcting these problems in the NRC, which includes disambiguating POS categories and aligning NRC entries with other emotion lexicons to infer the accuracy of labels.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":0,"sentence":"We demonstrate via an experimental benchmark that the quality of the resources is thus improved.","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":0,"sentence":"We release the revised resource and our code to enable other researchers to reproduce and build upon results.","offset":5,"pro":0.8333333333333334,"labels":"CTN"},{"idx":1,"sentence":"Automatic detection of toxic language plays an essential role in protecting social media users, especially minority groups, from verbal abuse.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"However, biases toward some attributes, including gender, race, and dialect, exist in most training datasets for toxicity detection.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":1,"sentence":"Considering that current debiasing methods for general natural language understanding tasks cannot effectively mitigate the biases in the toxicity detectors, we propose to use invariant rationalization (InvRat), a game-theoretic framework consisting of a rationale generator and a predictor, to rule out the spurious correlation of certain syntactic patterns (e.g., identity mentions, dialect) to toxicity labels.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":1,"sentence":"We empirically show that our method yields lower false positive rate in both lexical and dialectal attributes than previous debiasing methods.","offset":3,"pro":0.6,"labels":"RST"},{"idx":1,"sentence":"The biases make the learned models unfair and can even exacerbate the marginalization of people.","offset":4,"pro":0.8,"labels":"GAP"},{"idx":2,"sentence":"While earlier work on hyperpartisan news detection uses binary classification (i.e., hyperpartisan or not) and English data, we argue for a more fine-grained classification, covering the full political spectrum (i.e., far-left, left, centre, right, far-right) and for extending research to German data.","offset":0,"pro":0,"labels":"PUR"},{"idx":2,"sentence":"Understanding political bias helps in accurately detecting hate speech and online abuse.","offset":1,"pro":0.25,"labels":"BAC"},{"idx":2,"sentence":"We experiment with different classification methods for political bias detection.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":2,"sentence":"We present a data set consisting of German news articles labeled for political bias on a five-point scale in a semi-supervised way.","offset":3,"pro":0.75,"labels":"PUR"},{"idx":3,"sentence":"Online abuse and offensive language on social media have become widespread problems in today's digital age.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"In this paper, we contribute a Reddit-based dataset, consisting of 68,159 insults and 51,102 compliments targeted at individuals instead of targeting a particular community or race.","offset":1,"pro":0.25,"labels":"PUR"},{"idx":3,"sentence":"Secondly, we benchmark multiple existing state-of-the-art models for both classification and unsupervised style transfer on the dataset.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":3,"sentence":"Finally, we analyse the experimental results and conclude that the transfer task is challenging, requiring the models to understand the high degree of creativity exhibited in the data.","offset":3,"pro":0.75,"labels":"RST"},{"idx":4,"sentence":"User posts whose perceived toxicity depends on the conversational context are rare in current toxicity detection datasets.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"Hence, toxicity detectors trained on current datasets will also disregard context, making the detection of context-sensitive toxicity a lot harder when it occurs.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":4,"sentence":"We constructed and publicly release a dataset of 10k posts with two kinds of toxicity labels per post, obtained from annotators who considered (i) both the current post and the previous one as context, or (ii) only the current post.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":4,"sentence":"We introduce a new task, context-sensitivity estimation, which aims to identify posts whose perceived toxicity changes if the context (previous post) is also considered.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":4,"sentence":"Using the new dataset, we show that systems can be developed for this task.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":4,"sentence":"Such systems could be used to enhance toxicity detection datasets with more context-dependent posts or to suggest when moderators should consider the parent posts, which may not always be necessary and may introduce additional costs.","offset":5,"pro":0.8333333333333334,"labels":"GAP"},{"idx":5,"sentence":"In this paper, we introduce a new English Twitter-based dataset for cyberbullying detection and online abuse.","offset":0,"pro":0,"labels":"PUR"},{"idx":5,"sentence":"Comprising 62,587 tweets, this dataset was sourced from Twitter using specific query terms designed to retrieve tweets with high probabilities of various forms of bullying and offensive content, including insult, trolling, profanity, sarcasm, threat, porn and exclusion.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":5,"sentence":"We recruited a pool of 17 annotators to perform fine-grained annotation on the dataset with each tweet annotated by three annotators.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":5,"sentence":"All our annotators are high school educated and frequent users of social media.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":5,"sentence":"Analysis performed on the dataset confirmed common cyberbullying themes reported by other studies and revealed interesting relationships between the classes.","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":5,"sentence":"Inter-rater agreement for the dataset as measured by Krippendorff's Alpha is 0.67.","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":6,"sentence":"With the rise of research on toxic comment classification, more and more annotated datasets have been released.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"The wide variety of the task (different languages, different labeling processes and schemes) has led to a large amount of heterogeneous datasets that can be used for training and testing very specific settings.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":6,"sentence":"Despite recent efforts to create web pages that provide an overview, most publications still use only a single dataset.","offset":2,"pro":0.4,"labels":"GAP"},{"idx":6,"sentence":"They are not stored in one central database, they come in many different data formats and it is difficult to interpret their class labels and how to reuse these labels in other projects.","offset":3,"pro":0.6,"labels":"GAP"},{"idx":6,"sentence":"To overcome these issues, we present a collection of more than thirty datasets in the form of a software tool that automatizes downloading and processing of the data and presents them in a unified data format that also offers a mapping of compatible class labels.","offset":4,"pro":0.8,"labels":"PUR"},{"idx":7,"sentence":"Community-level bans are a common tool against groups that enable online harassment and harmful speech.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"Unfortunately, the efficacy of community bans has only been partially studied and with mixed results.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":7,"sentence":"We use a simple word frequency divergence to identify uncommon words overrepresented in a given community, not as a proxy for harmful speech but as a linguistic signature of the community.","offset":2,"pro":0.2222222222222222,"labels":"MTD"},{"idx":7,"sentence":"Top users were more likely to become less active overall, while random users often reduced use of in-group language without decreasing activity.","offset":3,"pro":0.3333333333333333,"labels":"RST"},{"idx":7,"sentence":"Finally, we find some evidence that the effectiveness of bans aligns with the content of a community.","offset":4,"pro":0.4444444444444444,"labels":"RST"},{"idx":7,"sentence":"Users of dark humor communities were largely unaffected by bans while users of communities organized around white supremacy and fascism were the most affected.","offset":5,"pro":0.5555555555555556,"labels":"RST"},{"idx":7,"sentence":"Altogether, our results show that bans do not affect all groups or users equally, and pave the way to understanding the effect of bans across communities.","offset":6,"pro":0.6666666666666666,"labels":"CLN"},{"idx":7,"sentence":"Here, we provide a flexible unsupervised methodology to identify in-group language and track user activity on Reddit both before and after the ban of a community (subreddit).","offset":7,"pro":0.7777777777777778,"labels":"PUR"},{"idx":7,"sentence":"We apply our method to 15 banned subreddits, ","offset":8,"pro":0.8888888888888888,"labels":"MTD"},{"idx":8,"sentence":"Mainstream research on hate speech focused so far predominantly on the task of classifying mainly social media posts with respect to predefined typologies of rather coarse-grained hate speech categories.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"However, removal is not always possible due to the legislation of a country.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":8,"sentence":"As the first approximation, we propose to adapt a generic state-of-the-art concept extraction model to the hate speech domain.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":8,"sentence":"The outcome of the experiments is promising and can serve as inspiration for further work on the task","offset":3,"pro":0.42857142857142855,"labels":"CLN"},{"idx":8,"sentence":"This may be sufficient if the goal is to detect and delete abusive language posts.","offset":4,"pro":0.5714285714285714,"labels":"BAC"},{"idx":8,"sentence":"Also, there is evidence that hate speech cannot be successfully combated by merely removing hate speech posts; they should be countered by education and counter-narratives.","offset":5,"pro":0.7142857142857143,"labels":"GAP"},{"idx":8,"sentence":"For this purpose, we need to identify (i) who is the target in a given hate speech post, and (ii) what aspects (or characteristics) of the target are attributed to the target in the post.","offset":6,"pro":0.8571428571428571,"labels":"PUR"},{"idx":9,"sentence":"Abusive language is a growing phenomenon on social media platforms.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"Its effects can reach beyond the online context, contributing to mental or emotional stress on users.","offset":1,"pro":0.09090909090909091,"labels":"BAC"},{"idx":9,"sentence":"Automatic tools for detecting abuse can alleviate the issue.","offset":2,"pro":0.18181818181818182,"labels":"BAC"},{"idx":9,"sentence":"In practice, developing automated methods to detect abusive language relies on good quality data.","offset":3,"pro":0.2727272727272727,"labels":"BAC"},{"idx":9,"sentence":"However, there is currently a lack of standards for creating datasets in the field.","offset":4,"pro":0.36363636363636365,"labels":"GAP"},{"idx":9,"sentence":"This paper introduces an annotation framework inspired by legal concepts to define abusive language in the context of online harassment.","offset":5,"pro":0.45454545454545453,"labels":"PUR"},{"idx":9,"sentence":"The framework uses a 7-point Likert scale for labelling instead of class labels.","offset":6,"pro":0.5454545454545454,"labels":"MTD"},{"idx":9,"sentence":"We also present ALYT - a dataset of Abusive Language on YouTube.","offset":7,"pro":0.6363636363636364,"labels":"MTD"},{"idx":9,"sentence":"ALYT includes YouTube comments in English extracted from videos on different controversial topics and labelled by Law students.","offset":8,"pro":0.7272727272727273,"labels":"MTD"},{"idx":9,"sentence":"The comments were sampled from the actual collected data, without artificial methods for increasing the abusive content.","offset":9,"pro":0.8181818181818182,"labels":"MTD"},{"idx":9,"sentence":"These standards include definitions of what is considered abusive language, annotation guidelines and reporting on the process.","offset":10,"pro":0.9090909090909091,"labels":"BAC"},{"idx":10,"sentence":"We present the results and main findings of the shared task at WOAH 5 on hateful memes detection.","offset":0,"pro":0,"labels":"PUR"},{"idx":10,"sentence":"The task include two subtasks relating to distinct challenges in the fine-grained detection of hateful memes: (1) the protected category attacked by the meme and (2) the attack type.","offset":1,"pro":0.25,"labels":"MTD"},{"idx":10,"sentence":"3 teams submitted system description papers.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":10,"sentence":"This shared task builds on the hateful memes detection task created by Facebook AI Research in 2020.","offset":3,"pro":0.75,"labels":"MTD"},{"idx":11,"sentence":"This paper describes our submission (winning solution for Task A) to the Shared Task on Hateful Meme Detection at WOAH 2021.","offset":0,"pro":0,"labels":"PUR"},{"idx":11,"sentence":"We build our system on top of a state-of-the-art system for binary hateful meme classification that already uses image tags such as race, gender, and web entities.","offset":1,"pro":0.3333333333333333,"labels":"MTD"},{"idx":11,"sentence":"We add further metadata such as emotions and experiment with data augmentation techniques, as hateful instances are underrepresented in the data set.","offset":2,"pro":0.6666666666666666,"labels":"MTD"},{"idx":12,"sentence":"Memes are the combinations of text and images that are often humorous in nature.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"But, that may not always be the case, and certain combinations of texts and images may depict hate, referred to as hateful memes.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":12,"sentence":"This work presents a multimodal pipeline that takes both visual and textual features from memes into account to (1) identify the protected category (e.g. race, sex etc.) that has been attacked; and (2) detect the type of attack (e.g. contempt, slurs etc.).","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":12,"sentence":"Our pipeline uses state-of-the-art pre-trained visual and textual representations, followed by a simple logistic regression classifier.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":12,"sentence":"We employ our pipeline on the Hateful Memes Challenge dataset with additional newly created fine-grained labels for protected category and type of attack.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":12,"sentence":"Our best model achieves an AUROC of 0.96 for identifying the protected category, and 0.97 for detecting the type of attack.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":12,"sentence":"We release our code at https://github.com/harisbinzia/HatefulMemes","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":13,"sentence":"The Shared Task on Hateful Memes is a challenge that aims at the detection of hateful content in memes by inviting the implementation of systems that understand memes, potentially by combining image and textual information.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"Our participation included a text-based BERT baseline (TxtBERT), the same but adding information from the image (ImgBERT), and neural retrieval approaches.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":13,"sentence":"We also experimented with retrieval augmented classification models.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":13,"sentence":"We found that an ensemble of TxtBERT and ImgBERT achieves the best performance in terms of ROC AUC score in two out of the three tasks on our development set.","offset":3,"pro":0.5,"labels":"RST"},{"idx":13,"sentence":"The challenge consists of three detection tasks: hate, protected category and attack type.","offset":4,"pro":0.6666666666666666,"labels":"BAC"},{"idx":13,"sentence":"The first is a binary classification task, while the other two are multi-label classification tasks.","offset":5,"pro":0.8333333333333334,"labels":"BAC"},{"idx":14,"sentence":"Text simplification is the process of splitting and rephrasing a sentence to a sequence of sentences making it easier to read and understand while preserving the content and approximating the original meaning.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"Text simplification has been exploited in NLP applications like machine translation, summarization, semantic role labeling, and information extraction, opening a broad avenue for its exploitation in comprehension-based question-answering downstream tasks.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":14,"sentence":"In this work, we investigate the effect of text simplification in the task of question-answering using a comprehension context.","offset":2,"pro":0.2222222222222222,"labels":"PUR"},{"idx":14,"sentence":"We release Simple-SQuAD, a simplified version of the widely-used SQuAD dataset.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":14,"sentence":"Firstly, we outline each step in the dataset creation pipeline, including style transfer, thresholding of sentences showing correct transfer, and offset finding for each answer.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":14,"sentence":"Secondly, we verify the quality of the transferred sentences through various methodologies involving both automated and human evaluation.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":14,"sentence":"Thirdly, we benchmark the newly created corpus and perform an ablation study for examining the effect of the simplification process in the SQuAD-based question answering task.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":14,"sentence":"Our experiments show that simplification leads to up to 2.04% and 1.74% increase in Exact Match and F1, respectively.","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":14,"sentence":"Finally, we conclude with an analysis of the transfer process, investigating the types of edits made by the model, and the effect of sentence length on the transfer model.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":15,"sentence":"Finding informative COVID-19 posts in a stream of tweets is very useful to monitor health-related updates.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"In this work, we introduce a new dataset of 5,000 tweets for finding informative COVID-19 tweets for Danish.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":15,"sentence":"In contrast to prior work, which balances the label distribution, we model the problem by keeping its natural distribution.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":15,"sentence":"We examine how well a simple probabilistic model and a convolutional neural network (CNN) perform on this task.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":15,"sentence":"We find a weighted CNN to work well but it is sensitive to embedding and hyperparameter choices.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":15,"sentence":"Prior work focused on a balanced data setup and on English,","offset":5,"pro":0.7142857142857143,"labels":"BAC"},{"idx":15,"sentence":"but informative tweets are rare, and English is only one of the many languages spoken in the world.","offset":6,"pro":0.8571428571428571,"labels":"GAP"},{"idx":16,"sentence":"Our corpus is compiled by expert verified cases of depression in several online blogs.","offset":0,"pro":0,"labels":"MTD"},{"idx":16,"sentence":"We experiment with two different LSTM based models and two different BERT based models.","offset":1,"pro":0.14285714285714285,"labels":"MTD"},{"idx":16,"sentence":"We achieve a 77.53% accuracy with a Thai BERT model in detecting depression.","offset":2,"pro":0.2857142857142857,"labels":"RST"},{"idx":16,"sentence":"This establishes a good baseline for future researcher on the same corpus.","offset":3,"pro":0.42857142857142855,"labels":"CLN"},{"idx":16,"sentence":"Furthermore, we identify a need for Thai embeddings that have been trained on a more varied corpus than Wikipedia.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":16,"sentence":"Our corpus, code and trained models have been released openly on Zenodo.","offset":5,"pro":0.7142857142857143,"labels":"CTN"},{"idx":16,"sentence":"We present the first openly available corpus for detecting depression in Thai.","offset":6,"pro":0.8571428571428571,"labels":"PUR"},{"idx":17,"sentence":"Extracting keyphrases that summarize the main points of a document is a fundamental task in natural language processing.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"Supervised approaches to keyphrase extraction(KPE) are largely developed based on the assumption that the training data is fully annotated.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":17,"sentence":"However, due to the difficulty of keyphrase annotating, KPE models severely suffer from incomplete annotated problem in many scenarios.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":17,"sentence":"To this end, we propose a more robust training method that learns to mitigate the misguidance brought by unlabeled keyphrases.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":17,"sentence":"We introduce negative sampling to adjust training loss, and conduct experiments under different scenarios.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":17,"sentence":"Empirical studies on synthetic datasets and open domain dataset show that our model is robust to incomplete annotated problem and surpasses prior baselines.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":17,"sentence":"Extensive experiments on five scientific domain datasets of different scales demonstrate that our model is competitive with the state-of-the-art method.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":18,"sentence":"Fine-grained temporal relation extraction (FineTempRel) aims to recognize the durations and timeline of event mentions in text.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"A missing part in the current deep learning models for FineTempRel is their failure to exploit the syntactic structures of the input sentences to enrich the representation vectors.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":18,"sentence":"In this work, we propose to fill this gap by introducing novel methods to integrate the syntactic structures into the deep learning models for FineTempRel.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":18,"sentence":"The proposed model focuses on two types of syntactic information from the dependency trees, i.e., the syntax-based importance scores for representation learning of the words and the syntactic connections to identify important context words for the event mentions.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":18,"sentence":"We also present two novel techniques to facilitate the knowledge transfer between the subtasks of FineTempRel,","offset":4,"pro":0.8,"labels":"MTD"},{"idx":19,"sentence":"The goal of Event Factuality Prediction (EFP) is to determine the factual degree of an event mention, representing how likely the event mention has happened in text.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"Current deep learning models has demonstrated the importance of syntactic and semantic structures of the sentences to identify important context words for EFP.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":19,"sentence":"However, the major problem with these EFP models is that they only encode the one-hop paths between the words (i.e., the direct connections) to form the sentence structures.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":19,"sentence":"In this work, we show that the multi-hop paths between the words are also necessary to compute the sentence structures for EFP.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":19,"sentence":"We demonstrate the effectiveness of the proposed model via the extensive experiments in this work.","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":19,"sentence":"To this end, we introduce a novel deep learning model for EFP that explicitly considers multi-hop paths with both syntax-based and semantic-based edges between the words to obtain sentence structures for representation learning in EFP.","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":0,"sentence":"Strategies that insert intentional noise into text when posting it are commonly observed in the online space, and sometimes they aim to let only certain community users understand the genuine semantics.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"In this paper, we explore the purpose of such actions by categorizing them into tricks, memes, fillers, and codes, and organize the linguistic strategies that are used for each purpose.","offset":1,"pro":0.25,"labels":"PUR"},{"idx":0,"sentence":"Through this, we identify that such strategies can be conducted by authors for multiple purposes, regarding the presence of stakeholders such as Peers' and Others'.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":0,"sentence":"We finally analyze how these strategies appear differently in each circumstance, along with the unified taxonomy accompanying examples.","offset":3,"pro":0.75,"labels":"MTD"},{"idx":1,"sentence":"ICD-9 coding is a relevant clinical billing task, where unstructured texts with information about a patient's diagnosis and treatments are annotated with multiple ICD-9 codes.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"Automated ICD-9 coding is an active research field, where CNN- and RNN-based model architectures represent the state-of-the-art approaches.","offset":1,"pro":0.3333333333333333,"labels":"BAC"},{"idx":1,"sentence":"In this work, we propose a description-based label attention classifier to improve the model explainability when dealing with noisy texts like clinical notes.","offset":2,"pro":0.6666666666666666,"labels":"PUR"},{"idx":2,"sentence":"Lexical normalization, in addition to word segmentation and part-of-speech tagging, is a fundamental task for Japanese user-generated text processing.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"In this paper, we propose a text editing model to solve the three task jointly and methods of pseudo-labeled data generation to overcome the problem of data deficiency.","offset":1,"pro":0.3333333333333333,"labels":"PUR"},{"idx":2,"sentence":"Our experiments showed that the proposed model achieved better normalization performance when trained on more diverse pseudo-labeled data.","offset":2,"pro":0.6666666666666666,"labels":"RST"},{"idx":3,"sentence":"Language models used in speech recognition are often either evaluated intrinsically using perplexity on test data, or extrinsically with an automatic speech recognition (ASR) system.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"The former evaluation does not always correlate well with ASR performance, while the latter could be specific to particular ASR systems.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":3,"sentence":"Recent work proposed to evaluate language models by using them to classify ground truth sentences among alternative phonetically similar sentences generated by a fine state transducer.","offset":2,"pro":0.2857142857142857,"labels":"BAC"},{"idx":3,"sentence":"Underlying such an evaluation is the assumption that the generated sentences are linguistically incorrect.","offset":3,"pro":0.42857142857142855,"labels":"BAC"},{"idx":3,"sentence":"In this paper, we first put this assumption into question, and observe that alternatively generated sentences could often be linguistically correct when they differ from the ground truth by only one edit.","offset":4,"pro":0.5714285714285714,"labels":"PUR"},{"idx":3,"sentence":"Secondly, we showed that by using multi-lingual BERT, we can achieve better performance than previous work on two code-switching data sets.","offset":5,"pro":0.7142857142857143,"labels":"PUR"},{"idx":3,"sentence":"Our implementation is publicly available on Github at https://github.com/sikfeng/language-modelling-for-code-switching.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":4,"sentence":"Multimodal named entity recognition (MNER) requires to bridge the gap between language understanding and visual context.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"While many multimodal neural techniques have been proposed to incorporate images into the MNER task, the model's ability to leverage multimodal interactions remains poorly understood.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":4,"sentence":"In this work, we conduct in-depth analyses of existing multimodal fusion techniques from different perspectives and describe the scenarios where adding information from the image does not always boost performance.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":4,"sentence":"We also study the use of captions as a way to enrich the context for MNER.","offset":3,"pro":0.6,"labels":"PUR"},{"idx":4,"sentence":"Experiments on three datasets from popular social platforms expose the bottleneck of existing multimodal models and the situations where using captions is beneficial.","offset":4,"pro":0.8,"labels":"RST"},{"idx":5,"sentence":"Existing sarcasm detection systems focus on exploiting linguistic markers, context, or user-level priors.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"However, social studies suggest that the relationship between the author and the audience can be equally relevant for the sarcasm usage and interpretation.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":5,"sentence":"In this work, we propose a framework jointly leveraging (1) a user context from their historical tweets together with (2) the social information from a user's conversational neighborhood in an interaction graph, to contextualize the interpretation of the post.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":5,"sentence":"We use graph attention networks (GAT) over users and tweets in a conversation thread, combined with dense user history representations.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":5,"sentence":"Apart from achieving state-of-the-art results on the recently published dataset of 19k Twitter users with 30K labeled tweets, adding 10M unlabeled tweets as context, our results indicate that the model contributes to interpreting the sarcastic intentions of an author more than to predicting the sarcasm perception by others.","offset":4,"pro":0.8,"labels":"RST"},{"idx":6,"sentence":"State-of-the-art approaches to spelling error correction problem include Transformer-based Seq2Seq models, which require large training sets and suffer from slow inference time; and sequence labeling models based on Transformer encoders like BERT, which involve token-level label space and therefore a large pre-defined vocabulary dictionary.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"In this paper we present a Hierarchical Character Tagger model, or HCTagger, for short text spelling error correction.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":6,"sentence":"We use a pre-trained language model at the character level as a text encoder, and then predict character-level edits to transform the original text into its error-free form with a much smaller label space.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":6,"sentence":"For decoding, we propose a hierarchical multi-task approach to alleviate the issue of long-tail label distribution without introducing extra model parameters.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":6,"sentence":"Experiments on two public misspelling correction datasets demonstrate that HCTagger is an accurate and much faster approach than many existing models.","offset":4,"pro":0.8,"labels":"RST"},{"idx":7,"sentence":"Large-scale language models such as ELMo and BERT have pushed the horizon of what is possible in semantic role labeling (SRL), solving the out-of-vocabulary problem and enabling end-to-end systems, but they have also introduced significant biases.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"We evaluate three SRL parsers on very simple transitive sentences with verbs usually associated with animate subjects and objects, such as, Mary babysat Tom: a state-of-the-art parser based on BERT, an older parser based on GloVe, and an even older parser from before the days of word embeddings.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":7,"sentence":"When arguments are word forms predominantly used as person names, aligning with common sense expectations of animacy, the BERT-based parser is unsurprisingly superior; yet, with abstract or random nouns, the opposite picture emerges.","offset":2,"pro":0.4,"labels":"RST"},{"idx":7,"sentence":"We refer to this as common sense bias and present a challenge dataset for evaluating the extent to which parsers are sensitive to such a bias.","offset":3,"pro":0.6,"labels":"CLN"},{"idx":7,"sentence":"Our code and challenge dataset are available here: github.com/coastalcph/comte","offset":4,"pro":0.8,"labels":"CTN"},{"idx":8,"sentence":"WhatsApp Messenger is one of the most popular channels for spreading information with a current reach of more than 180 countries and 2 billion people.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"Its widespread usage has made it one of the most popular media for information propagation among the masses during any socially engaging event.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":8,"sentence":"In the recent past, several countries have witnessed its effectiveness and influence in political and social campaigns.","offset":2,"pro":0.2222222222222222,"labels":"BAC"},{"idx":8,"sentence":"We observe a high surge in information and propaganda flow during election campaigning.","offset":3,"pro":0.3333333333333333,"labels":"BAC"},{"idx":8,"sentence":"In this paper, we explore a high-quality large-scale user-generated dataset curated from WhatsApp comprising of 281 groups, 31,078 unique users, and 223,404 messages shared before, during, and after the Indian General Elections 2019, encompassing all major Indian political parties and leaders.","offset":4,"pro":0.4444444444444444,"labels":"PUR"},{"idx":8,"sentence":"In addition to the raw noisy user-generated data, we present a fine-grained annotated dataset of 3,848 messages that will be useful to understand the various dimensions of WhatsApp political campaigning.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":8,"sentence":"We present several complementary insights into the investigative and sensational news stories from the same period.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":8,"sentence":"Exploratory data analysis and experiments showcase several exciting results and future research opportunities.","offset":7,"pro":0.7777777777777778,"labels":"IMP"},{"idx":8,"sentence":"To facilitate reproducible research, we make the anonymized datasets available in the public domain.","offset":8,"pro":0.8888888888888888,"labels":"CTN"},{"idx":9,"sentence":"As a result of unstructured sentences and some misspellings and errors, finding named entities in a noisy environment such as social media takes much more effort.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"ParsTwiNER contains about 250k tokens, based on standard instructions like MUC-6 or CoNLL 2003, gathered from Persian Twitter.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":9,"sentence":"Using Cohen's Kappa coefficient, the consistency of annotators is 0.95, a high score.","offset":2,"pro":0.4,"labels":"BAC"},{"idx":9,"sentence":"In this study, we demonstrate that some state-of-the-art models degrade on these corpora, and trained a new model using parallel transfer learning based on the BERT architecture.","offset":3,"pro":0.6,"labels":"PUR"},{"idx":9,"sentence":"Experimental results show that the model works well in informal Persian as well as in formal Persian.","offset":4,"pro":0.8,"labels":"RST"},{"idx":10,"sentence":"We present DreamDrug, a crowdsourced dataset for detecting mentions of drugs in noisy user-generated item listings from darknet markets.","offset":0,"pro":0,"labels":"PUR"},{"idx":10,"sentence":"Our dataset contains nearly 15,000 manually annotated drug entities in over 3,500 item listings scraped from the darknet market platform DreamMarket in 2017.","offset":1,"pro":0.3333333333333333,"labels":"MTD"},{"idx":10,"sentence":"We also train and evaluate baseline models for detecting these entities, using contextual language models fine-tuned in a few-shot setting and on the full dataset, and examine the effect of pretraining on in-domain unannotated corpora.","offset":2,"pro":0.6666666666666666,"labels":"MTD"},{"idx":11,"sentence":"Code-mixed text generation systems have found applications in many downstream tasks, including speech recognition, translation and dialogue.","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"A paradigm of these generation systems relies on well-defined grammatical theories of code-mixing, and there is a lack of comparison of these theories.","offset":1,"pro":0.25,"labels":"GAP"},{"idx":11,"sentence":"We present a large-scale human evaluation of two popular grammatical theories, Matrix-Embedded Language (ML) and Equivalence Constraint (EC).","offset":2,"pro":0.5,"labels":"PUR"},{"idx":11,"sentence":"We compare them against three heuristic-based models and quantitatively demonstrate the effectiveness of the two grammatical theories.","offset":3,"pro":0.75,"labels":"RST"},{"idx":12,"sentence":"Automatic Speech Recognition (ASR) systems generally do not produce punctuated transcripts.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"To make transcripts more readable and follow the expected input format for downstream language models, it is necessary to add punctuation marks.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":12,"sentence":"In this paper, we tackle the punctuation restoration problem specifically for the noisy text (e.g., phone conversation scenarios).","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":12,"sentence":"To leverage the available written text datasets, we introduce a data sampling technique based on an n-gram language model to sample more training data that are similar to our in-domain data.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":12,"sentence":"Moreover, we propose a two-stage fine-tuning approach that utilizes the sampled external data as well as our in-domain dataset for models based on BERT.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":12,"sentence":"Extensive experiments show that the proposed approach outperforms the baseline with an improvement of 1.12% F1 score.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":13,"sentence":"In this work, we propose a novel and easy-to-apply data augmentation strategy, namely Bilateral Generation (BiG), with a contrastive training objective for improving the performance of ranking question answer pairs with existing labeled data.","offset":0,"pro":0,"labels":"PUR"},{"idx":13,"sentence":"In specific, we synthesize pseudo-positive QA pairs in contrast to the original negative QA pairs with two pre-trained generation models, one for question generation, the other for answer generation, which are fine-tuned on the limited positive QA pairs from the original dataset.","offset":1,"pro":0.25,"labels":"MTD"},{"idx":13,"sentence":"With the augmented dataset, we design a contrastive training objective for learning to rank question answer pairs.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":13,"sentence":"Experimental results on three benchmark datasets show that our method significantly improves the performance of ranking models by making full use of existing labeled data and can be easily applied to different ranking models.","offset":3,"pro":0.75,"labels":"RST"},{"idx":14,"sentence":"Fake news causes significant damage to society.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"To deal with these fake news, several studies on building detection models and arranging datasets have been conducted.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":14,"sentence":"Most of the fake news datasets depend on a specific time period.","offset":2,"pro":0.2857142857142857,"labels":"BAC"},{"idx":14,"sentence":"Consequently, the detection models trained on such a dataset have difficulty detecting novel fake news generated by political changes and social changes; they may possibly result in biased output from the input, including specific person names and organizational names.","offset":3,"pro":0.42857142857142855,"labels":"GAP"},{"idx":14,"sentence":"We refer to this problem as Diachronic Bias because it is caused by the creation date of news in each dataset.","offset":4,"pro":0.5714285714285714,"labels":"GAP"},{"idx":14,"sentence":"In this study, we confirm the bias, especially proper nouns including person names, from the deviation of phrase appearances in each dataset.","offset":5,"pro":0.7142857142857143,"labels":"PUR"},{"idx":14,"sentence":"Based on these findings, we propose masking methods using Wikidata to mitigate the influence of person names and validate whether they make fake news detection models robust through experiments with in-domain and out-of-domain data.","offset":6,"pro":0.8571428571428571,"labels":"PUR"},{"idx":15,"sentence":"This work takes a critical look at the evaluation of user-generated content automatic translation, the well-known specificities of which raise many challenges for MT.","offset":0,"pro":0,"labels":"PUR"},{"idx":15,"sentence":"Our analyses show that measuring the average-case performance using a standard metric on a UGC test set falls far short of giving a reliable image of the UGC translation quality.","offset":1,"pro":0.25,"labels":"GAP"},{"idx":15,"sentence":"That is why we introduce a new data set for the evaluation of UGC translation in which UGC specificities have been manually annotated using a fine-grained typology.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":15,"sentence":"Using this data set, we conduct several experiments to measure the impact of different kinds of UGC specificities on translation quality, more precisely than previously possible.","offset":3,"pro":0.75,"labels":"RST"},{"idx":16,"sentence":"This work explores the capacities of character-based Neural Machine Translation to translate noisy User-Generated Content (UGC) with a strong focus on exploring the limits of such approaches to handle productive UGC phenomena, which almost by definition, cannot be seen at training time.","offset":0,"pro":0,"labels":"PUR"},{"idx":16,"sentence":"Within a strict zero-shot scenario, we first study the detrimental impact on translation performance of various user-generated content phenomena on a small annotated dataset we developed and then show that such models are indeed incapable of handling unknown letters, which leads to catastrophic translation failure once such characters are encountered.","offset":1,"pro":0.3333333333333333,"labels":"MTD"},{"idx":16,"sentence":"We further confirm this behavior with a simple, yet insightful, copy task experiment and highlight the importance of reducing the vocabulary size hyper-parameter to increase the robustness of character-based models for machine translation.","offset":2,"pro":0.6666666666666666,"labels":"CLN"},{"idx":17,"sentence":"Twitter data has become established as a valuable source of data for various application scenarios in the past years.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"For many such applications, it is necessary to know where Twitter posts (tweets) were sent from or what location they refer to.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":17,"sentence":"Researchers have frequently used exact coordinates provided in a small percentage of tweets, but Twitter removed the option to share these coordinates in mid-2019.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":17,"sentence":"Moreover, there is reason to suspect that a large share of the provided coordinates did not correspond to GPS coordinates of the user even before that.","offset":3,"pro":0.42857142857142855,"labels":"GAP"},{"idx":17,"sentence":"In this paper, we explain the situation and the 2019 policy change and shed light on the various options of still obtaining location information from tweets.","offset":4,"pro":0.5714285714285714,"labels":"PUR"},{"idx":17,"sentence":"We provide usage statistics including changes over time, and analyze what the removal of exact coordinates means for various common research tasks performed with Twitter data.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":17,"sentence":"Finally, we make suggestions for future research requiring geolocated tweets.","offset":6,"pro":0.8571428571428571,"labels":"MTD"},{"idx":18,"sentence":"Despite excellent performance on tasks such as question answering, Transformer-based architectures remain sensitive to syntactic and contextual ambiguities.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"Question Paraphrasing (QP) offers a promising solution as a means to augment existing datasets.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":18,"sentence":"The main challenges of current QP models include lack of training data and difficulty in generating diverse and natural questions.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":18,"sentence":"In this paper, we present Conquest, a framework for generating synthetic datasets for contextual question paraphrasing.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":18,"sentence":"To this end, Conquest first employs an answer-aware question generation (QG) model to create a question-pair dataset and then uses this data to train a contextualized question paraphrasing model.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":18,"sentence":"We extensively evaluate Conquest and show its ability to produce more diverse and fluent question pairs than existing approaches.","offset":5,"pro":0.625,"labels":"RST"},{"idx":18,"sentence":"Our contextual paraphrase model also establishes a strong baseline for end-to-end contextual paraphrasing.","offset":6,"pro":0.75,"labels":"RST"},{"idx":18,"sentence":"Further, We find that context can improve BLEU-1 score on contextual compression and expansion by 4.3 and 11.2 respectively, compared to a non-contextual model.","offset":7,"pro":0.875,"labels":"RST"},{"idx":19,"sentence":"Adverse Drug Event (ADE) extraction models can rapidly examine large collections of social media texts, detecting mentions of drug-related adverse reactions and trigger medical investigations.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"However, despite the recent advances in NLP, it is currently unknown if such models are robust in face of negation, which is pervasive across language varieties.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":19,"sentence":"In this paper we evaluate three state-of-the-art systems, showing their fragility against negation, and then we introduce two possible strategies to increase the robustness of these models: a pipeline approach, relying on a specific component for negation detection; an augmentation of an ADE extraction dataset to artificially create negated samples and further train the models.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":19,"sentence":"We show that both strategies bring significant increases in performance, lowering the number of spurious entities predicted by the models.","offset":3,"pro":0.6,"labels":"RST"},{"idx":19,"sentence":"Our dataset and code will be publicly released to encourage research on the topic.","offset":4,"pro":0.8,"labels":"CTN"},{"idx":0,"sentence":"This work investigates the use of interactively updated label suggestions to improve upon the efficiency of gathering annotations on the task of opinion mining in German Covid-19 social media data.","offset":0,"pro":0,"labels":"PUR"},{"idx":0,"sentence":"We develop guidelines to conduct a controlled annotation study with social science students and find that suggestions from a model trained on a small, expert-annotated dataset already lead to a substantial improvement - in terms of inter-annotator agreement (+.14 Fleiss' κ) and annotation quality - compared to students that do not receive any label suggestions.","offset":1,"pro":0.2,"labels":"RST"},{"idx":0,"sentence":"We further find that label suggestions from interactively trained models do not lead to an improvement over suggestions from a static model.","offset":2,"pro":0.4,"labels":"RST"},{"idx":0,"sentence":"Nonetheless, our analysis of suggestion bias shows that annotators remain capable of reflecting upon the suggested label in general.","offset":3,"pro":0.6,"labels":"CLN"},{"idx":0,"sentence":"Finally, we confirm the quality of the annotated data in transfer learning experiments between different annotator groups.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":1,"sentence":"Humor is an important social phenomenon, serving complex social and psychological functions.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"However, despite being studied for millennia humor is computationally not well understood, often considered an AI-complete problem.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":1,"sentence":"In this work, we introduce a novel setting in humor mining: automatically detecting funny and unusual scientific papers.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":1,"sentence":"This challenging task has unique characteristics that make it particularly suitable for automatic learning.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":1,"sentence":"We construct a dataset containing thousands of funny papers and use it to learn classifiers, combining findings from psychology and linguistics with recent advances in NLP.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":1,"sentence":"We use our models to identify potentially funny papers in a large dataset of over 630,000 articles.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":1,"sentence":"The results demonstrate the potential of our methods, and more broadly the utility of integrating state-of-the-art NLP methods with insights from more traditional disciplines","offset":6,"pro":0.75,"labels":"CLN"},{"idx":1,"sentence":"We are inspired by the Ig Nobel prize, a satirical prize awarded annually to celebrate funny scientific achievements (example past winner: Are cows more likely to lie down the longer they stand?).","offset":7,"pro":0.875,"labels":"BAC"},{"idx":2,"sentence":"This paper presents a novel task to generate poll questions for social media posts.","offset":0,"pro":0,"labels":"PUR"},{"idx":2,"sentence":"It offers an easy way to hear the voice from the public and learn from their feelings to important social topics.","offset":1,"pro":0.125,"labels":"MTD"},{"idx":2,"sentence":"While most related work tackles formal languages (e.g., exam papers), we generate poll questions for short and colloquial social media messages exhibiting severe data sparsity.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":2,"sentence":"They are then incorporated into a sequence-to-sequence (S2S) architecture for question generation and its extension with dual decoders to additionally yield poll choices (answers).","offset":3,"pro":0.375,"labels":"MTD"},{"idx":2,"sentence":"For experiments, we collect a large-scale Chinese dataset from Sina Weibo containing over 20K polls.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":2,"sentence":"The results show that our model outperforms the popular S2S models without exploiting topics from comments and the dual decoder design can further benefit the prediction of both questions and answers.","offset":5,"pro":0.625,"labels":"CLN"},{"idx":2,"sentence":"Human evaluations further exhibit our superiority in yielding high-quality polls helpful to draw user engagements.","offset":6,"pro":0.75,"labels":"CLN"},{"idx":2,"sentence":"To deal with that, we propose to encode user comments and discover latent topics therein as contexts.","offset":7,"pro":0.875,"labels":"MTD"},{"idx":3,"sentence":"Detecting online hate is a difficult task that even state-of-the-art models struggle with.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"Typically, hate speech detection models are evaluated by measuring their performance on held-out test data using metrics such as accuracy and F1 score.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":3,"sentence":"However, this approach makes it difficult to identify specific model weak points.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":3,"sentence":"It also risks overestimating generalisable model performance due to increasingly well-evidenced systematic gaps and biases in hate speech datasets.","offset":3,"pro":0.375,"labels":"GAP"},{"idx":3,"sentence":"To enable more targeted diagnostic insights, we introduce HateCheck, a suite of functional tests for hate speech detection models.","offset":4,"pro":0.5,"labels":"PUR"},{"idx":3,"sentence":"We specify 29 model functionalities motivated by a review of previous research and a series of interviews with civil society stakeholders.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":3,"sentence":"We craft test cases for each functionality and validate their quality through a structured annotation process.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":3,"sentence":"To illustrate HateCheck's utility, we test near-state-of-the-art transformer models as well as two popular commercial models, revealing critical model weaknesses.","offset":7,"pro":0.875,"labels":"MTD"},{"idx":4,"sentence":"Recent studies constructing direct interactions between the claim and each single user response (a comment or a relevant article) to capture evidence have shown remarkable success in interpretable claim verification.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"Owing to different single responses convey different cognition of individual users (i.e., audiences), the captured evidence belongs to the perspective of individual cognition.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":4,"sentence":"However, individuals' cognition of social things is not always able to truly reflect the objective.","offset":2,"pro":0.2,"labels":"GAP"},{"idx":4,"sentence":"There may be one-sided or biased semantics in their opinions on a claim.","offset":3,"pro":0.3,"labels":"GAP"},{"idx":4,"sentence":"The captured evidence correspondingly contains some unobjective and biased evidence fragments, deteriorating task performance.","offset":4,"pro":0.4,"labels":"GAP"},{"idx":4,"sentence":"In this paper, we propose a Dual-view model based on the views of Collective and Individual Cognition (CICD) for interpretable claim verification.","offset":5,"pro":0.5,"labels":"PUR"},{"idx":4,"sentence":"From the view of the collective cognition, we not only capture the word-level semantics based on individual users, but also focus on sentence-level semantics (i.e., the overall responses) among all users and adjust the proportion between them to generate global evidence.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":4,"sentence":"From the view of individual cognition, we select the top-k articles with high degree of difference and interact with the claim to explore the local key evidence fragments.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":4,"sentence":"Experiments on three benchmark datasets confirm that CICD achieves state-of-the-art performance.","offset":8,"pro":0.8,"labels":"CLN"},{"idx":4,"sentence":"To weaken the bias of individual cognition-view evidence, we devise inconsistent loss to suppress the divergence between global and local evidence for strengthening the consistent shared evidence between the both.","offset":9,"pro":0.9,"labels":"MTD"},{"idx":5,"sentence":"Rap generation, which aims to produce lyrics and corresponding singing beats, needs to model both rhymes and rhythms.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"Previous works for rap generation focused on rhyming lyrics, but ignored rhythmic beats, which are important for rap performance.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":5,"sentence":"In this paper, we develop DeepRapper, a Transformer-based rap generation system that can model both rhymes and rhythms.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":5,"sentence":"Since there is no available rap datasets with rhythmic beats, we develop a data mining pipeline to collect a large-scale rap dataset, which includes a large number of rap songs with aligned lyrics and rhythmic beats.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":5,"sentence":"Second, we design a Transformer-based autoregressive language model which carefully models rhymes and rhythms.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":5,"sentence":"Specifically, we generate lyrics in the reverse order with rhyme representation and constraint for rhyme enhancement, and insert a beat symbol into lyrics for rhythm/beat modeling.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":5,"sentence":"To our knowledge, DeepRapper is the first system to generate rap with both rhymes and rhythms.","offset":6,"pro":0.75,"labels":"CTN"},{"idx":5,"sentence":"Both objective and subjective evaluations demonstrate that DeepRapper generates creative and high-quality raps with rhymes and rhythms.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":6,"sentence":"In this paper, we formulate the personalized news headline generation problem whose goal is to output a user-specific title based on both a user's reading interests and a candidate news body to be exposed to her.","offset":0,"pro":0,"labels":"PUR"},{"idx":6,"sentence":"The training set is collected from user impressions logs of Microsoft News, and the test set is manually created by hundreds of native speakers to enable a fair testbed for evaluating models in an offline mode.","offset":1,"pro":0.14285714285714285,"labels":"MTD"},{"idx":6,"sentence":"We propose a generic framework as a preparatory solution to our problem.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":6,"sentence":"At its heart, user preference is learned by leveraging the user behavioral data, and three kinds of user preference injections are proposed to personalize a text generator and establish personalized headlines.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":6,"sentence":"We investigate our dataset by implementing several state-of-the-art user modeling methods in our framework to demonstrate a benchmark score for the proposed dataset.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":6,"sentence":"The dataset is available at https://msnews.github.io/pens.html.","offset":5,"pro":0.7142857142857143,"labels":"CTN"},{"idx":6,"sentence":"To build up a benchmark for this problem, we publicize a large-scale dataset named PENS (PErsonalized News headlineS).","offset":6,"pro":0.8571428571428571,"labels":"MTD"},{"idx":7,"sentence":"Text style transfer aims to alter the style (e.g., sentiment) of a sentence while preserving its content.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"A common approach is to map a given sentence to content representation that is free of style, and the content representation is fed to a decoder with a target style.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":7,"sentence":"Previous methods in filtering style completely remove tokens with style at the token level, which incurs the loss of content information.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":7,"sentence":"In this paper, we propose to enhance content preservation by implicitly removing the style information of each token with reverse attention, and thereby retain the content.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":7,"sentence":"Furthermore, we fuse content information when building the target style representation, making it dynamic with respect to the content.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":7,"sentence":"Our method creates not only style-independent content representation, but also content-dependent style representation in transferring style.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":7,"sentence":"Empirical results show that our method outperforms the state-of-the-art baselines by a large margin in terms of content preservation.","offset":6,"pro":0.75,"labels":"CLN"},{"idx":7,"sentence":"In addition, it is also competitive in terms of style transfer accuracy and fluency.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":8,"sentence":"Constrained decoding algorithms always produce hypotheses satisfying all constraints.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"However, they are computationally expensive and can lower the generated text quality.","offset":1,"pro":0.1,"labels":"GAP"},{"idx":8,"sentence":"In this paper, we propose Mention Flags (MF), which traces whether lexical constraints are satisfied in the generated outputs in an S2S decoder.","offset":2,"pro":0.2,"labels":"PUR"},{"idx":8,"sentence":"The MF models can be trained to generate tokens in a hypothesis until all constraints are satisfied, guaranteeing high constraint satisfaction.","offset":3,"pro":0.3,"labels":"MTD"},{"idx":8,"sentence":"Our experiments on the Common Sense Generation task (CommonGen) (Lin et al., 2020), End2end Restaurant Dialog task (E2ENLG) (Du{v{sek et al., 2020) and Novel Object Captioning task (nocaps) (Agrawal et al., 2019) show that the MF models maintain higher constraint satisfaction and text quality than the baseline models and other constrained decoding algorithms, achieving state-of-the-art performance on all three tasks.","offset":4,"pro":0.4,"labels":"CLN"},{"idx":8,"sentence":"These results are achieved with a much lower run-time than constrained decoding algorithms.","offset":5,"pro":0.5,"labels":"RST"},{"idx":8,"sentence":"We also show that the MF models work well in the low-resource setting.","offset":6,"pro":0.6,"labels":"CLN"},{"idx":8,"sentence":"This paper focuses on Seq2Seq (S2S) constrained text generation where the text generator is constrained to mention specific words which are inputs to the encoder in the generated outputs.","offset":7,"pro":0.7,"labels":"BAC"},{"idx":8,"sentence":"Pre-trained S2S models or a Copy Mechanism are trained to copy the surface tokens from encoders to decoders,","offset":8,"pro":0.8,"labels":"BAC"},{"idx":8,"sentence":"but they cannot guarantee constraint satisfaction.","offset":9,"pro":0.9,"labels":"GAP"},{"idx":9,"sentence":"Concept-to-text Natural Language Generation is the task of expressing an input meaning representation in natural language.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"Previous approaches in this task have been able to generalise to rare or unseen instances by relying on a delexicalisation of the input.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":9,"sentence":"However, this often requires that the input appears verbatim in the output text.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":9,"sentence":"This poses challenges in multilingual settings, where the task expands to generate the output text in multiple languages given the same input.","offset":3,"pro":0.42857142857142855,"labels":"GAP"},{"idx":9,"sentence":"Our experiments across five datasets and five languages show that multilingual models outperform monolingual models in concept-to-text and that our framework outperforms previous approaches, especially in low resource conditions.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":9,"sentence":"In this paper, we explore the application of multilingual models in concept-to-text and propose Language Agnostic Delexicalisation,","offset":5,"pro":0.7142857142857143,"labels":"PUR"},{"idx":9,"sentence":"a novel delexicalisation method that uses multilingual pretrained embeddings, and employs a character-level post-editing model to inflect words in their correct form during relexicalisation.","offset":6,"pro":0.8571428571428571,"labels":"MTD"},{"idx":10,"sentence":"Nowadays, open-domain dialogue models can generate acceptable responses according to the historical context based on the large-scale pre-trained language models.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"However, they generally concatenate the dialogue history directly as the model input to predict the response, which we named as the flat pattern and ignores the dynamic information flow across dialogue utterances.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":10,"sentence":"Experiments on the multi-reference Reddit Dataset and DailyDialog Dataset demonstrate that our DialoFlow significantly outperforms the DialoGPT on the dialogue generation task.","offset":2,"pro":0.2857142857142857,"labels":"CLN"},{"idx":10,"sentence":"Besides, we propose the Flow score, an effective automatic metric for evaluating interactive human-bot conversation quality based on the pre-trained DialoFlow, which presents high chatbot-level correlation (r=0.9) with human ratings among 11 chatbots.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":10,"sentence":"Code and pre-trained models will be public.","offset":4,"pro":0.5714285714285714,"labels":"CTN"},{"idx":10,"sentence":"In this work, we propose the DialoFlow model,","offset":5,"pro":0.7142857142857143,"labels":"PUR"},{"idx":10,"sentence":"in which we introduce a dynamic flow mechanism to model the context flow, and design three training objectives to capture the information dynamics across dialogue utterances by addressing the semantic influence brought about by each utterance in large-scale pre-training.","offset":6,"pro":0.8571428571428571,"labels":"MTD"},{"idx":11,"sentence":"The goal of dialogue state tracking (DST) is to predict the current dialogue state given all previous dialogue contexts.","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"Existing approaches generally predict the dialogue state at every turn from scratch.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":11,"sentence":"However, the overwhelming majority of the slots in each turn should simply inherit the slot values from the previous turn.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":11,"sentence":"Therefore, the mechanism of treating slots equally in each turn not only is inefficient but also may lead to additional errors because of the redundant slot value generation.","offset":3,"pro":0.375,"labels":"GAP"},{"idx":11,"sentence":"To address this problem, we devise the two-stage DSS-DST which consists of the Dual Slot Selector based on the current turn dialogue, and the Slot Value Generator based on the dialogue history.","offset":4,"pro":0.5,"labels":"PUR"},{"idx":11,"sentence":"The Dual Slot Selector determines each slot whether to update slot value or to inherit the slot value from the previous turn from two aspects: (1) if there is a strong relationship between it and the current turn dialogue utterances; (2) if a slot value with high reliability can be obtained for it through the current turn dialogue.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":11,"sentence":"The slots selected to be updated are permitted to enter the Slot Value Generator to update values by a hybrid method, while the other slots directly inherit the values from the previous turn.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":11,"sentence":"Empirical results show that our method achieves 56.93%, 60.73%, and 58.04% joint accuracy on MultiWOZ 2.0, MultiWOZ 2.1, and MultiWOZ 2.2 datasets respectively and achieves a new state-of-the-art performance with significant improvements.","offset":7,"pro":0.875,"labels":"RST"},{"idx":12,"sentence":"One of the difficulties in training dialogue systems is the lack of training data.","offset":0,"pro":0,"labels":"GAP"},{"idx":12,"sentence":"We explore the possibility of creating dialogue data through the interaction between a dialogue system and a user simulator.","offset":1,"pro":0.125,"labels":"PUR"},{"idx":12,"sentence":"Our goal is to develop a modelling framework that can incorporate new dialogue scenarios through self-play between the two agents.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":12,"sentence":"In this framework, we first pre-train the two agents on a collection of source domain dialogues, which equips the agents to converse with each other via natural language.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":12,"sentence":"In experiments on the MultiWOZ dataset, two practical transfer learning problems are investigated: 1) domain adaptation and 2) single-to-multiple domain transfer.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":12,"sentence":"We demonstrate that the proposed framework is highly effective in bootstrapping the performance of the two agents in transfer learning.","offset":5,"pro":0.625,"labels":"CLN"},{"idx":12,"sentence":"We also show that our method leads to improvements in dialogue system performance on complete datasets.","offset":6,"pro":0.75,"labels":"CLN"},{"idx":12,"sentence":"With further fine-tuning on a small amount of target domain data, the agents continue to interact with the aim of improving their behaviors using reinforcement learning with structured reward functions.","offset":7,"pro":0.875,"labels":"MTD"},{"idx":13,"sentence":"Maintaining a consistent persona is essential for dialogue agents.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"Although tremendous advancements have been brought, the limited-scale of annotated personalized dialogue datasets is still a barrier towards training robust and consistent persona-based dialogue models.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":13,"sentence":"This work shows how this challenge can be addressed by disentangling persona-based dialogue generation into two sub-tasks with a novel BERT-over-BERT (BoB) model.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":13,"sentence":"Specifically, the model consists of a BERT-based encoder and two BERT-based decoders, where one decoder is for response generation, and another is for consistency understanding.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":13,"sentence":"In particular, to learn the ability of consistency understanding from large-scale non-dialogue inference data, we train the second decoder in an unlikelihood manner.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":13,"sentence":"Under different limited data settings, both automatic and human evaluations demonstrate that the proposed model outperforms strong baselines in response quality and persona consistency.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":14,"sentence":"Multi-intent SLU can handle multiple intents in an utterance, which has attracted increasing attention.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"However, the state-of-the-art joint models heavily rely on autoregressive approaches, resulting in two issues: slow inference speed and information leakage.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":14,"sentence":"In this paper, we explore a non-autoregressive model for joint multiple intent detection and slot filling, achieving more fast and accurate.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":14,"sentence":"Specifically, we propose a Global-Locally Graph Interaction Network (GL-GIN) where a local slot-aware graph interaction layer is proposed to model slot dependency for alleviating uncoordinated slots problem while a global intent-slot graph interaction layer is introduced to model the interaction between multiple intents and all slots in the utterance.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":14,"sentence":"Experimental results on two public datasets show that our framework achieves state-of-the-art performance while being 11.5 times faster.","offset":4,"pro":0.8,"labels":"RST"},{"idx":15,"sentence":"Both performance and efficiency are crucial factors for sequence labeling tasks in many real-world scenarios.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"Although the pre-trained models (PTMs) have significantly improved the performance of various sequence labeling tasks, their computational cost is expensive.","offset":1,"pro":0.1,"labels":"GAP"},{"idx":15,"sentence":"To alleviate this problem, we extend the recent successful early-exit mechanism to accelerate the inference of PTMs for sequence labeling tasks.","offset":2,"pro":0.2,"labels":"PUR"},{"idx":15,"sentence":"However, existing early-exit mechanisms are specifically designed for sequence-level tasks, rather than sequence labeling.","offset":3,"pro":0.3,"labels":"GAP"},{"idx":15,"sentence":"In this paper, we first propose a simple extension of sentence-level early-exit for sequence labeling tasks.","offset":4,"pro":0.4,"labels":"PUR"},{"idx":15,"sentence":"To further reduce the computational cost, we also propose a token-level early-exit mechanism that allows partial tokens to exit early at different layers.","offset":5,"pro":0.5,"labels":"PUR"},{"idx":15,"sentence":"Considering the local dependency inherent in sequence labeling, we employed a window-based criterion to decide for a token whether or not to exit.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":15,"sentence":"The token-level early-exit brings the gap between training and inference, so we introduce an extra self-sampling fine-tuning stage to alleviate it.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":15,"sentence":"The extensive experiments on three popular sequence labeling tasks show that our approach can save up to 66%∼75% inference cost with minimal performance degradation.","offset":8,"pro":0.8,"labels":"RST"},{"idx":15,"sentence":"Compared with competitive compressed models such as DistilBERT, our approach can achieve better performance under the same speed-up ratios of 2{mbox{times, 3{mbox{times, and 4{mbox{times.","offset":9,"pro":0.9,"labels":"RST"},{"idx":16,"sentence":"Although the existing Named Entity Recognition (NER) models have achieved promising performance, they suffer from certain drawbacks.","offset":0,"pro":0,"labels":"GAP"},{"idx":16,"sentence":"The sequence labeling-based NER models do not perform well in recognizing long entities as they focus only on word-level information, while the segment-based NER models which focus on processing segment instead of single word are unable to capture the word-level dependencies within the segment.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":16,"sentence":"Moreover, as boundary detection and type prediction may cooperate with each other for the NER task, it is also important for the two sub-tasks to mutually reinforce each other by sharing their information.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":16,"sentence":"In this paper, we propose a novel Modularized Interaction Network (MIN) model which utilizes both segment-level information and word-level dependencies, and incorporates an interaction mechanism to support information sharing between boundary detection and type prediction to enhance the performance for the NER task.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":16,"sentence":"We have conducted extensive experiments based on three NER benchmark datasets.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":16,"sentence":"The performance results have shown that the proposed MIN model has outperformed the current state-of-the-art models.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":17,"sentence":"Capturing interactions among event arguments is an essential step towards robust event argument extraction (EAE).","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"However, existing efforts in this direction suffer from two limitations: 1) The argument role type information of contextual entities is mainly utilized as training signals, ignoring the potential merits of directly adopting it as semantically rich input features; 2) The argument-level sequential semantics, which implies the overall distribution pattern of argument roles over an event mention, is not well characterized.","offset":1,"pro":0.25,"labels":"GAP"},{"idx":17,"sentence":"To tackle the above two bottlenecks, we formalize EAE as a Seq2Seq-like learning problem for the first time, where a sentence with a specific event trigger is mapped to a sequence of event argument roles.","offset":2,"pro":0.5,"labels":"PUR"},{"idx":17,"sentence":"A neural architecture with a novel Bi-directional Entity-level Recurrent Decoder (BERD) is proposed to generate argument roles by incorporating contextual entities' argument role predictions, like a word-by-word text generation process, thereby distinguishing implicit argument distribution patterns within an event more accurately.","offset":3,"pro":0.75,"labels":"MTD"},{"idx":18,"sentence":"Many joint entity relation extraction models setup two separated label spaces for the two sub-tasks (i.e., entity detection and relation classification).","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"We argue that this setting may hinder the information interaction between entities and relations.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":18,"sentence":"In this work, we propose to eliminate the different treatment on the two sub-tasks' label spaces.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":18,"sentence":"The input of our model is a table containing all word pairs from a sentence.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":18,"sentence":"Entities and relations are represented by squares and rectangles in the table.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":18,"sentence":"We apply a unified classifier to predict each cell's label, which unifies the learning of two sub-tasks.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":18,"sentence":"For testing, an effective (yet fast) approximate decoder is proposed for finding squares and rectangles from tables.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":18,"sentence":"Experiments on three benchmarks (ACE04, ACE05, SciERC) show that, using only half the number of parameters, our model achieves competitive accuracy with the best extractor, and is faster.","offset":7,"pro":0.875,"labels":"RST"},{"idx":19,"sentence":"Continual learning has gained increasing attention in recent years, thanks to its biological interpretation and efficiency in many real-world applications.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"As a typical task of continual learning, continual relation extraction (CRE) aims to extract relations between entities from texts, where the samples of different relations are delivered into the model continuously.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":19,"sentence":"Some previous works have proved that storing typical samples of old relations in memory can help the model keep a stable understanding of old relations and avoid forgetting them.","offset":2,"pro":0.2,"labels":"BAC"},{"idx":19,"sentence":"However, most methods heavily depend on the memory size in that they simply replay these memorized samples in subsequent tasks.","offset":3,"pro":0.3,"labels":"GAP"},{"idx":19,"sentence":"To fully utilize memorized samples, in this paper, we employ relation prototype to extract useful information of each relation.","offset":4,"pro":0.4,"labels":"PUR"},{"idx":19,"sentence":"Specifically, the prototype embedding for a specific relation is computed based on memorized samples of this relation, which is collected by K-means algorithm.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":19,"sentence":"The prototypes of all observed relations at current learning stage are used to re-initialize a memory network to refine subsequent sample embeddings, which ensures the model's stable understanding on all observed relations when learning a new task.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":19,"sentence":"Compared with previous CRE models, our model utilizes the memory information sufficiently and efficiently, resulting in enhanced CRE performance.","offset":7,"pro":0.7,"labels":"CLN"},{"idx":19,"sentence":"Our experiments show that the proposed model outperforms the state-of-the-art CRE models and has great advantage in avoiding catastrophic forgetting.","offset":8,"pro":0.8,"labels":"CLN"},{"idx":19,"sentence":"The code and datasets are released on https://github.com/fd2014cl/RP-CRE.","offset":9,"pro":0.9,"labels":"CTN"},{"idx":0,"sentence":"Existing multilingual machine translation approaches mainly focus on English-centric directions, while the non-English directions still lag behind.","offset":0,"pro":0,"labels":"GAP"},{"idx":0,"sentence":"In this work, we aim to build a many-to-many translation system with an emphasis on the quality of non-English language directions.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":0,"sentence":"Our intuition is based on the hypothesis that a universal cross-language representation leads to better multilingual translation performance.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":0,"sentence":"To this end, we propose mRASP2, a training method to obtain a single unified multilingual translation model. mRASP2 is empowered by two techniques: a) a contrastive learning scheme to close the gap among representations of different languages, and b) data augmentation on both multiple parallel and monolingual data to further align token representations.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":0,"sentence":"For English-centric directions, mRASP2 achieves competitive or even better performance than a strong pre-trained model mBART on tens of WMT benchmarks.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":0,"sentence":"For non-English directions, mRASP2 achieves an improvement of average 10+ BLEU compared with the multilingual baseline","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":1,"sentence":"Neural Machine Translation (NMT) currently exhibits biases such as producing translations that are too short and overgenerating frequent words, and shows poor robustness to copy noise in training data or domain shift.","offset":0,"pro":0,"labels":"GAP"},{"idx":1,"sentence":"Recent work has tied these shortcomings to beam search - the de facto standard inference algorithm in NMT - and Eikema & Aziz (2020) propose to use Minimum Bayes Risk (MBR) decoding on unbiased samples instead.","offset":1,"pro":0.25,"labels":"BAC"},{"idx":1,"sentence":"In this paper, we empirically investigate the properties of MBR decoding on a number of previously reported biases and failure cases of beam search.","offset":2,"pro":0.5,"labels":"PUR"},{"idx":1,"sentence":"We find that MBR still exhibits a length and token frequency bias, owing to the MT metrics used as utility functions, but that MBR also increases robustness against copy noise in the training data and domain shift.","offset":3,"pro":0.75,"labels":"RST"},{"idx":2,"sentence":"One of the reasons Transformer translation models are popular is that self-attention networks for context modelling can be easily parallelized at sequence level.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"However, the computational complexity of a self-attention network is O(n^2), increasing quadratically with sequence length.","offset":1,"pro":0.08333333333333333,"labels":"GAP"},{"idx":2,"sentence":"By contrast, the complexity of LSTM-based approaches is only O(n).","offset":2,"pro":0.16666666666666666,"labels":"RST"},{"idx":2,"sentence":"In practice, however, LSTMs are much slower to train than self-attention networks as they cannot be parallelized at sequence level: to model context, the current LSTM state relies on the full LSTM computation of the preceding state.","offset":3,"pro":0.25,"labels":"GAP"},{"idx":2,"sentence":"This has to be computed n times for a sequence of length n.","offset":4,"pro":0.3333333333333333,"labels":"MTD"},{"idx":2,"sentence":"The linear transformations involved in the LSTM gate and state computations are the major cost factors in this.","offset":5,"pro":0.4166666666666667,"labels":"GAP"},{"idx":2,"sentence":"To enable sequence-level parallelization of LSTMs, we approximate full LSTM context modelling by computing hidden states and gates with the current input and a simple bag-of-words representation of the preceding tokens context.","offset":6,"pro":0.5,"labels":"PUR"},{"idx":2,"sentence":"This allows us to compute each input step efficiently in parallel, avoiding the formerly costly sequential linear transformations.","offset":7,"pro":0.5833333333333334,"labels":"MTD"},{"idx":2,"sentence":"We then connect the outputs of each parallel step with computationally cheap element-wise computations.","offset":8,"pro":0.6666666666666666,"labels":"MTD"},{"idx":2,"sentence":"We call this the Highly Parallelized LSTM.","offset":9,"pro":0.75,"labels":"MTD"},{"idx":2,"sentence":"To further constrain the number of LSTM parameters, we compute several small HPLSTMs in parallel like multi-head attention in the Transformer.","offset":10,"pro":0.8333333333333334,"labels":"MTD"},{"idx":2,"sentence":"The experiments show that our MHPLSTM decoder achieves significant BLEU improvements, while being even slightly faster than the self-attention network in training, and much faster than the standard LSTM.","offset":11,"pro":0.9166666666666666,"labels":"CLN"},{"idx":3,"sentence":"Word alignment and machine translation are two closely related tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"Neural translation models, such as RNN-based and Transformer models, employ a target-to-source attention mechanism which can provide rough word alignments, but with a rather low accuracy.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":3,"sentence":"High-quality word alignment can help neural machine translation in many different ways, such as missing word detection, annotation transfer and lexicon injection.","offset":2,"pro":0.25,"labels":"BAC"},{"idx":3,"sentence":"Existing methods for learning word alignment include statistical word aligners (e.g. GIZA++) and recently neural word alignment models.","offset":3,"pro":0.375,"labels":"BAC"},{"idx":3,"sentence":"This paper presents a bidirectional Transformer based alignment (BTBA) model for unsupervised learning of the word alignment task.","offset":4,"pro":0.5,"labels":"PUR"},{"idx":3,"sentence":"Our BTBA model predicts the current target word by attending the source context and both left-side and right-side target context to produce accurate target-to-source attention (alignment).","offset":5,"pro":0.625,"labels":"MTD"},{"idx":3,"sentence":"We further fine-tune the target-to-source attention in the BTBA model to obtain better alignments using a full context based optimization method and self-supervised training.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":3,"sentence":"We test our method on three word alignment tasks and show that our method outperforms both previous neural word alignment approaches and the popular statistical word aligner GIZA++.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":4,"sentence":"Multilingual neural machine translation aims at learning a single translation model for multiple languages.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"These jointly trained models often suffer from performance degradationon rich-resource language pairs.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":4,"sentence":"We attribute this degeneration to parameter interference.","offset":2,"pro":0.2222222222222222,"labels":"MTD"},{"idx":4,"sentence":"In this paper, we propose LaSS to jointly train a single unified multilingual MT model.","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":4,"sentence":"LaSS learns Language Specific Sub-network (LaSS) for each language pair to counter parameter interference.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":4,"sentence":"Comprehensive experiments on IWSLT and WMT datasets with various Transformer architectures show that LaSS obtains gains on 36 language pairs by up to 1.2 BLEU.","offset":5,"pro":0.5555555555555556,"labels":"RST"},{"idx":4,"sentence":"Besides, LaSS shows its strong generalization performance at easy adaptation to new language pairs and zero-shot translation.","offset":6,"pro":0.6666666666666666,"labels":"RST"},{"idx":4,"sentence":"LaSS boosts zero-shot translation with an average of 8.3 BLEU on 30 language pairs.","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":4,"sentence":"Codes and trained models are available at https://github.com/NLP-Playground/LaSS.","offset":8,"pro":0.8888888888888888,"labels":"CTN"},{"idx":5,"sentence":"While state-of-the-art NLP models have been achieving the excellent performance of a wide range of tasks in recent years, important questions are being raised about their robustness and their underlying sensitivity to systematic biases that may exist in their training and test data.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"Such issues come to be manifest in performance problems when faced with out-of-distribution data in the field.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":5,"sentence":"One recent solution has been to use counterfactually augmented datasets in order to reduce any reliance on spurious patterns that may exist in the original data.","offset":2,"pro":0.3333333333333333,"labels":"BAC"},{"idx":5,"sentence":"Producing high-quality augmented data can be costly and time-consuming as it usually needs to involve human feedback and crowdsourcing efforts.","offset":3,"pro":0.5,"labels":"GAP"},{"idx":5,"sentence":"In this work, we propose an alternative by describing and evaluating an approach to automatically generating counterfactual data for the purpose of data augmentation and explanation.","offset":4,"pro":0.6666666666666666,"labels":"PUR"},{"idx":5,"sentence":"A comprehensive evaluation on several different datasets and using a variety of state-of-the-art benchmarks demonstrate how our approach can achieve significant improvements in model performance when compared to models training on the original data and even when compared to models trained with the benefit of human-generated augmented data.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":6,"sentence":"As a fine-grained task, the annotation cost of aspect term extraction is extremely high.","offset":0,"pro":0,"labels":"GAP"},{"idx":6,"sentence":"Recent attempts alleviate this issue using domain adaptation that transfers common knowledge across domains.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":6,"sentence":"Since most aspect terms are domain-specific, they cannot be transferred directly.","offset":2,"pro":0.2,"labels":"GAP"},{"idx":6,"sentence":"Existing methods solve this problem by associating aspect terms with pivot words (we call this passive domain adaptation because the transfer of aspect terms relies on the links to pivots).","offset":3,"pro":0.3,"labels":"BAC"},{"idx":6,"sentence":"However, all these methods need either manually labeled pivot words or expensive computing resources to build associations.","offset":4,"pro":0.4,"labels":"GAP"},{"idx":6,"sentence":"In this paper, we propose a novel active domain adaptation method.","offset":5,"pro":0.5,"labels":"PUR"},{"idx":6,"sentence":"Our goal is to transfer aspect terms by actively supplementing transferable knowledge.","offset":6,"pro":0.6,"labels":"PUR"},{"idx":6,"sentence":"To this end, we construct syntactic bridges by recognizing syntactic roles as pivots instead of as links to pivots.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":6,"sentence":"We also build semantic bridges by retrieving transferable semantic prototypes.","offset":8,"pro":0.8,"labels":"MTD"},{"idx":6,"sentence":"Extensive experiments show that our method significantly outperforms previous approaches.","offset":9,"pro":0.9,"labels":"CLN"},{"idx":7,"sentence":"With the popularity of smartphones, we have witnessed the rapid proliferation of multimodal posts on various social media platforms.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"We observe that the multimodal sentiment expression has specific global characteristics, such as the interdependencies of objects or scenes within the image.","offset":1,"pro":0.125,"labels":"MTD"},{"idx":7,"sentence":"However, most previous studies only considered the representation of a single image-text post and failed to capture the global co-occurrence characteristics of the dataset.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":7,"sentence":"In this paper, we propose Multi-channel Graph Neural Networks with Sentiment-awareness (MGNNS) for image-text sentiment detection.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":7,"sentence":"Specifically, we first encode different modalities to capture hidden representations.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":7,"sentence":"Then, we introduce multi-channel graph neural networks to learn multimodal representations based on the global characteristics of the dataset.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":7,"sentence":"Finally, we implement multimodal in-depth fusion with the multi-head attention mechanism to predict the sentiment of image-text pairs.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":7,"sentence":"Extensive experiments conducted on three publicly available datasets demonstrate the effectiveness of our approach for multimodal sentiment detection.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":8,"sentence":"Product reviews contain a large number of implicit aspects and implicit opinions.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"However, most of the existing studies in aspect-based sentiment analysis ignored this problem.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":8,"sentence":"In this work, we introduce a new task, named Aspect-Category-Opinion-Sentiment (ACOS) Quadruple Extraction, with the goal to extract all aspect-category-opinion-sentiment quadruples in a review sentence and provide full support for aspect-based sentiment analysis with implicit aspects and opinions.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":8,"sentence":"We furthermore construct two new datasets, Restaurant-ACOS and Laptop-ACOS, for this new task, both of which contain the annotations of not only aspect-category-opinion-sentiment quadruples but also implicit aspects and opinions.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":8,"sentence":"The former is an extension of the SemEval Restaurant dataset; the latter is a newly collected and annotated Laptop dataset, twice the size of the SemEval Laptop dataset.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":8,"sentence":"We finally benchmark the task with four baseline systems.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":8,"sentence":"Experiments demonstrate the feasibility of the new task and its effectiveness in extracting and describing implicit aspects and implicit opinions.","offset":6,"pro":0.75,"labels":"CLN"},{"idx":8,"sentence":"The two datasets and source code of four systems are publicly released at url{https://github.com/NUSTM/ACOS.","offset":7,"pro":0.875,"labels":"CTN"},{"idx":9,"sentence":"The product reviews summarization task aims to automatically produce a short summary for a set of reviews of a given product.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"Such summaries are expected to aggregate a range of different opinions in a concise, coherent and informative manner.","offset":1,"pro":0.1111111111111111,"labels":"MTD"},{"idx":9,"sentence":"This challenging task gives rise to two shortcomings in existing work.","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":9,"sentence":"First, summarizers tend to favor generic content that appears in reviews for many different products, resulting in template-like, less informative summaries.","offset":3,"pro":0.3333333333333333,"labels":"GAP"},{"idx":9,"sentence":"Second, as reviewers often disagree on the pros and cons of a given product, summarizers sometimes yield inconsistent, self-contradicting summaries.","offset":4,"pro":0.4444444444444444,"labels":"GAP"},{"idx":9,"sentence":"We propose the PASS system (Perturb-and-Select Summarizer) that employs a large pre-trained Transformer-based model (T5 in our case), which follows a few-shot fine-tuning scheme.","offset":5,"pro":0.5555555555555556,"labels":"PUR"},{"idx":9,"sentence":"A key component of the PASS system relies on applying systematic perturbations to the model's input during inference, which allows it to generate multiple different summaries per product.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":9,"sentence":"We develop a method for ranking these summaries according to desired criteria, coherence in our case, enabling our system to almost entirely avoid the problem of self-contradiction.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":9,"sentence":"We compare our system against strong baselines on publicly available datasets, and show that it produces summaries which are more informative, diverse and coherent.","offset":8,"pro":0.8888888888888888,"labels":"RST"},{"idx":10,"sentence":"For sentence-level extractive summarization, there is a disproportionate ratio of selected and unselected sentences, leading to flatting the summary features when maximizing the accuracy.","offset":0,"pro":0,"labels":"GAP"},{"idx":10,"sentence":"The imbalanced classification of summarization is inherent, which can't be addressed by common algorithms easily.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":10,"sentence":"In this paper, we conceptualize the single-document extractive summarization as a rebalance problem and present a deep differential amplifier framework.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":10,"sentence":"Specifically, we first calculate and amplify the semantic difference between each sentence and all other sentences, and then apply the residual unit as the second item of the differential amplifier to deepen the architecture.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":10,"sentence":"Finally, to compensate for the imbalance, the corresponding objective loss of minority class is boosted by a weighted cross-entropy.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":10,"sentence":"In contrast to previous approaches, this model pays more attention to the pivotal information of one sentence, instead of all the informative context modeling by recurrent or Transformer architecture.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":10,"sentence":"We demonstrate experimentally on two benchmark datasets that our summarizer performs competitively against state-of-the-art methods.","offset":6,"pro":0.75,"labels":"CLN"},{"idx":10,"sentence":"Our source code will be available on Github.","offset":7,"pro":0.875,"labels":"CTN"},{"idx":11,"sentence":"In this paper, we address a novel task, Multiple TimeLine Summarization (MTLS), which extends the flexibility and versatility of Time-Line Summarization (TLS).","offset":0,"pro":0,"labels":"PUR"},{"idx":11,"sentence":"Given any collection of time-stamped news articles, MTLS automatically discovers important yet different stories and generates a corresponding time-line for each story.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":11,"sentence":"To achieve this, we propose a novel unsupervised summarization framework based on two-stage affinity propagation.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":11,"sentence":"We also introduce a quantitative evaluation measure for MTLS based on previousTLS evaluation methods.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":11,"sentence":"Experimental results show that our MTLS framework demonstrates high effectiveness and MTLS task can give bet-ter results than TLS.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":12,"sentence":"Recently, opinion summarization, which is the generation of a summary from multiple reviews, has been conducted in a self-supervised manner by considering a sampled review as a pseudo summary.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"However, non-text data such as image and metadata related to reviews have been considered less often.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":12,"sentence":"To use the abundant information contained in non-text data, we propose a self-supervised multimodal opinion summarization framework called MultimodalSum.","offset":2,"pro":0.2222222222222222,"labels":"PUR"},{"idx":12,"sentence":"Our framework obtains a representation of each modality using a separate encoder for each modality, and the text decoder generates a summary.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":12,"sentence":"To resolve the inherent heterogeneity of multimodal data, we propose a multimodal training pipeline.","offset":4,"pro":0.4444444444444444,"labels":"PUR"},{"idx":12,"sentence":"We first pretrain the text encoder-decoder based solely on text modality data.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":12,"sentence":"Subsequently, we pretrain the non-text modality encoders by considering the pretrained text decoder as a pivot for the homogeneous representation of multimodal data.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":12,"sentence":"Finally, to fuse multimodal representations, we train the entire framework in an end-to-end manner.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":12,"sentence":"We demonstrate the superiority of MultimodalSum by conducting experiments on Yelp and Amazon datasets.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":13,"sentence":"In recent years, reference-based and supervised summarization evaluation metrics have been widely explored.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"However, collecting human-annotated references and ratings are costly and time-consuming.","offset":1,"pro":0.1,"labels":"GAP"},{"idx":13,"sentence":"To avoid these limitations, we propose a training-free and reference-free summarization evaluation metric.","offset":2,"pro":0.2,"labels":"PUR"},{"idx":13,"sentence":"Our metric consists of a centrality-weighted relevance score and a self-referenced redundancy score.","offset":3,"pro":0.3,"labels":"MTD"},{"idx":13,"sentence":"The relevance score is computed between the pseudo reference built from the source document and the given summary, where the pseudo reference content is weighted by the sentence centrality to provide importance guidance.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":13,"sentence":"Besides an F_1-based relevance score, we also design an F_beta-based variant that pays more attention to the recall score.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":13,"sentence":"As for the redundancy score of the summary, we compute a self-masked similarity score with the summary itself to evaluate the redundant information in the summary.","offset":6,"pro":0.6,"labels":"RST"},{"idx":13,"sentence":"Finally, we combine the relevance and redundancy scores to produce the final evaluation score of the given summary.","offset":7,"pro":0.7,"labels":"RST"},{"idx":13,"sentence":"Extensive experiments show that our methods can significantly outperform existing methods on both multi-document and single-document summarization evaluation.","offset":8,"pro":0.8,"labels":"CLN"},{"idx":13,"sentence":"The source code is released at https://github.com/Chen-Wang-CUHK/Training-Free-and-Ref-Free-Summ-Evaluation.","offset":9,"pro":0.9,"labels":"CTN"},{"idx":14,"sentence":"Short textual descriptions of entities provide summaries of their key attributes and have been shown to be useful sources of background knowledge for tasks such as entity linking and question answering.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"However, generating entity descriptions, especially for new and long-tail entities, can be challenging since relevant information is often scattered across multiple sources with varied content and style.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":14,"sentence":"We introduce DESCGEN: given mentions spread over multiple documents, the goal is to generate an entity summary description.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":14,"sentence":"DESCGEN consists of 37K entity descriptions from Wikipedia and Fandom, each paired with nine evidence documents on average.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":14,"sentence":"The documents were collected using a combination of entity linking and hyperlinks into the entity pages, which together provide high-quality distant supervision.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":14,"sentence":"Compared to other multi-document summarization tasks, our task is entity-centric, more abstractive, and covers a wide range of domains.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":14,"sentence":"We also propose a two-stage extract-then-generate baseline and show that there exists a large gap (19.9% in ROUGE-L) between state-of-art models and human performance, suggesting that the data will support significant future work.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":15,"sentence":"With the recent success of pre-trained models in NLP, a significant focus was put on interpreting their representations.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"One of the most prominent approaches is structural probing (Hewitt and Manning, 2019), where a linear projection of word embeddings is performed in order to approximate the topology of dependency structures.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":15,"sentence":"In this work, we introduce a new type of structural probing, where the linear projection is decomposed into 1.iso-morphic space rotation; 2.linear scaling that identifies and scales the most relevant dimensions.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":15,"sentence":"In addition to syntactic dependency, we evaluate our method on two novel tasks (lexical hypernymy and position in a sentence).","offset":3,"pro":0.5,"labels":"MTD"},{"idx":15,"sentence":"We jointly train the probes for multiple tasks and experimentally show that lexical and syntactic information is separated in the representations.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":15,"sentence":"Moreover, the orthogonal constraint makes the Structural Probes less vulnerable to memorization.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":16,"sentence":"Backdoor attacks are a kind of insidious security threat against machine learning models.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"After being injected with a backdoor in training, the victim model will produce adversary-specified outputs on the inputs embedded with predesigned triggers but behave properly on normal inputs during inference.","offset":1,"pro":0.125,"labels":"MTD"},{"idx":16,"sentence":"As a sort of emergent attack, backdoor attacks in natural language processing (NLP) are investigated insufficiently.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":16,"sentence":"As far as we know, almost all existing textual backdoor attack methods insert additional contents into normal samples as triggers, which causes the trigger-embedded samples to be detected and the backdoor attacks to be blocked without much effort.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":16,"sentence":"In this paper, we propose to use the syntactic structure as the trigger in textual backdoor attacks.","offset":4,"pro":0.5,"labels":"PUR"},{"idx":16,"sentence":"We conduct extensive experiments to demonstrate that the syntactic trigger-based attack method can achieve comparable attack performance (almost 100% success rate) to the insertion-based methods but possesses much higher invisibility and stronger resistance to defenses.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":16,"sentence":"These results also reveal the significant insidiousness and harmfulness of textual backdoor attacks.","offset":6,"pro":0.75,"labels":"CLN"},{"idx":16,"sentence":"All the code and data of this paper can be obtained at https://github.com/thunlp/HiddenKiller.","offset":7,"pro":0.875,"labels":"CTN"},{"idx":17,"sentence":"Since language models are used to model a wide variety of languages, it is natural to ask whether the neural architectures used for the task have inductive biases towards modeling particular types of languages.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"Investigation of these biases has proved complicated due to the many variables that appear in the experimental setup.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":17,"sentence":"Languages vary in many typological dimensions, and it is difficult to single out one or two to investigate without the others acting as confounders.","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":17,"sentence":"We propose a novel method for investigating the inductive biases of language models using artificial languages.","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":17,"sentence":"These languages are constructed to allow us to create parallel corpora across languages that differ only in the typological feature being investigated, such as word order.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":17,"sentence":"We then use them to train and test language models.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":17,"sentence":"This constitutes a fully controlled causal framework, and demonstrates how grammar engineering can serve as a useful tool for analyzing neural models.","offset":6,"pro":0.6666666666666666,"labels":"CLN"},{"idx":17,"sentence":"Using this method, we find that commonly used neural architectures exhibit different inductive biases: LSTMs display little preference with respect to word ordering, while transformers display a clear preference for some orderings over others.","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":17,"sentence":"Further, we find that neither the inductive bias of the LSTM nor that of the transformer appear to reflect any tendencies that we see in attested natural languages.","offset":8,"pro":0.8888888888888888,"labels":"RST"},{"idx":18,"sentence":"Despite the success of contextualized language models on various NLP tasks, it is still unclear what these models really learn.","offset":0,"pro":0,"labels":"GAP"},{"idx":18,"sentence":"In this paper, we contribute to the current efforts of explaining such models by exploring the continuum between function and content words with respect to contextualization in BERT, based on linguistically-informed insights.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":18,"sentence":"In particular, we utilize scoring and visual analytics techniques: we use an existing similarity-based score to measure contextualization and integrate it into a novel visual analytics technique, presenting the model's layers simultaneously and highlighting intra-layer properties and inter-layer differences.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":18,"sentence":"We show that contextualization is neither driven by polysemy nor by pure context variation.","offset":3,"pro":0.6,"labels":"RST"},{"idx":18,"sentence":"We also provide insights on why BERT fails to model words in the middle of the functionality continuum.","offset":4,"pro":0.8,"labels":"CTN"},{"idx":19,"sentence":"Neural network architectures in natural language processing often use attention mechanisms to produce probability distributions over input token representations.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"Attention has empirically been demonstrated to improve performance in various tasks, while its weights have been extensively used as explanations for model predictions.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":19,"sentence":"Recent studies (Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019) have showed that it cannot generally be considered as a faithful explanation (Jacovi and Goldberg, 2020) across encoders and tasks.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":19,"sentence":"In this paper, we seek to improve the faithfulness of attention-based explanations for text classification.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":19,"sentence":"We achieve this by proposing a new family of Task-Scaling (TaSc) mechanisms that learn task-specific non-contextualised information to scale the original attention weights.","offset":4,"pro":0.5714285714285714,"labels":"PUR"},{"idx":19,"sentence":"Evaluation tests for explanation faithfulness, show that the three proposed variants of TaSc improve attention-based explanations across two attention mechanisms, five encoders and five text classification datasets without sacrificing predictive performance.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":19,"sentence":"Finally, we demonstrate that TaSc consistently provides more faithful attention-based explanations compared to three widely-used interpretability techniques.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":0,"sentence":"While previous works have shown that unlabeled data in a target language can be used to improve cross-lingual model performance, we propose a novel adversarial approach (AdvPicker) to better leverage such data and further improve results.","offset":0,"pro":0,"labels":"PUR"},{"idx":0,"sentence":"We design an adversarial learning framework in which an encoder learns entity domain knowledge from labeled source-language data and better shared features are captured via adversarial training - where a discriminator selects less language-dependent target-language data via similarity to the source language.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":0,"sentence":"Experimental results on standard benchmark datasets well demonstrate that the proposed method benefits strongly from this data selection process and outperforms existing state-of-the-art methods; without requiring any additional external resources (e.g., gazetteers or via machine translation).","offset":2,"pro":0.4,"labels":"CLN"},{"idx":0,"sentence":"Neural methods have been shown to achieve high performance in Named Entity Recognition (NER)","offset":3,"pro":0.6,"labels":"BAC"},{"idx":0,"sentence":"but rely on costly high-quality labeled data for training, which is not always available across language","offset":4,"pro":0.8,"labels":"GAP"},{"idx":1,"sentence":"Nowadays, fake news detection, which aims to verify whether a news document is trusted or fake, has become urgent and important.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"Most existing methods rely heavily on linguistic and semantic features from the news content, and fail to effectively exploit external knowledge which could help determine whether the news document is trusted.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":1,"sentence":"In this paper, we propose a novel end-to-end graph neural model called CompareNet, which compares the news to the knowledge base (KB) through entities for fake news detection.","offset":2,"pro":0.2222222222222222,"labels":"PUR"},{"idx":1,"sentence":"Considering that fake news detection is correlated with topics, we also incorporate topics to enrich the news representation.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":1,"sentence":"Specifically, we first construct a directed heterogeneous document graph for each news incorporating topics and entities.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":1,"sentence":"Based on the graph, we develop a heterogeneous graph attention network for learning the topic-enriched news representation as well as the contextual entity representations that encode the semantics of the news content.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":1,"sentence":"The contextual entity representations are then compared to the corresponding KB-based entity representations through a carefully designed entity comparison network, to capture the consistency between the news content and KB.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":1,"sentence":"Finally, the topic-enriched news representation combining the entity comparison features is fed into a fake news classifier.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":1,"sentence":"Experimental results on two benchmark datasets demonstrate that CompareNet significantly outperforms state-of-the-art methods.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":2,"sentence":"Named entity recognition (NER) remains challenging when entity mentions can be discontinuous.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"Existing methods break the recognition process into several sequential steps.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":2,"sentence":"In training, they predict conditioned on the golden intermediate results, while at inference relying on the model output of the previous steps, which introduces exposure bias.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":2,"sentence":"To solve this problem, we first construct a segment graph for each sentence, in which each node denotes a segment (a continuous entity on its own, or a part of discontinuous entities), and an edge links two nodes that belong to the same entity.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":2,"sentence":"The nodes and edges can be generated respectively in one stage with a grid tagging scheme and learned jointly using a novel architecture named Mac.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":2,"sentence":"Then discontinuous NER can be reformulated as a non-parametric process of discovering maximal cliques in the graph and concatenating the spans in each clique.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":2,"sentence":"Experiments on three benchmarks show that our method outperforms the state-of-the-art (SOTA) results, with up to 3.5 percentage points improvement on F1, and achieves 5x speedup over the SOTA model.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":3,"sentence":"Entity linking (EL) is the task of disambiguating mentions appearing in text by linking them to entities in a knowledge graph, a crucial task for text understanding, question answering or conversational systems.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"In the special case of short-text EL, which poses additional challenges due to limited context, prior approaches have reached good performance by employing heuristics-based methods or purely neural approaches.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":3,"sentence":"Here, we take a different, neuro-symbolic approach that combines the advantages of using interpretable rules based on first-order logic with the performance of neural learning.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":3,"sentence":"Even though constrained to use rules, we show that we reach competitive or better performance with SoTA black-box neural approaches.","offset":3,"pro":0.375,"labels":"RST"},{"idx":3,"sentence":"Furthermore, our framework has the benefits of extensibility and transferability.","offset":4,"pro":0.5,"labels":"CLN"},{"idx":3,"sentence":"We show that we can easily blend existing rule templates given by a human expert, with multiple types of features (priors, BERT encodings, box embeddings, etc), and even with scores resulting from previous EL methods, thus improving on such methods.","offset":5,"pro":0.625,"labels":"RST"},{"idx":3,"sentence":"As an example of improvement, on the LC-QuAD-1.0 dataset, we show more than 3% increase in F1 score relative to previous SoTA.","offset":6,"pro":0.75,"labels":"RST"},{"idx":3,"sentence":"Finally, we show that the inductive bias offered by using logic results in a set of learned rules that transfers from one dataset to another, sometimes without finetuning, while still having high accuracy.","offset":7,"pro":0.875,"labels":"RST"},{"idx":4,"sentence":"Context-aware machine translation models are designed to leverage contextual information, but often fail to do so.","offset":0,"pro":0,"labels":"GAP"},{"idx":4,"sentence":"As a result, they inaccurately disambiguate pronouns and polysemous words that require context for resolution.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":4,"sentence":"In this paper, we ask several questions: What contexts do human translators use to resolve ambiguous words?","offset":2,"pro":0.25,"labels":"PUR"},{"idx":4,"sentence":"Are models paying large amounts of attention to the same context?","offset":3,"pro":0.375,"labels":"PUR"},{"idx":4,"sentence":"What if we explicitly train them to do so?","offset":4,"pro":0.5,"labels":"PUR"},{"idx":4,"sentence":"To answer these questions, we introduce SCAT (Supporting Context for Ambiguous Translations), a new English-French dataset comprising supporting context words for 14K translations that professional translators found useful for pronoun disambiguation.","offset":5,"pro":0.625,"labels":"PUR"},{"idx":4,"sentence":"Using SCAT, we perform an in-depth analysis of the context used to disambiguate, examining positional and lexical characteristics of the supporting words.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":4,"sentence":"Furthermore, we measure the degree of alignment between the model's attention scores and the supporting context from SCAT, and apply a guided attention strategy to encourage agreement between the two.","offset":7,"pro":0.875,"labels":"MTD"},{"idx":5,"sentence":"The scarcity of parallel data is a major obstacle for training high-quality machine translation systems for low-resource languages.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"Fortunately, some low-resource languages are linguistically related or similar to high-resource languages; these related languages may share many lexical or syntactic structures.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":5,"sentence":"In this work, we exploit this linguistic overlap to facilitate translating to and from a low-resource language with only monolingual data, in addition to any parallel data in the related high-resource language.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":5,"sentence":"Our method, NMT-Adapt, combines denoising autoencoding, back-translation and adversarial objectives to utilize monolingual data for low-resource adaptation.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":5,"sentence":"We experiment on 7 languages from three different language families and show that our technique significantly improves translation into low-resource language compared to other translation baselines.","offset":4,"pro":0.8,"labels":"RST"},{"idx":6,"sentence":"Bilingual lexicons map words in one language to their translations in another, and are typically induced by learning linear projections to align monolingual word embedding spaces.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"In this paper, we show it is possible to produce much higher quality lexicons with methods that combine (1) unsupervised bitext mining and (2) unsupervised word alignment.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":6,"sentence":"Directly applying a pipeline that uses recent algorithms for both subproblems significantly improves induced lexicon quality and further gains are possible by learning to filter the resulting lexical entries, with both unsupervised and semi-supervised schemes.","offset":2,"pro":0.4,"labels":"CLN"},{"idx":6,"sentence":"Our final model outperforms the state of the art on the BUCC 2020 shared task by 14 F1 points averaged over 12 language pairs, while also providing a more interpretable approach that allows for rich reasoning of word meaning in context.","offset":3,"pro":0.6,"labels":"RST"},{"idx":6,"sentence":"Further analysis of our output and the standard reference lexicons suggests they are of comparable quality, and new benchmarks may be needed to measure further progress on this task.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":7,"sentence":"We present a simple yet effective approach to build multilingual speech-to-text (ST) translation through efficient transfer learning from a pretrained speech encoder and text decoder.","offset":0,"pro":0,"labels":"PUR"},{"idx":7,"sentence":"Our key finding is that a minimalistic LNA (LayerNorm and Attention) finetuning can achieve zero-shot crosslingual and cross-modality transfer ability by only finetuning 10 50% of the pretrained parameters.","offset":1,"pro":0.2,"labels":"RST"},{"idx":7,"sentence":"This effectively leverages large pretrained models at low training cost such as wav2vec 2.0 for acoustic modeling, and mBART for multilingual text generation.","offset":2,"pro":0.4,"labels":"RST"},{"idx":7,"sentence":"This sets a new state-of-the-art for 36 translation directions (and surpassing cascaded ST for 26 of them) on the large-scale multilingual ST benchmark CoVoST 2 (+6.4 BLEU on average for En-X directions and +6.7 BLEU for X-En directions).","offset":3,"pro":0.6,"labels":"RST"},{"idx":7,"sentence":"Our approach demonstrates strong zero-shot performance in a many-to-many multilingual model (+5.6 BLEU on average across 28 non-English directions), making it an appealing approach for attaining high-quality speech translation with improved parameter and data efficiency.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":8,"sentence":"Learning contextual text embeddings that represent causal graphs has been useful in improving the performance of downstream tasks like causal treatment effect estimation.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"However, existing causal embeddings which are trained to predict direct causal links, fail to capture other indirect causal links of the graph, thus leading to spurious correlations in downstream tasks.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":8,"sentence":"In this paper, we define the faithfulness property of contextual embeddings to capture geometric distance-based properties of directed acyclic causal graphs.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":8,"sentence":"By incorporating these faithfulness properties, we learn text embeddings that are 31.3% more faithful to human validated causal graphs with about 800K and 200K causal links and achieve 21.1% better Precision-Recall AUC in a link prediction fine-tuning task.","offset":3,"pro":0.5,"labels":"RST"},{"idx":8,"sentence":"Further, in a crowdsourced causal question-answering task on Yahoo!","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":8,"sentence":"Answers with questions of the form What causes X?, our faithful embeddings achieved a precision of the first ranked answer (P@1) of 41.07%, outperforming the existing baseline by 10.2%.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":9,"sentence":"Transformer-based language models benefit from conditioning on contexts of hundreds to thousands of previous tokens.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"What aspects of these contexts contribute to accurate model prediction?","offset":1,"pro":0.2,"labels":"PUR"},{"idx":9,"sentence":"We describe a series of experiments that measure usable information by selectively ablating lexical and structural information in transformer language models trained on English Wikipedia.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":9,"sentence":"In both mid- and long-range contexts, we find that several extremely destructive context manipulations-including shuffling word order within sentences and deleting all words other than nouns-remove less than 15% of the usable information.","offset":3,"pro":0.6,"labels":"RST"},{"idx":9,"sentence":"Our results suggest that long contexts, but not their detailed syntactic and propositional content, are important for the low perplexity of current transformer language models.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":10,"sentence":"In this paper, we introduce Integrated Directional Gradients (IDG), a method for attributing importance scores to groups of features, indicating their relevance to the output of a neural network model for a given input.","offset":0,"pro":0,"labels":"PUR"},{"idx":10,"sentence":"The success of Deep Neural Networks has been attributed to their ability to capture higher level feature interactions.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":10,"sentence":"Hence, in the last few years capturing the importance of these feature interactions has received increased prominence in ML interpretability literature.","offset":2,"pro":0.2,"labels":"BAC"},{"idx":10,"sentence":"In this paper, we formally define the feature group attribution problem and outline a set of axioms that any intuitive feature group attribution method should satisfy.","offset":3,"pro":0.3,"labels":"PUR"},{"idx":10,"sentence":"Earlier, cooperative game theory inspired axiomatic methods only borrowed axioms from solution concepts (such as Shapley value) for individual feature attributions and introduced their own extensions to model interactions.","offset":4,"pro":0.4,"labels":"BAC"},{"idx":10,"sentence":"In contrast, our formulation is inspired by axioms satisfied by characteristic functions as well as solution concepts in cooperative game theory literature.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":10,"sentence":"We believe that characteristic functions are much better suited to model importance of groups compared to just solution concepts.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":10,"sentence":"We demonstrate that our proposed method, IDG, satisfies all the axioms.","offset":7,"pro":0.7,"labels":"CLN"},{"idx":10,"sentence":"Using IDG we analyze two state-of-the-art text classifiers on three benchmark datasets for sentiment analysis.","offset":8,"pro":0.8,"labels":"MTD"},{"idx":10,"sentence":"Our experiments show that IDG is able to effectively capture semantic interactions in linguistic models via negations and conjunctions.","offset":9,"pro":0.9,"labels":"RST"},{"idx":11,"sentence":"Sentence embeddings are an important component of many natural language processing (NLP) systems.","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"Like word embeddings, sentence embeddings are typically learned on large text corpora and then transferred to various downstream tasks, such as clustering and retrieval.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":11,"sentence":"Unlike word embeddings, the highest performing solutions for learning sentence embeddings require labelled data, limiting their usefulness to languages and domains where labelled data is abundant.","offset":2,"pro":0.25,"labels":"BAC"},{"idx":11,"sentence":"In this paper, we present DeCLUTR: Deep Contrastive Learning for Unsupervised Textual Representations.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":11,"sentence":"Inspired by recent advances in deep metric learning (DML), we carefully design a self-supervised objective for learning universal sentence embeddings that does not require labelled training data.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":11,"sentence":"When used to extend the pretraining of transformer-based language models, our approach closes the performance gap between unsupervised and supervised pretraining for universal sentence encoders.","offset":5,"pro":0.625,"labels":"CLN"},{"idx":11,"sentence":"Importantly, our experiments suggest that the quality of the learned embeddings scale with both the number of trainable parameters and the amount of unlabelled training data.","offset":6,"pro":0.75,"labels":"CLN"},{"idx":11,"sentence":"Our code and pretrained models are publicly available and can be easily adapted to new domains or used to embed unseen text.","offset":7,"pro":0.875,"labels":"CTN"},{"idx":12,"sentence":"Due to the scarcity of annotated data, Abstract Meaning Representation (AMR) research is relatively limited and challenging for languages other than English.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"Upon the availability of English AMR dataset and English-to- X parallel datasets, in this paper we propose a novel cross-lingual pre-training approach via multi-task learning (MTL) for both zeroshot AMR parsing and AMR-to-text generation.","offset":1,"pro":0.125,"labels":"PUR"},{"idx":12,"sentence":"Specifically, we consider three types of relevant tasks, including AMR parsing, AMR-to-text generation, and machine translation.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":12,"sentence":"We hope that knowledge gained while learning for English AMR parsing and text generation can be transferred to the counterparts of other languages.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":12,"sentence":"With properly pretrained models, we explore four different finetuning methods, i.e., vanilla fine-tuning with a single task, one-for-all MTL fine-tuning, targeted MTL fine-tuning, and teacher-studentbased MTL fine-tuning.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":12,"sentence":"Experimental results on AMR parsing and text generation of multiple non-English languages demonstrate that our approach significantly outperforms a strong baseline of pre-training approach, and greatly advances the state of the art.","offset":5,"pro":0.625,"labels":"CLN"},{"idx":12,"sentence":"In detail, on LDC2020T07 we have achieved 70.45%, 71.76%, and 70.80% in Smatch F1 for AMR parsing of German, Spanish, and Italian, respectively, while for AMR-to-text generation of the languages, we have obtained 25.69, 31.36, and 28.42 in BLEU respectively.","offset":6,"pro":0.75,"labels":"RST"},{"idx":12,"sentence":"We make our code available on github https://github.com/xdqkid/XLPT-AMR.","offset":7,"pro":0.875,"labels":"CTN"},{"idx":13,"sentence":"Despite the success of sequence-to-sequence (seq2seq) models in semantic parsing, recent work has shown that they fail in compositional generalization, i.e., the ability to generalize to new structures built of components observed during training.","offset":0,"pro":0,"labels":"GAP"},{"idx":13,"sentence":"In this work, we posit that a span-based parser should lead to better compositional generalization. we propose SpanBasedSP, a parser that predicts a span tree over an input utterance, explicitly encoding how partial programs compose over spans in the input.","offset":1,"pro":0.25,"labels":"PUR"},{"idx":13,"sentence":"SpanBasedSP extends Pasupat et al.(2019) to be comparable to seq2seq models by (i) training from programs, without access to gold trees, treating trees as latent variables, (ii) parsing a class of non-projective trees through an extension to standard CKY.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":13,"sentence":"On GeoQuery, SCAN and CLOSURE datasets, SpanBasedSP performs similarly to strong seq2seq baselines on random splits, but dramatically improves performance compared to baselines on splits that require compositional generalization: from 61.0 → 88.9 average accuracy.","offset":3,"pro":0.75,"labels":"RST"},{"idx":14,"sentence":"Sequence-to-sequence models excel at handling natural language variation, but have been shown to struggle with out-of-distribution compositional generalization.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"This has motivated new specialized architectures with stronger compositional biases, but most of these approaches have only been evaluated on synthetically-generated datasets, which are not representative of natural language variation.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":14,"sentence":"In this work we ask: can we develop a semantic parsing approach that handles both natural language variation and compositional generalization?","offset":2,"pro":0.25,"labels":"PUR"},{"idx":14,"sentence":"To better assess this capability, we propose new train and test splits of non-synthetic datasets.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":14,"sentence":"We demonstrate that strong existing approaches do not perform well across a broad set of evaluations.","offset":4,"pro":0.5,"labels":"CLN"},{"idx":14,"sentence":"We also propose NQG-T5, a hybrid model that combines a high-precision grammar-based approach with a pre-trained sequence-to-sequence model.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":14,"sentence":"It outperforms existing approaches across several compositional generalization challenges on non-synthetic data, while also being competitive with the state-of-the-art on standard evaluations.","offset":6,"pro":0.75,"labels":"RST"},{"idx":14,"sentence":"While still far from solving this problem, our study highlights the importance of diverse evaluations and the open challenge of handling both compositional generalization and natural language variation in semantic parsing.","offset":7,"pro":0.875,"labels":"IMP"},{"idx":15,"sentence":"We present a targeted, scaled-up comparison of incremental processing in humans and neural language models by collecting by-word reaction time data for sixteen different syntactic test suites across a range of structural phenomena.","offset":0,"pro":0,"labels":"PUR"},{"idx":15,"sentence":"Human reaction time data comes from a novel online experimental paradigm called the Interpolated Maze task.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":15,"sentence":"We compare human reaction times to by-word probabilities for four contemporary language models, with different architectures and trained on a range of data set sizes.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":15,"sentence":"We find that across many phenomena, both humans and language models show increased processing difficulty in ungrammatical sentence regions with human and model accuracy' scores a la Marvin and Linzen (2018) about equal.","offset":3,"pro":0.42857142857142855,"labels":"RST"},{"idx":15,"sentence":"However, although language model outputs match humans in direction, we show that models systematically under-predict the difference in magnitude of incremental processing difficulty between grammatical and ungrammatical sentences.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":15,"sentence":"Specifically, when models encounter syntactic violations they fail to accurately predict the longer reading times observed in the human data.","offset":5,"pro":0.7142857142857143,"labels":"GAP"},{"idx":15,"sentence":"These results call into question whether contemporary language models are approaching human-like performance for sensitivity to syntactic violations.","offset":6,"pro":0.8571428571428571,"labels":"GAP"},{"idx":16,"sentence":"Modality is the linguistic ability to describe vents with added information such as how desirable, plausible, or feasible they are.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"Modality is important for many NLP downstream tasks such as the detection of hedging, uncertainty, speculation, and more.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":16,"sentence":"Previous studies that address modality detection in NLP often restrict modal expressions to a closed syntactic class, and the modal sense labels are vastly different across different studies, lacking an accepted standard.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":16,"sentence":"Furthermore, these senses are often analyzed independently of the events that they modify.","offset":3,"pro":0.42857142857142855,"labels":"BAC"},{"idx":16,"sentence":"This work builds on the theoretical foundations of the Georgetown Gradable Modal Expressions (GME) work by Rubinstein et al.(2013) to propose an event-based modality detection task where modal expressions can be words of any syntactic class and sense labels are drawn from a comprehensive taxonomy which harmonizes the modal concepts contributed by the different studies.","offset":4,"pro":0.5714285714285714,"labels":"PUR"},{"idx":16,"sentence":"We present experiments on the GME corpus aiming to detect and classify fine-grained modal concepts and associate them with their modified events.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":16,"sentence":"We show that detecting and classifying modal expressions is not only feasible, it also improves the detection of modal events in their own right.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":17,"sentence":"Part-of-Speech (POS) tags are routinely included as features in many NLP tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"However, the importance and usefulness of POS tags needs to be examined as NLP expands to low-resource languages because linguists who provide many annotated resources do not place priority on early identification and tagging of POS.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":17,"sentence":"This paper describes an empirical study about the effect that POS tags have on two computational morphological tasks with the Transformer architecture.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":17,"sentence":"Each task is tested twice on identical data except for the presence/absence of POS tags, using published data in ten high- to low-resource languages or unpublished linguistic field data in five low-resource languages.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":17,"sentence":"We find that the presence or absence of POS tags does not have a significant bearing on performance.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":17,"sentence":"In joint segmentation and glossing, the largest average difference is an .09 improvement in F1-scores by removing POS tags.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":17,"sentence":"In reinflection, the greatest average difference is 1.2% in accuracy for published data and 5% for unpublished and noisy field data.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":18,"sentence":"Parsing spoken dialogue poses unique difficulties, including disfluencies and unmarked boundaries between sentence-like units.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"Previous work has shown that prosody can help with parsing disfluent speech (Tran et al.2018), but has assumed that the input to the parser is already segmented into sentence-like units (SUs), which isn't true in existing speech applications.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":18,"sentence":"We investigate how prosody affects a parser that receives an entire dialogue turn as input (a turn-based model), instead of gold standard pre-segmented SUs (an SU-based model).","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":18,"sentence":"In experiments on the English Switchboard corpus, we find that when using transcripts alone, the turn-based model has trouble segmenting SUs, leading to worse parse performance than the SU-based model.","offset":3,"pro":0.5,"labels":"RST"},{"idx":18,"sentence":"However, prosody can effectively replace gold standard SU boundaries: with prosody, the turn-based model performs as well as the SU-based model (91.38 vs. 91.06 F1 score, respectively), despite performing two tasks (SU segmentation and parsing) rather than one (parsing alone).","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":18,"sentence":"Analysis shows that pitch and intensity features are the most important for this corpus, since they allow the model to correctly distinguish an SU boundary from a speech disfluency - a distinction that the model otherwise struggles to make.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":19,"sentence":"We introduce VoxPopuli, a large-scale multilingual corpus providing 400K hours of unlabeled speech data in 23 languages.","offset":0,"pro":0,"labels":"PUR"},{"idx":19,"sentence":"It is the largest open data to date for unsupervised representation learning as well as semi-supervised learning.","offset":1,"pro":0.2,"labels":"CTN"},{"idx":19,"sentence":"VoxPopuli also contains 1.8K hours of transcribed speeches in 15 languages and their aligned oral interpretations into 15 target languages totaling 17.3K hours.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":19,"sentence":"We provide speech recognition (ASR) baselines and validate the versatility of VoxPopuli unlabeled data in semi-supervised ASR and speech-to-text translation under challenging out-of-domain settings.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":19,"sentence":"The corpus is available at https://github.com/facebookresearch/voxpopuli.","offset":4,"pro":0.8,"labels":"CTN"},{"idx":0,"sentence":"Cafocused navigation services are based on turns and distances of named streets, whereas navigation instructions naturally used by humans are centered around physical objects called landmarks.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"We present a neural model that takes OpenStreetMap representations as input and learns to generate navigation instructions that contain visible and salient landmarks from human natural language instructions.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":0,"sentence":"Routes on the map are encoded in a location- and rotation-invariant graph representation that is decoded into natural language instructions.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":0,"sentence":"Our work is based on a novel dataset of 7,672 crowd-sourced instances that have been verified by human navigation in Street View.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":0,"sentence":"Our evaluation shows that the navigation instructions generated by our system have similar properties as human-generated instructions, and lead to successful human navigation in Street View.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":1,"sentence":"Vision-language pre-training (VLP) on large-scale image-text pairs has achieved huge success for the cross-modal downstream tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"The most existing pre-training methods mainly adopt a two-step training procedure, which firstly employs a pre-trained object detector to extract region-based visual features, then concatenates the image representation and text embedding as the input of Transformer to train.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":1,"sentence":"However, these methods face problems of using task-specific visual representation of the specific object detector for generic cross-modal understanding, and the computation inefficiency of two-stage pipeline.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":1,"sentence":"In this paper, we propose the first end-to-end vision-language pre-trained model for both V+L understanding and generation, namely E2E-VLP, where we build a unified Transformer framework to jointly learn visual representation, and semantic alignments between image and text.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":1,"sentence":"We incorporate the tasks of object detection and image captioning into pre-training with a unified Transformer encoder-decoder architecture for enhancing visual learning.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":1,"sentence":"An extensive set of experiments have been conducted on well-established vision-language downstream tasks to demonstrate the effectiveness of this novel VLP paradigm.","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":2,"sentence":"Despite the achievements of large-scale multimodal pre-training approaches, cross-modal retrieval, e.g., image-text retrieval, remains a challenging task.","offset":0,"pro":0,"labels":"GAP"},{"idx":2,"sentence":"To bridge the semantic gap between the two modalities, previous studies mainly focus on word-region alignment at the object level, lacking the matching between the linguistic relation among the words and the visual relation among the regions.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":2,"sentence":"The neglect of such relation consistency impairs the contextualized representation of image-text pairs and hinders the model performance and the interpretability.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":2,"sentence":"In this paper, we first propose a novel metric, Intra-modal Self-attention Distance (ISD), to quantify the relation consistency by measuring the semantic distance between linguistic and visual relations.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":2,"sentence":"In response, we present Inter-modal Alignment on Intra-modal Self-attentions (IAIS), a regularized training method to optimize the ISD and calibrate intra-modal self-attentions from the two modalities mutually via inter-modal alignment.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":2,"sentence":"The IAIS regularizer boosts the performance of prevailing models on Flickr30k and MS COCO datasets by a considerable margin, which demonstrates the superiority of our approach.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":3,"sentence":"We present Knowledge Enhanced Multimodal BART (KM-BART), which is a Transformer-based sequence-to-sequence model capable of reasoning about commonsense knowledge from multimodal inputs of images and texts.","offset":0,"pro":0,"labels":"PUR"},{"idx":3,"sentence":"We adapt the generative BART architecture (Lewis et al., 2020) to a multimodal model with visual and textual inputs.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":3,"sentence":"We further develop novel pretraining tasks to improve the model performance on the Visual Commonsense Generation (VCG) task.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":3,"sentence":"In particular, our pretraining task of Knowledge-based Commonsense Generation (KCG) boosts model performance on the VCG task by leveraging commonsense knowledge from a large language model pretrained on external commonsense knowledge graphs.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":3,"sentence":"To the best of our knowledge, we are the first to propose a dedicated task for improving model performance on the VCG task.","offset":4,"pro":0.6666666666666666,"labels":"CTN"},{"idx":3,"sentence":"Experimental results show that our model reaches state-of-the-art performance on the VCG task (Park et al., 2020) by applying these novel pretraining tasks.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":4,"sentence":"Transformers have advanced the field of natural language processing (NLP) on a variety of important tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"At the cornerstone of the Transformer architecture is the multi-head attention (MHA) mechanism which models pairwise interactions between the elements of the sequence.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":4,"sentence":"Despite its massive success, the current framework ignores interactions among different heads, leading to the problem that many of the heads are redundant in practice, which greatly wastes the capacity of the model.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":4,"sentence":"To improve parameter efficiency, we re-formulate the MHA as a latent variable model from a probabilistic perspective.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":4,"sentence":"We present cascaded head-colliding attention (CODA) which explicitly models the interactions between attention heads through a hierarchical variational distribution.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":4,"sentence":"We conduct extensive experiments and demonstrate that CODA outperforms the transformer baseline, by 0.6 perplexity on Wikitext-103 in language modeling, and by 0.6 BLEU on WMT14 EN-DE in machine translation, due to its improvements on the parameter efficiency.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":5,"sentence":"Knowledge distillation is a critical technique to transfer knowledge between models, typically from a large model (the teacher) to a more fine-grained one (the student).","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"The objective function of knowledge distillation is typically the cross-entropy between the teacher and the student's output distributions.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":5,"sentence":"However, for structured prediction problems, the output space is exponential in size; therefore, the cross-entropy objective becomes intractable to compute and optimize directly.","offset":2,"pro":0.4,"labels":"GAP"},{"idx":5,"sentence":"In this paper, we derive a factorized form of the knowledge distillation objective for structured prediction, which is tractable for many typical choices of the teacher and student models.","offset":3,"pro":0.6,"labels":"PUR"},{"idx":5,"sentence":"In particular, we show the tractability and empirical effectiveness of structural knowledge distillation between sequence labeling and dependency parsing models under four different scenarios: 1) the teacher and student share the same factorization form of the output structure scoring function; 2) the student factorization produces more fine-grained substructures than the teacher factorization; 3) the teacher factorization produces more fine-grained substructures than the student factorization; 4) the factorization forms from the teacher and the student are incompatible.","offset":4,"pro":0.8,"labels":"MTD"},{"idx":6,"sentence":"State-of-the-art parameter-efficient fine-tuning methods rely on introducing adapter modules between the layers of a pretrained language model.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"However, such modules are trained separately for each task and thus do not enable sharing information across tasks.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":6,"sentence":"In this paper, we show that we can learn adapter parameters for all layers and tasks by generating them using shared hypernetworks, which condition on task, adapter position, and layer id in a transformer model.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":6,"sentence":"This parameter-efficient multi-task learning framework allows us to achieve the best of both worlds by sharing knowledge across tasks via hypernetworks while enabling the model to adapt to each individual task through task-specific adapters.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":6,"sentence":"Experiments on the well-known GLUE benchmark show improved performance in multi-task learning while adding only 0.29% parameters per task.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":6,"sentence":"We additionally demonstrate substantial performance improvements in few-shot domain generalization across a variety of tasks.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":6,"sentence":"Our code is publicly available in https://github.com/rabeehk/hyperformer.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":7,"sentence":"Pre-trained multilingual language models, e.g., multilingual-BERT, are widely used in cross-lingual tasks, yielding the state-of-the-art performance.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"However, such models suffer from a large performance gap between source and target languages, especially in the zero-shot setting, where the models are fine-tuned only on English but tested on other languages for the same task.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":7,"sentence":"We tackle this issue by incorporating language-agnostic information, specifically, universal syntax such as dependency relations and POS tags, into language models, based on the observation that universal syntax is transferable across different languages.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":7,"sentence":"Our approach, called COunterfactual SYntax (COSY), includes the design of SYntax-aware networks as well as a COunterfactual training method to implicitly force the networks to learn not only the semantics but also the syntax.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":7,"sentence":"To evaluate COSY, we conduct cross-lingual experiments on natural language inference and question answering using mBERT and XLM-R as network backbones.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":7,"sentence":"Our results show that COSY achieves the state-of-the-art performance for both tasks, without using auxiliary training data.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":8,"sentence":"Recent studies on neural networks with pre-trained weights (i.e., BERT) have mainly focused on a low-dimensional subspace, where the embedding vectors computed from input words (or their contexts) are located.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"In this work, we propose a new approach, called OoMMix, to finding and regularizing the remainder of the space, referred to as out-of-manifold, which cannot be accessed through the words.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":8,"sentence":"Specifically, we synthesize the out-of-manifold embeddings based on two embeddings obtained from actually-observed words, to utilize them for fine-tuning the network.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":8,"sentence":"A discriminator is trained to detect whether an input embedding is located inside the manifold or not, and simultaneously, a generator is optimized to produce new embeddings that can be easily identified as out-of-manifold by the discriminator.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":8,"sentence":"These two modules successfully collaborate in a unified and end-to-end manner for regularizing the out-of-manifold.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":8,"sentence":"Our extensive evaluation on various text classification benchmarks demonstrates the effectiveness of our approach, as well as its good compatibility with existing data augmentation techniques which aim to enhance the manifold.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":9,"sentence":"Stereotypical language expresses widely-held beliefs about different social categories.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"Many stereotypes are overtly negative, while others may appear positive on the surface, but still lead to negative consequences.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":9,"sentence":"In this work, we present a computational approach to interpreting stereotypes in text through the Stereotype Content Model (SCM), a comprehensive causal theory from social psychology.","offset":2,"pro":0.2222222222222222,"labels":"PUR"},{"idx":9,"sentence":"The SCM proposes that stereotypes can be understood along two primary dimensions: warmth and competence.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":9,"sentence":"We present a method for defining warmth and competence axes in semantic embedding space, and show that the four quadrants defined by this subspace accurately represent the warmth and competence concepts, according to annotated lexicons.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":9,"sentence":"We then apply our computational SCM model to textual stereotype data and show that it compares favourably with survey-based studies in the psychological literature.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":9,"sentence":"Furthermore, we explore various strategies to counter stereotypical beliefs with anti-stereotypes.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":9,"sentence":"It is known that countering stereotypes with anti-stereotypical examples is one of the most effective ways to reduce biased thinking, yet the problem of generating anti-stereotypes has not been previously studied.","offset":7,"pro":0.7777777777777778,"labels":"BAC"},{"idx":9,"sentence":"Thus, a better understanding of how to generate realistic and effective anti-stereotypes can contribute to addressing pressing societal concerns of stereotyping, prejudice, and discrimination.","offset":8,"pro":0.8888888888888888,"labels":"CTN"},{"idx":10,"sentence":"Misinformation has recently become a well-documented matter of public concern.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"Existing studies on this topic have hitherto adopted a coarse concept of misinformation, which incorporates a broad spectrum of story types ranging from political conspiracies to misinterpreted pranks.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":10,"sentence":"This paper aims to structurize these misinformation stories by leveraging fact-check articles.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":10,"sentence":"Our intuition is that key phrases in a fact-check article that identify the misinformation type(s) (e.g., doctored images, urban legends) also act as rationales that determine the verdict of the fact-check (e.g., false).","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":10,"sentence":"We experiment on rationalized models with domain knowledge as weak supervision to extract these phrases as rationales, and then cluster semantically similar rationales to summarize prevalent misinformation types.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":10,"sentence":"Using archived fact-checks from Snopes/com, we identify ten types of misinformation stories.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":10,"sentence":"We discuss how these types have evolved over the last ten years and compare their prevalence between the 2016/2020 US presidential elections and the H1N1/COVID-19 pandemics.","offset":6,"pro":0.8571428571428571,"labels":"MTD"},{"idx":11,"sentence":"While there is an abundance of advice to podcast creators on how to speak in ways that engage their listeners, there has been little data-driven analysis of podcasts that relates linguistic style with engagement.","offset":0,"pro":0,"labels":"GAP"},{"idx":11,"sentence":"In this paper, we investigate how various factors - vocabulary diversity, distinctiveness, emotion, and syntax, among others - correlate with engagement, based on analysis of the creators' written descriptions and transcripts of the audio.","offset":1,"pro":0.25,"labels":"PUR"},{"idx":11,"sentence":"Our analysis tests popular wisdom about stylistic elements in high-engagement podcasts, corroborating some pieces of advice and adding new perspectives on others.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":11,"sentence":"We build models with different textual representations, ","offset":3,"pro":0.75,"labels":"MTD"},{"idx":12,"sentence":"People debate on a variety of topics on online platforms such as Reddit, or Facebook.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"Debates can be lengthy, with users exchanging a wealth of information and opinions.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":12,"sentence":"However, conversations do not always go smoothly, and users sometimes engage in unsound argumentation techniques to prove a claim.","offset":2,"pro":0.2,"labels":"GAP"},{"idx":12,"sentence":"These techniques are called fallacies.","offset":3,"pro":0.3,"labels":"BAC"},{"idx":12,"sentence":"Fallacies are persuasive arguments that provide insufficient or incorrect evidence to support the claim.","offset":4,"pro":0.4,"labels":"BAC"},{"idx":12,"sentence":"In this paper, we study the most frequent fallacies on Reddit, and we present them using the pragma-dialectical theory of argumentation.","offset":5,"pro":0.5,"labels":"PUR"},{"idx":12,"sentence":"We construct a new annotated dataset of fallacies, using user comments containing fallacy mentions as noisy labels, and cleaning the data via crowdsourcing.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":12,"sentence":"Finally, we study the task of classifying fallacies using neural models.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":12,"sentence":"We find that generally the models perform better in the presence of conversational context.","offset":8,"pro":0.8,"labels":"CLN"},{"idx":12,"sentence":"We have released the data and the code at github.com/sahaisaumya/informal{_fallacies.","offset":9,"pro":0.9,"labels":"CTN"},{"idx":13,"sentence":"Inferring social relations from dialogues is vital for building emotionally intelligent robots to interpret human language better and act accordingly.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"We model the social network as an And-or Graph, named SocAoG, for the consistency of relations among a group and leveraging attributes as inference cues.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":13,"sentence":"Moreover, we formulate a sequential structure prediction task, and propose an alpha-beta-gamma strategy to incrementally parse SocAoG for the dynamic inference upon any incoming utterance: (i) an alpha process predicting attributes and relations conditioned on the semantics of dialogues, (ii) a beta process updating the social relations based on related attributes, and (iii) a gamma process updating individual's attributes based on interpersonal social relations.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":13,"sentence":"Empirical results on DialogRE and MovieGraph show that our model infers social relations more accurately than the state-of-the-art methods.","offset":3,"pro":0.6,"labels":"CLN"},{"idx":13,"sentence":"Moreover, the ablation study shows the three processes complement each other, and the case study demonstrates the dynamic relational inference.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":14,"sentence":"We present a data-driven, end-to-end approach to transaction-based dialog systems that performs at near-human levels in terms of verbal response quality and factual grounding accuracy.","offset":0,"pro":0,"labels":"PUR"},{"idx":14,"sentence":"We show that two essential components of the system produce these results: a sufficiently large and diverse, in-domain labeled dataset, and a neural network-based, pre-trained model that generates both verbal responses and API call predictions.","offset":1,"pro":0.08333333333333333,"labels":"RST"},{"idx":14,"sentence":"In terms of data, we introduce TicketTalk, a movie ticketing dialog dataset with 23,789 annotated conversations.","offset":2,"pro":0.16666666666666666,"labels":"MTD"},{"idx":14,"sentence":"The conversations range from completely open-ended and unrestricted to more structured, both in terms of their knowledge base, discourse features, and number of turns.","offset":3,"pro":0.25,"labels":"MTD"},{"idx":14,"sentence":"In qualitative human evaluations, model-generated responses trained on just 10,000 TicketTalk dialogs were rated to make sense 86.5% of the time, almost the same as human responses in the same contexts.","offset":4,"pro":0.3333333333333333,"labels":"RST"},{"idx":14,"sentence":"Our simple, API-focused annotation schema results in a much easier labeling task making it faster and more cost effective.","offset":5,"pro":0.4166666666666667,"labels":"RST"},{"idx":14,"sentence":"It is also the key component for being able to predict API calls accurately.","offset":6,"pro":0.5,"labels":"MTD"},{"idx":14,"sentence":"We handle factual grounding by incorporating API calls in the training data, allowing our model to learn which actions to take and when.","offset":7,"pro":0.5833333333333334,"labels":"MTD"},{"idx":14,"sentence":"Trained on the same 10,000-dialog set, the model's API call predictions were rated to be correct 93.9% of the time in our evaluations, surpassing the ratings for the corresponding human labels.","offset":8,"pro":0.6666666666666666,"labels":"RST"},{"idx":14,"sentence":"We show how API prediction and response generation scores improve as the dataset size incrementally increases from 5000 to 21,000 dialogs.","offset":9,"pro":0.75,"labels":"CLN"},{"idx":14,"sentence":"Our analysis also clearly illustrates the benefits of pre-training.","offset":10,"pro":0.8333333333333334,"labels":"CLN"},{"idx":14,"sentence":"To facilitate future work on transaction-based dialog systems, we are publicly releasing the TicketTalk dataset at https://git.io/JL8an.","offset":11,"pro":0.9166666666666666,"labels":"CTN"},{"idx":15,"sentence":"In this paper, we explore the ability to model and infer personality types of opponents, predict their responses, and use this information to adapt a dialog agent's high-level strategy in negotiation tasks.","offset":0,"pro":0,"labels":"PUR"},{"idx":15,"sentence":"Inspired by the idea of incorporating a theory of mind (ToM) into machines, we introduce a probabilistic formulation to encapsulate the opponent's personality type during both learning and inference.","offset":1,"pro":0.25,"labels":"MTD"},{"idx":15,"sentence":"We test our approach on the CraigslistBargain dataset (He et al. 2018) and show that our method using ToM inference achieves a 20% higher dialog agreement rate compared to baselines on a mixed population of opponents.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":15,"sentence":"We also demonstrate that our model displays diverse negotiation behavior with different types of opponents.","offset":3,"pro":0.75,"labels":"CLN"},{"idx":16,"sentence":"In this paper, we propose Inverse Adversarial Training (IAT) algorithm for training neural dialogue systems to avoid generic responses and model dialogue history better.","offset":0,"pro":0,"labels":"PUR"},{"idx":16,"sentence":"In contrast to standard adversarial training algorithms, IAT encourages the model to be sensitive to the perturbation in the dialogue history and therefore learning from perturbations.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":16,"sentence":"By giving higher rewards for responses whose output probability reduces more significantly when dialogue history is perturbed, the model is encouraged to generate more diverse and consistent responses.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":16,"sentence":"By penalizing the model when generating the same response given perturbed dialogue history, the model is forced to better capture dialogue history and generate more informative responses.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":16,"sentence":"Experimental results on two benchmark datasets show that our approach can better model dialogue history and generate more diverse and consistent responses.","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":16,"sentence":"In addition, we point out a problem of the widely used maximum mutual information (MMI) based methods for improving the diversity of dialogue response generation models and demonstrate it empirically.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":17,"sentence":"Knowledge-grounded dialogue systems are intended to convey information that is based on evidence provided in a given source text.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"We discuss the challenges of training a generative neural dialogue model for such systems that is controlled to stay faithful to the evidence.","offset":1,"pro":0.125,"labels":"PUR"},{"idx":17,"sentence":"Existing datasets contain a mix of conversational responses that are faithful to selected evidence as well as more subjective or chit-chat style responses.","offset":2,"pro":0.25,"labels":"BAC"},{"idx":17,"sentence":"We propose different evaluation measures to disentangle these different styles of responses by quantifying the informativeness and objectivity.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":17,"sentence":"At training time, additional inputs based on these evaluation measures are given to the dialogue model.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":17,"sentence":"At generation time, these additional inputs act as stylistic controls that encourage the model to generate responses that are faithful to the provided evidence.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":17,"sentence":"We also investigate the usage of additional controls at decoding time using resampling techniques.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":17,"sentence":"In addition to automatic metrics, we perform a human evaluation study where raters judge the output of these controlled generation models to be generally more objective and faithful to the evidence compared to baseline dialogue systems.","offset":7,"pro":0.875,"labels":"MTD"},{"idx":18,"sentence":"Automatically extracting key information from scientific documents has the potential to help scientists work more efficiently and accelerate the pace of scientific progress.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"Prior work has considered extracting document-level entity clusters and relations end-to-end from raw scientific text, which can improve literature search and help identify methods and materials for a given problem.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":18,"sentence":"Despite the importance of this task, most existing works on scientific information extraction (SciIE) consider extraction solely based on the content of an individual paper, without considering the paper's place in the broader literature.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":18,"sentence":"In contrast to prior work, we augment our text representations by leveraging a complementary source of document context: the citation graph of referential links between citing and cited papers.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":18,"sentence":"On a test set of English-language scientific documents, we show that simple ways of utilizing the structure and content of the citation graph can each lead to significant gains in different scientific information extraction tasks.","offset":4,"pro":0.5714285714285714,"labels":"CLN"},{"idx":18,"sentence":"When these tasks are combined, we observe a sizable improvement in end-to-end information extraction over the state-of-the-art, suggesting the potential for future work along this direction.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":18,"sentence":"We release software tools to facilitate citation-aware SciIE development.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":19,"sentence":"Current event-centric knowledge graphs highly rely on explicit connectives to mine relations between events.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"Unfortunately, due to the sparsity of connectives, these methods severely undermine the coverage of EventKGs.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":19,"sentence":"The lack of high-quality labelled corpora further exacerbates that problem.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":19,"sentence":"In this paper, we propose a knowledge projection paradigm for event relation extraction: projecting discourse knowledge to narratives by exploiting the commonalities between them.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":19,"sentence":"Specifically, we propose Multi-tier Knowledge Projection Network (MKPNet), which can leverage multi-tier discourse knowledge effectively for event relation extraction.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":19,"sentence":"In this way, the labelled data requirement is significantly reduced, and implicit event relations can be effectively extracted.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":19,"sentence":"Intrinsic experimental results show that MKPNet achieves the new state-of-the-art performance and extrinsic experimental results verify the value of the extracted event relations.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":0,"sentence":"Multilingual neural machine translation has shown the capability of directly translating between language pairs unseen in training, i.e. zero-shot translation.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"Despite being conceptually attractive, it often suffers from low output quality.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":0,"sentence":"The difficulty of generalizing to new translation directions suggests the model representations are highly specific to those language pairs seen in training.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":0,"sentence":"With this modification, we gain up to 18.5 BLEU points on zero-shot translation while retaining quality on supervised directions.","offset":3,"pro":0.375,"labels":"RST"},{"idx":0,"sentence":"The improvements are particularly prominent between related languages, where our proposed model outperforms pivot-based translation.","offset":4,"pro":0.5,"labels":"CLN"},{"idx":0,"sentence":"By thorough inspections of the hidden layer outputs, we show that our approach indeed leads to more language-independent representations.","offset":5,"pro":0.625,"labels":"CLN"},{"idx":0,"sentence":"We demonstrate that a main factor causing the language-specific representations is the positional correspondence to input tokens. ","offset":6,"pro":0.75,"labels":"PUR"},{"idx":0,"sentence":"Moreover, our approach allows easy integration of new languages, which substantially expands translation coverage.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":1,"sentence":"Commonsense reasoning research has so far been limited to English.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"We aim to evaluate and improve popular multilingual language models (ML-LMs) to help advance commonsense reasoning (CSR) beyond English.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":1,"sentence":"We collect the Mickey corpus, consisting of 561k sentences in 11 different languages, which can be used for analyzing and improving ML-LMs.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":1,"sentence":"We propose Mickey Probe, a language-general probing task for fairly evaluating the common sense of popular ML-LMs across different languages.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":1,"sentence":"In addition, we also create two new datasets, X-CSQA and X-CODAH, by translating their English versions to 14 other languages, so that we can evaluate popular ML-LMs for cross-lingual commonsense reasoning.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":1,"sentence":"It significantly enhances sentence representations, yielding a large performance gain on both benchmarks (e.g., +2.7% accuracy for X-CSQA over XLM-R{_L).","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":1,"sentence":"To improve the performance beyond English, we propose a simple yet effective method - multilingual contrastive pretraining (MCP).","offset":6,"pro":0.8571428571428571,"labels":"MTD"},{"idx":2,"sentence":"Attention mechanisms have achieved substantial improvements in neural machine translation by dynamically selecting relevant inputs for different predictions.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"However, recent studies have questioned the attention mechanisms' capability for discovering decisive inputs.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":2,"sentence":"In this paper, we propose to calibrate the attention weights by introducing a mask perturbation model that automatically evaluates each input's contribution to the model outputs.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":2,"sentence":"We increase the attention weights assigned to the indispensable tokens, whose removal leads to a dramatic performance decrease.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":2,"sentence":"The extensive experiments on the Transformer-based translation have demonstrated the effectiveness of our model.","offset":4,"pro":0.5714285714285714,"labels":"CLN"},{"idx":2,"sentence":"Detailed analyses also show a great need for calibration in the attention weights with high entropy where the model is unconfident about its decision.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":2,"sentence":"We further find that the calibrated attention weights are more uniform at lower layers to collect multiple information while more concentrated on the specific inputs at higher layers.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":3,"sentence":"We propose a new architecture for adapting a sentence-level sequence-to-sequence transformer by incorporating multiple pre-trained document context signals and assess the impact on translation performance of (1) different pretraining approaches for generating these signals, (2) the quantity of parallel data for which document context is available, and (3) conditioning on source, target, or source and target contexts.","offset":0,"pro":0,"labels":"PUR"},{"idx":3,"sentence":"Experiments on the NIST Chinese-English, and IWSLT and WMT English-German tasks support four general conclusions: that using pre-trained context representations markedly improves sample efficiency, that adequate parallel data resources are crucial for learning to use document context, that jointly conditioning on multiple context representations outperforms any single representation, and that source context is more valuable for translation performance than target side context.","offset":1,"pro":0.5,"labels":"CLN"},{"idx":4,"sentence":"Recent research in multilingual language models (LM) has demonstrated their ability to effectively handle multiple languages in a single model.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"This holds promise for low web-resource languages (LRL) as multilingual models can enable transfer of supervision from high resource languages to LRLs.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":4,"sentence":"However, incorporating a new language in an LM still remains a challenge, particularly for languages with limited corpora and in unseen scripts.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":4,"sentence":"In this paper we argue that relatedness among languages in a language family may be exploited to overcome some of the corpora limitations of LRLs, and propose RelateLM.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":4,"sentence":"We focus on Indian languages, and exploit relatedness along two dimensions: (1) script (since many Indic scripts originated from the Brahmic script), and (2) sentence structure.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":4,"sentence":"RelateLM uses transliteration to convert the unseen script of limited LRL text into the script of a Related Prominent Language (RPL) (Hindi in our case).","offset":5,"pro":0.625,"labels":"MTD"},{"idx":4,"sentence":"While exploiting similar sentence structures, RelateLM utilizes readily available bilingual dictionaries to pseudo translate RPL text into LRL corpora.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":4,"sentence":"Experiments on multiple real-world benchmark datasets provide validation to our hypothesis that using a related language as pivot, along with transliteration and pseudo translation based data augmentation, can be an effective way to adapt LMs for LRLs, rather than direct training or pivoting through English.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":5,"sentence":"The connection between the maximum spanning tree in a directed graph and the best dependency tree of a sentence has been exploited by the NLP community.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"While work has been done to efficiently solve this problem for finding the one-best dependency tree, no research has attempted to extend this solution to finding the K-best dependency trees.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":5,"sentence":"In this paper, we provide a simplification of the K-best spanning tree algorithm of Camerini et al.(1980).","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":5,"sentence":"Our simplification allows us to obtain a constant time speed-up over the original algorithm.","offset":3,"pro":0.5,"labels":"RST"},{"idx":5,"sentence":"Furthermore, we present a novel extension of the algorithm for decoding the K-best dependency trees of a graph which are subject to a root constraint.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":5,"sentence":"This is arguably a more important extension as a larger proportion of decoded trees will not be subject to the root constraint of dependency trees.","offset":5,"pro":0.8333333333333334,"labels":"GAP"},{"idx":6,"sentence":"This survey builds an interdisciplinary picture of Argument Mining (AM), with a strong focus on its potential to address issues related to Social and Political Science.","offset":0,"pro":0,"labels":"PUR"},{"idx":6,"sentence":"Under our definition, the quality of a contribution needs to be assessed at multiple levels: the contribution itself, its preceding context, and the consequential effect on the development of the upcoming discourse.","offset":1,"pro":0.25,"labels":"MTD"},{"idx":6,"sentence":"We finally define an application of AM for Social Good: (semi-)automatic moderation, a highly integrative application which (a) represents a challenging testbed for the integrated notion of quality we advocate, (b) allows the empirical quantification of argument/deliberative quality to benefit from the developments in other NLP fields (i.e. hate speech detection, fact checking, debiasing), and (c) has a clearly beneficial potential at the level of its societal thanks to its real-world application (even if extremely ambitious).","offset":2,"pro":0.5,"labels":"MTD"},{"idx":6,"sentence":"We propose a novel definition of argument quality which is integrated with that of deliberative quality from the Social Science literature.","offset":3,"pro":0.75,"labels":"MTD"},{"idx":7,"sentence":"Research on the application of NLP in symbol-based Augmentative and Alternative Communication (AAC) tools for improving social interaction support is scarce.","offset":0,"pro":0,"labels":"GAP"},{"idx":7,"sentence":"We contribute a novel method for generating context-related vocabulary from photographs of personally relevant events aimed at supporting people with language impairments in retelling their past experiences.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":7,"sentence":"Performance was calculated with information retrieval concepts on the relevance of vocabulary generated for communicating a corpus of 9730 narrative phrases about events depicted in 1946 photographs.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":7,"sentence":"In comparison to a baseline generation composed of frequent English words, our method generated vocabulary with a 4.6 gain in mean average precision, regardless of the level of contextual information in the input photographs, and 6.9 for photographs in which contextual information was extracted correctly.","offset":3,"pro":0.6,"labels":"RST"},{"idx":7,"sentence":"We conclude by discussing how our findings provide insights for system optimization and usage.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":8,"sentence":"Continuity of care is crucial to ensuring positive health outcomes for patients discharged from an inpatient hospital setting, and improved information sharing can help.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"In this work, we describe our creation of a dataset of clinical action items annotated over MIMIC-III, the largest publicly available dataset of real clinical notes.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":8,"sentence":"This dataset, which we call CLIP, is annotated by physicians and covers 718 documents representing 100K sentences.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":8,"sentence":"We describe the task of extracting the action items from these documents as multi-aspect extractive summarization, with each aspect representing a type of action to be taken.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":8,"sentence":"We evaluate several machine learning models on this task, and show that the best models exploit in-domain language model pre-training on 59K unannotated documents, and incorporate context from neighboring sentences.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":8,"sentence":"We also propose an approach to pre-training data selection that allows us to explore the trade-off between size and domain-specificity of pre-training datasets for this task.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":8,"sentence":"To share information, caregivers write discharge notes containing action items to share with patients and their future caregivers,","offset":6,"pro":0.8571428571428571,"labels":"BAC"},{"idx":9,"sentence":"Emojis have become ubiquitous in digital communication, due to their visual appeal as well as their ability to vividly convey human emotion, among other factors.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"This also leads to an increased need for systems and tools to operate on text containing emojis.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":9,"sentence":"In this study, we assess this support by considering test sets of tweets with emojis, based on which we perform a series of experiments investigating the ability of prominent NLP and text processing tools to adequately process them.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":9,"sentence":"In particular, we consider tokenization, part-of-speech tagging, dependency parsing, as well as sentiment analysis.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":9,"sentence":"Our findings show that many systems still have notable shortcomings when operating on text containing emojis.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":10,"sentence":"Natural language processing techniques have demonstrated promising results in keyphrase generation.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"However, one of the major challenges in neural keyphrase generation is processing long documents using deep neural networks.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":10,"sentence":"Consequently, the models may miss essential points conveyed in the target document.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":10,"sentence":"To overcome this limitation, we propose SEG-Net, a neural keyphrase generation model that is composed of two major components, (1) a selector that selects the salient sentences in a document and (2) an extractor-generator that jointly extracts and generates keyphrases from the selected sentences.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":10,"sentence":"SEG-Net uses Transformer, a self-attentive architecture, as the basic building block with a novel layer-wise coverage attention to summarize most of the points discussed in the document.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":10,"sentence":"Generally, documents are truncated before given as inputs to neural networks.","offset":5,"pro":0.8333333333333334,"labels":"GAP"},{"idx":11,"sentence":"We propose a method for generating paraphrases of English questions that retain the original intent but use a different surface form.","offset":0,"pro":0,"labels":"PUR"},{"idx":11,"sentence":"Our model combines a careful choice of training objective with a principled information bottleneck, to induce a latent encoding space that disentangles meaning and form.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":11,"sentence":"We train an encoder-decoder model to reconstruct a question from a paraphrase with the same meaning and an exemplar with the same surface form, leading to separated encoding spaces.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":11,"sentence":"We use a Vector-Quantized Variational Autoencoder to represent the surface form as a set of discrete latent variables, allowing us to use a classifier to select a different surface form at test time.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":11,"sentence":"Crucially, our method does not require access to an external source of target exemplars.","offset":4,"pro":0.8,"labels":"MTD"},{"idx":12,"sentence":"In contrast to previous work using sentence planning, our model is still end-to-end: AggGen performs sentence planning at the same time as generating text by learning latent alignments (via semantic facts) between input representation and target text.","offset":0,"pro":0,"labels":"MTD"},{"idx":12,"sentence":"Our code is available at https://github.com/XinnuoXu/AggGen.","offset":1,"pro":0.25,"labels":"CTN"},{"idx":12,"sentence":"We present AggGen (pronounced again') a data-to-text model which re-introduces two explicit sentence planning stages into neural data-to-text systems: input ordering and input aggregation.","offset":2,"pro":0.5,"labels":"PUR"},{"idx":12,"sentence":"Experiments on the WebNLG and E2E challenge data show that by using fact-based alignments our approach is more interpretable, expressive, robust to noise, and easier to control, while retaining the advantages of end-to-end systems in terms of fluency.","offset":3,"pro":0.75,"labels":"CLN"},{"idx":13,"sentence":"Publicly available, large pretrained Language Models (LMs) generate text with remarkable quality, but only sequentially from left to right.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"As a result, they are not immediately applicable to generation tasks that break the unidirectional assumption, such as paraphrasing or text-infilling, necessitating task-specific supervision.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":13,"sentence":"In this paper, we present Reflective Decoding, a novel unsupervised algorithm that allows for direct application of unidirectional LMs to non-sequential tasks.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":13,"sentence":"Our 2-step approach requires no supervision or even parallel corpora, only two off-the-shelf pretrained LMs in opposite directions: forward and backward.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":13,"sentence":"First, in the contextualization step, we use LMs to generate ensembles of past and future contexts which collectively capture the input (e.g. the source sentence for paraphrasing).","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":13,"sentence":"Second, in the reflection step, we condition on these context ensembles, generating outputs that are compatible with them.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":13,"sentence":"Comprehensive empirical results demonstrate that Reflective Decoding outperforms strong unsupervised baselines on both paraphrasing and abductive text infilling, significantly narrowing the gap between unsupervised and supervised methods.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":14,"sentence":"Recent neural text generation models have shown significant improvement in generating descriptive text from structured data such as table formats.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"One of the remaining important challenges is generating more analytical descriptions that can be inferred from facts in a data source.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":14,"sentence":"The use of a template-based generator and a pointer-generator is among the potential alternatives for table-to-text generators.","offset":2,"pro":0.3333333333333333,"labels":"BAC"},{"idx":14,"sentence":"In this paper, we propose a framework consisting of a pre-trained model and a copy mechanism.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":14,"sentence":"The pre-trained models are fine-tuned to produce fluent text that is enriched with numerical reasoning.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":14,"sentence":"The copy mechanism is incorporated in the fine-tuning step by using general placeholders to avoid producing hallucinated phrases that are not supported by a table while preserving high fluency.","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":15,"sentence":"In this paper, we focus on the problem of citing sentence generation, which entails generating a short text to capture the salient information in a cited paper and the connection between the citing and cited paper.","offset":0,"pro":0,"labels":"PUR"},{"idx":15,"sentence":"We present BACO, a BAckground knowledge- and COntent-based framework for citing sentence generation, which considers two types of information: (1) background knowledge by leveraging structural information from a citation network; and (2) content, which represents in-depth information about what to cite and why to cite.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":15,"sentence":"First, a citation network is encoded to provide background knowledge.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":15,"sentence":"Second, we apply salience estimation to identify what to cite by estimating the importance of sentences in the cited paper.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":15,"sentence":"During the decoding stage, both types of information are combined to facilitate the text generation, and then we conduct a joint training for the generator and citation function classification to make the model aware of why to cite.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":15,"sentence":"Our experimental results show that our framework outperforms comparative baselines.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":16,"sentence":"Current dialogue summarization systems usually encode the text with a number of general semantic features (e.g., keywords and topics) to gain more powerful dialogue modeling capabilities.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"In this paper, we show how DialoGPT, a pre-trained model for conversational response generation, can be developed as an unsupervised dialogue annotator, which takes advantage of dialogue background knowledge encoded in DialoGPT.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":16,"sentence":"We apply DialoGPT to label three types of features on two dialogue summarization datasets, SAMSum and AMI, and employ pre-trained and non pre-trained models as our summarizers.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":16,"sentence":"Experimental results show that our proposed method can obtain remarkable improvements on both datasets and achieves new state-of-the-art performance on the SAMSum dataset.","offset":3,"pro":0.6,"labels":"CLN"},{"idx":16,"sentence":"However, these features are obtained via open-domain toolkits that are dialog-agnostic or heavily relied on human annotations.","offset":4,"pro":0.8,"labels":"GAP"},{"idx":17,"sentence":"Recent pretrained language models solved many reading comprehension benchmarks, where questions are written with access to the evidence document.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"However, datasets containing information-seeking queries where evidence documents are provided after the queries are written independently remain challenging.","offset":1,"pro":0.1,"labels":"GAP"},{"idx":17,"sentence":"We manually annotate 800 unanswerable examples across six languages on what makes them challenging to answer.","offset":2,"pro":0.2,"labels":"MTD"},{"idx":17,"sentence":"Together, our study points to avenues for future research in information-seeking question answering, both for dataset creation and model development.","offset":3,"pro":0.3,"labels":"IMP"},{"idx":17,"sentence":"Our code and annotated data is publicly available at https://github.com/AkariAsai/unanswerableqa.","offset":4,"pro":0.4,"labels":"CTN"},{"idx":17,"sentence":"We analyze why answering information-seeking queries is more challenging and where their prevalent unanswerabilities arise, on Natural Questions and TyDi QA.","offset":5,"pro":0.5,"labels":"PUR"},{"idx":17,"sentence":"When provided with a gold paragraph and knowing when to abstain from answering, existing models easily outperform a human annotator.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":17,"sentence":"With this new data, we conduct per-category answerability prediction, revealing issues in the current dataset collection as well as task formulation.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":17,"sentence":"However, predicting answerability itself remains challenging.","offset":8,"pro":0.8,"labels":"MTD"},{"idx":17,"sentence":"Our controlled experiments suggest two headrooms - paragraph selection and answerability prediction, i.e. whether the paired evidence document contains the answer to the query or not.","offset":9,"pro":0.9,"labels":"PUR"},{"idx":18,"sentence":"Users of medical question answering systems often submit long and detailed questions, making it hard to achieve high recall in answer retrieval.","offset":0,"pro":0,"labels":"GAP"},{"idx":18,"sentence":"To alleviate this problem, we propose a novel Multi-Task Learning (MTL) method with data augmentation for medical question understanding.","offset":1,"pro":0.125,"labels":"PUR"},{"idx":18,"sentence":"We first establish an equivalence between the tasks of question summarization and Recognizing Question Entailment (RQE) using their definitions in the medical domain.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":18,"sentence":"Based on this equivalence, we propose a data augmentation algorithm to use just one dataset to optimize for both tasks, with a weighted MTL loss.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":18,"sentence":"We introduce gradually soft parameter-sharing: a constraint for decoder parameters to be close, that is gradually loosened as we move to the highest layer.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":18,"sentence":"We show through ablation studies that our proposed novelties improve performance.","offset":5,"pro":0.625,"labels":"CLN"},{"idx":18,"sentence":"Our method outperforms existing MTL methods across 4 datasets of medical question pairs, in ROUGE scores, RQE accuracy and human evaluation.","offset":6,"pro":0.75,"labels":"RST"},{"idx":18,"sentence":"Finally, we show that our method fares better than single-task learning under 4 low-resource settings.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":19,"sentence":"A common issue in real-world applications of named entity recognition and classification (NERC) is the absence of annotated data for the target entity classes during training.","offset":0,"pro":0,"labels":"GAP"},{"idx":19,"sentence":"This paper presents the first approach for zero-shot NERC, introducing novel architectures that leverage the fact that textual descriptions for many entity classes occur naturally.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":19,"sentence":"For evaluation, we adapt two datasets, OntoNotes and MedMentions, emulating the difficulty of real-world zero-shot learning by testing models on the rarest entity classes.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":19,"sentence":"Our proposed approach outperforms baselines adapted from machine reading comprehension and zero-shot text classification.","offset":3,"pro":0.42857142857142855,"labels":"RST"},{"idx":19,"sentence":"Furthermore, we assess the effect of different class descriptions for this task.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":19,"sentence":"Zero-shot learning approaches address this issue by learning models from classes with training data that can predict classes without it.","offset":5,"pro":0.7142857142857143,"labels":"BAC"},{"idx":19,"sentence":"We address the zero-shot NERC specific challenge that the not-an-entity class is not well defined as different entity classes are considered in training and testing.","offset":6,"pro":0.8571428571428571,"labels":"MTD"},{"idx":0,"sentence":"Auditing NLP systems for computational harms like surfacing stereotypes is an elusive goal.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"Several recent efforts have focused on benchmark datasets consisting of pairs of contrastive sentences, which are often accompanied by metrics that aggregate an NLP system's behavior on these pairs into measurements of harms.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":0,"sentence":"We examine four such benchmarks constructed for two NLP tasks: language modeling and coreference resolution.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":0,"sentence":"We apply a measurement modeling lens-originating from the social sciences-to inventory a range of pitfalls that threaten these benchmarks' validity as measurement models for stereotyping.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":0,"sentence":"We find that these benchmarks frequently lack clear articulations of what is being measured, and we highlight a range of ambiguities and unstated assumptions that affect how these benchmarks conceptualize and operationalize stereotyping.","offset":4,"pro":0.8,"labels":"RST"},{"idx":1,"sentence":"Knowledge Graph (KG) completion research usually focuses on densely connected benchmark datasets that are not representative of real KGs.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"We curate two KG datasets that include biomedical and encyclopedic knowledge and use an existing commonsense KG dataset to explore KG completion in the more realistic setting where dense connectivity is not guaranteed.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":1,"sentence":"We develop a deep convolutional network that utilizes textual entity representations and demonstrate that our model outperforms recent KG completion methods in this challenging setting.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":1,"sentence":"We find that our model's performance improvements stem primarily from its robustness to sparsity.","offset":3,"pro":0.5,"labels":"RST"},{"idx":1,"sentence":"We then distill the knowledge from the convolutional network into a student network that re-ranks promising candidate entities.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":1,"sentence":"This re-ranking stage leads to further improvements in performance and demonstrates the effectiveness of entity re-ranking for KG completion.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":2,"sentence":"Computing precise evidences, namely minimal sets of sentences that support or refute a given claim, rather than larger evidences is crucial in fact verification (FV), since larger evidences may contain conflicting pieces some of which support the claim while the other refute, thereby misleading FV.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"Despite being important, precise evidences are rarely studied by existing methods for FV.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":2,"sentence":"It is challenging to find precise evidences due to a large search space with lots of local optimums.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":2,"sentence":"Inspired by the strong exploration ability of the deep Q-learning network (DQN), we propose a DQN-based approach to retrieval of precise evidences.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":2,"sentence":"In addition, to tackle the label bias on Q-values computed by DQN, we design a post-processing strategy which seeks best thresholds for determining the true labels of computed evidences.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":2,"sentence":"Experimental results confirm the effectiveness of DQN in computing precise evidences and demonstrate improvements in achieving accurate claim verification.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":3,"sentence":"In selective prediction, a classifier is allowed to abstain from making predictions on low-confidence examples.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"Though this setting is interesting and important, selective prediction has rarely been examined in natural language processing (NLP) tasks.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":3,"sentence":"To fill this void in the literature, we study in this paper selective prediction for NLP, comparing different models and confidence estimators.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":3,"sentence":"We further propose a simple error regularization trick that improves confidence estimation without substantially increasing the computation budget.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":3,"sentence":"We show that recent pre-trained transformer models simultaneously improve both model accuracy and confidence estimation effectiveness.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":3,"sentence":"We also find that our proposed regularization improves confidence estimation and can be applied to other relevant scenarios, such as using classifier cascades for accuracy-efficiency trade-offs.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":3,"sentence":"Source code for this paper can be found at https://github.com/castorini/transformers-selective.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":4,"sentence":"Deployed real-world machine learning applications are often subject to uncontrolled and even potentially malicious inputs.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"Those out-of-domain inputs can lead to unpredictable outputs and sometimes catastrophic safety issues.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":4,"sentence":"Prior studies on out-of-domain detection require in-domain task labels and are limited to supervised classification scenarios.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":4,"sentence":"Our work tackles the problem of detecting out-of-domain samples with only unsupervised in-domain data.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":4,"sentence":"We utilize the latent representations of pre-trained transformers and propose a simple yet effective method to transform features across all layers to construct out-of-domain detectors efficiently.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":4,"sentence":"Two domain-specific fine-tuning approaches are further proposed to boost detection accuracy.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":4,"sentence":"Our empirical evaluations of related methods on two datasets validate that our method greatly improves out-of-domain detection ability in a more general scenario.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":5,"sentence":"The advent of large pre-trained language models has given rise to rapid progress in the field of Natural Language Processing (NLP).","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"While the performance of these models on standard benchmarks has scaled with size, compression techniques such as knowledge distillation have been key in making them practical.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":5,"sentence":"We present MATE-KD, a novel text-based adversarial training algorithm which improves the performance of knowledge distillation.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":5,"sentence":"MATE-KD first trains a masked language model-based generator to perturb text by maximizing the divergence between teacher and student logits.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":5,"sentence":"Then using knowledge distillation a student is trained on both the original and the perturbed training samples.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":5,"sentence":"We evaluate our algorithm, using BERT-based models, on the GLUE benchmark and demonstrate that MATE-KD outperforms competitive adversarial learning and data augmentation baselines.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":5,"sentence":"On the GLUE test set our 6 layer RoBERTa based model outperforms BERT-large.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":6,"sentence":"Language model fine-tuning is essential for modern natural language processing, but is computationally expensive and time-consuming.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"Further, the effectiveness of fine-tuning is limited by the inclusion of training examples that negatively affect performance.","offset":1,"pro":0.1,"labels":"GAP"},{"idx":6,"sentence":"Here we present a general fine-tuning method that we call information gain filtration for improving the overall training efficiency and final performance of language model fine-tuning.","offset":2,"pro":0.2,"labels":"PUR"},{"idx":6,"sentence":"We define the information gain of an example as the improvement on a validation metric after training on that example.","offset":3,"pro":0.3,"labels":"MTD"},{"idx":6,"sentence":"A secondary learner is then trained to approximate this quantity.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":6,"sentence":"During fine-tuning, this learner selects informative examples and skips uninformative ones.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":6,"sentence":"We show that our method has consistent improvement across datasets, fine-tuning tasks, and language model architectures.","offset":6,"pro":0.6,"labels":"RST"},{"idx":6,"sentence":"For example, we achieve a median perplexity of 54.0 on a books dataset compared to 57.3 for standard fine-tuning.","offset":7,"pro":0.7,"labels":"RST"},{"idx":6,"sentence":"We present statistical evidence that offers insight into the improvements of our method over standard fine-tuning.","offset":8,"pro":0.8,"labels":"CTN"},{"idx":6,"sentence":"The generality of our method leads us to propose a new paradigm for language model fine-tuning - we encourage researchers to release pretrained secondary learners on common corpora to promote efficient and effective fine-tuning, thereby improving the performance and reducing the overall energy footprint of language model fine-tuning.","offset":9,"pro":0.9,"labels":"CTN"},{"idx":7,"sentence":"Text simplification reduces the language complexity of professional content for accessibility purposes.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"End-to-end neural network models have been widely adopted to directly generate the simplified version of input text, usually functioning as a blackbox.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":7,"sentence":"We show that text simplification can be decomposed into a compact pipeline of tasks to ensure the transparency and explainability of the process.","offset":2,"pro":0.3333333333333333,"labels":"BAC"},{"idx":7,"sentence":"The first two steps in this pipeline are often neglected: 1) to predict whether a given piece of text needs to be simplified, and 2) if yes, to identify complex parts of the text.","offset":3,"pro":0.5,"labels":"GAP"},{"idx":7,"sentence":"The two tasks can be solved separately using either lexical or deep learning methods, or solved jointly.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":7,"sentence":"Simply applying explainable complexity prediction as a preliminary step, the out-of-sample text simplification performance of the state-of-the-art, black-box simplification models can be improved by a large margin.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":8,"sentence":"Retrieving relevant contexts from a large corpus is a crucial step for tasks such as open-domain question answering and fact checking.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"Although neural retrieval outperforms traditional methods like tf-idf and BM25, its performance degrades considerably when applied to out-of-domain data.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":8,"sentence":"Driven by the question of whether a neural retrieval model can be universal and perform robustly on a wide variety of problems, we propose a multi-task trained model.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":8,"sentence":"Our approach not only outperforms previous methods in the few-shot setting, but also rivals specialised neural retrievers, even when in-domain training data is abundant.","offset":3,"pro":0.6,"labels":"RST"},{"idx":8,"sentence":"With the help of our retriever, we improve existing models for downstream tasks and closely match or improve the state of the art on multiple benchmarks.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":9,"sentence":"NLP is currently dominated by language models like RoBERTa which are pretrained on billions of words.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"But what exact knowledge or skills do Transformer LMs learn from large-scale pretraining that they cannot learn from less data?","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":9,"sentence":"To explore this question, we adopt five styles of evaluation: classifier probing, information-theoretic probing, unsupervised relative acceptability judgments, unsupervised language model knowledge probing, and fine-tuning on NLU tasks.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":9,"sentence":"We then draw learning curves that track the growth of these different measures of model ability with respect to pretraining data volume using the MiniBERTas, a group of RoBERTa models pretrained on 1M, 10M, 100M and 1B words.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":9,"sentence":"We find that these LMs require only about 10M to 100M words to learn to reliably encode most syntactic and semantic features we test.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":9,"sentence":"They need a much larger quantity of data in order to acquire enough commonsense knowledge and other skills required to master typical downstream NLU tasks.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":9,"sentence":"The results suggest that, while the ability to encode linguistic features is almost certainly necessary for language understanding, it is likely that other, unidentified, forms of knowledge are the major drivers of recent improvements in language understanding among large pretrained models.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":10,"sentence":"In Neural Machine Translation (and, more generally, conditional language modeling), the generation of a target token is influenced by two types of context: the source and the prefix of the target sequence.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"While many attempts to understand the internal workings of NMT models have been made, none of them explicitly evaluates relative source and target contributions to a generation decision.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":10,"sentence":"We argue that this relative contribution can be evaluated by adopting a variant of Layerwise Relevance Propagation (LRP).","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":10,"sentence":"Its underlying conservation principle' makes relevance propagation unique: differently from other methods, it evaluates not an abstract quantity reflecting token importance, but the proportion of each token's influence.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":10,"sentence":"We extend LRP to the Transformer and conduct an analysis of NMT models which explicitly evaluates the source and target relative contributions to the generation process.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":10,"sentence":"We analyze changes in these contributions when conditioning on different types of prefixes, when varying the training objective or the amount of training data, and during the training process.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":10,"sentence":"We find that models trained with more data tend to rely on source information more and to have more sharp token contributions; the training process is non-monotonic with several stages of different nature.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":11,"sentence":"Recent years have seen numerous NLP datasets introduced to evaluate the performance of fine-tuned models on natural language understanding tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"Recent results from large pretrained models, though, show that many of these datasets are largely saturated and unlikely to be able to detect further progress.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":11,"sentence":"What kind of datasets are still effective at discriminating among strong models, and what kind of datasets should we expect to be able to detect future improvements?","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":11,"sentence":"To measure this uniformly across datasets, we draw on Item Response Theory and evaluate 29 datasets using predictions from 18 pretrained Transformer models on individual test examples.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":11,"sentence":"We find that Quoref, HellaSwag, and MC-TACO are best suited for distinguishing among state-of-the-art models, while SNLI, MNLI, and CommitmentBank seem to be saturated for current strong models.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":11,"sentence":"We also observe span selection task format, which is used for QA datasets like QAMR or SQuAD2.0, is effective in differentiating between strong and weak models.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":12,"sentence":"A growing body of literature has focused on detailing the linguistic knowledge embedded in large, pretrained language models.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"Existing work has shown that non-linguistic biases in models can drive model behavior away from linguistic generalizations.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":12,"sentence":"We hypothesized that competing linguistic processes within a language, rather than just non-linguistic model biases, could obscure underlying linguistic knowledge.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":12,"sentence":"We tested this claim by exploring a single phenomenon in four languages: English, Chinese, Spanish, and Italian.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":12,"sentence":"While human behavior has been found to be similar across languages, we find cross-linguistic variation in model behavior.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":12,"sentence":"We show that competing processes in a language act as constraints on model behavior and demonstrate that targeted fine-tuning can re-weight the learned constraints, uncovering otherwise dormant linguistic knowledge in models.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":12,"sentence":"Our results suggest that models need to learn both the linguistic constraints in a language and their relative ranking, with mismatches in either producing non-human-like behavior.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":13,"sentence":"Interpretability is an important aspect of the trustworthiness of a model's predictions.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"Transformer's predictions are widely explained by the attention weights, i.e., a probability distribution generated at its self-attention unit (head).","offset":1,"pro":0.09090909090909091,"labels":"BAC"},{"idx":13,"sentence":"Current empirical studies provide shreds of evidence that attention weights are not explanations by proving that they are not unique.","offset":2,"pro":0.18181818181818182,"labels":"GAP"},{"idx":13,"sentence":"A recent study showed theoretical justifications to this observation by proving the non-identifiability of attention weights.","offset":3,"pro":0.2727272727272727,"labels":"GAP"},{"idx":13,"sentence":"For a given input to a head and its output, if the attention weights generated in it are unique, we call the weights identifiable.","offset":4,"pro":0.36363636363636365,"labels":"GAP"},{"idx":13,"sentence":"In this work, we provide deeper theoretical analysis and empirical observations on the identifiability of attention weights.","offset":5,"pro":0.45454545454545453,"labels":"PUR"},{"idx":13,"sentence":"Ignored in the previous works, we find the attention weights are more identifiable than we currently perceive by uncovering the hidden role of the key vector.","offset":6,"pro":0.5454545454545454,"labels":"MTD"},{"idx":13,"sentence":"However, the weights are still prone to be non-unique attentions that make them unfit for interpretation.","offset":7,"pro":0.6363636363636364,"labels":"RST"},{"idx":13,"sentence":"To tackle this issue, we provide a variant of the encoder layer that decouples the relationship between key and value vector and provides identifiable weights up to the desired length of the input.","offset":8,"pro":0.7272727272727273,"labels":"MTD"},{"idx":13,"sentence":"We prove the applicability of such variations by providing empirical justifications on varied text classification tasks.","offset":9,"pro":0.8181818181818182,"labels":"CLN"},{"idx":13,"sentence":"The implementations are available at https://github.com/declare-lab/identifiable-transformers.","offset":10,"pro":0.9090909090909091,"labels":"CTN"},{"idx":14,"sentence":"Natural Language Generation (NLG) is a key component in a task-oriented dialogue system, which converts the structured meaning representation (MR) to the natural language.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"For large-scale conversational systems, where it is common to have over hundreds of intents and thousands of slots, neither template-based approaches nor model-based approaches are scalable.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":14,"sentence":"Recently, neural NLGs started leveraging transfer learning and showed promising results in few-shot settings.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":14,"sentence":"This paper proposes AugNLG, a novel data augmentation approach that combines a self-trained neural retrieval model with a few-shot learned NLU model, to automatically create MR-to-Text data from open-domain texts.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":14,"sentence":"The proposed system mostly outperforms the state-of-the-art methods on the FewshotWOZ data in both BLEU and Slot Error Rate.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":14,"sentence":"We further confirm improved results on the FewshotSGD data and provide comprehensive analysis results on key components of our system.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":14,"sentence":"Our code and data are available at https://github.com/XinnuoXu/AugNLG.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":15,"sentence":"In this paper we implement and compare 7 different data augmentation strategies for the task of automatic scoring of children's ability to understand others' thoughts, feelings, and desires (or mindreading).","offset":0,"pro":0,"labels":"PUR"},{"idx":15,"sentence":"We recruit in-domain experts to re-annotate augmented samples and determine to what extent each strategy preserves the original rating.","offset":1,"pro":0.1,"labels":"MTD"},{"idx":15,"sentence":"We also carry out multiple experiments to measure how much each augmentation strategy improves the performance of automatic scoring systems.","offset":2,"pro":0.2,"labels":"MTD"},{"idx":15,"sentence":"To determine the capabilities of automatic systems to generalize to unseen data, we create UK-MIND-20 - a new corpus of children's performance on tests of mindreading, consisting of 10,320 question-answer pairs.","offset":3,"pro":0.3,"labels":"MTD"},{"idx":15,"sentence":"We obtain a new state-of-the-art performance on the MIND-CA corpus, improving macro-F1-score by 6 points.","offset":4,"pro":0.4,"labels":"RST"},{"idx":15,"sentence":"Results indicate that both the number of training examples and the quality of the augmentation strategies affect the performance of the systems.","offset":5,"pro":0.5,"labels":"CLN"},{"idx":15,"sentence":"The task-specific augmentations generally outperform task-agnostic augmentations.","offset":6,"pro":0.6,"labels":"CLN"},{"idx":15,"sentence":"Automatic augmentations based on vectors (GloVe, FastText) perform the worst.","offset":7,"pro":0.7,"labels":"CLN"},{"idx":15,"sentence":"We find that systems trained on MIND-CA generalize well to UK-MIND-20.","offset":8,"pro":0.8,"labels":"RST"},{"idx":15,"sentence":"We demonstrate that data augmentation strategies also improve the performance on unseen data.","offset":9,"pro":0.9,"labels":"CLN"},{"idx":16,"sentence":"Reply suggestion models help users process emails and chats faster.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"Previous work only studies English reply suggestion.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":16,"sentence":"Instead, we present MRS, a multilingual reply suggestion dataset with ten languages.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":16,"sentence":"MRS can be used to compare two families of models: 1) retrieval models that select the reply from a fixed set and 2) generation models that produce the reply from scratch.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":16,"sentence":"Therefore, MRS complements existing cross-lingual generalization benchmarks that focus on classification and sequence labeling tasks.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":16,"sentence":"MRS is publicly available at https://github.com/zhangmozhi/mrs.","offset":5,"pro":0.625,"labels":"CTN"},{"idx":16,"sentence":"We build a generation model and a retrieval model as baselines for MRS.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":16,"sentence":"The two models have different strengths in the monolingual setting, and they require different strategies to generalize across languages.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":17,"sentence":"Crowdsourcing is widely used to create data for common natural language understanding tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"Despite the importance of these datasets for measuring and refining model understanding of language, there has been little focus on the crowdsourcing methods used for collecting the datasets.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":17,"sentence":"In this paper, we compare the efficacy of interventions that have been proposed in prior work as ways of improving data quality.","offset":2,"pro":0.2222222222222222,"labels":"PUR"},{"idx":17,"sentence":"We use multiple-choice question answering as a testbed and run a randomized trial by assigning crowdworkers to write questions under one of four different data collection protocols.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":17,"sentence":"We find that asking workers to write explanations for their examples is an ineffective stand-alone strategy for boosting NLU example difficulty.","offset":4,"pro":0.4444444444444444,"labels":"RST"},{"idx":17,"sentence":"However, we find that training crowdworkers, and then using an iterative process of collecting data, sending feedback, and qualifying workers based on expert judgments is an effective means of collecting challenging data.","offset":5,"pro":0.5555555555555556,"labels":"RST"},{"idx":17,"sentence":"But using crowdsourced, instead of expert judgments, to qualify workers and send feedback does not prove to be effective.","offset":6,"pro":0.6666666666666666,"labels":"RST"},{"idx":17,"sentence":"We observe that the data from the iterative protocol with expert assessments is more challenging by several measures.","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":17,"sentence":"Notably, the human-model gap on the unanimous agreement portion of this data is, on average, twice as large as the gap for the baseline protocol data.","offset":8,"pro":0.8888888888888888,"labels":"RST"},{"idx":18,"sentence":"Legislators is typically estimated by ideal point models from historical records of votes.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"It represents legislators and legislation as points in a latent space and shows promising results for modeling voting behavior.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":18,"sentence":"However, it fails to capture more specific attitudes of legislators toward emerging issues and is unable to model newly-elected legislators without voting histories.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":18,"sentence":"In order to mitigate these two problems, we explore to incorporate both voting behavior and public statements on Twitter to jointly model legislators.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":18,"sentence":"In addition, we propose a novel task, namely hashtag usage prediction to model the ideology of legislators on Twitter.","offset":4,"pro":0.5,"labels":"PUR"},{"idx":18,"sentence":"In practice, we construct a heterogeneous graph for the legislative context and use relational graph neural networks to learn the representation of legislators with the guidance of historical records of their voting and hashtag usage.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":18,"sentence":"Experiment results indicate that our model yields significant improvements for the task of roll call vote prediction.","offset":6,"pro":0.75,"labels":"CLN"},{"idx":18,"sentence":"Further analysis further demonstrates that legislator representation we learned captures nuances in statements.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":19,"sentence":"Languages are dynamic systems: word usage may change over time, reflecting various societal factors.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"However, all languages do not evolve identically: the impact of an event, the influence of a trend or thinking, can differ between communities.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":19,"sentence":"In this paper, we propose to track these divergences by comparing the evolution of a word and its translation across two languages.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":19,"sentence":"We investigate several methods of building time-varying and bilingual word embeddings, using contextualised and non-contextualised embeddings.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":19,"sentence":"We propose a set of scenarios to characterize semantic divergence across two languages, along with a setup to differentiate them in a bilingual corpus.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":19,"sentence":"We evaluate the different methods by generating a corpus of synthetic semantic change across two languages, English and French, before applying them to newspaper corpora to detect bilingual semantic divergence and provide qualitative insight for the task.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":19,"sentence":"We conclude that BERT embeddings coupled with a clustering step lead to the best performance on synthetic corpora; however, the performance of CBOW embeddings is very competitive and more adapted to an exploratory analysis on a large corpus.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":0,"sentence":"Recently, word enhancement has become very popular for Chinese Named Entity Recognition (NER), reducing segmentation errors and increasing the semantic and boundary information of Chinese words.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"However, these methods tend to ignore the information of the Chinese character structure after integrating the lexical information.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":0,"sentence":"Chinese characters have evolved from pictographs since ancient times, and their structure often reflects more information about the characters.","offset":2,"pro":0.2857142857142857,"labels":"BAC"},{"idx":0,"sentence":"This paper presents a novel Multi-metadata Embedding based Cross-Transformer (MECT) to improve the performance of Chinese NER by fusing the structural information of Chinese characters.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":0,"sentence":"Specifically, we use multi-metadata embedding in a two-stream Transformer to integrate Chinese character features with the radical-level embedding.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":0,"sentence":"With the structural characteristics of Chinese characters, MECT can better capture the semantic information of Chinese characters for NER.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":0,"sentence":"The experimental results obtained on several well-known benchmarking datasets demonstrate the merits and superiority of the proposed MECT method.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":1,"sentence":"As the sources of information that we consume everyday rapidly diversify, it is becoming increasingly important to develop NLP tools that help to evaluate the credibility of the information we receive.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"In this paper, we frame factuality assessment as a modal dependency parsing task that identifies the events and their sources, formally known as conceivers, and then determine the level of certainty that the sources are asserting with respect to the events.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":1,"sentence":"We crowdsource the first large-scale data set annotated with modal dependency structures that consists of 353 Covid-19 related news articles, 24,016 events, and 2,938 conceivers.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":1,"sentence":"We also develop the first modal dependency parser that jointly extracts events, conceivers and constructs the modal dependency structure of a text.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":1,"sentence":"We evaluate the joint model against a pipeline model and demonstrate the advantage of the joint model in conceiver extraction and modal dependency structure construction when events and conceivers are automatically extracted.","offset":4,"pro":0.5714285714285714,"labels":"CLN"},{"idx":1,"sentence":"We believe the dataset and the models will be a valuable resource for a whole host of NLP applications such as fact checking and rumor detection.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":1,"sentence":"A critical step towards this goal is to determine the factuality of events in text.","offset":6,"pro":0.8571428571428571,"labels":"BAC"},{"idx":2,"sentence":"The modeling of conversational context plays a vital role in emotion recognition from conversation (ERC).","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"In this paper, we put forward a novel idea of encoding the utterances with a directed acyclic graph (DAG) to better model the intrinsic structure within a conversation, and design a directed acyclic neural network, namely DAG-ERC, to implement this idea.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":2,"sentence":"Extensive experiments are conducted on four ERC benchmarks with state-of-the-art models employed as baselines for comparison.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":2,"sentence":"The empirical results demonstrate the superiority of this new model and confirm the motivation of the directed acyclic graph architecture for ERC.","offset":3,"pro":0.6,"labels":"CLN"},{"idx":2,"sentence":"In an attempt to combine the strengths of conventional graph-based neural models and recurrence-based neural models, DAG-ERC provides a more intuitive way to model the information flow between long-distance conversation background and nearby context.","offset":4,"pro":0.8,"labels":"MTD"},{"idx":3,"sentence":"Models pre-trained on large-scale regular text corpora often do not work well for user-generated data where the language styles differ significantly from the mainstream text.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"Here we present Context-Aware Rule Injection (CARI), an innovative method for formality style transfer (FST) by injecting multiple rules into an end-to-end BERT-based encoder and decoder model.","offset":1,"pro":0.1,"labels":"PUR"},{"idx":3,"sentence":"CARI is able to learn to select optimal rules based on context.","offset":2,"pro":0.2,"labels":"MTD"},{"idx":3,"sentence":"The intrinsic evaluation showed that CARI achieved the new highest performance on the FST benchmark dataset.","offset":3,"pro":0.3,"labels":"CLN"},{"idx":3,"sentence":"Our extrinsic evaluation showed that CARI can greatly improve the regular pre-trained models' performance on several tweet sentiment analysis tasks.","offset":4,"pro":0.4,"labels":"CLN"},{"idx":3,"sentence":"We show that CARI outperformed existing rule-based FST approaches for sentiment classification.","offset":5,"pro":0.5,"labels":"CLN"},{"idx":3,"sentence":"Our contributions are as follows: 1)We propose a new method, CARI, to integrate rules for pre-trained language models.","offset":6,"pro":0.6,"labels":"CTN"},{"idx":3,"sentence":"CARI is context-aware and can trained end-to-end with the downstream NLP applications.","offset":7,"pro":0.7,"labels":"CTN"},{"idx":3,"sentence":"2)We have achieved new state-of-the-art results for FST on the benchmark GYAFC dataset.","offset":8,"pro":0.8,"labels":"CTN"},{"idx":3,"sentence":"3)We are the first to evaluate FST methods with extrinsic evaluation and specifically on sentiment classification tasks.","offset":9,"pro":0.9,"labels":"CTN"},{"idx":4,"sentence":"Emotion detection in dialogues is challenging as it often requires the identification of thematic topics underlying a conversation, the relevant commonsense knowledge, and the intricate transition patterns between the affective states.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"In this paper, we propose a Topic-Driven Knowledge-Aware Transformer to handle the challenges above.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":4,"sentence":"We firstly design a topic-augmented language model (LM) with an additional layer specialized for topic detection.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":4,"sentence":"The topic-augmented LM is then combined with commonsense statements derived from a knowledge base based on the dialogue contextual information.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":4,"sentence":"Finally, a transformer-based encoder-decoder architecture fuses the topical and commonsense information, and performs the emotion label sequence prediction.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":4,"sentence":"The model has been experimented on four datasets in dialogue emotion detection, demonstrating its superiority empirically over the existing state-of-the-art approaches.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":4,"sentence":"Quantitative and qualitative results show that the model can discover topics which help in distinguishing emotion categories.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":5,"sentence":"Approaches to computational argumentation tasks such as stance detection and aspect detection have largely focused on the text of independent claims, losing out on potentially valuable context provided by the rest of the collection.","offset":0,"pro":0,"labels":"GAP"},{"idx":5,"sentence":"We introduce a general approach to these tasks motivated by syntopical reading, a reading process that emphasizes comparing and contrasting viewpoints in order to improve topic understanding.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":5,"sentence":"Experiments applying syntopical graphs to the problems of detecting stance and aspects demonstrate state-of-the-art performance in each domain, significantly outperforming approaches that do not utilize collection-level information.","offset":2,"pro":0.4,"labels":"RST"},{"idx":5,"sentence":"To capture collection-level context, we introduce the syntopical graph, a data structure for linking claims within a collection.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":5,"sentence":"A syntopical graph is a typed multi-graph where nodes represent claims and edges represent different possible pairwise relationships, such as entailment, paraphrase, or support.","offset":4,"pro":0.8,"labels":"MTD"},{"idx":6,"sentence":"The prevalence of the COVID-19 pandemic in day-to-day life has yielded large amounts of stance detection data on social media sites, as users turn to social media to share their views regarding various issues related to the pandemic, e.g. stay at home mandates and wearing face masks when out in public.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"We set out to make use of this data by collecting the stance expressed by Twitter users, with respect to topics revolving around the pandemic.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":6,"sentence":"We annotate a new stance detection dataset, called COVID-19-Stance.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":6,"sentence":"Using this newly annotated dataset, we train several established stance detection models to ascertain a baseline performance for this specific task.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":6,"sentence":"To further improve the performance, we employ self-training and domain adaptation approaches to take advantage of large amounts of unlabeled data and existing stance detection datasets.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":6,"sentence":"The dataset, code, and other resources are available on GitHub.","offset":5,"pro":0.8333333333333334,"labels":"CTN"},{"idx":7,"sentence":"Fact verification is a challenging task that requires simultaneously reasoning and aggregating over multiple retrieved pieces of evidence to evaluate the truthfulness of a claim.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"Existing approaches typically (i) explore the semantic interaction between the claim and evidence at different granularity levels but fail to capture their topical consistency during the reasoning process, which we believe is crucial for verification; (ii) aggregate multiple pieces of evidence equally without considering their implicit stances to the claim, thereby introducing spurious information.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":7,"sentence":"To alleviate the above issues, we propose a novel topic-aware evidence reasoning and stance-aware aggregation model for more accurate fact verification, with the following four key properties: 1) checking topical consistency between the claim and evidence; 2) maintaining topical coherence among multiple pieces of evidence; 3) ensuring semantic similarity between the global topic information and the semantic representation of evidence; 4) aggregating evidence based on their implicit stances to the claim.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":7,"sentence":"Extensive experiments conducted on the two benchmark datasets demonstrate the superiority of the proposed model over several state-of-the-art approaches for fact verification.","offset":3,"pro":0.6,"labels":"CLN"},{"idx":7,"sentence":"The source code can be obtained from https://github.com/jasenchn/TARSA.","offset":4,"pro":0.8,"labels":"CTN"},{"idx":8,"sentence":"We introduce the well-established social scientific concept of social solidarity and its contestation, anti-solidarity, as a new problem setting to supervised machine learning in NLP to assess how European solidarity discourses changed before and after the COVID-19 outbreak was declared a global pandemic.","offset":0,"pro":0,"labels":"PUR"},{"idx":8,"sentence":"To this end, we annotate 2.3k English and German tweets for (anti-)solidarity expressions, utilizing multiple human annotators and two annotation approaches (experts vs. crowds).","offset":1,"pro":0.14285714285714285,"labels":"MTD"},{"idx":8,"sentence":"We use these annotations to train a BERT model with multiple data augmentation strategies.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":8,"sentence":"Our augmented BERT model that combines both expert and crowd annotations outperforms the baseline BERT classifier trained with expert annotations only by over 25 points, from 58% macro-F1 to almost 85%.","offset":3,"pro":0.42857142857142855,"labels":"RST"},{"idx":8,"sentence":"We use this high-quality model to automatically label over 270k tweets between September 2019 and December 2020.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":8,"sentence":"We then assess the automatically labeled data for how statements related to European (anti-)solidarity discourses developed over time and in relation to one another, before and during the COVID-19 crisis.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":8,"sentence":"Our results show that solidarity became increasingly salient and contested during the crisis.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":9,"sentence":"In conversation, uptake happens when a speaker builds on the contribution of their interlocutor by, for example, acknowledging, repeating or reformulating what they have said.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"In education, teachers' uptake of student contributions has been linked to higher student achievement.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":9,"sentence":"Yet measuring and improving teachers' uptake at scale is challenging, as existing methods require expensive annotation by experts.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":9,"sentence":"We find that although repetition captures a significant part of uptake, pJSD outperforms repetition-based baselines, as it is capable of identifying a wider range of uptake phenomena like question answering and reformulation.","offset":3,"pro":0.375,"labels":"RST"},{"idx":9,"sentence":"We apply our uptake measure to three different educational datasets with outcome indicators.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":9,"sentence":"Unlike baseline measures, pJSD correlates significantly with instruction quality in all three, providing evidence for its generalizability and for its potential to serve as an automated professional development tool for teachers.","offset":5,"pro":0.625,"labels":"CLN"},{"idx":9,"sentence":"We propose a framework for computationally measuring uptake,","offset":6,"pro":0.75,"labels":"PUR"},{"idx":9,"sentence":"by (1) releasing a dataset of student-teacher exchanges extracted from US math classroom transcripts annotated for uptake by experts; (2) formalizing uptake as pointwise Jensen-Shannon Divergence (pJSD), estimated via next utterance classification; (3) conducting a linguistically-motivated comparison of different unsupervised measures and (4) correlating these measures with educational outcomes.","offset":7,"pro":0.875,"labels":"MTD"},{"idx":10,"sentence":"The analysis of data in which multiple languages are represented has gained popularity among computational linguists in recent years.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"So far, much of this research focuses mainly on the improvement of computational methods and largely ignores linguistic and social aspects of C-S discussed across a wide range of languages within the long-established literature in linguistics.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":10,"sentence":"To fill this gap, we offer a survey of code-switching (C-S) covering the literature in linguistics with a reflection on the key issues in language technologies.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":10,"sentence":"From the linguistic perspective, we provide an overview of structural and functional patterns of C-S focusing on the literature from European and Indian contexts as highly multilingual areas.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":10,"sentence":"From the language technologies perspective, we discuss how massive language models fail to represent diverse C-S types due to lack of appropriate training data, lack of robust evaluation benchmarks for C-S (across multilingual situations and types of C-S) and lack of end-to- end systems that cover sociolinguistic aspects of C-S as well.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":10,"sentence":"Our survey will be a step to- wards an outcome of mutual benefit for computational scientists and linguists with a shared interest in multilingualism and C-S.","offset":5,"pro":0.8333333333333334,"labels":"IMP"},{"idx":11,"sentence":"We provide a new dataset of 40,000 entries, generated and labelled by trained annotators over four rounds of dynamic data creation.","offset":0,"pro":0,"labels":"MTD"},{"idx":11,"sentence":"It includes 15,000 challenging perturbations and each hateful entry has fine-grained labels for the type and target of hate.","offset":1,"pro":0.125,"labels":"MTD"},{"idx":11,"sentence":"Hateful entries make up 54% of the dataset, which is substantially higher than comparable datasets.","offset":2,"pro":0.25,"labels":"RST"},{"idx":11,"sentence":"We show that model performance is substantially improved using this approach.","offset":3,"pro":0.375,"labels":"CLN"},{"idx":11,"sentence":"Models trained on later rounds of data collection perform better on test sets and are harder for annotators to trick.","offset":4,"pro":0.5,"labels":"CLN"},{"idx":11,"sentence":"We provide the code, dataset and annotation guidelines for other researchers to use.","offset":5,"pro":0.625,"labels":"CTN"},{"idx":11,"sentence":"We present a human-and-model-in-the-loop process for dynamically generating datasets and training better performing and more robust hate detection models.","offset":6,"pro":0.75,"labels":"PUR"},{"idx":11,"sentence":"They also have better performance on HateCheck, a suite of functional tests for online hate detection.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":12,"sentence":"To defend against machine-generated fake news, an effective mechanism is urgently needed.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"Due to training data scarcity, we also formulate a novel data synthesis method by manipulating knowledge elements within the knowledge graph to generate noisy training data with specific, hard to detect, known inconsistencies.","offset":1,"pro":0.25,"labels":"MTD"},{"idx":12,"sentence":"Our detection approach outperforms the state-of-the-art (up to 16.8% accuracy gain), and more critically, yields fine-grained explanations.","offset":2,"pro":0.5,"labels":"RST"},{"idx":12,"sentence":"We contribute a novel benchmark for fake news detection at the knowledge element level, as well as a solution for this task which incorporates cross-media consistency checking to detect the fine-grained knowledge elements making news articles misinformative.","offset":3,"pro":0.75,"labels":"PUR"},{"idx":13,"sentence":"To quantify how well natural language understanding models can capture consistency in a general conversation, we introduce the DialoguE COntradiction DEtection task (DECODE) and a new conversational dataset containing both human-human and human-bot contradictory dialogues.","offset":0,"pro":0,"labels":"PUR"},{"idx":13,"sentence":"We show that: (i) our newly collected dataset is notably more effective at providing supervision for the dialogue contradiction detection task than existing NLI data including those aimed to cover the dialogue domain; (ii) Transformer models that explicitly hinge on utterance structures for dialogue contradiction detection are more robust and generalize well on both analysis and out-of-distribution dialogues than standard (unstructured) Transformers.","offset":1,"pro":0.3333333333333333,"labels":"CLN"},{"idx":13,"sentence":"We also show that our best contradiction detection model correlates well with human judgments and further provide evidence for its usage in both automatically evaluating and improving the consistency of state-of-the-art generative chatbots.","offset":2,"pro":0.6666666666666666,"labels":"CLN"},{"idx":14,"sentence":"This paper is concerned with dialogue state tracking (DST) in a task-oriented dialogue system.","offset":0,"pro":0,"labels":"PUR"},{"idx":14,"sentence":"Seq2Seq-DU employs two BERT-based encoders to respectively encode the utterances in the dialogue and the descriptions of schemas, an attender to calculate attentions between the utterance embeddings and the schema embeddings, and a decoder to generate pointers to represent the current state of dialogue.","offset":1,"pro":0.125,"labels":"MTD"},{"idx":14,"sentence":"Seq2Seq-DU has the following advantages.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":14,"sentence":"It can jointly model intents, slots, and slot values; it can leverage the rich representations of utterances and schemas based on BERT; it can effectively deal with categorical and non-categorical slots, and unseen schemas.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":14,"sentence":"In addition, Seq2Seq-DU can also be used in the NLU (natural language understanding) module of a dialogue system.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":14,"sentence":"Experimental results on benchmark datasets in different settings show that Seq2Seq-DU outperforms the existing methods.","offset":5,"pro":0.625,"labels":"RST"},{"idx":14,"sentence":"This paper proposes a new approach to dialogue state tracking, referred to as Seq2Seq-DU, which formalizes DST as a sequence-to-sequence problem","offset":6,"pro":0.75,"labels":"MTD"},{"idx":14,"sentence":"Building a DST module that is highly effective is still a challenging issue, although significant progresses have been made recently.","offset":7,"pro":0.875,"labels":"GAP"},{"idx":15,"sentence":"Learning discrete dialog structure graph from human-human dialogs yields basic insights into the structure of conversation, and also provides background knowledge to facilitate dialog generation.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"However, this problem is less studied in open-domain dialogue.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":15,"sentence":"In this paper, we conduct unsupervised discovery of discrete dialog structure from chitchat corpora, and then leverage it to facilitate coherent dialog generation in downstream systems.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":15,"sentence":"Then we leverage it as background knowledge to facilitate dialog management in a RL based dialog system.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":15,"sentence":"Experimental results on two benchmark corpora confirm that DVAE-GNN can discover meaningful dialog structure graph, and the use of dialog structure as background knowledge can significantly improve multi-turn coherence.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":15,"sentence":"To this end, we present an unsupervised model, Discrete Variational Auto-Encoder with Graph Neural Network (DVAE-GNN), to discover discrete hierarchical latent dialog states (at the level of both session and utterance) and their transitions from corpus as a dialog structure graph.","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":16,"sentence":"We study the learning of a matching model for dialogue response selection.","offset":0,"pro":0,"labels":"PUR"},{"idx":16,"sentence":"Motivated by the recent finding that models trained with random negative samples are not ideal in real-world scenarios, we propose a hierarchical curriculum learning framework that trains the matching model in an easy-to-difficult scheme.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":16,"sentence":"Our learning framework consists of two complementary curricula: (1) corpus-level curriculum (CC); and (2) instance-level curriculum (IC).","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":16,"sentence":"In CC, the model gradually increases its ability in finding the matching clues between the dialogue context and a response candidate.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":16,"sentence":"As for IC, it progressively strengthens the model's ability in identifying the mismatching information between the dialogue context and a response candidate.","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":16,"sentence":"Empirical studies on three benchmark datasets with three state-of-the-art matching models demonstrate that the proposed learning framework significantly improves the model performance across various evaluation metrics.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":17,"sentence":"In this paper, we present a neural model for joint dropped pronoun recovery (DPR) and conversational discourse parsing (CDP) in Chinese conversational speech.","offset":0,"pro":0,"labels":"PUR"},{"idx":17,"sentence":"We refer to our model as DiscProReco, and it first encodes the tokens in each utterance in a conversation with a directed Graph Convolutional Network (GCN).","offset":1,"pro":0.125,"labels":"MTD"},{"idx":17,"sentence":"The token states for an utterance are then aggregated to produce a single state for each utterance.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":17,"sentence":"The utterance states are then fed into a biaffine classifier to construct a conversational discourse graph.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":17,"sentence":"A second (multi-relational) GCN is then applied to the utterance states to produce a discourse relation-augmented representation for the utterances, which are then fused together with token states in each utterance as input to a dropped pronoun recovery layer.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":17,"sentence":"The joint model is trained and evaluated on a new Structure Parsing-enhanced Dropped Pronoun Recovery (SPDPR) data set that we annotated with both two types of information.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":17,"sentence":"Experimental results on the SPDPR dataset and other benchmarks show that DiscProReco significantly outperforms the state-of-the-art baselines of both tasks.","offset":6,"pro":0.75,"labels":"RST"},{"idx":17,"sentence":"We show that DPR and CDP are closely related, and a joint model benefits both tasks.","offset":7,"pro":0.875,"labels":"MTD"},{"idx":18,"sentence":"Knowledge bases (KBs) and text often contain complementary knowledge: KBs store structured knowledge that can support long range reasoning, while text stores more comprehensive and timely knowledge in an unstructured way.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"Separately embedding the individual knowledge sources into vector spaces has demonstrated tremendous successes in encoding the respective knowledge, but how to jointly embed and reason with both knowledge sources to fully leverage the complementary information is still largely an open problem.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":18,"sentence":"We set up a novel evaluation framework with two evaluation tasks, few-shot link prediction and analogical reasoning, and evaluate an array of KB-text embedding alignment methods.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":18,"sentence":"We also demonstrate how such alignment can infuse textual information into KB embeddings for more accurate link prediction on emerging entities and events, using COVID-19 as a case study.","offset":3,"pro":0.6,"labels":"CLN"},{"idx":18,"sentence":"We conduct a large-scale, systematic investigation of aligning KB and text embeddings for joint reasoning.","offset":4,"pro":0.8,"labels":"PUR"},{"idx":19,"sentence":"Weak supervision has shown promising results in many natural language processing tasks, such as Named Entity Recognition (NER).","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"Existing work mainly focuses on learning deep NER models only with weak supervision, i.e., without any human annotation, and shows that by merely using weakly labeled data, one can achieve good performance, though still underperforms fully supervised NER with manually/strongly labeled data.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":19,"sentence":"In this paper, we consider a more practical scenario, where we have both a small amount of strongly labeled data and a large amount of weakly labeled data.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":19,"sentence":"Through experiments on E-commerce query NER and Biomedical NER, we demonstrate that NEEDLE can effectively suppress the noise of the weak labels and outperforms existing methods.","offset":3,"pro":0.42857142857142855,"labels":"RST"},{"idx":19,"sentence":"In particular, we achieve new SOTA F1-scores on 3 Biomedical NER datasets: BC5CDR-chem 93.74, BC5CDR-disease 90.69, NCBI-disease 92.28.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":19,"sentence":"To address this issue, we propose a new multi-stage computational framework - NEEDLE with three essential ingredients: (1) weak label completion, (2) noise-aware loss function, and (3) final fine-tuning over the strongly labeled data.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":19,"sentence":"Unfortunately, we observe that weakly labeled data does not necessarily improve, or even deteriorate the model performance (due to the extensive noise in the weak labels) when we train deep NER models over a simple or weighted combination of the strongly labeled and weakly labeled data.","offset":6,"pro":0.8571428571428571,"labels":"GAP"},{"idx":0,"sentence":"Recently, there is an effort to extend fine-grained entity typing by using a richer and ultra-fine set of types, and labeling noun phrases including pronouns and nominal nouns instead of just named entity mentions.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"A key challenge for this ultra-fine entity typing task is that human annotated data are extremely scarce, and the annotation ability of existing distant or weak supervision approaches is very limited.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":0,"sentence":"To remedy this problem, in this paper, we propose to obtain training data for ultra-fine entity typing by using a BERT Masked Language Model (MLM).","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":0,"sentence":"Given a mention in a sentence, our approach constructs an input for the BERT MLM so that it predicts context dependent hypernyms of the mention, which can be used as type labels.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":0,"sentence":"Experimental results demonstrate that, with the help of these automatically generated labels, the performance of an ultra-fine entity typing model can be improved substantially.","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":0,"sentence":"We also show that our approach can be applied to improve traditional fine-grained entity typing after performing simple type mapping.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":1,"sentence":"Recent advances in Named Entity Recognition (NER) show that document-level contexts can significantly improve model performance.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"In many application scenarios, however, such contexts are not available.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":1,"sentence":"In this paper, we propose to find external contexts of a sentence by retrieving and selecting a set of semantically relevant texts through a search engine, with the original sentence as the query.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":1,"sentence":"We find empirically that the contextual representations computed on the retrieval-based input view, constructed through the concatenation of a sentence and its external contexts, can achieve significantly improved performance compared to the original input view based only on the sentence.","offset":3,"pro":0.5,"labels":"RST"},{"idx":1,"sentence":"Furthermore, we can improve the model performance of both input views by Cooperative Learning, a training method that encourages the two input views to produce similar contextual representations or output label distributions.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":1,"sentence":"Experiments show that our approach can achieve new state-of-the-art performance on 8 NER data sets across 5 domains.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":2,"sentence":"Does the effectiveness of neural language models derive entirely from accurate modeling of surface word co-occurrence statistics, or do these models represent and reason about the world they describe?","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"In BART and T5 transformer language models, we identify contextual word representations that function as *models of entities and situations* as they evolve throughout a discourse.","offset":1,"pro":0.25,"labels":"MTD"},{"idx":2,"sentence":"These neural representations have functional similarities to linguistic models of dynamic semantics: they support a linear readout of each entity's current properties and relations, and can be manipulated with predictable effects on language generation.","offset":2,"pro":0.5,"labels":"BAC"},{"idx":2,"sentence":"Our results indicate that prediction in pretrained neural language models is supported, at least in part, by dynamic representations of meaning and implicit simulation of entity state, and that this behavior can be learned with only text as training data.","offset":3,"pro":0.75,"labels":"CLN"},{"idx":3,"sentence":"Targeted syntactic evaluations have demonstrated the ability of language models to perform subject-verb agreement given difficult contexts.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"To elucidate the mechanisms by which the models accomplish this behavior, this study applies causal mediation analysis to pre-trained neural language models.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":3,"sentence":"We investigate the magnitude of models' preferences for grammatical inflections, as well as whether neurons process subject-verb agreement similarly across sentences with different syntactic structures.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":3,"sentence":"We uncover similarities and differences across architectures and model sizes-notably, that larger models do not necessarily learn stronger preferences.","offset":3,"pro":0.5,"labels":"RST"},{"idx":3,"sentence":"We also observe two distinct mechanisms for producing subject-verb agreement depending on the syntactic structure of the input sentence.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":3,"sentence":"Finally, we find that language models rely on similar sets of neurons when given sentences with similar syntactic structure.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":4,"sentence":"NLP has a rich history of representing our prior understanding of language in the form of graphs.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"Recent work on analyzing contextualized text representations has focused on hand-designed probe models to understand how and to what extent do these representations encode a particular linguistic phenomenon.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":4,"sentence":"However, due to the inter-dependence of various phenomena and randomness of training probe models, detecting how these representations encode the rich information in these linguistic graphs remains a challenging problem.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":4,"sentence":"In this paper, we propose a new information-theoretic probe, Bird's Eye, which is a fairly simple probe method for detecting if and how these representations encode the information in these linguistic graphs.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":4,"sentence":"Instead of using model performance, our probe takes an information-theoretic view of probing and estimates the mutual information between the linguistic graph embedded in a continuous space and the contextualized word representations.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":4,"sentence":"Furthermore, we also propose an approach to use our probe to investigate localized linguistic information in the linguistic graphs using perturbation analysis.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":4,"sentence":"We call this probing setup Worm's Eye.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":4,"sentence":"Using these probes, we analyze the BERT models on its ability to encode a syntactic and a semantic graph structure, and find that these models encode to some degree both syntactic as well as semantic information; albeit syntactic information to a greater extent.","offset":7,"pro":0.875,"labels":"MTD"},{"idx":5,"sentence":"Previous literatures show that pre-trained masked language models (MLMs) such as BERT can achieve competitive factual knowledge extraction performance on some datasets, indicating that MLMs can potentially be a reliable knowledge source.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"In this paper, we conduct a rigorous study to explore the underlying predicting mechanisms of MLMs over different extraction paradigms.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":5,"sentence":"By investigating the behaviors of MLMs, we find that previous decent performance mainly owes to the biased prompts which overfit dataset artifacts.","offset":2,"pro":0.4,"labels":"RST"},{"idx":5,"sentence":"Furthermore, incorporating illustrative cases and external contexts improve knowledge prediction mainly due to entity type guidance and golden answer leakage.","offset":3,"pro":0.6,"labels":"CLN"},{"idx":5,"sentence":"Our findings shed light on the underlying predicting mechanisms of MLMs, and strongly question the previous conclusion that current MLMs can potentially serve as reliable factual knowledge bases.","offset":4,"pro":0.8,"labels":"IMP"},{"idx":6,"sentence":"We study the problem of generating data poisoning attacks against Knowledge Graph Embedding (KGE) models for the task of link prediction in knowledge graphs.","offset":0,"pro":0,"labels":"PUR"},{"idx":6,"sentence":"To poison KGE models, we propose to exploit their inductive abilities which are captured through the relationship patterns like symmetry, inversion and composition in the knowledge graph.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":6,"sentence":"Specifically, to degrade the model's prediction confidence on target facts, we propose to improve the model's prediction confidence on a set of decoy facts.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":6,"sentence":"Thus, we craft adversarial additions that can improve the model's prediction confidence on decoy facts through different inference patterns.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":6,"sentence":"Our experiments demonstrate that the proposed poisoning attacks outperform state-of-art baselines on four KGE models for two publicly available datasets.","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":6,"sentence":"We also find that the symmetry pattern based attacks generalize across all model-dataset combinations which indicates the sensitivity of KGE models to this pattern.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":7,"sentence":"A common factor in bias measurement methods is the use of hand-curated seed lexicons, but there remains little guidance for their selection.","offset":0,"pro":0,"labels":"GAP"},{"idx":7,"sentence":"We gather seeds used in prior work, documenting their common sources and rationales, and in case studies of three English-language corpora, we enumerate the different types of social biases and linguistic features that, once encoded in the seeds, can affect subsequent bias measurements.","offset":1,"pro":0.3333333333333333,"labels":"MTD"},{"idx":7,"sentence":"Seeds developed in one context are often re-used in other contexts, but documentation and evaluation remain necessary precursors to relying on seeds for sensitive measurements.","offset":2,"pro":0.6666666666666666,"labels":"GAP"},{"idx":8,"sentence":"Despite inextricable ties between race and language, little work has considered race in NLP research and development.","offset":0,"pro":0,"labels":"GAP"},{"idx":8,"sentence":"In this work, we survey 79 papers from the ACL anthology that mention race.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":8,"sentence":"These papers reveal various types of race-related bias in all stages of NLP model development, highlighting the need for proactive consideration of how NLP systems can uphold racial hierarchies.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":8,"sentence":"However, persistent gaps in research on race and NLP remain: race has been siloed as a niche topic and remains ignored in many NLP tasks; most work operationalizes race as a fixed single-dimensional variable with a ground-truth label, which risks reinforcing differences produced by historical racism; and the voices of historically marginalized people are nearly absent in NLP literature.","offset":3,"pro":0.6,"labels":"GAP"},{"idx":8,"sentence":"By identifying where and how NLP literature has and has not considered race, especially in comparison to related fields, our work calls for inclusion and racial justice in NLP research practices.","offset":4,"pro":0.8,"labels":"PUR"},{"idx":9,"sentence":"Natural Language Processing (NLP) systems learn harmful societal biases that cause them to amplify inequality as they are deployed in more and more situations.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"To guide efforts at debiasing these systems, the NLP community relies on a variety of metrics that quantify bias in models.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":9,"sentence":"Some of these metrics are intrinsic, measuring bias in word embedding spaces, and some are extrinsic, measuring bias in downstream tasks that the word embeddings enable.","offset":2,"pro":0.25,"labels":"BAC"},{"idx":9,"sentence":"Do these intrinsic and extrinsic metrics correlate with each other?","offset":3,"pro":0.375,"labels":"BAC"},{"idx":9,"sentence":"We compare intrinsic and extrinsic metrics across hundreds of trained models covering different tasks and experimental conditions.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":9,"sentence":"Our results show no reliable correlation between these metrics that holds in all scenarios across tasks and languages.","offset":5,"pro":0.625,"labels":"RST"},{"idx":9,"sentence":"We urge researchers working on debiasing to focus on extrinsic measures of bias, and to make using these measures more feasible via creation of new challenge sets and annotated test data.","offset":6,"pro":0.75,"labels":"PUR"},{"idx":9,"sentence":"To aid this effort, we release code, a new intrinsic metric, and an annotated test set focused on gender bias in hate speech.","offset":7,"pro":0.875,"labels":"PUR"},{"idx":10,"sentence":"Text representation models are prone to exhibit a range of societal biases, reflecting the non-controlled and biased nature of the underlying pretraining data, which consequently leads to severe ethical issues and even bias amplification.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"Recent work has predominantly focused on measuring and mitigating bias in pretrained language models.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":10,"sentence":"Surprisingly, the landscape of bias measurements and mitigation resources and methods for conversational language models is still very scarce: it is limited to only a few types of bias, artificially constructed resources, and completely ignores the impact that debiasing methods may have on the final perfor mance in dialog tasks, e.g., conversational response generation.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":10,"sentence":"In this work, we present REDDITBIAS, the first conversational data set grounded in the actual human conversations from Reddit, allowing for bias measurement and mitigation across four important bias dimensions: gender,race,religion, and queerness.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":10,"sentence":"Further, we develop an evaluation framework which simultaneously 1)measures bias on the developed REDDITBIAS resource, and 2)evaluates model capability in dialog tasks after model debiasing.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":10,"sentence":"We use the evaluation framework to benchmark the widely used conversational DialoGPT model along with the adaptations of four debiasing methods.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":10,"sentence":"Our results indicate that DialoGPT is biased with respect to religious groups and that some debiasing techniques can remove this bias while preserving downstream task performance.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":11,"sentence":"This paper studies the relative importance of attention heads in Transformer-based models to aid their interpretability in cross-lingual and multi-lingual tasks.","offset":0,"pro":0,"labels":"PUR"},{"idx":11,"sentence":"Prior research has found that only a few attention heads are important in each mono-lingual Natural Language Processing (NLP) task and pruning the remaining heads leads to comparable or improved performance of the model.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":11,"sentence":"However, the impact of pruning attention heads is not yet clear in cross-lingual and multi-lingual tasks.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":11,"sentence":"Through extensive experiments, we show that (1) pruning a number of attention heads in a multi-lingual Transformer-based model has, in general, positive effects on its performance in cross-lingual and multi-lingual tasks and (2) the attention heads to be pruned can be ranked using gradients and identified with a few trial experiments.","offset":3,"pro":0.42857142857142855,"labels":"RST"},{"idx":11,"sentence":"Our experiments focus on sequence labeling tasks, with potential applicability on other cross-lingual and multi-lingual tasks.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":11,"sentence":"For comprehensiveness, we examine two pre-trained multi-lingual models, namely multi-lingual BERT (mBERT) and XLM-R, on three tasks across 9 languages each.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":11,"sentence":"We also discuss the validity of our findings and their extensibility to truly resource-scarce languages and other task settings.","offset":6,"pro":0.8571428571428571,"labels":"MTD"},{"idx":12,"sentence":"Effective adversary generation for neural machine translation (NMT) is a crucial prerequisite for building robust machine translation systems.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"In this work, we investigate veritable evaluations of NMT adversarial attacks, and propose a novel method to craft NMT adversarial examples.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":12,"sentence":"We first show the current NMT adversarial attacks may be improperly estimated by the commonly used mono-directional translation, and we propose to leverage the round-trip translation technique to build valid metrics for evaluating NMT adversarial attacks.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":12,"sentence":"Our intuition is that an effective NMT adversarial example, which imposes minor shifting on the source and degrades the translation dramatically, would naturally lead to a semantic-destroyed round-trip translation result.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":12,"sentence":"We then propose a promising black-box attack method called Word Saliency speedup Local Search (WSLS) that could effectively attack the mainstream NMT architectures.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":12,"sentence":"Comprehensive experiments demonstrate that the proposed metrics could accurately evaluate the attack effectiveness, and the proposed WSLS could significantly break the state-of-art NMT models with small perturbation.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":12,"sentence":"Besides, WSLS exhibits strong transferability on attacking Baidu and Bing online translators.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":13,"sentence":"Transfer learning has yielded state-of-the-art (SoTA) results in many supervised NLP tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"However, annotated data for every target task in every target language is rare, especially for low-resource languages.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":13,"sentence":"We propose UXLA, a novel unsupervised data augmentation framework for zero-resource transfer learning scenarios.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":13,"sentence":"In particular, UXLA aims to solve cross-lingual adaptation problems from a source language task distribution to an unknown target language task distribution, assuming no training label in the target language.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":13,"sentence":"At its core, UXLA performs simultaneous self-training with data augmentation and unsupervised sample selection.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":13,"sentence":"To show its effectiveness, we conduct extensive experiments on three diverse zero-resource cross-lingual transfer tasks.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":13,"sentence":"UXLA achieves SoTA results in all the tasks, outperforming the baselines by a good margin.","offset":6,"pro":0.75,"labels":"RST"},{"idx":13,"sentence":"With an in-depth framework dissection, we demonstrate the cumulative contributions of different components to its success.","offset":7,"pro":0.875,"labels":"RST"},{"idx":14,"sentence":"Recent work on non-autoregressive neural machine translation (NAT) aims at improving the efficiency by parallel decoding without sacrificing the quality.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"However, existing NAT methods are either inferior to Transformer or require multiple decoding passes, leading to reduced speedup.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":14,"sentence":"We propose the Glancing Language Model (GLM) for single-pass parallel generation models.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":14,"sentence":"With GLM, we develop Glancing Transformer (GLAT) for machine translation.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":14,"sentence":"With only single-pass parallel decoding, GLAT is able to generate high-quality translation with 8{mbox{times-15{mbox{times speedup.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":14,"sentence":"Note that GLAT does not modify the network architecture, which is a training method to learn word interdependency.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":14,"sentence":"Experiments on multiple WMT language directions show that GLAT outperforms all previous single pass non-autoregressive methods, and is nearly comparable to Transformer, reducing the gap to 0.25-0.9 BLEU points.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":15,"sentence":"Dense video event captioning aims to generate a sequence of descriptive captions for each event in a long untrimmed video.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"Video-level context provides important information and facilities the model to generate consistent and less redundant captions between events.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":15,"sentence":"In this paper, we introduce a novel Hierarchical Context-aware Network for dense video event captioning (HCN) to capture context from various aspects.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":15,"sentence":"In detail, the model leverages local and global context with different mechanisms to jointly learn to generate coherent captions.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":15,"sentence":"The local context module performs full interaction between neighbor frames and the global context module selectively attends to previous or future events.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":15,"sentence":"According to our extensive experiment on both Youcook2 and Activitynet Captioning datasets, the video-level HCN model outperforms the event-level context-agnostic model by a large margin.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":15,"sentence":"The code is available at https://github.com/KirkGuo/HCN.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":16,"sentence":"Generating image captions with user intention is an emerging need.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"The recently published Localized Narratives dataset takes mouse traces as another input to the image captioning task, which is an intuitive and efficient way for a user to control what to describe in the image.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":16,"sentence":"However, how to effectively employ traces to improve generation quality and controllability is still under exploration.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":16,"sentence":"This paper aims to solve this problem by proposing a novel model called LoopCAG, which connects Contrastive constraints and Attention Guidance in a Loop manner, engaged explicit spatial and temporal constraints to the generating process.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":16,"sentence":"Precisely, each generated sentence is temporally aligned to the corresponding trace sequence through a contrastive learning strategy.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":16,"sentence":"Besides, each generated text token is supervised to attend to the correct visual objects under heuristic spatial attention guidance.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":16,"sentence":"Comprehensive experimental results demonstrate that our LoopCAG model learns better correspondence among the three modalities (vision, language, and traces) and achieves SOTA performance on trace-controlled image captioning task.","offset":6,"pro":0.75,"labels":"CLN"},{"idx":16,"sentence":"Moreover, the controllability and explainability of LoopCAG are validated by analyzing spatial and temporal sensitivity during the generation process.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":17,"sentence":"Understanding manipulated media, from automatically generated deepfakes' to manually edited ones, raises novel research challenges.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"Because the vast majority of edited or manipulated images are benign, such as photoshopped images for visual enhancements, the key challenge is to understand the complex layers of underlying intents of media edits and their implications with respect to disinformation.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":17,"sentence":"In this paper, we study Edited Media Frames, a new formalism to understand visual media manipulation as structured annotations with respect to the intents, emotional reactions, attacks on individuals, and the overall implications of disinformation.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":17,"sentence":"We introduce a dataset for our task, EMU, with 56k question-answer pairs written in rich natural language.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":17,"sentence":"We evaluate a wide variety of vision-and-language models for our task, and introduce a new model PELICAN, which builds upon recent progress in pretrained multimodal representations.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":17,"sentence":"Our model obtains promising results on our dataset, with humans rating its answers as accurate 48.2% of the time.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":17,"sentence":"At the same time, there is still much work to be done - and we provide analysis that highlights areas for further progress.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":18,"sentence":"We propose PIGLeT: a model that learns physical commonsense knowledge through interaction, and then uses this knowledge to ground language.","offset":0,"pro":0,"labels":"MTD"},{"idx":18,"sentence":"We factorize PIGLeT into a physical dynamics model, and a separate language model.","offset":1,"pro":0.1111111111111111,"labels":"MTD"},{"idx":18,"sentence":"Our dynamics model learns not just what objects are but also what they do: glass cups break when thrown, plastic ones don't.","offset":2,"pro":0.2222222222222222,"labels":"RST"},{"idx":18,"sentence":"We then use it as the interface to our language model, giving us a unified model of linguistic form and grounded meaning.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":18,"sentence":"PIGLeT can read a sentence, simulate neurally what might happen next, and then communicate that result through a literal symbolic representation, or natural language.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":18,"sentence":"Experimental results show that our model effectively learns world dynamics, along with how to communicate them.","offset":5,"pro":0.5555555555555556,"labels":"RST"},{"idx":18,"sentence":"It is able to correctly forecast what happens next given an English sentence over 80% of the time, outperforming a 100x larger, text-to-text approach by over 10%.","offset":6,"pro":0.6666666666666666,"labels":"RST"},{"idx":18,"sentence":"Likewise, its natural language summaries of physical interactions are also judged by humans as more accurate than LM alternatives.","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":18,"sentence":"We present comprehensive analysis showing room for future work.","offset":8,"pro":0.8888888888888888,"labels":"CTN"},{"idx":19,"sentence":"Neural entity typing models typically represent fine-grained entity types as vectors in a high-dimensional space, but such spaces are not well-suited to modeling these types' complex interdependencies.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"We study the ability of box embeddings, which embed concepts as d-dimensional hyperrectangles, to capture hierarchies of types even when these relationships are not defined explicitly in the ontology.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":19,"sentence":"Our model represents both types and entity mentions as boxes.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":19,"sentence":"Each mention and its context are fed into a BERT-based model to embed that mention in our box space; essentially, this model leverages typological clues present in the surface text to hypothesize a type representation for the mention.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":19,"sentence":"Box containment can then be used to derive both the posterior probability of a mention exhibiting a given type and the conditional probability relations between types themselves.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":19,"sentence":"We compare our approach with a vector-based typing model and observe state-of-the-art performance on several entity typing benchmarks.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":19,"sentence":"In addition to competitive typing performance, our box-based model shows better performance in prediction consistency (predicting a supertype and a subtype together) and confidence (i.e., calibration), demonstrating that the box-based model captures the latent type hierarchies better than the vector-based model does.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":0,"sentence":"Recent pretraining models in Chinese neglect two important aspects specific to the Chinese language: glyph and pinyin, which carry significant syntax and semantic information for language understanding.","offset":0,"pro":0,"labels":"GAP"},{"idx":0,"sentence":"In this work, we propose ChineseBERT, which incorporates both the glyph and pinyin information of Chinese characters into language model pretraining.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":0,"sentence":"The glyph embedding is obtained based on different fonts of a Chinese character, being able to capture character semantics from the visual features, and the pinyin embedding characterizes the pronunciation of Chinese characters, which handles the highly prevalent heteronym phenomenon in Chinese (the same character has different pronunciations with different meanings).","offset":2,"pro":0.4,"labels":"MTD"},{"idx":0,"sentence":"Pretrained on large-scale unlabeled Chinese corpus, the proposed ChineseBERT model yields significant performance boost over baseline models with fewer training steps.","offset":3,"pro":0.6,"labels":"CLN"},{"idx":0,"sentence":"The proposed model achieves new SOTA performances on a wide range of Chinese NLP tasks, including machine reading comprehension, natural language inference, text classification, sentence pair matching, and competitive performances in named entity recognition and word segmentation.","offset":4,"pro":0.8,"labels":"CTN"},{"idx":1,"sentence":"Knowledge distillation has been proven to be effective in model acceleration and compression.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"It transfers knowledge from a large neural network to a small one by using the large neural network predictions as targets of the small neural network.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":1,"sentence":"But this way ignores the knowledge inside the large neural networks, e.g., parameters.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":1,"sentence":"Our preliminary study as well as the recent success in pre-training suggests that transferring parameters are more effective in distilling knowledge.","offset":3,"pro":0.42857142857142855,"labels":"CLN"},{"idx":1,"sentence":"In this paper, we propose Weight Distillation to transfer the knowledge in parameters of a large neural network to a small neural network through a parameter generator.","offset":4,"pro":0.5714285714285714,"labels":"PUR"},{"idx":1,"sentence":"On the WMT16 En-Ro, NIST12 Zh-En, and WMT14 En-De machine translation tasks, our experiments show that weight distillation learns a small network that is 1.88 2.94x faster than the large network but with competitive BLEU performance.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":1,"sentence":"When fixing the size of small networks, weight distillation outperforms knowledge distillation by 0.51 1.82 BLEU points.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":2,"sentence":"It is a common belief that training deep transformers from scratch requires large datasets.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"Consequently, for small datasets, people usually use shallow and simple additional layers on top of pre-trained models during fine-tuning.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":2,"sentence":"This work shows that this does not always need to be the case: with proper initialization and optimization, the benefits of very deep transformers can carry over to challenging tasks with small datasets, including Text-to-SQL semantic parsing and logical reading comprehension.","offset":2,"pro":0.2857142857142857,"labels":"CLN"},{"idx":2,"sentence":"In particular, we successfully train 48 layers of transformers, comprising 24 fine-tuned layers from pre-trained RoBERTa and 24 relation-aware layers trained from scratch.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":2,"sentence":"With fewer training steps and no task-specific pre-training, we obtain the state of the art performance on the challenging cross-domain Text-to-SQL parsing benchmark Spider.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":2,"sentence":"We achieve this by deriving a novel Data dependent Transformer Fixed-update initialization scheme (DT-Fixup), inspired by the prior T-Fixup work.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":2,"sentence":"Further error analysis shows that increasing depth can help improve generalization on small datasets for hard cases that require reasoning and structural understanding.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":3,"sentence":"Transformer-based language models (TLMs), such as BERT, ALBERT and GPT-3, have shown strong performance in a wide range of NLP tasks and currently dominate the field of NLP.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"However, many researchers wonder whether these models can maintain their dominance forever.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":3,"sentence":"Of course, we do not have answers now, but, as an attempt to find better neural architectures and training schemes, we pretrain a simple CNN using a GAN-style learning scheme and Wikipedia data, and then integrate it with standard TLMs.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":3,"sentence":"We show that on the GLUE tasks, the combination of our pretrained CNN with ALBERT outperforms the original ALBERT and achieves a similar performance to that of SOTA.","offset":3,"pro":0.42857142857142855,"labels":"CLN"},{"idx":3,"sentence":"Furthermore, on open-domain QA (Quasar-T and SearchQA), the combination of the CNN with ALBERT or RoBERTa achieved stronger performance than SOTA and the original TLMs.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":3,"sentence":"We hope that this work provides a hint for developing a novel strong network architecture along with its training scheme.","offset":5,"pro":0.7142857142857143,"labels":"IMP"},{"idx":3,"sentence":"Our source code and models are available at https://github.com/nict-wisdom/bertac.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":4,"sentence":"We introduce a FEVER-like dataset COVID-Fact of 4,086 claims concerning the COVID-19 pandemic.","offset":0,"pro":0,"labels":"PUR"},{"idx":4,"sentence":"The dataset contains claims, evidence for the claims, and contradictory claims refuted by the evidence.","offset":1,"pro":0.14285714285714285,"labels":"MTD"},{"idx":4,"sentence":"Unlike previous approaches, we automatically detect true claims and their source articles and then generate counter-claims using automatic methods rather than employing human annotators.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":4,"sentence":"Along with our constructed resource, we formally present the task of identifying relevant evidence for the claims and verifying whether the evidence refutes or supports a given claim.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":4,"sentence":"In addition to scientific claims, our data contains simplified general claims from media sources, making it better suited for detecting general misinformation regarding COVID-19.","offset":4,"pro":0.5714285714285714,"labels":"CTN"},{"idx":4,"sentence":"Our experiments indicate that COVID-Fact will provide a challenging testbed for the development of new systems","offset":5,"pro":0.7142857142857143,"labels":"IMP"},{"idx":4,"sentence":"and our approach will reduce the costs of building domain-specific datasets for detecting misinformation.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":5,"sentence":"We address the task of explaining relationships between two scientific documents using natural language text.","offset":0,"pro":0,"labels":"PUR"},{"idx":5,"sentence":"This task requires modeling the complex content of long technical documents, deducing a relationship between these documents, and expressing the details of that relationship in text.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":5,"sentence":"In addition to the theoretical interest of this task, successful solutions can help improve researcher efficiency in search and review.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":5,"sentence":"In this paper we establish a dataset of 622K examples from 154K documents.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":5,"sentence":"We pretrain a large language model to serve as the foundation for autoregressive approaches to the task.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":5,"sentence":"We explore the impact of taking different views on the two documents, including the use of dense representations extracted with scientific IE systems.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":5,"sentence":"We provide extensive automatic and human evaluations which show the promise of such models, but make clear challenges for future work.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":6,"sentence":"Existing software-based energy measurements of NLP models are not accurate because they do not consider the complex interactions between energy consumption and model execution.","offset":0,"pro":0,"labels":"GAP"},{"idx":6,"sentence":"We present IrEne, an interpretable and extensible energy prediction system that accurately predicts the inference energy consumption of a wide range of Transformer-based NLP models.","offset":1,"pro":0.1111111111111111,"labels":"MTD"},{"idx":6,"sentence":"IrEne constructs a model tree graph that breaks down the NLP model into modules that are further broken down into low-level machine learning (ML) primitives.","offset":2,"pro":0.2222222222222222,"labels":"MTD"},{"idx":6,"sentence":"IrEne predicts the inference energy consumption of the ML primitives as a function of generalizable features and fine-grained runtime resource usage.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":6,"sentence":"IrEne then aggregates these low-level predictions recursively to predict the energy of each module and finally of the entire model.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":6,"sentence":"Experiments across multiple Transformer models show IrEne predicts inference energy consumption of transformer models with an error of under 7% compared to the ground truth.","offset":5,"pro":0.5555555555555556,"labels":"RST"},{"idx":6,"sentence":"In contrast, existing energy models see an error of over 50%.","offset":6,"pro":0.6666666666666666,"labels":"RST"},{"idx":6,"sentence":"We also show how IrEne can be used to conduct energy bottleneck analysis and to easily evaluate the energy impact of different architectural choices.","offset":7,"pro":0.7777777777777778,"labels":"CLN"},{"idx":6,"sentence":"We release the code and data at https://github.com/StonyBrookNLP/irene.","offset":8,"pro":0.8888888888888888,"labels":"CTN"},{"idx":7,"sentence":"The element of repetition in cyberbullying behavior has directed recent computational studies toward detecting cyberbullying based on a social media session.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"In contrast to a single text, a session may consist of an initial post and an associated sequence of comments.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":7,"sentence":"Yet, emerging efforts to enhance the performance of session-based cyberbullying detection have largely overlooked unintended social biases in existing cyberbullying datasets.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":7,"sentence":"For example, a session containing certain demographic-identity terms (e.g., gay or black) is more likely to be classified as an instance of cyberbullying.","offset":3,"pro":0.42857142857142855,"labels":"GAP"},{"idx":7,"sentence":"In this paper, we first show evidence of such bias in models trained on sessions collected from different social media platforms (e.g., Instagram).","offset":4,"pro":0.5714285714285714,"labels":"PUR"},{"idx":7,"sentence":"We then propose a context-aware and model-agnostic debiasing strategy that leverages a reinforcement learning technique, without requiring any extra resources or annotations apart from a pre-defined set of sensitive triggers commonly used for identifying cyberbullying instances.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":7,"sentence":"Empirical evaluations show that the proposed strategy can simultaneously alleviate the impacts of the unintended biases and improve the detection performance.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":8,"sentence":"Creating effective visualization is an important part of data analytics.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"While there are many libraries for creating visualization, writing such code remains difficult given the myriad of parameters that users need to provide.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":8,"sentence":"In this paper, we propose the new task of synthesizing visualization programs from a combination of natural language utterances and code context.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":8,"sentence":"To tackle the learning problem, we introduce PlotCoder, a new hierarchical encoder-decoder architecture that models both the code context and the input utterance.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":8,"sentence":"We use PlotCoder to first determine the template of the visualization code, followed by predicting the data to be plotted.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":8,"sentence":"We use Jupyter notebooks containing visualization programs crawled from GitHub to train PlotCoder.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":8,"sentence":"On a comprehensive set of test samples from those notebooks, we show that PlotCoder correctly predicts the plot type of about 70% samples, and synthesizes the correct programs for 35% samples, performing 3-4.5% better than the baselines.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":9,"sentence":"NLP community is currently investing a lot more research and resources into development of deep learning models than training data.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"While we have made a lot of progress, it is now clear that our models learn all kinds of spurious patterns, social biases, and annotation artifacts.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":9,"sentence":"Algorithmic solutions have so far had limited success.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":9,"sentence":"An alternative that is being actively discussed is more careful design of datasets so as to deliver specific signals.","offset":3,"pro":0.5,"labels":"BAC"},{"idx":9,"sentence":"This position paper maps out the arguments for and against data curation, and argues that fundamentally the point is moot: curation already is and will be happening, and it is changing the world.","offset":4,"pro":0.6666666666666666,"labels":"PUR"},{"idx":9,"sentence":"The question is only how much thought we want to invest into that process.","offset":5,"pro":0.8333333333333334,"labels":"PUR"},{"idx":10,"sentence":"Heavily overparameterized language models such as BERT, XLNet and T5 have achieved impressive success in many NLP tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"However, their high model complexity requires enormous computation resources and extremely long training time for both pre-training and fine-tuning.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":10,"sentence":"Many works have studied model compression on large NLP models, but only focusing on reducing inference time while still requiring an expensive training process.","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":10,"sentence":"Other works use extremely large batch sizes to shorten the pre-training time, at the expense of higher computational resource demands.","offset":3,"pro":0.3333333333333333,"labels":"GAP"},{"idx":10,"sentence":"In this paper, inspired by the Early-Bird Lottery Tickets recently studied for computer vision tasks, we propose EarlyBERT, a general computationally-efficient training algorithm applicable to both pre-training and fine-tuning of large-scale language models.","offset":4,"pro":0.4444444444444444,"labels":"PUR"},{"idx":10,"sentence":"By slimming the self-attention and fully-connected sub-layers inside a transformer, we are the first to identify structured winning tickets in the early stage of BERT training.","offset":5,"pro":0.5555555555555556,"labels":"RST"},{"idx":10,"sentence":"We apply those tickets towards efficient BERT training, and conduct comprehensive pre-training and fine-tuning experiments on GLUE and SQuAD downstream tasks.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":10,"sentence":"Our results show that EarlyBERT achieves comparable performance to standard BERT, with 35 45% less training time.","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":10,"sentence":"Code is available at https://github.com/VITA-Group/EarlyBERT.","offset":8,"pro":0.8888888888888888,"labels":"CTN"},{"idx":11,"sentence":"Adapter-based tuning has recently arisen as an alternative to fine-tuning.","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"It works by adding light-weight adapter modules to a pretrained language model (PrLM) and only updating the parameters of adapter modules when learning on a downstream task.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":11,"sentence":"As such, it adds only a few trainable parameters per new task, allowing a high degree of parameter sharing.","offset":2,"pro":0.2222222222222222,"labels":"MTD"},{"idx":11,"sentence":"Prior studies have shown that adapter-based tuning often achieves comparable results to fine-tuning.","offset":3,"pro":0.3333333333333333,"labels":"BAC"},{"idx":11,"sentence":"However, existing work only focuses on the parameter-efficient aspect of adapter-based tuning while lacking further investigation on its effectiveness.","offset":4,"pro":0.4444444444444444,"labels":"GAP"},{"idx":11,"sentence":"In this paper, we study the latter.","offset":5,"pro":0.5555555555555556,"labels":"PUR"},{"idx":11,"sentence":"We first show that adapter-based tuning better mitigates forgetting issues than fine-tuning since it yields representations with less deviation from those generated by the initial PrLM.","offset":6,"pro":0.6666666666666666,"labels":"RST"},{"idx":11,"sentence":"We then empirically compare the two tuning methods on several downstream NLP tasks and settings.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":11,"sentence":"We demonstrate that 1) adapter-based tuning outperforms fine-tuning on low-resource and cross-lingual tasks; 2) it is more robust to overfitting and less sensitive to changes in learning rates.","offset":8,"pro":0.8888888888888888,"labels":"RST"},{"idx":12,"sentence":"Data augmentation is an effective way to improve the performance of many neural text generation models.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"However, current data augmentation methods need to define or choose proper data mapping functions that map the original samples into the augmented samples.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":12,"sentence":"In this work, we derive an objective to formulate the problem of data augmentation on text generation tasks without any use of augmented data constructed by specific mapping functions.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":12,"sentence":"Our proposed objective can be efficiently optimized and applied to popular loss functions on text generation tasks with a convergence rate guarantee.","offset":3,"pro":0.6,"labels":"RST"},{"idx":12,"sentence":"Experiments on five datasets of two text generation tasks show that our approach can approximate or even surpass popular data augmentation methods.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":13,"sentence":"With the need of fast retrieval speed and small memory footprint, document hashing has been playing a crucial role in large-scale information retrieval.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"To generate high-quality hashing code, both semantics and neighborhood information are crucial.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":13,"sentence":"However, most existing methods leverage only one of them or simply combine them via some intuitive criteria, lacking a theoretical principle to guide the integration process.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":13,"sentence":"In this paper, we encode the neighborhood information with a graph-induced Gaussian distribution, and propose to integrate the two types of information with a graph-driven generative model.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":13,"sentence":"To deal with the complicated correlations among documents, we further propose a tree-structured approximation method for learning.","offset":4,"pro":0.5714285714285714,"labels":"PUR"},{"idx":13,"sentence":"Under the approximation, we prove that the training objective can be decomposed into terms involving only singleton or pairwise documents, enabling the model to be trained as efficiently as uncorrelated ones.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":13,"sentence":"Extensive experimental results on three benchmark datasets show that our method achieves superior performance over state-of-the-art methods, demonstrating the effectiveness of the proposed model for simultaneously preserving semantic and neighborhood information.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":14,"sentence":"The open-ended nature of visual captioning makes it a challenging area for evaluation.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"The majority of proposed models rely on specialized training to improve human-correlation, resulting in limited adoption, generalizability, and explainabilty.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":14,"sentence":"We introduce typicality, a new formulation of evaluation rooted in information theory, which is uniquely suited for problems lacking a definite ground truth.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":14,"sentence":"Typicality serves as our framework to develop a novel semantic comparison, SPARCS, as well as referenceless fluency evaluation metrics.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":14,"sentence":"Over the course of our analysis, two separate dimensions of fluency naturally emerge: style, captured by metric SPURTS, and grammar, captured in the form of grammatical outlier penalties.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":14,"sentence":"Through extensive experiments and ablation studies on benchmark datasets, we show how these decomposed dimensions of semantics and fluency provide greater system-level insight into captioner differences.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":14,"sentence":"Our proposed metrics along with their combination, SMURF, achieve state-of-the-art correlation with human judgment when compared with other rule-based evaluation metrics.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":15,"sentence":"The goal of database question answering is to enable natural language querying of real-life relational databases in diverse application domains.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"Recently, large-scale datasets such as Spider and WikiSQL facilitated novel modeling techniques for text-to-SQL parsing, improving zero-shot generalization to unseen databases.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":15,"sentence":"In this work, we examine the challenges that still prevent these techniques from practical deployment.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":15,"sentence":"First, we present KaggleDBQA, a new cross-domain evaluation dataset of real Web databases, with domain-specific data types, original formatting, and unrestricted questions.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":15,"sentence":"Second, we re-examine the choice of evaluation tasks for text-to-SQL parsers as applied in real-life settings.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":15,"sentence":"Finally, we augment our in-domain evaluation task with database documentation, a naturally occurring source of implicit domain knowledge.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":15,"sentence":"We show that KaggleDBQA presents a challenge to state-of-the-art zero-shot parsers but a more realistic evaluation setting and creative use of associated database documentation boosts their accuracy by over 13.2%, doubling their performance.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":16,"sentence":"We introduce the largest transcribed Arabic speech corpus, QASR, collected from the broadcast domain.","offset":0,"pro":0,"labels":"MTD"},{"idx":16,"sentence":"This multi-dialect speech dataset contains 2,000 hours of speech sampled at 16kHz crawled from Aljazeera news channel.","offset":1,"pro":0.1,"labels":"MTD"},{"idx":16,"sentence":"The dataset is released with lightly supervised transcriptions, aligned with the audio segments.","offset":2,"pro":0.2,"labels":"MTD"},{"idx":16,"sentence":"Unlike previous datasets, QASR contains linguistically motivated segmentation, punctuation, speaker information among others.","offset":3,"pro":0.3,"labels":"MTD"},{"idx":16,"sentence":"QASR is suitable for training and evaluating speech recognition systems, acoustics- and/or linguistics- based Arabic dialect identification, punctuation restoration, speaker identification, speaker linking, and potentially other NLP modules for spoken data.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":16,"sentence":"In addition to QASR transcription, we release a dataset of 130M words to aid in designing and training a better language model.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":16,"sentence":"We show that end-to-end automatic speech recognition trained on QASR reports a competitive word error rate compared to the previous MGB-2 corpus.","offset":6,"pro":0.6,"labels":"RST"},{"idx":16,"sentence":"We report baseline results for downstream natural language processing tasks such as named entity recognition using speech transcript.","offset":7,"pro":0.7,"labels":"RST"},{"idx":16,"sentence":"We also report the first baseline for Arabic punctuation restoration.","offset":8,"pro":0.8,"labels":"RST"},{"idx":16,"sentence":"We make the corpus available for the research community.","offset":9,"pro":0.9,"labels":"CTN"},{"idx":17,"sentence":"The performance of fine-tuning pre-trained language models largely depends on the hyperparameter configuration.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"In this paper, we investigate the performance of modern hyperparameter optimization methods (HPO) on fine-tuning pre-trained language models.","offset":1,"pro":0.125,"labels":"PUR"},{"idx":17,"sentence":"First, we study and report three HPO algorithms' performances on fine-tuning two state-of-the-art language models on the GLUE dataset.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":17,"sentence":"We find that using the same time budget, HPO often fails to outperform grid search due to two reasons: insufficient time budget and overfitting.","offset":3,"pro":0.375,"labels":"RST"},{"idx":17,"sentence":"We propose two general strategies and an experimental procedure to systematically troubleshoot HPO's failure cases.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":17,"sentence":"By applying the procedure, we observe that HPO can succeed with more appropriate settings in the search space and time budget; however, in certain cases overfitting remains.","offset":5,"pro":0.625,"labels":"RST"},{"idx":17,"sentence":"Finally, we make suggestions for future work.","offset":6,"pro":0.75,"labels":"BAC"},{"idx":17,"sentence":"Our implementation can be found in url{https://github.com/microsoft/FLAML/tree/main/flaml/nlp/","offset":7,"pro":0.875,"labels":"CTN"},{"idx":18,"sentence":"Evaluation in NLP is usually done by comparing the scores of competing systems independently averaged over a common set of test instances.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"In this work, we question the use of averages for aggregating evaluation scores into a final number used to decide which system is best, since the average, as well as alternatives such as the median, ignores the pairing arising from the fact that systems are evaluated on the same test instances.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":18,"sentence":"We illustrate the importance of taking the instancelevel pairing of evaluation scores into account and demonstrate, both theoretically and empirically, the advantages of aggregation methods based on pairwise comparisons, such as the Bradley-Terry (BT) model, a mechanism based on the estimated probability that a given system scores better than another on the test set.","offset":2,"pro":0.4,"labels":"CLN"},{"idx":18,"sentence":"By re-evaluating 296 real NLP evaluation setups across four tasks and 18 evaluation metrics, we show that the choice of aggregation mechanism matters and yields different conclusions as to which systems are state of the art in about 30% of the setups.","offset":3,"pro":0.6,"labels":"RST"},{"idx":18,"sentence":"To facilitate the adoption of pairwise evaluation, we release a practical tool for performing the full analysis of evaluation scores with the mean, median, BT, and two variants of BT (Elo and TrueSkill), alongside functionality for appropriate statistical testing.","offset":4,"pro":0.8,"labels":"PUR"},{"idx":19,"sentence":"The cross-database context-dependent Text-to-SQL (XDTS) problem has attracted considerable attention in recent years due to its wide range of potential applications.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"However, we identify two biases in existing datasets for XDTS: (1) a high proportion of context-independent questions and (2) a high proportion of easy SQL queries.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":19,"sentence":"These biases conceal the major challenges in XDTS to some extent.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":19,"sentence":"In this work, we present Chase, a large-scale and pragmatic Chinese dataset for XDTS.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":19,"sentence":"It consists of 5,459 coherent question sequences (17,940 questions with their SQL queries annotated) over 280 databases, in which only 35% of questions are context-independent, and 28% of SQL queries are easy.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":19,"sentence":"We experiment on Chase with three state-of-the-art XDTS approaches.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":19,"sentence":"The best approach only achieves an exact match accuracy of 40% over all questions and 16% over all question sequences, indicating that Chase highlights the challenging problems of XDTS.","offset":6,"pro":0.75,"labels":"RST"},{"idx":19,"sentence":"We believe that XDTS can provide fertile soil for addressing the problems.","offset":7,"pro":0.875,"labels":"PUR"},{"idx":0,"sentence":"Despite pre-trained language models have proven useful for learning high-quality semantic representations, these models are still vulnerable to simple perturbations.","offset":0,"pro":0,"labels":"GAP"},{"idx":0,"sentence":"Recent works aimed to improve the robustness of pre-trained models mainly focus on adversarial training from perturbed examples with similar semantics, neglecting the utilization of different or even opposite semantics.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":0,"sentence":"Different from the image processing field, the text is discrete and few word substitutions can cause significant semantic changes.","offset":2,"pro":0.25,"labels":"BAC"},{"idx":0,"sentence":"To study the impact of semantics caused by small perturbations, we conduct a series of pilot experiments and surprisingly find that adversarial training is useless or even harmful for the model to detect these semantic changes.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":0,"sentence":"To address this problem, we propose Contrastive Learning with semantIc Negative Examples (CLINE), which constructs semantic negative examples unsupervised to improve the robustness under semantically adversarial attacking.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":0,"sentence":"By comparing with similar and opposite semantic examples, the model can effectively perceive the semantic changes caused by small perturbations.","offset":5,"pro":0.625,"labels":"RST"},{"idx":0,"sentence":"Empirical results show that our approach yields substantial improvements on a range of sentiment analysis, reasoning, and reading comprehension tasks.","offset":6,"pro":0.75,"labels":"CLN"},{"idx":0,"sentence":"And CLINE also ensures the compactness within the same semantics and separability across different semantics in sentence-level.","offset":7,"pro":0.875,"labels":"RST"},{"idx":1,"sentence":"Topic modeling has been widely used for discovering the latent semantic structure of documents, but most existing methods learn topics with a flat structure.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"Although probabilistic models can generate topic hierarchies by introducing nonparametric priors like Chinese restaurant process, such methods have data scalability issues.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":1,"sentence":"In this study, we develop a tree-structured topic model by leveraging nonparametric neural variational inference.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":1,"sentence":"Particularly, the latent components of the stick-breaking process are first learned for each document, then the affiliations of latent components are modeled by the dependency matrices between network layers.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":1,"sentence":"Utilizing this network structure, we can efficiently extract a tree-structured topic hierarchy with reasonable structure, low redundancy, and adaptable widths.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":1,"sentence":"Experiments on real-world datasets validate the effectiveness of our method.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":2,"sentence":"Prior work infers the causation between events mainly based on the knowledge induced from the annotated causal event pairs.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"However, additional evidence information intermediate to the cause and effect remains unexploited.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":2,"sentence":"By incorporating such information, the logical law behind the causality can be unveiled, and the interpretability and stability of the causal reasoning system can be improved.","offset":2,"pro":0.2222222222222222,"labels":"BAC"},{"idx":2,"sentence":"To facilitate this, we present an Event graph knowledge enhanced explainable CAusal Reasoning framework (ExCAR).","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":2,"sentence":"ExCAR first acquires additional evidence information from a large-scale causal event graph as logical rules for causal reasoning.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":2,"sentence":"To learn the conditional probabilistic of logical rules, we propose the Conditional Markov Neural Logic Network (CMNLN) that combines the representation learning and structure learning of logical rules in an end-to-end differentiable manner.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":2,"sentence":"Experimental results demonstrate that ExCAR outperforms previous state-of-the-art methods.","offset":6,"pro":0.6666666666666666,"labels":"CLN"},{"idx":2,"sentence":"Adversarial evaluation shows the improved stability of ExCAR over baseline systems.","offset":7,"pro":0.7777777777777778,"labels":"CLN"},{"idx":2,"sentence":"Human evaluation shows that ExCAR can achieve a promising explainable performance.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":3,"sentence":"Emotion category is usually divided into different ones by human beings, but it is indeed difficult to clearly distinguish and define the boundaries between different emotion categories.","offset":0,"pro":0,"labels":"GAP"},{"idx":3,"sentence":"The existing studies working on emotion detection usually focus on how to improve the performance of model prediction, in which emotions are represented with one-hot vectors.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":3,"sentence":"However, emotion relations are ignored in one-hot representations.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":3,"sentence":"In this article, we first propose a general framework to learn the distributed representations for emotion categories in emotion space from a given emotion classification dataset.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":3,"sentence":"Furthermore, based on the soft labels predicted by the pre-trained neural network model, we derive a simple and effective algorithm.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":3,"sentence":"Experiments have validated that the proposed representations in emotion space can express emotion relations much better than word vectors in semantic space.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":4,"sentence":"Every natural text is written in some style.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"Style is formed by a complex combination of different stylistic factors, including formality markers, emotions, metaphors, etc. One cannot form a complete understanding of a text without considering these factors.","offset":1,"pro":0.07692307692307693,"labels":"BAC"},{"idx":4,"sentence":"The factors combine and co-vary in complex ways to form styles.","offset":2,"pro":0.15384615384615385,"labels":"MTD"},{"idx":4,"sentence":"Studying the nature of the covarying combinations sheds light on stylistic language in general, sometimes called cross-style language understanding.","offset":3,"pro":0.23076923076923078,"labels":"CTN"},{"idx":4,"sentence":"This paper provides the benchmark corpus (XSLUE) that combines existing datasets and collects a new one for sentence-level cross-style language understanding and evaluation.","offset":4,"pro":0.3076923076923077,"labels":"PUR"},{"idx":4,"sentence":"The benchmark contains text in 15 different styles under the proposed four theoretical groupings: figurative, personal, affective, and interpersonal groups.","offset":5,"pro":0.38461538461538464,"labels":"MTD"},{"idx":4,"sentence":"For valid evaluation, we collect an additional diagnostic set by annotating all 15 styles on the same text.","offset":6,"pro":0.46153846153846156,"labels":"MTD"},{"idx":4,"sentence":"Using XSLUE, we propose three interesting cross-style applications in classification, correlation, and generation.","offset":7,"pro":0.5384615384615384,"labels":"MTD"},{"idx":4,"sentence":"First, our proposed cross-style classifier trained with multiple styles together helps improve overall classification performance against individually-trained style classifiers.","offset":8,"pro":0.6153846153846154,"labels":"RST"},{"idx":4,"sentence":"Second, our study shows that some styles are highly dependent on each other in human-written text.","offset":9,"pro":0.6923076923076923,"labels":"PUR"},{"idx":4,"sentence":"Finally, we find that combinations of some contradictive styles likely generate stylistically less appropriate text.","offset":10,"pro":0.7692307692307693,"labels":"RST"},{"idx":4,"sentence":"We believe our benchmark and case studies help explore interesting future directions for cross-style research.","offset":11,"pro":0.8461538461538461,"labels":"CLN"},{"idx":4,"sentence":"The preprocessed datasets and code are publicly available.","offset":12,"pro":0.9230769230769231,"labels":"CTN"},{"idx":5,"sentence":"We introduce DynaSent (Dynamic Sentiment'), a new English-language benchmark task for ternary (positive/negative/neutral) sentiment analysis.","offset":0,"pro":0,"labels":"PUR"},{"idx":5,"sentence":"DynaSent combines naturally occurring sentences with sentences created using the open-source Dynabench Platform, which facilities human-and-model-in-the-loop dataset creation.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":5,"sentence":"DynaSent has a total of 121,634 sentences, each validated by five crowdworkers, and its development and test splits are designed to produce chance performance for even the best models we have been able to develop; when future models solve this task, we will use them to create DynaSent version 2, continuing the dynamic evolution of this benchmark.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":5,"sentence":"Here, we report on the dataset creation effort, focusing on the steps we took to increase quality and reduce artifacts.","offset":3,"pro":0.6,"labels":"PUR"},{"idx":5,"sentence":"We also present evidence that DynaSent's Neutral category is more coherent than the comparable category in other benchmarks, and we motivate training models from scratch for each round over successive fine-tuning.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":6,"sentence":"In this digital age, online users expect personalized content.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"To cater to diverse group of audiences across online platforms it is necessary to generate multiple variants of same content with differing degree of characteristics (sentiment, style, formality, etc.).","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":6,"sentence":"Though text-style transfer is a well explored related area, it focuses on flipping the style attribute polarity instead of regulating a fine-grained attribute transfer.","offset":2,"pro":0.3333333333333333,"labels":"BAC"},{"idx":6,"sentence":"In this paper we propose a hierarchical architecture for finer control over the at- tribute, preserving content using attribute dis- entanglement.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":6,"sentence":"We demonstrate the effective- ness of the generative process for two different attributes with varied complexity, namely sentiment and formality.","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":6,"sentence":"With extensive experiments and human evaluation on five real-world datasets, we show that the framework can generate natural looking sentences with finer degree of control of intensity of a given attribute.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":7,"sentence":"Aspect-based Sentiment Analysis (ABSA) aims to identify the aspect terms, their corresponding sentiment polarities, and the opinion terms.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"There exist seven subtasks in ABSA.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":7,"sentence":"Most studies only focus on the subsets of these subtasks, which leads to various complicated ABSA models while hard to solve these subtasks in a unified framework.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":7,"sentence":"In this paper, we redefine every subtask target as a sequence mixed by pointer indexes and sentiment class indexes, which converts all ABSA subtasks into a unified generative formulation.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":7,"sentence":"Based on the unified formulation, we exploit the pre-training sequence-to-sequence model BART to solve all ABSA subtasks in an end-to-end framework.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":7,"sentence":"Extensive experiments on four ABSA datasets for seven subtasks demonstrate that our framework achieves substantial performance gain and provides a real unified end-to-end solution for the whole ABSA subtasks, which could benefit multiple tasks.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":8,"sentence":"Task-oriented dialogue systems typically require manual annotation of dialogue slots in training data, which is costly to obtain.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"We propose a method that eliminates this requirement: We use weak supervision from existing linguistic annotation models to identify potential slot candidates, then automatically identify domain-relevant slots by using clustering algorithms.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":8,"sentence":"Furthermore, we use the resulting slot annotation to train a neural-network-based tagger that is able to perform slot tagging with no human intervention.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":8,"sentence":"This tagger is trained solely on the outputs of our method and thus does not rely on any labeled data.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":8,"sentence":"Our model demonstrates state-of-the-art performance in slot tagging without labeled training data on four different dialogue domains.","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":8,"sentence":"Moreover, we find that slot annotations discovered by our model significantly improve the performance of an end-to-end dialogue response generation model, compared to using no slot annotation at all.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":9,"sentence":"Intent classification is a major task in spoken language understanding (SLU).","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"Since most models are built with pre-collected in-domain (IND) training utterances, their ability to detect unsupported out-of-domain (OOD) utterances has a critical effect in practical use.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":9,"sentence":"Recent works have shown that using extra data and labels can improve the OOD detection performance, yet it could be costly to collect such data.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":9,"sentence":"This paper proposes to train a model with only IND data while supporting both IND intent classification and OOD detection.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":9,"sentence":"Our method designs a novel domain-regularized module (DRM) to reduce the overconfident phenomenon of a vanilla classifier, achieving a better generalization in both cases.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":9,"sentence":"Besides, DRM can be used as a drop-in replacement for the last layer in any neural network-based intent classifier, providing a low-cost strategy for a significant improvement.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":9,"sentence":"The evaluation on four datasets shows that our method built on BERT and RoBERTa models achieves state-of-the-art performance against existing approaches and the strong baselines we created for the comparisons.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":10,"sentence":"Recent research considers few-shot intent detection as a meta-learning problem: the model is learning to learn from a consecutive set of small tasks named episodes.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"In this work, we propose ProtAugment, a meta-learning algorithm for short texts classification (the intent detection task).","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":10,"sentence":"ProtAugment is a novel extension of Prototypical Networks, that limits overfitting on the bias introduced by the few-shots classification objective at each episode.","offset":2,"pro":0.3333333333333333,"labels":"BAC"},{"idx":10,"sentence":"It relies on diverse paraphrasing: a conditional language model is first fine-tuned for paraphrasing, and diversity is later introduced at the decoding stage at each meta-learning episode.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":10,"sentence":"The diverse paraphrasing is unsupervised as it is applied to unlabelled data, and then fueled to the Prototypical Network training objective as a consistency loss.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":10,"sentence":"ProtAugment is the state-of-the-art method for intent detection meta-learning, at no extra labeling efforts and without the need to fine-tune a conditional language model on a given application domain.","offset":5,"pro":0.8333333333333334,"labels":"BAC"},{"idx":11,"sentence":"Most language understanding models in task-oriented dialog systems are trained on a small amount of annotated training data, and evaluated in a small set from the same distribution.","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"However, these models can lead to system failure or undesirable output when being exposed to natural language perturbation or variation in practice.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":11,"sentence":"In this paper, we conduct comprehensive evaluation and analysis with respect to the robustness of natural language understanding models, and introduce three important aspects related to language understanding in real-world dialog systems, namely, language variety, speech characteristics, and noise perturbation.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":11,"sentence":"We propose a model-agnostic toolkit LAUG to approximate natural language perturbations for testing the robustness issues in task-oriented dialog.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":11,"sentence":"Four data augmentation approaches covering the three aspects are assembled in LAUG, which reveals critical robustness issues in state-of-the-art models.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":11,"sentence":"The augmented dataset through LAUG can be used to facilitate future research on the robustness testing of language understanding in task-oriented dialog.","offset":5,"pro":0.8333333333333334,"labels":"IMP"},{"idx":12,"sentence":"Dialogue state tracking (DST) plays a key role in task-oriented dialogue systems to monitor the user's goal.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"In general, there are two strategies to track a dialogue state: predicting it from scratch and updating it from previous state.","offset":1,"pro":0.1,"labels":"MTD"},{"idx":12,"sentence":"The scratch-based strategy obtains each slot value by inquiring all the dialogue history, and the previous-based strategy relies on the current turn dialogue to update the previous dialogue state.","offset":2,"pro":0.2,"labels":"MTD"},{"idx":12,"sentence":"However, it is hard for the scratch-based strategy to correctly track short-dependency dialogue state because of noise; meanwhile, the previous-based strategy is not very useful for long-dependency dialogue state tracking.","offset":3,"pro":0.3,"labels":"GAP"},{"idx":12,"sentence":"Obviously, it plays different roles for the context information of different granularity to track different kinds of dialogue states.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":12,"sentence":"Thus, in this paper, we will study and discuss how the context information of different granularity affects dialogue state tracking.","offset":5,"pro":0.5,"labels":"PUR"},{"idx":12,"sentence":"First, we explore how greatly different granularities affect dialogue state tracking.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":12,"sentence":"Then, we further discuss how to combine multiple granularities for dialogue state tracking.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":12,"sentence":"Finally, we apply the findings about context granularity to few-shot learning scenario.","offset":8,"pro":0.8,"labels":"MTD"},{"idx":12,"sentence":"Besides, we have publicly released all codes.","offset":9,"pro":0.9,"labels":"CTN"},{"idx":13,"sentence":"Mixed initiative in open-domain dialogue requires a system to pro-actively introduce new topics.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"The one-turn topic transition task explores how a system connects two topics in a cooperative and coherent manner.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":13,"sentence":"The goal of the task is to generate a bridging utterance connecting the new topic to the topic of the previous conversation turn.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":13,"sentence":"We are especially interested in commonsense explanations of how a new topic relates to what has been mentioned before.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":13,"sentence":"We first collect a new dataset of human one-turn topic transitions, which we callOTTers.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":13,"sentence":"We then explore different strategies used by humans when asked to complete such a task, and notice that the use of a bridging utterance to connect the two topics is the approach used the most.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":13,"sentence":"We finally show how existing state-of-the-art text generation models can be adapted to this task and examine the performance of these baselines on different splits of the OTTers data.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":14,"sentence":"Recently, there has been significant progress in studying neural networks to translate text descriptions into SQL queries.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"Despite achieving good performance on some public benchmarks, existing text-to-SQL models typically rely on the lexical matching between words in natural language (NL) questions and tokens in table schemas, which may render the models vulnerable to attacks that break the schema linking mechanism.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":14,"sentence":"In this work, we investigate the robustness of text-to-SQL models to synonym substitution.","offset":2,"pro":0.2222222222222222,"labels":"PUR"},{"idx":14,"sentence":"In particular, we introduce Spider-Syn, a human-curated dataset based on the Spider benchmark for text-to-SQL translation.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":14,"sentence":"NL questions in Spider-Syn are modified from Spider, by replacing their schema-related words with manually selected synonyms that reflect real-world question paraphrases.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":14,"sentence":"We observe that the accuracy dramatically drops by eliminating such explicit correspondence between NL questions and table schemas, even if the synonyms are not adversarially selected to conduct worst-case attacks.","offset":5,"pro":0.5555555555555556,"labels":"RST"},{"idx":14,"sentence":"Finally, we present two categories of approaches to improve the model robustness.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":14,"sentence":"The first category of approaches utilizes additional synonym annotations for table schemas by modifying the model input, while the second category is based on adversarial training.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":14,"sentence":"We demonstrate that both categories of approaches significantly outperform their counterparts without the defense, and the first category of approaches are more effective.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":15,"sentence":"In order to better understand the reason behind model behaviors (i.e., making predictions), most recent works have exploited generative models to provide complementary explanations.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"However, existing approaches in NLP mainly focus on WHY A rather than contrastive WHY A NOT B, which is shown to be able to better distinguish confusing candidates and improve data efficiency in other research fields.In this paper, we focus on generating contrastive explanations with counterfactual examples in NLI and propose a novel Knowledge-Aware Contrastive Explanation generation framework (KACE).Specifically, we first identify rationales (i.e., key phrases) from input sentences, and use them as key perturbations for generating counterfactual examples.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":15,"sentence":"After obtaining qualified counterfactual examples, we take them along with original examples and external knowledge as input, and employ a knowledge-aware generative pre-trained language model to generate contrastive explanations.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":15,"sentence":"Experimental results show that contrastive explanations are beneficial to fit the scenarios by clarifying the difference between the predicted answer and other possible wrong ones.","offset":3,"pro":0.6,"labels":"RST"},{"idx":15,"sentence":"Moreover, we train an NLI model enhanced with contrastive explanations and achieves an accuracy of 91.9% on SNLI, gaining improvements of 5.7% against ETPA (Explain-Then-Predict-Attention) and 0.6% against NILE (WHY A).","offset":4,"pro":0.8,"labels":"RST"},{"idx":16,"sentence":"Although BERT and its variants have reshaped the NLP landscape, it still remains unclear how best to derive sentence embeddings from such pre-trained Transformers.","offset":0,"pro":0,"labels":"GAP"},{"idx":16,"sentence":"In this work, we propose a contrastive learning method that utilizes self-guidance for improving the quality of BERT sentence representations.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":16,"sentence":"Our method fine-tunes BERT in a self-supervised fashion, does not rely on data augmentation, and enables the usual [CLS] token embeddings to function as sentence vectors.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":16,"sentence":"Moreover, we redesign the contrastive learning objective (NT-Xent) and apply it to sentence representation learning.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":16,"sentence":"We demonstrate with extensive experiments that our approach is more effective than competitive baselines on diverse sentence-related tasks.","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":16,"sentence":"We also show it is efficient at inference and robust to domain shifts.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":17,"sentence":"This work aims to tackle the challenging heterogeneous graph encoding problem in the text-to-SQL task.","offset":0,"pro":0,"labels":"PUR"},{"idx":17,"sentence":"Previous methods are typically node-centric and merely utilize different weight matrices to parameterize edge types, which 1) ignore the rich semantics embedded in the topological structure of edges, and 2) fail to distinguish local and non-local relations for each node.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":17,"sentence":"To this end, we propose a Line Graph Enhanced Text-to-SQL (LGESQL) model to mine the underlying relational features without constructing meta-paths.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":17,"sentence":"By virtue of the line graph, messages propagate more efficiently through not only connections between nodes, but also the topology of directed edges.","offset":3,"pro":0.42857142857142855,"labels":"BAC"},{"idx":17,"sentence":"Furthermore, both local and non-local relations are integrated distinctively during the graph iteration.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":17,"sentence":"We also design an auxiliary task called graph pruning to improve the discriminative capability of the encoder.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":17,"sentence":"Our framework achieves state-of-the-art results (62.8% with Glove, 72.0% with Electra) on the cross-domain text-to-SQL benchmark Spider at the time of writing.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":18,"sentence":"Multimodal pre-training models, such as LXMERT, have achieved excellent results in downstream tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"However, current pre-trained models require large amounts of training data and have huge model sizes, which make them impossible to apply in low-resource situations.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":18,"sentence":"How to obtain similar or even better performance than a larger model under the premise of less pre-training data and smaller model size has become an important problem.","offset":2,"pro":0.2857142857142857,"labels":"BAC"},{"idx":18,"sentence":"In this paper, we propose a new Multi-stage Pre-training (MSP) method, which uses information at different granularities from word, phrase to sentence in both texts and images to pre-train a model in stages.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":18,"sentence":"We also design several different pre-training tasks suitable for the information granularity in different stage in order to efficiently capture the diverse knowledge from a limited corpus.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":18,"sentence":"We take a Simplified LXMERT (LXMERT-S) which is with 45.9% parameters of the original LXMERT model and only 11.44% of the original pre-training data as the testbed of our MSP method.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":18,"sentence":"Experimental results show that our method achieves comparable performance to the original LXMERT model in all downstream tasks, and even outperforms the original model in Image-Text Retrieval task.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":19,"sentence":"Document-level contextual information has shown benefits to text-based machine translation, but whether and how context helps end-to-end (E2E) speech translation (ST) is still under-studied.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"We fill this gap through extensive experiments using a simple concatenation-based context-aware ST model, paired with adaptive feature selection on speech encodings for computational efficiency.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":19,"sentence":"We investigate several decoding approaches, and introduce in-model ensemble decoding which jointly performs document- and sentence-level translation using the same model.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":19,"sentence":"Our results on the MuST-C benchmark with Transformer demonstrate the effectiveness of context to E2E ST.","offset":3,"pro":0.6,"labels":"CLN"},{"idx":19,"sentence":"Compared to sentence-level ST, context-aware ST obtains better translation quality (+0.18-2.61 BLEU), improves pronoun and homophone translation, shows better robustness to (artificial) audio segmentation errors, and reduces latency and flicker to deliver higher quality for simultaneous translation.","offset":4,"pro":0.8,"labels":"RST"},{"idx":0,"sentence":"Pre-training of text and layout has proved effective in a variety of visually-rich document understanding tasks due to its effective model architecture and the advantage of large-scale unlabeled scanned/digital-born documents.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"Specifically, with a two-stream multi-modal Transformer encoder, LayoutLMv2 uses not only the existing masked visual-language modeling task but also the new text-image alignment and text-image matching tasks, which make it better capture the cross-modality interaction in the pre-training stage.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":0,"sentence":"Meanwhile, it also integrates a spatial-aware self-attention mechanism into the Transformer architecture so that the model can fully understand the relative positional relationship among different text blocks.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":0,"sentence":"Experiment results show that LayoutLMv2 outperforms LayoutLM by a large margin and achieves new state-of-the-art results on a wide variety of downstream visually-rich document understanding tasks, including FUNSD (0.7895 to 0.8420), CORD (0.9493 to 0.9601), SROIE (0.9524 to 0.9781), Kleister-NDA (0.8340 to 0.8520), RVL-CDIP (0.9443 to 0.9564), and DocVQA (0.7295 to 0.8672).","offset":3,"pro":0.6,"labels":"RST"},{"idx":0,"sentence":"We propose LayoutLMv2 architecture with new pre-training tasks to model the interaction among text, layout, and image in a single multi-modal framework.","offset":4,"pro":0.8,"labels":"PUR"},{"idx":1,"sentence":"Existed pre-training methods either focus on single-modal tasks or multi-modal tasks, and cannot effectively adapt to each other.","offset":0,"pro":0,"labels":"GAP"},{"idx":1,"sentence":"In this work, we propose a UNIfied-MOdal pre-training architecture, namely UNIMO, which can effectively adapt to both single-modal and multi-modal understanding and generation tasks.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":1,"sentence":"With the help of rich non-paired single-modal data, our model is able to learn more generalizable representations, by allowing textual knowledge and visual knowledge to enhance each other in the unified semantic space.","offset":2,"pro":0.2857142857142857,"labels":"RST"},{"idx":1,"sentence":"The experimental results show that UNIMO greatly improves the performance of several single-modal and multi-modal downstream tasks.","offset":3,"pro":0.42857142857142855,"labels":"CLN"},{"idx":1,"sentence":"Our code and pre-trained models are public at https://github.com/PaddlePaddle/Research/tree/master/NLP/UNIMO.","offset":4,"pro":0.5714285714285714,"labels":"CTN"},{"idx":1,"sentence":"Large scale of free text corpus and image collections are utilized to improve the capability of visual and textual understanding, and cross-modal contrastive learning (CMCL) is leveraged to align the textual and visual information into a unified semantic space, over a corpus of image-text pairs augmented with related images and texts.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":1,"sentence":"They can only utilize single-modal data (i.e., text or image) or limited multi-modal data (i.e., image-text pairs)","offset":6,"pro":0.8571428571428571,"labels":"BAC"},{"idx":2,"sentence":"Multimodal fusion has been proved to improve emotion recognition performance in previous works.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"However, in real-world applications, we often encounter the problem of missing modality, and which modalities will be missing is uncertain.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":2,"sentence":"It makes the fixed multimodal fusion fail in such cases.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":2,"sentence":"In this work, we propose a unified model, Missing Modality Imagination Network (MMIN), to deal with the uncertain missing modality problem.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":2,"sentence":"The code will be available at https://github.com/AIM3-RUC/MMIN.","offset":4,"pro":0.6666666666666666,"labels":"CTN"},{"idx":2,"sentence":"Comprehensive experiments on two benchmark datasets demonstrate that the unified MMIN model significantly improves emotion recognition performance under both uncertain missing-modality testing conditions and full-modality ideal testing condition.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":3,"sentence":"Encoder pre-training is promising in end-to-end Speech Translation (ST), given the fact that speech-to-translation data is scarce.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"But ST encoders are not simple instances of Automatic Speech Recognition (ASR) or Machine Translation (MT) encoders.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":3,"sentence":"In this work, we propose a Stacked Acoustic-and-Textual Encoding (SATE) method for speech translation.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":3,"sentence":"In this way, it is straightforward to incorporate the pre-trained models into the system.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":3,"sentence":"Also, we develop an adaptor module to alleviate the representation inconsistency between the pre-trained ASR encoder and MT encoder, and develop a multi-teacher knowledge distillation method to preserve the pre-training knowledge.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":3,"sentence":"Experimental results on the LibriSpeech En-Fr and MuST-C En-De ST tasks show that our method achieves state-of-the-art BLEU scores of 18.3 and 25.2.","offset":5,"pro":0.625,"labels":"RST"},{"idx":3,"sentence":"To our knowledge, we are the first to develop an end-to-end ST system that achieves comparable or even better BLEU performance than the cascaded ST counterpart when large-scale ASR and MT data is available.","offset":6,"pro":0.75,"labels":"CTN"},{"idx":3,"sentence":"For example, we find that ASR encoders lack the global context representation, which is necessary for translation, whereas MT encoders are not designed to deal with long but locally attentive acoustic sequences.","offset":7,"pro":0.875,"labels":"GAP"},{"idx":4,"sentence":"In this paper, we study the task of graph-based constituent parsing in the setting that binarization is not conducted as a pre-processing step, where a constituent tree may consist of nodes with more than two children.","offset":0,"pro":0,"labels":"PUR"},{"idx":4,"sentence":"Previous graph-based methods on this setting typically generate hidden nodes with the dummy label inside the n-ary nodes, in order to transform the tree into a binary tree for prediction.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":4,"sentence":"The limitation is that the hidden nodes break the sibling relations of the n-ary node's children.","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":4,"sentence":"Consequently, the dependencies of such sibling constituents might not be accurately modeled and is being ignored.","offset":3,"pro":0.3333333333333333,"labels":"GAP"},{"idx":4,"sentence":"The main idea is to utilize 1-order semi-Markov model to predict the immediate children sequence of a constituent candidate, which then recursively serves as a child candidate of its parent.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":4,"sentence":"In this manner, the dependencies of sibling constituents can be described by 1-order transition features, which solves the above limitation.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":4,"sentence":"Through experiments, the proposed framework obtains the F1 of 95.92% and 92.50% on the datasets of PTB and CTB 5.1 respectively.","offset":6,"pro":0.6666666666666666,"labels":"RST"},{"idx":4,"sentence":"Specially, the recursive semi-Markov model shows advantages in modeling nodes with more than two children, whose average F1 can be improved by 0.3-1.1 points in PTB and 2.3-6.8 points in CTB 5.1.","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":4,"sentence":"To solve this limitation, we propose a novel graph-based framework, which is called recursive semi-Markov model.","offset":8,"pro":0.8888888888888888,"labels":"MTD"},{"idx":5,"sentence":"Pretrained contextualized embeddings are powerful word representations for structured prediction tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"Recent work found that better word representations can be obtained by concatenating different types of embeddings.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":5,"sentence":"However, the selection of embeddings to form the best concatenated representation usually varies depending on the task and the collection of candidate embeddings, and the ever-increasing number of embedding types makes it a more difficult problem.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":5,"sentence":"In this paper, we propose Automated Concatenation of Embeddings (ACE) to automate the process of finding better concatenations of embeddings for structured prediction tasks, based on a formulation inspired by recent progress on neural architecture search.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":5,"sentence":"Specifically, a controller alternately samples a concatenation of embeddings, according to its current belief of the effectiveness of individual embedding types in consideration for a task, and updates the belief based on a reward.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":5,"sentence":"We follow strategies in reinforcement learning to optimize the parameters of the controller and compute the reward based on the accuracy of a task model, which is fed with the sampled concatenation as input and trained on a task dataset.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":5,"sentence":"Empirical results on 6 tasks and 21 datasets show that our approach outperforms strong baselines and achieves state-of-the-art performance with fine-tuned embeddings in all the evaluations.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":6,"sentence":"In structured prediction problems, cross-lingual transfer learning is an efficient way to train quality models for low-resource languages, and further improvement can be obtained by learning from multiple source languages.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"However, not all source models are created equal and some may hurt performance on the target language.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":6,"sentence":"Previous work has explored the similarity between source and target sentences as an approximate measure of strength for different source models.","offset":2,"pro":0.3333333333333333,"labels":"BAC"},{"idx":6,"sentence":"Experiments for three structured prediction tasks on sixteen data sets show that our framework achieves significant improvement over all existing approaches, including these with access to additional source language data.","offset":3,"pro":0.5,"labels":"CLN"},{"idx":6,"sentence":"In this paper, we propose a multi-view framework, by leveraging a small number of labeled target sentences, to effectively combine multiple source models into an aggregated source view at different granularity levels (language, sentence, or sub-structure), and transfer it to a target view based on a task-specific model.","offset":4,"pro":0.6666666666666666,"labels":"PUR"},{"idx":6,"sentence":"By encouraging the two views to interact with each other, our framework can dynamically adjust the confidence level of each source model and improve the performance of both views during training.","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":7,"sentence":"Incorporating syntax into neural approaches in NLP has a multitude of practical and scientific benefits.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"Rapid progress in recent years was arguably spurred on by the empirical success of the Parsing-Reading-Predict architecture of (Shen et al., 2018a), later simplified by the Order Neuron LSTM of (Shen et al., 2019).","offset":1,"pro":0.125,"labels":"BAC"},{"idx":7,"sentence":"However, even heuristic (much less fully mathematical) understanding of why and when these architectures work is lagging severely behind.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":7,"sentence":"In this work, we answer representational questions raised by the architectures in (Shen et al., 2018a, 2019), as well as some transition-based syntax-aware language models (Dyer et al., 2016): what kind of syntactic structure can current neural approaches to syntax represent?","offset":3,"pro":0.375,"labels":"PUR"},{"idx":7,"sentence":"Concretely, we ground this question in the sandbox of probabilistic context-free-grammars (PCFGs), and identify a key aspect of the representational power of these approaches: the amount and directionality of context that the predictor has access to when forced to make parsing decision.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":7,"sentence":"We show that with limited context (either bounded, or unidirectional), there are PCFGs, for which these approaches cannot represent the max-likelihood parse; conversely, if the context is unlimited, they can represent the max-likelihood parse of any PCFG.","offset":5,"pro":0.625,"labels":"RST"},{"idx":7,"sentence":"For instance, a language model that is syntax-aware is likely to be able to produce better samples; even a discriminative model like BERT with a syntax module could be used for core NLP tasks like unsupervised syntactic parsing.","offset":6,"pro":0.75,"labels":"BAC"},{"idx":7,"sentence":"Most notably, this is the first time neural approaches were able to successfully perform unsupervised syntactic parsing (evaluated by various metrics like F-1 score).","offset":7,"pro":0.875,"labels":"BAC"},{"idx":8,"sentence":"Neural lexicalized PCFGs (L-PCFGs) have been shown effective in grammar induction.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"However, to reduce computational complexity, they make a strong independence assumption on the generation of the child word and thus bilexical dependencies are ignored.","offset":1,"pro":0.25,"labels":"GAP"},{"idx":8,"sentence":"In this paper, we propose an approach to parameterize L-PCFGs without making implausible independence assumptions.","offset":2,"pro":0.5,"labels":"PUR"},{"idx":8,"sentence":"Experimental results on the English WSJ dataset confirm the effectiveness of our approach in improving both running speed and unsupervised parsing performance.","offset":3,"pro":0.75,"labels":"CLN"},{"idx":9,"sentence":"On social media platforms, hateful and offensive language negatively impact the mental well-being of users and the participation of people from diverse backgrounds.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"Automatic methods to detect offensive language have largely relied on datasets with categorical labels.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":9,"sentence":"However, comments can vary in their degree of offensiveness.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":9,"sentence":"The dataset was annotated using Best-Worst Scaling, a form of comparative annotation that has been shown to alleviate known biases of using rating scales.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":9,"sentence":"We show that the method produces highly reliable offensiveness scores.","offset":4,"pro":0.5714285714285714,"labels":"CLN"},{"idx":9,"sentence":"Finally, we evaluate the ability of widely-used neural models to predict offensiveness scores on this new dataset.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":9,"sentence":"We create the first dataset of English language Reddit comments that has fine-grained, real-valued scores between -1 (maximally supportive) and 1 (maximally offensive).","offset":6,"pro":0.8571428571428571,"labels":"PUR"},{"idx":10,"sentence":"Automatic dialogue coherence evaluation has attracted increasing attention and is crucial for developing promising dialogue systems.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"However, existing metrics have two major limitations: (a) they are mostly trained in a simplified two-level setting (coherent vs. incoherent), while humans give Likert-type multi-level coherence scores, dubbed as quantifiable; (b) their predicted coherence scores cannot align with the actual human rating standards due to the absence of human guidance during training.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":10,"sentence":"To address these limitations, we propose Quantifiable Dialogue Coherence Evaluation (QuantiDCE), a novel framework aiming to train a quantifiable dialogue coherence metric that can reflect the actual human rating standards.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":10,"sentence":"Specifically, QuantiDCE includes two training stages, Multi-Level Ranking (MLR) pre-training and Knowledge Distillation (KD) fine-tuning.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":10,"sentence":"During MLR pre-training, a new MLR loss is proposed for enabling the model to learn the coarse judgement of coherence degrees.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":10,"sentence":"Then, during KD fine-tuning, the pretrained model is further finetuned to learn the actual human rating standards with only very few human-annotated data.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":10,"sentence":"To advocate the generalizability even with limited fine-tuning data, a novel KD regularization is introduced to retain the knowledge learned at the pre-training stage.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":10,"sentence":"Experimental results show that the model trained by QuantiDCE presents stronger correlations with human judgements than the other state-of-the-art metrics.","offset":7,"pro":0.875,"labels":"RST"},{"idx":11,"sentence":"However, most existing resources with annotation of idiomaticity include ratings only at type level.","offset":0,"pro":0,"labels":"GAP"},{"idx":11,"sentence":"This paper presents the Noun Compound Type and Token Idiomaticity (NCTTI) dataset, with human annotations for 280 noun compounds in English and 180 in Portuguese at both type and token level.","offset":1,"pro":0.25,"labels":"PUR"},{"idx":11,"sentence":"The NCTTI dataset is used to explore how vector space models reflect the variability of idiomaticity across sentences.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":11,"sentence":"Several experiments using state-of-the-art contextualised models suggest that their representations are not capturing the noun compounds idiomaticity as human annotators.","offset":3,"pro":0.75,"labels":"CLN"},{"idx":12,"sentence":"Statutory reasoning is the task of determining whether a legal statute, stated in natural language, applies to the text description of a case.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"Prior work introduced a resource that approached statutory reasoning as a monolithic textual entailment problem, with neural baselines performing nearly at-chance.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":12,"sentence":"To address this challenge, we decompose statutory reasoning into four types of language-understanding challenge problems, through the introduction of concepts and structure found in Prolog programs.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":12,"sentence":"Augmenting an existing benchmark, we provide annotations for the four tasks, and baselines for three of them.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":12,"sentence":"Models for statutory reasoning are shown to benefit from the additional structure, improving on prior baselines.","offset":4,"pro":0.8,"labels":"RST"},{"idx":13,"sentence":"Ordinal Classification (OC) is an important classification task where the classes are ordinal.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"Ordinal Quantification (OQ) is a related task where the gold data is a distribution over ordinal classes, and the system is required to estimate this distribution.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":13,"sentence":"Evaluation measures for an OQ task should also take the ordinal nature of the classes into account.","offset":2,"pro":0.3333333333333333,"labels":"BAC"},{"idx":13,"sentence":"However, for both OC and OQ, there are only a small number of known evaluation measures that meet this basic requirement.","offset":3,"pro":0.5,"labels":"GAP"},{"idx":13,"sentence":"In the present study, we utilise data from the SemEval and NTCIR communities to clarify the properties of nine evaluation measures in the context of OC tasks, and six measures in the context of OQ tasks.","offset":4,"pro":0.6666666666666666,"labels":"PUR"},{"idx":13,"sentence":"For example, an OC task for sentiment analysis could have the following classes: highly positive, positive, neutral, negative, highly negative. ","offset":5,"pro":0.8333333333333334,"labels":"BAC"},{"idx":14,"sentence":"Entity Matching (EM) aims at recognizing entity records that denote the same real-world object.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"Neural EM models learn vector representation of entity descriptions and match entities end-to-end.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":14,"sentence":"Though robust, these methods require many annotated resources for training, and lack of interpretability.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":14,"sentence":"In this paper, we propose a novel EM framework that consists of Heterogeneous Information Fusion (HIF) and Key Attribute Tree (KAT) Induction to decouple feature representation from matching decision.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":14,"sentence":"Using self-supervised learning and mask mechanism in pre-trained language modeling, HIF learns the embeddings of noisy attribute values by inter-attribute attention with unlabeled data.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":14,"sentence":"Using a set of comparison features and a limited amount of annotated data, KAT Induction learns an efficient decision tree that can be interpreted by generating entity matching rules whose structure is advocated by domain experts.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":14,"sentence":"Experiments on 6 public datasets and 3 industrial datasets show that our method is highly efficient and outperforms SOTA EM models in most cases.","offset":6,"pro":0.75,"labels":"CLN"},{"idx":14,"sentence":"We will release the codes upon acceptance.","offset":7,"pro":0.875,"labels":"CTN"},{"idx":15,"sentence":"Named entity recognition (NER) is a well-studied task in natural language processing.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"Traditional NER research only deals with flat entities and ignores nested entities.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":15,"sentence":"The span-based methods treat entity recognition as a span classification task.","offset":2,"pro":0.2,"labels":"BAC"},{"idx":15,"sentence":"Although these methods have the innate ability to handle nested NER, they suffer from high computational cost, ignorance of boundary information, under-utilization of the spans that partially match with entities, and difficulties in long entity recognition.","offset":3,"pro":0.3,"labels":"GAP"},{"idx":15,"sentence":"To tackle these issues, we propose a two-stage entity identifier.","offset":4,"pro":0.4,"labels":"PUR"},{"idx":15,"sentence":"First we generate span proposals by filtering and boundary regression on the seed spans to locate the entities, and then label the boundary-adjusted span proposals with the corresponding categories.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":15,"sentence":"Our method effectively utilizes the boundary information of entities and partially matched spans during training.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":15,"sentence":"In addition, many low-quality seed spans are filtered out in the first stage, which reduces the time complexity of inference.","offset":7,"pro":0.7,"labels":"RST"},{"idx":15,"sentence":"Experiments on nested NER datasets demonstrate that our proposed method outperforms previous state-of-the-art models.","offset":8,"pro":0.8,"labels":"CLN"},{"idx":15,"sentence":"Through boundary regression, entities of any length can be covered theoretically, which improves the ability to recognize long entities.","offset":9,"pro":0.9,"labels":"RST"},{"idx":16,"sentence":"Traditional methods usually extract event records by decomposing the complex structure prediction task into multiple subtasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"In this paper, we propose Text2Event, a sequence-to-structure generation paradigm that can directly extract events from the text in an end-to-end manner.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":16,"sentence":"Specifically, we design a sequence-to-structure network for unified event extraction, a constrained decoding algorithm for event knowledge injection during inference, and a curriculum learning algorithm for efficient model learning.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":16,"sentence":"Experimental results show that, by uniformly modeling all tasks in a single model and universally predicting different labels, our method can achieve competitive performance using only record-level annotations in both supervised learning and transfer learning settings.","offset":3,"pro":0.6,"labels":"CLN"},{"idx":16,"sentence":"Event extraction is challenging due to the complex structure of event records and the semantic gap between text and event.","offset":4,"pro":0.8,"labels":"BAC"},{"idx":17,"sentence":"In this paper, we aim to explore an uncharted territory, which is Chinese multimodal named entity recognition (NER) with both textual and acoustic contents.","offset":0,"pro":0,"labels":"PUR"},{"idx":17,"sentence":"Our corpus totally contains 42,987 annotated sentences accompanying by 71 hours of speech data.","offset":1,"pro":0.125,"labels":"MTD"},{"idx":17,"sentence":"Based on this dataset, we propose a family of strong and representative baseline models, which can leverage textual features or multimodal features.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":17,"sentence":"Upon these baselines, to capture the natural monotonic alignment between the textual modality and the acoustic modality, we further propose a simple multimodal multitask model by introducing a speech-to-text alignment auxiliary task.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":17,"sentence":"Through extensive experiments, we observe that: (1) Progressive performance boosts as we move from unimodal to multimodal, verifying the necessity of integrating speech clues into Chinese NER.","offset":4,"pro":0.5,"labels":"RST"},{"idx":17,"sentence":"(2) Our proposed model yields state-of-the-art (SoTA) results on CNERTA, demonstrating its effectiveness.","offset":5,"pro":0.625,"labels":"RST"},{"idx":17,"sentence":"For further research, the annotated dataset is publicly available at http://github.com/DianboWork/CNERTA.","offset":6,"pro":0.75,"labels":"CTN"},{"idx":17,"sentence":"To achieve this, we construct a large-scale human-annotated Chinese multimodal NER dataset, named CNERTA.","offset":7,"pro":0.875,"labels":"MTD"},{"idx":18,"sentence":"Disease is one of the fundamental entities in biomedical research.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"Recognizing such entities from biomedical text and then normalizing them to a standardized disease vocabulary offer a tremendous opportunity for many downstream applications.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":18,"sentence":"Previous studies have demonstrated that joint modeling of the two sub-tasks has superior performance than the pipelined counterpart.","offset":2,"pro":0.2222222222222222,"labels":"BAC"},{"idx":18,"sentence":"Although the neural joint model based on multi-task learning framework has achieved state-of-the-art performance, it suffers from the boundary inconsistency problem due to the separate decoding procedures.","offset":3,"pro":0.3333333333333333,"labels":"GAP"},{"idx":18,"sentence":"Moreover, it ignores the rich information (e.g., the text surface form) of each candidate concept in the vocabulary, which is quite essential for entity normalization.","offset":4,"pro":0.4444444444444444,"labels":"GAP"},{"idx":18,"sentence":"In this work, we propose a neural transition-based joint model to alleviate these two issues.","offset":5,"pro":0.5555555555555556,"labels":"PUR"},{"idx":18,"sentence":"We transform the end-to-end disease recognition and normalization task as an action sequence prediction task, which not only jointly learns the model with shared representations of the input, but also jointly searches the output by state transitions in one search space.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":18,"sentence":"Moreover, we introduce attention mechanisms to take advantage of the text surface form of each candidate concept for better normalization performance.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":18,"sentence":"Experimental results conducted on two publicly available datasets show the effectiveness of the proposed method.","offset":8,"pro":0.8888888888888888,"labels":"RST"},{"idx":19,"sentence":"Event Detection (ED) aims to identify event trigger words from a given text and classify it into an event type.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"Most current methods to ED rely heavily on training instances, and almost ignore the correlation of event types.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":19,"sentence":"Hence, they tend to suffer from data scarcity and fail to handle new unseen event types.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":19,"sentence":"To address these problems, we formulate ED as a process of event ontology population: linking event instances to pre-defined event types in event ontology, and propose a novel ED framework entitled OntoED with ontology embedding.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":19,"sentence":"We enrich event ontology with linkages among event types, and further induce more event-event correlations.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":19,"sentence":"Experiments indicate that OntoED is more predominant and robust than previous approaches to ED, especially in data-scarce scenarios.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":19,"sentence":"Based on the event ontology, OntoED can leverage and propagate correlation knowledge, particularly from data-rich to data-poor event types. ","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":0,"sentence":"Self-training has proven effective for improving NMT performance by augmenting model training with synthetic parallel data.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"The common practice is to construct synthetic data based on a randomly sampled subset of large-scale monolingual data, which we empirically show is sub-optimal.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":0,"sentence":"In this work, we propose to improve the sampling procedure by selecting the most informative monolingual sentences to complement the parallel data.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":0,"sentence":"To this end, we compute the uncertainty of monolingual sentences using the bilingual dictionary extracted from the parallel data.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":0,"sentence":"Intuitively, monolingual sentences with lower uncertainty generally correspond to easy-to-translate patterns which may not provide additional gains.","offset":4,"pro":0.5,"labels":"RST"},{"idx":0,"sentence":"Accordingly, we design an uncertainty-based sampling strategy to efficiently exploit the monolingual data for self-training, in which monolingual sentences with higher uncertainty would be sampled with higher probability.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":0,"sentence":"Experimental results on large-scale WMT English⇒German and English⇒Chinese datasets demonstrate the effectiveness of the proposed approach.","offset":6,"pro":0.75,"labels":"RST"},{"idx":0,"sentence":"Extensive analyses suggest that emphasizing the learning on uncertain monolingual sentences by our approach does improve the translation quality of high-uncertainty sentences and also benefits the prediction of low-frequency words at the target side.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":1,"sentence":"Context-aware neural machine translation (NMT) remains challenging due to the lack of large-scale document-level parallel corpora.","offset":0,"pro":0,"labels":"GAP"},{"idx":1,"sentence":"To break the corpus bottleneck, in this paper we aim to improve context-aware NMT by taking the advantage of the availability of both large-scale sentence-level parallel dataset and source-side monolingual documents.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":1,"sentence":"To this end, we propose two pre-training tasks.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":1,"sentence":"One learns to translate a sentence from source language to target language on the sentence-level parallel dataset while the other learns to translate a document from deliberately noised to original on the monolingual documents.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":1,"sentence":"Importantly, the two pre-training tasks are jointly and simultaneously learned via the same model, thereafter fine-tuned on scale-limited parallel documents from both sentence-level and document-level perspectives.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":1,"sentence":"Experimental results on four translation tasks show that our approach significantly improves translation performance.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":1,"sentence":"One nice property of our approach is that the fine-tuned model can be used to translate both sentences and documents.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":2,"sentence":"Although teacher forcing has become the main training paradigm for neural machine translation, it usually makes predictions only conditioned on past information, and hence lacks global planning for the future.","offset":0,"pro":0,"labels":"GAP"},{"idx":2,"sentence":"To address this problem, we introduce another decoder, called seer decoder, into the encoder-decoder framework during training, which involves future information in target predictions.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":2,"sentence":"Meanwhile, we force the conventional decoder to simulate the behaviors of the seer decoder via knowledge distillation.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":2,"sentence":"In this way, at test the conventional decoder can perform like the seer decoder without the attendance of it.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":2,"sentence":"Experiment results on the Chinese-English, English-German and English-Romanian translation tasks show our method can outperform competitive baselines significantly and achieves greater improvements on the bigger data sets.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":2,"sentence":"Besides, the experiments also prove knowledge distillation the best way to transfer knowledge from the seer decoder to the conventional decoder compared to adversarial learning and L2 regularization.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":3,"sentence":"Five years after the first published proofs of concept, direct approaches to speech translation (ST) are now competing with traditional cascade solutions.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"In light of this steady progress, can we claim that the performance gap between the two is closed?","offset":1,"pro":0.2,"labels":"PUR"},{"idx":3,"sentence":"Starting from this question, we present a systematic comparison between state-of-the-art systems representative of the two paradigms.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":3,"sentence":"Focusing on three language directions (English-German/Italian/Spanish), we conduct automatic and manual evaluations, exploiting high-quality professional post-edits and annotations.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":3,"sentence":"Our multi-faceted analysis on one of the few publicly available ST benchmarks attests for the first time that: i) the gap between the two paradigms is now closed, and ii) the subtle differences observed in their behavior are not sufficient for humans neither to distinguish them nor to prefer one over the other.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":4,"sentence":"Unsupervised machine translation, which utilizes unpaired monolingual corpora as training data, has achieved comparable performance against supervised machine translation.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"However, it still suffers from data-scarce domains.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":4,"sentence":"To address this issue, this paper presents a novel meta-learning algorithm for unsupervised neural machine translation (UNMT) that trains the model to adapt to another domain by utilizing only a small amount of training data.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":4,"sentence":"We assume that domain-general knowledge is a significant factor in handling data-scarce domains.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":4,"sentence":"Hence, we extend the meta-learning algorithm, which utilizes knowledge learned from high-resource domains, to boost the performance of low-resource UNMT.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":4,"sentence":"Our model surpasses a transfer learning-based approach by up to 2-3 BLEU scores.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":4,"sentence":"Extensive experimental results show that our proposed algorithm is pertinent for fast adaptation and consistently outperforms other baselines.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":5,"sentence":"Large-scale models for learning fixed-dimensional cross-lingual sentence representations like LASER (Artetxe and Schwenk, 2019b) lead to significant improvement in performance on downstream tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"However, further increases and modifications based on such large-scale models are usually impractical due to memory limitations.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":5,"sentence":"In this work, we introduce a lightweight dual-transformer architecture with just 2 layers for generating memory-efficient cross-lingual sentence representations.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":5,"sentence":"We explore different training tasks and observe that current cross-lingual training tasks leave a lot to be desired for this shallow architecture.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":5,"sentence":"To ameliorate this, we propose a novel cross-lingual language model, which combines the existing single-word masked language model with the newly proposed cross-lingual token-level reconstruction task.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":5,"sentence":"We further augment the training task by the introduction of two computationally-lite sentence-level contrastive learning tasks to enhance the alignment of cross-lingual sentence representation space, which compensates for the learning bottleneck of the lightweight transformer for generative tasks.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":5,"sentence":"Our comparisons with competing models on cross-lingual sentence retrieval and multilingual document classification confirm the effectiveness of the newly proposed training tasks for a shallow model.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":6,"sentence":"Transformers are not suited for processing long documents, due to their quadratically increasing memory and time consumption.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"Simply truncating a long document or applying the sparse attention mechanism will incur the context fragmentation problem or lead to an inferior modeling capability against comparable model sizes.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":6,"sentence":"In this paper, we propose ERNIE-Doc, a document-level language pretraining model based on Recurrence Transformers.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":6,"sentence":"Two well-designed techniques, namely the retrospective feed mechanism and the enhanced recurrence mechanism, enable ERNIE-Doc, which has a much longer effective context length, to capture the contextual information of a complete document.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":6,"sentence":"We pretrain ERNIE-Doc to explicitly learn the relationships among segments with an additional document-aware segment-reordering objective.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":6,"sentence":"Various experiments were conducted on both English and Chinese document-level tasks.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":6,"sentence":"ERNIE-Doc improved the state-of-the-art language modeling result of perplexity to 16.8 on WikiText-103.","offset":6,"pro":0.75,"labels":"RST"},{"idx":6,"sentence":"Moreover, it outperformed competitive pretraining models by a large margin on most language understanding tasks, such as text classification and question answering.","offset":7,"pro":0.875,"labels":"RST"},{"idx":7,"sentence":"Recently, knowledge distillation (KD) has shown great success in BERT compression.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"Instead of only learning from the teacher's soft label as in conventional KD, researchers find that the rich information contained in the hidden layers of BERT is conducive to the student's performance.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":7,"sentence":"To better exploit the hidden knowledge, a common practice is to force the student to deeply mimic the teacher's hidden states of all the tokens in a layer-wise manner.","offset":2,"pro":0.2,"labels":"BAC"},{"idx":7,"sentence":"In this paper, however, we observe that although distilling the teacher's hidden state knowledge (HSK) is helpful, the performance gain (marginal utility) diminishes quickly as more HSK is distilled.","offset":3,"pro":0.3,"labels":"PUR"},{"idx":7,"sentence":"To understand this effect, we conduct a series of analysis.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":7,"sentence":"Specifically, we divide the HSK of BERT into three dimensions, namely depth, length and width.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":7,"sentence":"We first investigate a variety of strategies to extract crucial knowledge for each single dimension and then jointly compress the three dimensions.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":7,"sentence":"In this way, we show that 1) the student's performance can be improved by extracting and distilling the crucial HSK, and 2) using a tiny fraction of HSK can achieve the same performance as extensive HSK distillation.","offset":7,"pro":0.7,"labels":"RST"},{"idx":7,"sentence":"Based on the second finding, we further propose an efficient KD paradigm to compress BERT, which does not require loading the teacher during the training of student.","offset":8,"pro":0.8,"labels":"MTD"},{"idx":7,"sentence":"For two kinds of student models and computing devices, the proposed KD paradigm gives rise to training speedup of 2.7x 3.4x.","offset":9,"pro":0.9,"labels":"RST"},{"idx":8,"sentence":"Lifelong learning (LL) aims to train a neural network on a stream of tasks while retaining knowledge from previous tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"However, many prior attempts in NLP still suffer from the catastrophic forgetting issue, where the model completely forgets what it just learned in the previous tasks.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":8,"sentence":"In this paper, we introduce Rational LAMOL, a novel end-to-end LL framework for language models.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":8,"sentence":"In order to alleviate catastrophic forgetting, Rational LAMOL enhances LAMOL, a recent LL model, by applying critical freezing guided by human rationales.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":8,"sentence":"When the human rationales are not available, we propose exploiting unsupervised generated rationales as substitutions.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":8,"sentence":"In the experiment, we tested Rational LAMOL on permutations of three datasets from the ERASER benchmark.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":8,"sentence":"The results show that our proposed framework outperformed vanilla LAMOL on most permutations.","offset":6,"pro":0.75,"labels":"RST"},{"idx":8,"sentence":"Furthermore, unsupervised rationale generation was able to consistently improve the overall LL performance from the baseline without relying on human-annotated rationales.","offset":7,"pro":0.875,"labels":"RST"},{"idx":9,"sentence":"Natural language processing (NLP) often faces the problem of data diversity such as different domains, themes, styles, and so on.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"Therefore, a single language model (LM) is insufficient to learn all knowledge from diverse samples.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":9,"sentence":"To solve this problem, we firstly propose an autoencoding topic model with a mixture prior (mATM) to perform clustering for the data, where the clusters defined in semantic space describes the data diversity.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":9,"sentence":"Having obtained the clustering assignment for each sample, we develop the ensemble LM (EnsLM) with the technique of weight modulation.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":9,"sentence":"Specifically, EnsLM contains a backbone that is adjusted by a few modulated weights to fit for different sample clusters.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":9,"sentence":"As a result, the backbone learns the shared knowledge among all clusters while modulated weights extract the cluster-specific features.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":9,"sentence":"EnsLM can be trained jointly with mATM with a flexible LM backbone.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":9,"sentence":"We evaluate the effectiveness of both mATM and EnsLM on various tasks.","offset":7,"pro":0.875,"labels":"MTD"},{"idx":10,"sentence":"Pre-trained language models like BERT are performant in a wide range of natural language tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"However, they are resource exhaustive and computationally expensive for industrial scenarios.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":10,"sentence":"Thus, early exits are adopted at each layer of BERT to perform adaptive computation by predicting easier samples with the first few layers to speed up the inference.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":10,"sentence":"In this work, to improve efficiency without performance drop, we propose a novel training scheme called Learned Early Exit for BERT (LeeBERT).","offset":3,"pro":0.375,"labels":"PUR"},{"idx":10,"sentence":"First, we ask each exit to learn from each other, rather than learning only from the last layer.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":10,"sentence":"Second, the weights of different loss terms are learned, thus balancing off different objectives.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":10,"sentence":"We formulate the optimization of LeeBERT as a bi-level optimization problem, and we propose a novel cross-level optimization (CLO) algorithm to improve the optimization results.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":10,"sentence":"Experiments on the GLUE benchmark show that our proposed methods improve the performance of the state-of-the-art (SOTA) early exit methods for pre-trained models.","offset":7,"pro":0.875,"labels":"RST"},{"idx":11,"sentence":"We pioneer the first extractive summarization-based collaborative filtering model called ESCOFILT.","offset":0,"pro":0,"labels":"PUR"},{"idx":11,"sentence":"Our proposed model specifically produces extractive summaries for each item and user.","offset":1,"pro":0.125,"labels":"MTD"},{"idx":11,"sentence":"Unlike other types of explanations, summary-level explanations closely resemble real-life explanations.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":11,"sentence":"Our model uniquely integrates BERT, K-Means embedding clustering, and multilayer perceptron to learn sentence embeddings, representation-explanations, and user-item interactions, respectively.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":11,"sentence":"We argue that our approach enhances both rating prediction accuracy and user/item explainability.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":11,"sentence":"Our experiments illustrate that ESCOFILT's prediction accuracy is better than the other state-of-the-art recommender models.","offset":5,"pro":0.625,"labels":"CLN"},{"idx":11,"sentence":"Furthermore, we propose a comprehensive set of criteria that assesses the real-life explainability of explanations.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":11,"sentence":"Our explainability study demonstrates the superiority of and preference for summary-level explanations over other explanation types.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":12,"sentence":"Chinese spelling correction (CSC) is a task to detect and correct spelling errors in texts.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"CSC is essentially a linguistic problem, thus the ability of language understanding is crucial to this task.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":12,"sentence":"In this paper, we propose a Pre-trained masked Language model with Misspelled knowledgE (PLOME) for CSC, which jointly learns how to understand language and correct spelling errors.","offset":2,"pro":0.2,"labels":"PUR"},{"idx":12,"sentence":"To this end, PLOME masks the chosen tokens with similar characters according to a confusion set rather than the fixed token [MASK] as in BERT.","offset":3,"pro":0.3,"labels":"MTD"},{"idx":12,"sentence":"Besides character prediction, PLOME also introduces pronunciation prediction to learn the misspelled knowledge on phonic level.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":12,"sentence":"Moreover, phonological and visual similarity knowledge is important to this task.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":12,"sentence":"PLOME utilizes GRU networks to model such knowledge based on characters' phonics and strokes.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":12,"sentence":"Experiments are conducted on widely used benchmarks.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":12,"sentence":"Our method achieves superior performance against state-of-the-art approaches by a remarkable margin.","offset":8,"pro":0.8,"labels":"RST"},{"idx":12,"sentence":"We release the source code and pre-trained model for further use by the community (https://github.com/liushulinle/PLOME).","offset":9,"pro":0.9,"labels":"CTN"},{"idx":13,"sentence":"Medical report generation task, which targets to produce long and coherent descriptions of medical images, has attracted growing research interests recently.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"Different from the general image captioning tasks, medical report generation is more challenging for data-driven neural models.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":13,"sentence":"This is mainly due to 1) the serious data bias and 2) the limited medical data.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":13,"sentence":"To alleviate the data bias and make best use of available data, we propose a Competence-based Multimodal Curriculum Learning framework (CMCL).","offset":3,"pro":0.375,"labels":"PUR"},{"idx":13,"sentence":"Specifically, CMCL simulates the learning process of radiologists and optimizes the model in a step by step manner.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":13,"sentence":"Firstly, CMCL estimates the difficulty of each training instance and evaluates the competence of current model; Secondly, CMCL selects the most suitable batch of training instances considering current model competence.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":13,"sentence":"By iterating above two steps, CMCL can gradually improve the model's performance.","offset":6,"pro":0.75,"labels":"RST"},{"idx":13,"sentence":"The experiments on the public IU-Xray and MIMIC-CXR datasets show that CMCL can be incorporated into existing models to improve their performance.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":14,"sentence":"Deep learning models for automatic readability assessment generally discard linguistic features traditionally used in machine learning models for the task.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"We propose to incorporate linguistic features into neural network models by learning syntactic dense embeddings based on linguistic features.","offset":1,"pro":0.25,"labels":"PUR"},{"idx":14,"sentence":"To cope with the relationships between the features, we form a correlation graph among features and use it to learn their embeddings so that similar features will be represented by similar embeddings.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":14,"sentence":"Experiments with six data sets of two proficiency levels demonstrate that our proposed methodology can complement BERT-only model to achieve significantly better performances for automatic readability assessment.","offset":3,"pro":0.75,"labels":"RST"},{"idx":15,"sentence":"Pre-trained language models have been applied to various NLP tasks with considerable performance gains.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"However, the large model sizes, together with the long inference time, limit the deployment of such models in real-time applications.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":15,"sentence":"One line of model compression approaches considers knowledge distillation to distill large teacher models into small student models.","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":15,"sentence":"Most of these studies focus on single-domain only, which ignores the transferable knowledge from other domains.","offset":3,"pro":0.3333333333333333,"labels":"GAP"},{"idx":15,"sentence":"We notice that training a teacher with transferable knowledge digested across domains can achieve better generalization capability to help knowledge distillation.","offset":4,"pro":0.4444444444444444,"labels":"GAP"},{"idx":15,"sentence":"Hence we propose a Meta-Knowledge Distillation (Meta-KD) framework to build a meta-teacher model that captures transferable knowledge across domains and passes such knowledge to students.","offset":5,"pro":0.5555555555555556,"labels":"PUR"},{"idx":15,"sentence":"Specifically, we explicitly force the meta-teacher to capture transferable knowledge at both instance-level and feature-level from multiple domains, and then propose a meta-distillation algorithm to learn single-domain student models with guidance from the meta-teacher.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":15,"sentence":"Experiments on public multi-domain NLP tasks show the effectiveness and superiority of the proposed Meta-KD framework.","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":15,"sentence":"Further, we also demonstrate the capability of Meta-KD in the settings where the training data is scarce.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":16,"sentence":"Unsupervised commonsense question answering is appealing since it does not rely on any labeled task data.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"Among existing work, a popular solution is to use pre-trained language models to score candidate choices directly conditioned on the question or context.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":16,"sentence":"However, such scores from language models can be easily affected by irrelevant factors, such as word frequencies, sentence structures, etc. These distracting factors may not only mislead the model to choose a wrong answer but also make it oversensitive to lexical perturbations in candidate answers.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":16,"sentence":"In this paper, we present a novel SEmantic-based Question Answering method (SEQA) for unsupervised commonsense question answering.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":16,"sentence":"Instead of directly scoring each answer choice, our method first generates a set of plausible answers with generative models (e.g., GPT-2), and then uses these plausible answers to select the correct choice by considering the semantic similarity between each plausible answer and each choice.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":16,"sentence":"We devise a simple, yet sound formalism for this idea and verify its effectiveness and robustness with extensive experiments.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":16,"sentence":"We evaluate the proposed method on four benchmark datasets, and our method achieves the best results in unsupervised settings.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":16,"sentence":"Moreover, when attacked by TextFooler with synonym replacement, SEQA demonstrates much less performance drops than baselines, thereby indicating stronger robustness.","offset":7,"pro":0.875,"labels":"RST"},{"idx":17,"sentence":"CommonsenseQA (CQA) (Talmor et al., 2019) dataset was recently released to advance the research on common-sense question answering (QA) task.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"Our explanation definition is based on certain desiderata, and translates an explanation into a set of positive and negative common-sense properties (aka facts) which not only explain the correct answer choice but also refute the incorrect ones.","offset":1,"pro":0.125,"labels":"MTD"},{"idx":17,"sentence":"We human-annotate a first-of-its-kind dataset (called ECQA) of positive and negative properties, as well as free-flow explanations, for 11K QA pairs taken from the CQA dataset.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":17,"sentence":"We propose a latent representation based property retrieval model as well as a GPT-2 based property generation model with a novel two step fine-tuning procedure.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":17,"sentence":"We also propose a free-flow explanation generation model.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":17,"sentence":"Extensive experiments show that our retrieval model beats BM25 baseline by a relative gain of 100% in F_1 score, property generation model achieves a respectable F_1 score of 36.4, and free-flow generation model achieves a similarity score of 61.9, where last two scores are based on a human correlated semantic similarity metric.","offset":5,"pro":0.625,"labels":"RST"},{"idx":17,"sentence":"Whereas the prior work has mostly focused on proposing QA models for this dataset","offset":6,"pro":0.75,"labels":"BAC"},{"idx":17,"sentence":"our aim is to retrieve as well as generate explanation for a given (question, correct answer choice, incorrect answer choices) tuple from this dataset.","offset":7,"pro":0.875,"labels":"PUR"},{"idx":18,"sentence":"In several question answering benchmarks, pretrained models have reached human parity through fine-tuning on an order of 100,000 annotated questions and answers.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"We explore the more realistic few-shot setting, where only a few hundred training examples are available, and observe that standard models perform poorly, highlighting the discrepancy between current pretraining objectives and question answering.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":18,"sentence":"We propose a new pretraining scheme tailored for question answering: recurring span selection.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":18,"sentence":"Given a passage with multiple sets of recurring spans, we mask in each set all recurring spans but one, and ask the model to select the correct span in the passage for each masked span.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":18,"sentence":"Masked spans are replaced with a special token, viewed as a question representation, that is later used during fine-tuning to select the answer span.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":18,"sentence":"The resulting model obtains surprisingly good results on multiple benchmarks (e.g., 72.7 F1 on SQuAD with only 128 training examples), while maintaining competitive performance in the high-resource setting.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":19,"sentence":"To date, most of recent work under the retrieval-reader framework for open-domain QA focuses on either extractive or generative reader exclusively.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"In this paper, we study a hybrid approach for leveraging the strengths of both models.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":19,"sentence":"We apply novel techniques to enhance both extractive and generative readers built upon recent pretrained neural language models, and find that proper training methods can provide large improvement over previous state-of-the-art models.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":19,"sentence":"We demonstrate that a simple hybrid approach by combining answers from both readers can efficiently take advantages of extractive and generative answer inference strategies and outperforms single models as well as homogeneous ensembles.","offset":3,"pro":0.6,"labels":"CLN"},{"idx":19,"sentence":"Our approach outperforms previous state-of-the-art models by 3.3 and 2.7 points in exact match on NaturalQuestions and TriviaQA respectively.","offset":4,"pro":0.8,"labels":"RST"},{"idx":0,"sentence":"Pre-trained language models have been applied to various NLP tasks with considerable performance gains.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"However, the large model sizes, together with the long inference time, limit the deployment of such models in real-time applications.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":0,"sentence":"One line of model compression approaches considers knowledge distillation to distill large teacher models into small student models.","offset":2,"pro":0.2222222222222222,"labels":"BAC"},{"idx":0,"sentence":"Most of these studies focus on single-domain only, which ignores the transferable knowledge from other domains.","offset":3,"pro":0.3333333333333333,"labels":"GAP"},{"idx":0,"sentence":"Specifically, we explicitly force the meta-teacher to capture transferable knowledge at both instance-level and feature-level from multiple domains, and then propose a meta-distillation algorithm to learn single-domain student models with guidance from the meta-teacher.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":0,"sentence":"Experiments on public multi-domain NLP tasks show the effectiveness and superiority of the proposed Meta-KD framework.","offset":5,"pro":0.5555555555555556,"labels":"CLN"},{"idx":0,"sentence":"Further, we also demonstrate the capability of Meta-KD in the settings where the training data is scarce.","offset":6,"pro":0.6666666666666666,"labels":"CLN"},{"idx":0,"sentence":"We notice that training a teacher with transferable knowledge digested across domains can achieve better generalization capability to help knowledge distillation.","offset":7,"pro":0.7777777777777778,"labels":"BAC"},{"idx":0,"sentence":"Hence we propose a Meta-Knowledge Distillation (Meta-KD) framework to build a meta-teacher model that captures transferable knowledge across domains and passes such knowledge to students.","offset":8,"pro":0.8888888888888888,"labels":"PUR"},{"idx":1,"sentence":"Unsupervised commonsense question answering is appealing since it does not rely on any labeled task data.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"Among existing work, a popular solution is to use pre-trained language models to score candidate choices directly conditioned on the question or context.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":1,"sentence":"In this paper, we present a novel SEmantic-based Question Answering method (SEQA) for unsupervised commonsense question answering.","offset":2,"pro":0.2,"labels":"PUR"},{"idx":1,"sentence":"Instead of directly scoring each answer choice, our method first generates a set of plausible answers with generative models (e.g., GPT-2), and then uses these plausible answers to select the correct choice by considering the semantic similarity between each plausible answer and each choice.","offset":3,"pro":0.3,"labels":"MTD"},{"idx":1,"sentence":"We devise a simple, yet sound formalism for this idea and verify its effectiveness and robustness with extensive experiments.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":1,"sentence":"Moreover, when attacked by TextFooler with synonym replacement, SEQA demonstrates much less performance drops than baselines, thereby indicating stronger robustness.","offset":5,"pro":0.5,"labels":"RST"},{"idx":1,"sentence":"However, such scores from language models can be easily affected by irrelevant factors, such as word frequencies, sentence structures, etc.","offset":6,"pro":0.6,"labels":"GAP"},{"idx":1,"sentence":"These distracting factors may not only mislead the model to choose a wrong answer but also make it oversensitive to lexical perturbations in candidate answers.","offset":7,"pro":0.7,"labels":"GAP"},{"idx":1,"sentence":"We evaluate the proposed method on four benchmark datasets,","offset":8,"pro":0.8,"labels":"MTD"},{"idx":1,"sentence":"and our method achieves the best results in unsupervised settings.","offset":9,"pro":0.9,"labels":"RST"},{"idx":2,"sentence":"CommonsenseQA (CQA) (Talmor et al., 2019) dataset was recently released to advance the research on common-sense question answering (QA) task.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"Whereas the prior work has mostly focused on proposing QA models for this dataset, our aim is to retrieve as well as generate explanation for a given (question, correct answer choice, incorrect answer choices) tuple from this dataset.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":2,"sentence":"Our explanation definition is based on certain desiderata, and translates an explanation into a set of positive and negative common-sense properties (aka facts) which not only explain the correct answer choice but also refute the incorrect ones.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":2,"sentence":"We human-annotate a first-of-its-kind dataset (called ECQA) of positive and negative properties, as well as free-flow explanations, for 11K QA pairs taken from the CQA dataset.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":2,"sentence":"We propose a latent representation based property retrieval model as well as a GPT-2 based property generation model with a novel two step fine-tuning procedure.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":2,"sentence":"We also propose a free-flow explanation generation model.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":2,"sentence":"Extensive experiments show that our retrieval model beats BM25 baseline by a relative gain of 100% in F_1 score, property generation model achieves a respectable F_1 score of 36.4, and free-flow generation model achieves a similarity score of 61.9, where last two scores are based on a human correlated semantic similarity metric.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":3,"sentence":"In several question answering benchmarks, pretrained models have reached human parity through fine-tuning on an order of 100,000 annotated questions and answers.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"We propose a new pretraining scheme tailored for question answering: recurring span selection.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":3,"sentence":"Given a passage with multiple sets of recurring spans, we mask in each set all recurring spans but one, and ask the model to select the correct span in the passage for each masked span.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":3,"sentence":"Masked spans are replaced with a special token, viewed as a question representation, that is later used during fine-tuning to select the answer span.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":3,"sentence":"The resulting model obtains surprisingly good results on multiple benchmarks (e.g., 72.7 F1 on SQuAD with only 128 training examples), while maintaining competitive performance in the high-resource setting.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":3,"sentence":"We explore the more realistic few-shot setting, where only a few hundred training examples are available, and observe that standard models perform poorly, highlighting the discrepancy between current pretraining objectives and question answering.","offset":5,"pro":0.8333333333333334,"labels":"GAP"},{"idx":4,"sentence":"To date, most of recent work under the retrieval-reader framework for open-domain QA focuses on either extractive or generative reader exclusively.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"In this paper, we study a hybrid approach for leveraging the strengths of both models.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":4,"sentence":"We apply novel techniques to enhance both extractive and generative readers built upon recent pretrained neural language models, and find that proper training methods can provide large improvement over previous state-of-the-art models.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":4,"sentence":"We demonstrate that a simple hybrid approach by combining answers from both readers can efficiently take advantages of extractive and generative answer inference strategies and outperforms single models as well as homogeneous ensembles.","offset":3,"pro":0.6,"labels":"CLN"},{"idx":4,"sentence":"Our approach outperforms previous state-of-the-art models by 3.3 and 2.7 points in exact match on NaturalQuestions and TriviaQA respectively.","offset":4,"pro":0.8,"labels":"RST"},{"idx":5,"sentence":"Neural models have shown impressive performance gains in answering queries from natural language text.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"However, existing works are unable to support database queries, such as List/Count all female athletes who were born in 20th century, which require reasoning over sets of relevant facts with operations such as join, filtering and aggregation.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":5,"sentence":"We evaluate the architecture using WikiNLDB, a novel dataset for exploring such queries.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":5,"sentence":"Our architecture scales to databases containing thousands of facts whereas contemporary models are limited by how many facts can be encoded.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":5,"sentence":"In direct comparison on small databases, our approach increases overall answer accuracy from 85% to 90%.","offset":4,"pro":0.5,"labels":"RST"},{"idx":5,"sentence":"On larger databases, our approach retains its accuracy whereas transformer baselines could not encode the context.","offset":5,"pro":0.625,"labels":"RST"},{"idx":5,"sentence":"We show that while state-of-the-art transformer models perform very well for small databases, they exhibit limitations in processing noisy data, numerical operations, and queries that aggregate facts.","offset":6,"pro":0.75,"labels":"GAP"},{"idx":5,"sentence":"We propose a modular architecture to answer these database-style queries over multiple spans from text and aggregating these at scale.","offset":7,"pro":0.875,"labels":"PUR"},{"idx":6,"sentence":"In Machine Translation, assessing the quality of a large amount of automatic translations can be challenging.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"Automatic metrics are not reliable when it comes to high performing systems.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":6,"sentence":"In addition, resorting to human evaluators can be expensive, especially when evaluating multiple systems.","offset":2,"pro":0.4,"labels":"GAP"},{"idx":6,"sentence":"To overcome the latter challenge, we propose a novel application of online learning that, given an ensemble of Machine Translation systems, dynamically converges to the best systems, by taking advantage of the human feedback available.","offset":3,"pro":0.6,"labels":"PUR"},{"idx":6,"sentence":"Our experiments on WMT'19 datasets show that our online approach quickly converges to the top-3 ranked systems for the language pairs considered, despite the lack of human feedback for many translations.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":7,"sentence":"In this work, we provide a systematic and comprehensive empirical comparison of pretrained multilingual language models versus their monolingual counterparts with regard to their monolingual task performance.","offset":0,"pro":0,"labels":"PUR"},{"idx":7,"sentence":"We study a set of nine typologically diverse languages with readily available pretrained monolingual models on a set of five diverse monolingual downstream tasks.","offset":1,"pro":0.14285714285714285,"labels":"MTD"},{"idx":7,"sentence":"We first aim to establish, via fair and controlled comparisons, if a gap between the multilingual and the corresponding monolingual representation of that language exists, and subsequently investigate the reason for any performance difference.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":7,"sentence":"To disentangle conflating factors, we train new monolingual models on the same data, with monolingually and multilingually trained tokenizers.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":7,"sentence":"We find that while the pretraining data size is an important factor, a designated monolingual tokenizer plays an equally important role in the downstream performance.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":7,"sentence":"Our results show that languages that are adequately represented in the multilingual model's vocabulary exhibit negligible performance decreases over their monolingual counterparts.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":7,"sentence":"We further find that replacing the original multilingual tokenizer with the specialized monolingual tokenizer improves the downstream performance of the multilingual model for almost every task and language.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":8,"sentence":"Cross-lingual transfer has improved greatly through multi-lingual language model pretraining, reducing the need for parallel data and increasing absolute performance.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"In this paper, we address what effects morphological typology has on zero-shot cross-lingual transfer for two tasks: Part-of-speech tagging and sentiment analysis.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":8,"sentence":"We perform experiments on 19 languages from four language typologies (fusional, isolating, agglutinative, and introflexive) and find that transfer to another morphological type generally implies a higher loss than transfer to another language with the same morphological typology.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":8,"sentence":"Furthermore, POS tagging is more sensitive to morphological typology than sentiment analysis and, on this task, models perform much better on fusional languages than on the other typologies.","offset":3,"pro":0.5,"labels":"CLN"},{"idx":8,"sentence":"However, this progress has also brought to light the differences in performance across languages.","offset":4,"pro":0.6666666666666666,"labels":"GAP"},{"idx":8,"sentence":"Specifically, certain language families and typologies seem to consistently perform worse in these models.","offset":5,"pro":0.8333333333333334,"labels":"GAP"},{"idx":9,"sentence":"Generating code-switched text is a problem of growing interest, especially given the scarcity of corpora containing large volumes of real code-switched text.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"In this work, we adapt a state-of-the-art neural machine translation model to generate Hindi-English code-switched sentences starting from monolingual Hindi sentences.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":9,"sentence":"We outline a carefully designed curriculum of pretraining steps, including the use of synthetic code-switched text, that enable the model to generate high-quality code-switched text.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":9,"sentence":"Using text generated from our model as data augmentation, we show significant reductions in perplexity on a language modeling task, compared to using text from other generative models of CS text.","offset":3,"pro":0.6,"labels":"RST"},{"idx":9,"sentence":"We also show improvements using our text for a downstream code-switched natural language inference task.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":10,"sentence":"It is generally believed that a translation memory (TM) should be beneficial for machine translation tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"Unfortunately, existing wisdom demonstrates the superiority of TM-based neural machine translation (NMT) only on the TM-specialized translation tasks rather than general tasks, with a non-negligible computational overhead.","offset":1,"pro":0.25,"labels":"GAP"},{"idx":10,"sentence":"In this paper, we propose a fast and accurate approach to TM-based NMT within the Transformer framework: the model architecture is simple and employs a single bilingual sentence as its TM, leading to efficient training and inference; and its parameters are effectively optimized through a novel training criterion.","offset":2,"pro":0.5,"labels":"PUR"},{"idx":10,"sentence":"Extensive experiments on six TM-specialized tasks show that the proposed approach substantially surpasses several strong baselines that use multiple TMs, in terms of BLEU and running time.","offset":3,"pro":0.75,"labels":"CLN"},{"idx":11,"sentence":"Online misogyny, a category of online abusive language, has serious and harmful social consequences.","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"Automatic detection of misogynistic language online, while imperative, poses complicated challenges to both data gathering, data annotation, and bias mitigation, as this type of data is linguistically complex and diverse.","offset":1,"pro":0.25,"labels":"GAP"},{"idx":11,"sentence":"This paper makes three contributions in this area: Firstly, we describe the detailed design of our iterative annotation process and codebook.","offset":2,"pro":0.5,"labels":"PUR"},{"idx":11,"sentence":"Secondly, we present a comprehensive taxonomy of labels for annotating misogyny in natural written language, and finally, we introduce a high-quality dataset of annotated posts sampled from social media posts.","offset":3,"pro":0.75,"labels":"MTD"},{"idx":12,"sentence":"Recently, considerable literature has grown up around the theme of few-shot named entity recognition (NER), but little published benchmark data specifically focused on the practical and challenging task.","offset":0,"pro":0,"labels":"GAP"},{"idx":12,"sentence":"Current approaches collect existing supervised NER datasets and re-organize them to the few-shot setting for empirical study.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":12,"sentence":"These strategies conventionally aim to recognize coarse-grained entity types with few examples, while in practice, most unseen entity types are fine-grained.","offset":2,"pro":0.2222222222222222,"labels":"BAC"},{"idx":12,"sentence":"In this paper, we present Few-NERD, a large-scale human-annotated few-shot NER dataset with a hierarchy of 8 coarse-grained and 66 fine-grained entity types.","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":12,"sentence":"Few-NERD consists of 188,238 sentences from Wikipedia, 4,601,160 words are included and each is annotated as context or a part of the two-level entity type.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":12,"sentence":"To the best of our knowledge, this is the first few-shot NER dataset and the largest human-crafted NER dataset.","offset":5,"pro":0.5555555555555556,"labels":"CTN"},{"idx":12,"sentence":"We construct benchmark tasks with different emphases to comprehensively assess the generalization capability of models.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":12,"sentence":"Extensive empirical results and analysis show that Few-NERD is challenging and the problem requires further research.","offset":7,"pro":0.7777777777777778,"labels":"CLN"},{"idx":12,"sentence":"The Few-NERD dataset and the baselines will be publicly available to facilitate the research on this problem.","offset":8,"pro":0.8888888888888888,"labels":"CTN"},{"idx":13,"sentence":"Metaphor involves not only a linguistic phenomenon, but also a cognitive phenomenon structuring human thought, which makes understanding it challenging.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"As a means of cognition, metaphor is rendered by more than texts alone, and multimodal information in which vision/audio content is integrated with the text can play an important role in expressing and understanding metaphor.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":13,"sentence":"However, previous metaphor processing and understanding has focused on texts, partly due to the unavailability of large-scale datasets with ground truth labels of multimodal metaphor.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":13,"sentence":"In this paper, we introduce MultiMET, a novel multimodal metaphor dataset to facilitate understanding metaphorical information from multimodal text and image.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":13,"sentence":"It contains 10,437 text-image pairs from a range of sources with multimodal annotations of the occurrence of metaphors, domain relations, sentiments metaphors convey, and author intents.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":13,"sentence":"Moreover, we propose a range of strong baselines and show the importance of combining multimodal cues for metaphor understanding.","offset":5,"pro":0.625,"labels":"RST"},{"idx":13,"sentence":"MultiMET will be released publicly for research.","offset":6,"pro":0.75,"labels":"CTN"},{"idx":13,"sentence":"MultiMET opens the door to automatic metaphor understanding by investigating multimodal cues and their interplay.","offset":7,"pro":0.875,"labels":"CTN"},{"idx":14,"sentence":"Undermining the impact of hateful content with informed and non-aggressive responses, called counter narratives, has emerged as a possible solution for having healthier online communities.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"Thus, some NLP studies have started addressing the task of counter narrative generation.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":14,"sentence":"Although such studies have made an effort to build hate speech / counter narrative (HS/CN) datasets for neural generation, they fall short in reaching either high-quality and/or high-quantity.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":14,"sentence":"In this paper, we propose a novel human-in-the-loop data collection methodology in which a generative language model is refined iteratively by using its own data from the previous loops to generate new training samples that experts review and/or post-edit.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":14,"sentence":"Our experiments comprised several loops including diverse dynamic variations.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":14,"sentence":"Results show that the methodology is scalable and facilitates diverse, novel, and cost-effective data collection.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":14,"sentence":"To our knowledge, the resulting dataset is the only expert-based multi-target HS/CN dataset available to the community.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":15,"sentence":"Recent work has investigated the interesting question using pre-trained language models (PLMs) as knowledge bases for answering open questions.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"However, existing work is limited in using small benchmarks with high test-train overlaps.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":15,"sentence":"Experiments show that it is challenging for BART to remember training facts in high precision, and also challenging to answer closed-book questions even if relevant knowledge is retained.","offset":2,"pro":0.4,"labels":"RST"},{"idx":15,"sentence":"We construct a new dataset of closed-book QA using SQuAD, and investigate the performance of BART.","offset":3,"pro":0.6,"labels":"PUR"},{"idx":15,"sentence":"Some promising directions are found, including decoupling the knowledge memorizing process and the QA finetune process, forcing the model to recall relevant knowledge when question answering.","offset":4,"pro":0.8,"labels":"RST"},{"idx":16,"sentence":"This paper studies joint models for selecting correct answer sentences among the top k provided by answer sentence selection (AS2) modules, which are core components of retrieval-based Question Answering (QA) systems.","offset":0,"pro":0,"labels":"PUR"},{"idx":16,"sentence":"Our work shows that a critical step to effectively exploiting an answer set regards modeling the interrelated information between pair of answers.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":16,"sentence":"For this purpose, we build a three-way multi-classifier, which decides if an answer supports, refutes, or is neutral with respect to another one.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":16,"sentence":"More specifically, our neural architecture integrates a state-of-the-art AS2 module with the multi-classifier, and a joint layer connecting all components.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":16,"sentence":"We tested our models on WikiQA, TREC-QA, and a real-world dataset.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":16,"sentence":"The results show that our models obtain the new state of the art in AS2.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":17,"sentence":"In open-domain question answering, questions are highly likely to be ambiguous because users may not know the scope of relevant topics when formulating them.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"Therefore, a system needs to find possible interpretations of the question, and predict one or multiple plausible answers.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":17,"sentence":"In this paper, we present a model that aggregates and combines evidence from multiple passages to adaptively predict a single answer or a set of question-answer pairs for ambiguous questions.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":17,"sentence":"In addition, we propose a novel round-trip prediction approach to iteratively generate additional interpretations that our model fails to find in the first pass, and then verify and filter out the incorrect question-answer pairs to arrive at the final disambiguated output.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":17,"sentence":"Our model, named Refuel, achieves a new state-of-the-art performance on the AmbigQA dataset, and shows competitive performance on NQ-Open and TriviaQA.","offset":4,"pro":0.5,"labels":"CLN"},{"idx":17,"sentence":"The proposed round-trip prediction is a model-agnostic general approach for answering ambiguous open-domain questions, which improves our Refuel as well as several baseline models.","offset":5,"pro":0.625,"labels":"CLN"},{"idx":17,"sentence":"We release source code for our models and experiments at https://github.com/amzn/refuel-open-domain-qa.","offset":6,"pro":0.75,"labels":"CTN"},{"idx":17,"sentence":"When multiple plausible answers are found, the system should rewrite the question for each answer to resolve the ambiguity.","offset":7,"pro":0.875,"labels":"BAC"},{"idx":18,"sentence":"Hybrid data combining both tabular and textual content (e.g., financial reports) are quite pervasive in the real world.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"However, Question Answering (QA) over such hybrid data is largely neglected in existing research.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":18,"sentence":"In this work, we extract samples from real financial reports to build a new large-scale QA dataset containing both Tabular And Textual data, named TAT-QA, where numerical reasoning is usually required to infer the answer, such as addition, subtraction, multiplication, division, counting, comparison/sorting, and the compositions.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":18,"sentence":"We further propose a novel QA model termed TAGOP, which is capable of reasoning over both tables and text.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":18,"sentence":"It adopts sequence tagging to extract relevant cells from the table along with relevant spans from the text to infer their semantics, and then applies symbolic reasoning over them with a set of aggregation operators to arrive at the final answer.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":18,"sentence":"TAGOP achieves 58.0% inF1, which is an 11.1% absolute increase over the previous best baseline model, according to our experiments on TAT-QA.","offset":5,"pro":0.625,"labels":"RST"},{"idx":18,"sentence":"But this result still lags far behind performance of expert human, i.e.90.8% in F1.","offset":6,"pro":0.75,"labels":"RST"},{"idx":18,"sentence":"It is demonstrated that our TAT-QA is very challenging and can serve as a benchmark for training and testing powerful QA models that address hybrid form data.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":19,"sentence":"Conversational KBQA is about answering a sequence of questions related to a KB.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"Follow-up questions in conversational KBQA often have missing information referring to entities from the conversation history.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":19,"sentence":"In this paper, we propose to model these implied entities, which we refer to as the focal entities of the conversation.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":19,"sentence":"We propose a novel graph-based model to capture the transitions of focal entities and apply a graph neural network to derive a probability distribution of focal entities for each question, which is then combined with a standard KBQA module to perform answer ranking.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":19,"sentence":"Our experiments on two datasets demonstrate the effectiveness of our proposed method.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":0,"sentence":"This paper introduces the task of factual error correction: performing edits to a claim so that the generated rewrite is better supported by evidence.","offset":0,"pro":0,"labels":"PUR"},{"idx":0,"sentence":"This extends the well-studied task of fact verification by providing a mechanism to correct written texts that are refuted or only partially supported by evidence.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":0,"sentence":"We demonstrate that it is feasible to train factual error correction systems from existing fact checking datasets which only contain labeled claims accompanied by evidence, but not the correction.","offset":2,"pro":0.3333333333333333,"labels":"CLN"},{"idx":0,"sentence":"We achieve this by employing a two-stage distant supervision approach that incorporates evidence into masked claims when generating corrections.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":0,"sentence":"Our approach, based on the T5 transformer and using retrieved evidence, achieved better results than existing work which used a pointer copy network and gold evidence, producing accurate factual error corrections for 5x more instances in human evaluation and a .125 increase in SARI score.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":0,"sentence":"The evaluation is conducted on a dataset of 65,000 instances based on a recent fact verification shared task and we release it to enable further work on the task.","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":1,"sentence":"We present algorithms for aligning components of Abstract Meaning Representation (AMR) graphs to spans in English sentences.","offset":0,"pro":0,"labels":"MTD"},{"idx":1,"sentence":"We leverage unsupervised learning in combination with heuristics, taking the best of both worlds from previous AMR aligners.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":1,"sentence":"Our unsupervised models, however, are more sensitive to graph substructures, without requiring a separate syntactic parse.","offset":2,"pro":0.4,"labels":"RST"},{"idx":1,"sentence":"Our approach covers a wider variety of AMR substructures than previously considered, achieves higher coverage of nodes and edges, and does so with higher accuracy.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":1,"sentence":"We will release our LEAMR datasets and aligner for use in research on AMR parsing, generation, and evaluation.","offset":4,"pro":0.8,"labels":"CTN"},{"idx":2,"sentence":"Natural language is compositional; the meaning of a sentence is a function of the meaning of its parts.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"This property allows humans to create and interpret novel sentences, generalizing robustly outside their prior experience.","offset":1,"pro":0.125,"labels":"MTD"},{"idx":2,"sentence":"Neural networks have been shown to struggle with this kind of generalization, in particular performing poorly on tasks designed to assess compositional generalization (i.e. where training and testing distributions differ in ways that would be trivial for a compositional strategy to resolve).","offset":2,"pro":0.25,"labels":"BAC"},{"idx":2,"sentence":"Their poor performance on these tasks may in part be due to the nature of supervised learning which assumes training and testing data to be drawn from the same distribution.","offset":3,"pro":0.375,"labels":"GAP"},{"idx":2,"sentence":"We implement a meta-learning augmented version of supervised learning whose objective directly optimizes for out-of-distribution generalization.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":2,"sentence":"We construct pairs of tasks for meta-learning by sub-sampling existing training data.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":2,"sentence":"Each pair of tasks is constructed to contain relevant examples, as determined by a similarity metric, in an effort to inhibit models from memorizing their input.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":2,"sentence":"Experimental results on the COGS and SCAN datasets show that our similarity-driven meta-learning can improve generalization performance.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":3,"sentence":"Large pre-trained models such as BERT are known to improve different downstream NLP tasks, even when such a model is trained on a generic domain.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"Moreover, recent studies have shown that when large domain-specific corpora are available, continued pre-training on domain-specific data can further improve the performance of in-domain tasks.","offset":1,"pro":0.1,"labels":"GAP"},{"idx":3,"sentence":"However, this practice requires significant domain-specific data and computational resources which may not always be available.","offset":2,"pro":0.2,"labels":"GAP"},{"idx":3,"sentence":"In this paper, we aim to adapt a generic pretrained model with a relatively small amount of domain-specific data.","offset":3,"pro":0.3,"labels":"PUR"},{"idx":3,"sentence":"We demonstrate that by explicitly incorporating multi-granularity information of unseen and domain-specific words via the adaptation of (word based) n-grams, the performance of a generic pretrained model can be greatly improved.","offset":4,"pro":0.4,"labels":"CLN"},{"idx":3,"sentence":"Specifically, we introduce a Transformer-based Domain-aware N-gram Adaptor, T-DNA, to effectively learn and incorporate the semantic representation of different combinations of words in the new domain.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":3,"sentence":"Experimental results illustrate the effectiveness of T-DNA on eight low-resource downstream tasks from four domains.","offset":6,"pro":0.6,"labels":"CLN"},{"idx":3,"sentence":"We show that T-DNA is able to achieve significant improvements compared to existing methods on most tasks using limited data with lower computational costs.","offset":7,"pro":0.7,"labels":"CLN"},{"idx":3,"sentence":"Moreover, further analyses demonstrate the importance and effectiveness of both unseen words and the information of different granularities.","offset":8,"pro":0.8,"labels":"CLN"},{"idx":3,"sentence":"Our code is available at https://github.com/shizhediao/T-DNA.","offset":9,"pro":0.9,"labels":"CTN"},{"idx":4,"sentence":"Pre-trained Language Models (PLMs) have shown superior performance on various downstream Natural Language Processing (NLP) tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"However, conventional pre-training objectives do not explicitly model relational facts in text, which are crucial for textual understanding.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":4,"sentence":"To address this issue, we propose a novel contrastive learning framework ERICA to obtain a deep understanding of the entities and their relations in text.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":4,"sentence":"Specifically, we define two novel pre-training tasks to better understand entities and relations: (1) the entity discrimination task to distinguish which tail entity can be inferred by the given head entity and relation; (2) the relation discrimination task to distinguish whether two relations are close or not semantically, which involves complex relational reasoning.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":4,"sentence":"Experimental results demonstrate that ERICA can improve typical PLMs (BERT and RoBERTa) on several language understanding tasks, including relation extraction, entity typing and question answering, especially under low-resource settings.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":5,"sentence":"The Emotion Cause Extraction (ECE) task aims to identify clauses which contain emotion-evoking information for a particular emotion expressed in text.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"We observe that a widely-used ECE dataset exhibits a bias that the majority of annotated cause clauses are either directly before their associated emotion clauses or are the emotion clauses themselves.","offset":1,"pro":0.14285714285714285,"labels":"RST"},{"idx":5,"sentence":"Existing models for ECE tend to explore such relative position information and suffer from the dataset bias.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":5,"sentence":"To investigate the degree of reliance of existing ECE models on clause relative positions, we propose a novel strategy to generate adversarial examples in which the relative position information is no longer the indicative feature of cause clauses.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":5,"sentence":"We test the performance of existing models on such adversarial examples and observe a significant performance drop.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":5,"sentence":"To address the dataset bias, we propose a novel graph-based method to explicitly model the emotion triggering paths by leveraging the commonsense knowledge to enhance the semantic dependencies between a candidate clause and an emotion clause.","offset":5,"pro":0.7142857142857143,"labels":"PUR"},{"idx":5,"sentence":"Experimental results show that our proposed approach performs on par with the existing state-of-the-art methods on the original ECE dataset, and is more robust against adversarial attacks compared to existing models.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":6,"sentence":"Previous work on review summarization focused on measuring the sentiment toward the main aspects of the reviewed product or business, or on creating a textual summary.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"These approaches provide only a partial view of the data: aspect-based sentiment summaries lack sufficient explanation or justification for the aspect rating, while textual summaries do not quantify the significance of each element, and are not well-suited for representing conflicting views.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":6,"sentence":"Recently, Key Point Analysis (KPA) has been proposed as a summarization framework that provides both textual and quantitative summary of the main points in the data.","offset":2,"pro":0.3333333333333333,"labels":"BAC"},{"idx":6,"sentence":"We adapt KPA to review data by introducing Collective Key Point Mining for better key point extraction; integrating sentiment analysis into KPA; identifying good key point candidates for review summaries; and leveraging the massive amount of available reviews and their metadata.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":6,"sentence":"We show empirically that these novel extensions of KPA substantially improve its performance.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":6,"sentence":"We demonstrate that promising results can be achieved without any domain-specific annotation, while human supervision can lead to further improvement.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":7,"sentence":"Structured sentiment analysis attempts to extract full opinion tuples from a text, but over time this task has been subdivided into smaller and smaller sub-tasks, e.g., target extraction or targeted polarity classification.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"We argue that this division has become counterproductive and propose a new unified framework to remedy the situation.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":7,"sentence":"We cast the structured sentiment problem as dependency graph parsing, where the nodes are spans of sentiment holders, targets and expressions, and the arcs are the relations between them.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":7,"sentence":"We perform experiments on five datasets in four languages (English, Norwegian, Basque, and Catalan) and show that this approach leads to strong improvements over state-of-the-art baselines.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":7,"sentence":"Our analysis shows that refining the sentiment graphs with syntactic dependency information further improves results.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":8,"sentence":"Fine-tuning pre-trained cross-lingual language models can transfer task-specific supervision from one language to the others.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"In this work, we propose to improve cross-lingual fine-tuning with consistency regularization.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":8,"sentence":"Specifically, we use example consistency regularization to penalize the prediction sensitivity to four types of data augmentations, i.e., subword sampling, Gaussian noise, code-switch substitution, and machine translation.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":8,"sentence":"In addition, we employ model consistency to regularize the models trained with two augmented versions of the same training set.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":8,"sentence":"Experimental results on the XTREME benchmark show that our method significantly improves cross-lingual fine-tuning across various tasks, including text classification, question answering, and sequence labeling.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":9,"sentence":"The cross-lingual language models are typically pretrained with masked language modeling on multilingual text or parallel sentences.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"In this paper, we introduce denoising word alignment as a new cross-lingual pre-training task.","offset":1,"pro":0.1111111111111111,"labels":"PUR"},{"idx":9,"sentence":"Specifically, the model first self-label word alignments for parallel sentences.","offset":2,"pro":0.2222222222222222,"labels":"MTD"},{"idx":9,"sentence":"Then we randomly mask tokens in a bitext pair.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":9,"sentence":"Given a masked token, the model uses a pointer network to predict the aligned token in the other language.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":9,"sentence":"We alternately perform the above two steps in an expectation-maximization manner.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":9,"sentence":"Experimental results show that our method improves cross-lingual transferability on various datasets, especially on the token-level tasks, such as question answering, and structured prediction.","offset":6,"pro":0.6666666666666666,"labels":"CLN"},{"idx":9,"sentence":"Moreover, the model can serve as a pretrained word aligner, which achieves reasonably low error rate on the alignment benchmarks.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":9,"sentence":"The code and pretrained parameters are available at github.com/CZWin32768/XLM-Align.","offset":8,"pro":0.8888888888888888,"labels":"CTN"},{"idx":10,"sentence":"Knowledge distillation (KD) is commonly used to construct synthetic data for training non-autoregressive translation (NAT) models.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"However, there exists a discrepancy on low-frequency words between the distilled and the original data, leading to more errors on predicting low-frequency words.","offset":1,"pro":0.1,"labels":"GAP"},{"idx":10,"sentence":"To alleviate the problem, we directly expose the raw data into NAT by leveraging pretraining.","offset":2,"pro":0.2,"labels":"PUR"},{"idx":10,"sentence":"By analyzing directed alignments, we found that KD makes low-frequency source words aligned with targets more deterministically but fails to align sufficient low-frequency words from target to source.","offset":3,"pro":0.3,"labels":"RST"},{"idx":10,"sentence":"Accordingly, we propose reverse KD to rejuvenate more alignments for low-frequency target words.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":10,"sentence":"To make the most of authentic and synthetic data, we combine these complementary approaches as a new training strategy for further boosting NAT performance.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":10,"sentence":"We conduct experiments on five translation benchmarks over two advanced architectures.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":10,"sentence":"Results demonstrate that the proposed approach can significantly and universally improve translation quality by reducing translation errors on low-frequency words.","offset":7,"pro":0.7,"labels":"CLN"},{"idx":10,"sentence":"Encouragingly, our approach achieves 28.2 and 33.9 BLEU points on the WMT14 English-German and WMT16 Romanian-English datasets, respectively.","offset":8,"pro":0.8,"labels":"RST"},{"idx":10,"sentence":"Our code, data, and trained models are available at url{https://github.com/longyuewangdcu/RLFW-NAT.","offset":9,"pro":0.9,"labels":"CTN"},{"idx":11,"sentence":"Document-level MT models are still far from satisfactory.","offset":0,"pro":0,"labels":"RST"},{"idx":11,"sentence":"Existing work extend translation unit from single sentence to multiple sentences.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":11,"sentence":"However, study shows that when we further enlarge the translation unit to a whole document, supervised training of Transformer can fail.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":11,"sentence":"In this paper, we find such failure is not caused by overfitting, but by sticking around local minima during training.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":11,"sentence":"Our analysis shows that the increased complexity of target-to-source attention is a reason for the failure.","offset":4,"pro":0.5714285714285714,"labels":"CLN"},{"idx":11,"sentence":"As a solution, we propose G-Transformer, introducing locality assumption as an inductive bias into Transformer, reducing the hypothesis space of the attention from target to source.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":11,"sentence":"Experiments show that G-Transformer converges faster and more stably than Transformer, achieving new state-of-the-art BLEU scores for both nonpretraining and pre-training settings on three benchmark datasets.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":12,"sentence":"The Neural Machine Translation (NMT) model is essentially a joint language model conditioned on both the source sentence and partial translation.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"Therefore, the NMT model naturally involves the mechanism of the Language Model (LM) that predicts the next token only based on partial translation.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":12,"sentence":"Despite its success, NMT still suffers from the hallucination problem, generating fluent but inadequate translations.","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":12,"sentence":"The main reason is that NMT pays excessive attention to the partial translation while neglecting the source sentence to some extent, namely overconfidence of the LM.","offset":3,"pro":0.3333333333333333,"labels":"GAP"},{"idx":12,"sentence":"Accordingly, we define the Margin between the NMT and the LM, calculated by subtracting the predicted probability of the LM from that of the NMT model for each token.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":12,"sentence":"The Margin is negatively correlated to the overconfidence degree of the LM.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":12,"sentence":"Based on the property, we propose a Margin-based Token-level Objective (MTO) and a Margin-based Sentence-level Objective (MSO) to maximize the Margin for preventing the LM from being overconfident.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":12,"sentence":"Experiments on WMT14 English-to-German, WMT19 Chinese-to-English, and WMT14 English-to-French translation tasks demonstrate the effectiveness of our approach, with 1.36, 1.50, and 0.63 BLEU improvements, respectively, compared to the Transformer baseline.","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":12,"sentence":"The human evaluation further verifies that our approaches improve translation adequacy as well as fluency.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":13,"sentence":"Emotional support is a crucial ability for many conversation scenarios, including social interactions, mental health support, and customer service chats.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"Following reasonable procedures and using various support skills can help to effectively provide support.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":13,"sentence":"However, due to the lack of a well-designed task and corpora of effective emotional support conversations, research on building emotional support into dialog systems remains lacking.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":13,"sentence":"In this paper, we define the Emotional Support Conversation (ESC) task and propose an ESC Framework, which is grounded on the Helping Skills Theory.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":13,"sentence":"We construct an Emotion Support Conversation dataset (ESConv) with rich annotation (especially support strategy) in a help-seeker and supporter mode.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":13,"sentence":"To ensure a corpus of high-quality conversations that provide examples of effective emotional support, we take extensive effort to design training tutorials for supporters and several mechanisms for quality control during data collection.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":13,"sentence":"Finally, we evaluate state-of-the-art dialog models with respect to the ability to provide emotional support.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":13,"sentence":"Our results show the importance of support strategies in providing effective emotional support and the utility of ESConv in training more emotional support systems.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":14,"sentence":"Existing slot filling models can only recognize pre-defined in-domain slot types from a limited slot set.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"In the practical application, a reliable dialogue system should know what it does not know.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":14,"sentence":"In this paper, we introduce a new task, Novel Slot Detection (NSD), in the task-oriented dialogue system.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":14,"sentence":"NSD aims to discover unknown or out-of-domain slot types to strengthen the capability of a dialogue system based on in-domain training data.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":14,"sentence":"Besides, we construct two public NSD datasets, propose several strong NSD baselines, and establish a benchmark for future work.","offset":4,"pro":0.6666666666666666,"labels":"CTN"},{"idx":14,"sentence":"Finally, we conduct exhaustive experiments and qualitative analysis to comprehend key challenges and provide new guidance for future directions.","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":15,"sentence":"Generating some appealing questions in open-domain conversations is an effective way to improve human-machine interactions and lead the topic to a broader or deeper direction.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"To avoid dull or deviated questions, some researchers tried to utilize answer, the future information, to guide question generation.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":15,"sentence":"However, they separate a post-question-answer (PQA) triple into two parts: post-question (PQ) and question-answer (QA) pairs, which may hurt the overall coherence.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":15,"sentence":"Besides, the QA relationship is modeled as a one-to-one mapping that is not reasonable in open-domain conversations.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":15,"sentence":"To tackle these problems, we propose a generative triple-wise model with hierarchical variations for open-domain conversational question generation (CQG).","offset":4,"pro":0.5714285714285714,"labels":"PUR"},{"idx":15,"sentence":"Latent variables in three hierarchies are used to represent the shared background of a triple and one-to-many semantic mappings in both PQ and QA pairs.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":15,"sentence":"Experimental results on a large-scale CQG dataset show that our method significantly improves the quality of questions in terms of fluency, coherence and diversity over competitive baselines.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":16,"sentence":"Neural dialogue generation models trained with the one-hot target distribution suffer from the over-confidence issue, which leads to poor generation diversity as widely reported in the literature.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"Although existing approaches such as label smoothing can alleviate this issue, they fail to adapt to diverse dialog contexts.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":16,"sentence":"In this paper, we propose an Adaptive Label Smoothing (AdaLabel) approach that can adaptively estimate a target label distribution at each time step for different contexts.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":16,"sentence":"The maximum probability in the predicted distribution is used to modify the soft target distribution produced by a novel light-weight bi-directional decoder module.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":16,"sentence":"The resulting target distribution is aware of both previous and future contexts and is adjusted to avoid over-training the dialogue model.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":16,"sentence":"Our model can be trained in an endto-end manner.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":16,"sentence":"Extensive experiments on two benchmark datasets show that our approach outperforms various competitive baselines in producing diverse responses.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":17,"sentence":"Out-of-scope intent detection is of practical importance in task-oriented dialogue systems.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"Since the distribution of outlier utterances is arbitrary and unknown in the training stage, existing methods commonly rely on strong assumptions on data distribution such as mixture of Gaussians to make inference, resulting in either complex multi-step training procedures or hand-crafted rules such as confidence threshold selection for outlier detection.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":17,"sentence":"In this paper, we propose a simple yet effective method to train an out-of-scope intent classifier in a fully end-to-end manner by simulating the test scenario in training, which requires no assumption on data distribution and no additional post-processing or threshold setting.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":17,"sentence":"Specifically, we construct a set of pseudo outliers in the training stage, by generating synthetic outliers using inliner features via self-supervision and sampling out-of-scope sentences from easily available open-domain datasets.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":17,"sentence":"The pseudo outliers are used to train a discriminative classifier that can be directly applied to and generalize well on the test task.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":17,"sentence":"We evaluate our method extensively on four benchmark dialogue datasets and observe significant improvements over state-of-the-art approaches.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":17,"sentence":"Our code has been released at https://github.com/liam0949/DCLOOS.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":18,"sentence":"Document-level event extraction aims to recognize event information from a whole piece of article.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"Existing methods are not effective due to two challenges of this task: a) the target event arguments are scattered across sentences; b) the correlation among events in a document is non-trivial to model.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":18,"sentence":"In this paper, we propose Heterogeneous Graph-based Interaction Model with a Tracker (GIT) to solve the aforementioned two challenges.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":18,"sentence":"For the first challenge, GIT constructs a heterogeneous graph interaction network to capture global interactions among different sentences and entity mentions.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":18,"sentence":"For the second, GIT introduces a Tracker module to track the extracted events and hence capture the interdependency among the events.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":18,"sentence":"Experiments on a large-scale dataset (Zheng et al, 2019) show GIT outperforms the previous methods by 2.8 F1.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":18,"sentence":"Further analysis reveals is effective in extracting multiple correlated events and event arguments that scatter across the document.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":19,"sentence":"This paper presents a novel method for nested named entity recognition.","offset":0,"pro":0,"labels":"PUR"},{"idx":19,"sentence":"As a layered method, our method extends the prior second-best path recognition method by explicitly excluding the influence of the best path.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":19,"sentence":"Our method maintains a set of hidden states at each time step and selectively leverages them to build a different potential function for recognition at each level.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":19,"sentence":"In addition, we demonstrate that recognizing innermost entities first results in better performance than the conventional outermost entities first scheme.","offset":3,"pro":0.6,"labels":"CLN"},{"idx":19,"sentence":"We provide extensive experimental results on ACE2004, ACE2005, and GENIA datasets to show the effectiveness and efficiency of our proposed method.","offset":4,"pro":0.8,"labels":"MTD"},{"idx":0,"sentence":"Modern models for event causality identification (ECI) are mainly based on supervised learning, which are prone to the data lacking problem.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"Unfortunately, the existing NLP-related augmentation methods cannot directly produce available data required for this task.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":0,"sentence":"To solve the data lacking problem, we introduce a new approach to augment training data for event causality identification, by iteratively generating new examples and classifying event causality in a dual learning framework.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":0,"sentence":"On the one hand, our approach is knowledge guided, which can leverage existing knowledge bases to generate well-formed new sentences.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":0,"sentence":"On the other hand, our approach employs a dual mechanism, which is a learnable augmentation framework, and can interactively adjust the generation process to generate task-related sentences.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":0,"sentence":"Experimental results on two benchmarks EventStoryLine and Causal-TimeBank show that 1) our method can augment suitable task-related training data for ECI; 2) our method outperforms previous methods on EventStoryLine and Causal-TimeBank (+2.5 and +2.1 points on F1 value respectively).","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":1,"sentence":"Distantly supervision automatically generates plenty of training samples for relation extraction.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"However, it also incurs two major problems: noisy labels and imbalanced training data.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":1,"sentence":"Previous works focus more on reducing wrongly labeled relations (false positives) while few explore the missing relations that are caused by incompleteness of knowledge base (false negatives).","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":1,"sentence":"Furthermore, the quantity of negative labels overwhelmingly surpasses the positive ones in previous problem formulations.","offset":3,"pro":0.3333333333333333,"labels":"GAP"},{"idx":1,"sentence":"In this paper, we first provide a thorough analysis of the above challenges caused by negative data.","offset":4,"pro":0.4444444444444444,"labels":"PUR"},{"idx":1,"sentence":"Next, we formulate the problem of relation extraction into as a positive unlabeled learning task to alleviate false negative problem.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":1,"sentence":"Thirdly, we propose a pipeline approach, dubbed ReRe, that first performs sentence classification with relational labels and then extracts the subjects/objects.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":1,"sentence":"Experimental results show that the proposed method consistently outperforms existing approaches and remains excellent performance even learned with a large quantity of false positive samples.","offset":7,"pro":0.7777777777777778,"labels":"CLN"},{"idx":1,"sentence":"Source code is available online at https://github.com/redreamality/RERE-relation-extraction.","offset":8,"pro":0.8888888888888888,"labels":"CTN"},{"idx":2,"sentence":"This paper studies a new problem setting of entity alignment for knowledge graphs (KGs).","offset":0,"pro":0,"labels":"PUR"},{"idx":2,"sentence":"Since KGs possess different sets of entities, there could be entities that cannot find alignment across them, leading to the problem of dangling entities.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":2,"sentence":"As the first attempt to this problem, we construct a new dataset and design a multi-task learning framework for both entity alignment and dangling entity detection.","offset":2,"pro":0.2222222222222222,"labels":"MTD"},{"idx":2,"sentence":"The framework can opt to abstain from predicting alignment for the detected dangling entities.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":2,"sentence":"We propose three techniques for dangling entity detection that are based on the distribution of nearest-neighbor distances, i.e., nearest neighbor classification, marginal ranking and background ranking.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":2,"sentence":"After detecting and removing dangling entities, an incorporated entity alignment model in our framework can provide more robust alignment for remaining entities.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":2,"sentence":"Comprehensive experiments and analyses demonstrate the effectiveness of our framework.","offset":6,"pro":0.6666666666666666,"labels":"CLN"},{"idx":2,"sentence":"We further discover that the dangling entity detection module can, in turn, improve alignment learning and the final performance.","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":2,"sentence":"The contributed resource is publicly available to foster further research.","offset":8,"pro":0.8888888888888888,"labels":"CTN"},{"idx":3,"sentence":"How does the input segmentation of pretrained language models (PLMs) affect their interpretations of complex words?","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"We present the first study investigating this question, taking BERT as the example PLM and focusing on its semantic representations of English derivatives.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":3,"sentence":"We show that PLMs can be interpreted as serial dual-route models, i.e., the meanings of complex words are either stored or else need to be computed from the subwords, which implies that maximally meaningful input tokens should allow for the best generalization on new words.","offset":2,"pro":0.4,"labels":"CLN"},{"idx":3,"sentence":"This hypothesis is confirmed by a series of semantic probing tasks on which DelBERT (Derivation leveraging BERT), a model with derivational input segmentation, substantially outperforms BERT with WordPiece segmentation.","offset":3,"pro":0.6,"labels":"CLN"},{"idx":3,"sentence":"Our results suggest that the generalization capabilities of PLMs could be further improved if a morphologically-informed vocabulary of input tokens were used.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":4,"sentence":"Analogies play a central role in human commonsense reasoning.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"The ability to recognize analogies such as eye is to seeing what ear is to hearing, sometimes referred to as analogical proportions, shape how we structure knowledge and understand language.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":4,"sentence":"Surprisingly, however, the task of identifying such analogies has not yet received much attention in the language model era.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":4,"sentence":"In this paper, we analyze the capabilities of transformer-based language models on this unsupervised task, using benchmarks obtained from educational settings, as well as more commonly used datasets.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":4,"sentence":"We find that off-the-shelf language models can identify analogies to a certain extent, but struggle with abstract and complex relations, and results are highly sensitive to model architecture and hyperparameters.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":4,"sentence":"Overall the best results were obtained with GPT-2 and RoBERTa, while configurations using BERT were not able to outperform word embedding models.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":4,"sentence":"Our results raise important questions for future work about how, and to what extent, pre-trained language models capture knowledge about abstract semantic relations.","offset":6,"pro":0.8571428571428571,"labels":"IMP"},{"idx":5,"sentence":"This paper presents a multilingual study of word meaning representations in context.","offset":0,"pro":0,"labels":"PUR"},{"idx":5,"sentence":"We assess the ability of both static and contextualized models to adequately represent different lexical-semantic relations, such as homonymy and synonymy.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":5,"sentence":"To do so, we created a new multilingual dataset that allows us to perform a controlled evaluation of several factors such as the impact of the surrounding context or the overlap between words, conveying the same or different senses.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":5,"sentence":"A systematic assessment on four scenarios shows that the best monolingual models based on Transformers can adequately disambiguate homonyms in context.","offset":3,"pro":0.5,"labels":"CLN"},{"idx":5,"sentence":"However, as they rely heavily on context, these models fail at representing words with different senses when occurring in similar sentences.","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":5,"sentence":"Experiments are performed in Galician, Portuguese, English, and Spanish, and both the dataset (with more than 3,000 evaluation items) and new models are freely released with this study.","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":6,"sentence":"We propose to measure fine-grained domain relevance- the degree that a term is relevant to a broad (e.g., computer science) or narrow (e.g., deep learning) domain.","offset":0,"pro":0,"labels":"PUR"},{"idx":6,"sentence":"Such measurement is crucial for many downstream tasks in natural language processing.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":6,"sentence":"To handle long-tail terms, we build a core-anchored semantic graph, which uses core terms with rich description information to bridge the vast remaining fringe terms semantically.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":6,"sentence":"To support a fine-grained domain without relying on a matching corpus for supervision, we develop hierarchical core-fringe learning, which learns core and fringe terms jointly in a semi-supervised manner contextualized in the hierarchy of the domain.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":6,"sentence":"To reduce expensive human efforts, we employ automatic annotation and hierarchical positive-unlabeled learning.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":6,"sentence":"Our approach applies to big or small domains, covers head or tail terms, and requires little human effort.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":6,"sentence":"Extensive experiments demonstrate that our methods outperform strong baselines and even surpass professional human performance.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":7,"sentence":"Open-domain dialog systems have a user-centric goal: to provide humans with an engaging conversation experience.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"User engagement is one of the most important metrics for evaluating open-domain dialog systems, and could also be used as real-time feedback to benefit dialog policy learning.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":7,"sentence":"Existing work on detecting user disengagement typically requires hand-labeling many dialog samples.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":7,"sentence":"We propose HERALD, an efficient annotation framework that reframes the training data annotation process as a denoising problem.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":7,"sentence":"Specifically, instead of manually labeling training samples, we first use a set of labeling heuristics to label training samples automatically.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":7,"sentence":"We then denoise the weakly labeled data using the Shapley algorithm.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":7,"sentence":"Finally, we use the denoised data to train a user engagement detector.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":7,"sentence":"Our experiments show that HERALD improves annotation efficiency significantly and achieves 86% user disengagement detection accuracy in two dialog corpora.","offset":7,"pro":0.875,"labels":"RST"},{"idx":8,"sentence":"Conversational semantic parsers map user utterances to executable programs given dialogue histories composed of previous utterances, programs, and system responses.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"Existing parsers typically condition on rich representations of history that include the complete set of values and computations previously discussed.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":8,"sentence":"We propose a model that abstracts over values to focus prediction on type- and function-level context.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":8,"sentence":"This approach provides a compact encoding of dialogue histories and predicted programs, improving generalization and computational efficiency.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":8,"sentence":"Our model incorporates several other components, including an atomic span copy operation and structural enforcement of well-formedness constraints on predicted programs, that are particularly advantageous in the low-data regime.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":8,"sentence":"Trained on the SMCalFlow and TreeDST datasets, our model outperforms prior work by 7.3% and 10.6% respectively in terms of absolute accuracy.","offset":5,"pro":0.625,"labels":"RST"},{"idx":8,"sentence":"Trained on only a thousand examples from each dataset, it outperforms strong baselines by 12.4% and 6.4%.","offset":6,"pro":0.75,"labels":"RST"},{"idx":8,"sentence":"These results indicate that simple representations are key to effective generalization in conversational semantic parsing.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":9,"sentence":"Recently, various neural models for multi-party conversation (MPC) have achieved impressive improvements on a variety of tasks such as addressee recognition, speaker identification and response prediction.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"However, these existing methods on MPC usually represent interlocutors and utterances individually and ignore the inherent complicated structure in MPC which may provide crucial interlocutor and utterance semantics and would enhance the conversation understanding process.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":9,"sentence":"To this end, we present MPC-BERT, a pre-trained model for MPC understanding that considers learning who says what to whom in a unified model with several elaborated self-supervised tasks.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":9,"sentence":"Particularly, these tasks can be generally categorized into (1) interlocutor structure modeling including reply-to utterance recognition, identical speaker searching and pointer consistency distinction, and (2) utterance semantics modeling including masked shared utterance restoration and shared node detection.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":9,"sentence":"We evaluate MPC-BERT on three downstream tasks including addressee recognition, speaker identification and response selection.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":9,"sentence":"Experimental results show that MPC-BERT outperforms previous methods by large margins and achieves new state-of-the-art performance on all three downstream tasks at two benchmarks.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":10,"sentence":"While Transformer-based text classifiers pre-trained on large volumes of text have yielded significant improvements on a wide range of computational linguistics tasks, their implementations have been unsuitable for live incremental processing thus far, operating only on the level of complete sentence inputs.","offset":0,"pro":0,"labels":"GAP"},{"idx":10,"sentence":"We address the challenge of introducing methods for word-by-word left-to-right incremental processing to Transformers such as BERT, models without an intrinsic sense of linear order.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":10,"sentence":"We modify the training method and live decoding of non-incremental models to detect speech disfluencies with minimum latency and without pre-segmentation of dialogue acts.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":10,"sentence":"We experiment with several decoding methods to predict the rightward context of the word currently being processed using a GPT-2 language model and apply a BERT-based disfluency detector to sequences, including predicted words.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":10,"sentence":"We show our method of incrementalising Transformers maintains most of their high non-incremental performance while operating strictly incrementally.","offset":4,"pro":0.5714285714285714,"labels":"CLN"},{"idx":10,"sentence":"We also evaluate our models' incremental performance to establish the trade-off between incremental performance and final performance, using different prediction strategies.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":10,"sentence":"We apply our system to incremental speech recognition results as they arrive into a live system and achieve state-of-the-art results in this setting.","offset":6,"pro":0.8571428571428571,"labels":"MTD"},{"idx":11,"sentence":"We propose NeuralWOZ, a novel dialogue collection framework that uses model-based dialogue simulation.","offset":0,"pro":0,"labels":"PUR"},{"idx":11,"sentence":"NeuralWOZ has two pipelined models, Collector and Labeler.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":11,"sentence":"Collector generates dialogues from (1) user's goal instructions, which are the user context and task constraints in natural language, and (2) system's API call results, which is a list of possible query responses for user requests from the given knowledge base.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":11,"sentence":"Labeler annotates the generated dialogue by formulating the annotation as a multiple-choice problem, in which the candidate labels are extracted from goal instructions and API call results.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":11,"sentence":"We demonstrate the effectiveness of the proposed method in the zero-shot domain transfer learning for dialogue state tracking.","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":11,"sentence":"In the evaluation, the synthetic dialogue corpus generated from NeuralWOZ achieves a new state-of-the-art with improvements of 4.4% point joint goal accuracy on average across domains, and improvements of 5.7% point of zero-shot coverage against the MultiWOZ 2.1 dataset.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":12,"sentence":"The human mind is a dynamical system, yet many analysis techniques used to study it are limited in their ability to capture the complex dynamics that may characterize mental processes.","offset":0,"pro":0,"labels":"GAP"},{"idx":12,"sentence":"This study proposes the continuous-time deconvolutional regressive neural network (CDRNN), a deep neural extension of continuous-time deconvolutional regression (Shain {& Schuler, 2021) that jointly captures time-varying, non-linear, and delayed influences of predictors (e.g. word surprisal) on the response (e.g. reading time).","offset":1,"pro":0.25,"labels":"PUR"},{"idx":12,"sentence":"Despite this flexibility, CDRNN is interpretable and able to illuminate patterns in human cognition that are otherwise difficult to study.","offset":2,"pro":0.5,"labels":"RST"},{"idx":12,"sentence":"Behavioral and fMRI experiments reveal detailed and plausible estimates of human language processing dynamics that generalize better than CDR and other baselines, supporting a potential role for CDRNN in studying human language processing.","offset":3,"pro":0.75,"labels":"CLN"},{"idx":13,"sentence":"Transformer-based language models pre-trained on large amounts of text data have proven remarkably successful in learning generic transferable linguistic representations.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"Here we study whether structural guidance leads to more human-like systematic linguistic generalization in Transformer language models without resorting to pre-training on very large amounts of data.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":13,"sentence":"We explore two general ideas.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":13,"sentence":"The Generative Parsing idea jointly models the incremental parse and word sequence as part of the same sequence modeling task.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":13,"sentence":"The Structural Scaffold idea guides the language model's representation via additional structure loss that separately predicts the incremental constituency parse.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":13,"sentence":"We train the proposed models along with a vanilla Transformer language model baseline on a 14 million-token and a 46 million-token subset of the BLLIP dataset, and evaluate models' syntactic generalization performances on SG Test Suites and sized BLiMP.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":13,"sentence":"Experiment results across two benchmarks suggest converging evidence that generative structural supervisions can induce more robust and humanlike linguistic generalization in Transformer language models without the need for data intensive pre-training.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":14,"sentence":"While the use of character models has been popular in NLP applications, it has not been explored much in the context of psycholinguistic modeling.","offset":0,"pro":0,"labels":"GAP"},{"idx":14,"sentence":"This paper presents a character model that can be applied to a structural parser-based processing model to calculate word generation probabilities.","offset":1,"pro":0.25,"labels":"PUR"},{"idx":14,"sentence":"Experimental results show that surprisal estimates from a structural processing model using this character model deliver substantially better fits to self-paced reading, eye-tracking, and fMRI data than those from large-scale language models trained on much more data.","offset":2,"pro":0.5,"labels":"RST"},{"idx":14,"sentence":"This may suggest that the proposed processing model provides a more humanlike account of sentence processing, which assumes a larger role of morphology, phonotactics, and orthographic complexity than was previously thought.","offset":3,"pro":0.75,"labels":"CLN"},{"idx":15,"sentence":"Most previous studies integrate cognitive language processing signals (e.g., eye-tracking or EEG data) into neural models of natural language processing (NLP) just by directly concatenating word embeddings with cognitive features, ignoring the gap between the two modalities (i.e., textual vs. cognitive) and noise in cognitive features.","offset":0,"pro":0,"labels":"GAP"},{"idx":15,"sentence":"In this paper, we propose a CogAlign approach to these issues, which learns to align textual neural representations to cognitive features.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":15,"sentence":"In CogAlign, we use a shared encoder equipped with a modality discriminator to alternatively encode textual and cognitive inputs to capture their differences and commonalities.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":15,"sentence":"Additionally, a text-aware attention mechanism is proposed to detect task-related information and to avoid using noise in cognitive features.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":15,"sentence":"Experimental results on three NLP tasks, namely named entity recognition, sentiment analysis and relation extraction, show that CogAlign achieves significant improvements with multiple cognitive features over state-of-the-art models on public datasets.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":15,"sentence":"Moreover, our model is able to transfer cognitive information to other datasets that do not have any cognitive processing signals.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":16,"sentence":"Despite their impressive performance in NLP, self-attention networks were recently proved to be limited for processing formal languages with hierarchical structure, such as Dyck-k, the language consisting of well-nested parentheses of k types.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"This suggested that natural language can be approximated well with models that are too weak for formal languages, or that the role of hierarchy and recursion in natural language might be limited.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":16,"sentence":"We qualify this implication by proving that self-attention networks can process Dyck-(k, D), the subset of Dyck-k with depth bounded by D, which arguably better captures the bounded hierarchical structure of natural language.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":16,"sentence":"Specifically, we construct a hard-attention network with D+1 layers and O(log k) memory size (per token per layer) that recognizes Dyck-(k, D), and a soft-attention network with two layers and O(log k) memory size that generates Dyck-(k, D).","offset":3,"pro":0.6,"labels":"MTD"},{"idx":16,"sentence":"Experiments show that self-attention networks trained on Dyck-(k, D) generalize to longer inputs with near-perfect accuracy, and also verify the theoretical memory advantage of self-attention networks over recurrent networks.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":17,"sentence":"We present a novel approach to the problem of text style transfer.","offset":0,"pro":0,"labels":"PUR"},{"idx":17,"sentence":"Unlike previous approaches requiring style-labeled training data, our method makes use of readily-available unlabeled text by relying on the implicit connection in style between adjacent sentences, and uses labeled data only at inference time.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":17,"sentence":"We adapt T5 (Raffel et al., 2020), a strong pretrained text-to-text model, to extract a style vector from text and use it to condition the decoder to perform style transfer.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":17,"sentence":"As our label-free training results in a style vector space encoding many facets of style, we recast transfers as targeted restyling vector operations that adjust specific attributes of the input while preserving others.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":17,"sentence":"We demonstrate that training on unlabeled Amazon reviews data results in a model that is competitive on sentiment transfer, even compared to models trained fully on labeled data.","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":17,"sentence":"Furthermore, applying our novel method to a diverse corpus of unlabeled web text results in a single model capable of transferring along multiple dimensions of style (dialect, emotiveness, formality, politeness, sentiment) despite no additional training and using only a handful of exemplars at inference time.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":18,"sentence":"We describe an efficient hierarchical method to compute attention in the Transformer architecture.","offset":0,"pro":0,"labels":"PUR"},{"idx":18,"sentence":"The proposed attention mechanism exploits a matrix structure similar to the Hierarchical Matrix (H-Matrix) developed by the numerical analysis community, and has linear run time and memory complexity.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":18,"sentence":"We perform extensive experiments to show that the inductive bias embodied by our hierarchical attention is effective in capturing the hierarchical structure in the sequences typical for natural language and vision tasks.","offset":2,"pro":0.4,"labels":"RST"},{"idx":18,"sentence":"Our method is superior to alternative sub-quadratic proposals by over +6 points on average on the Long Range Arena benchmark.","offset":3,"pro":0.6,"labels":"RST"},{"idx":18,"sentence":"It also sets a new SOTA test perplexity on One-Billion Word dataset with 5x fewer model parameters than that of the previous-best Transformer-based models.","offset":4,"pro":0.8,"labels":"RST"},{"idx":19,"sentence":"The recent GPT-3 model (Brown et al., 2020) achieves remarkable few-shot performance solely by leveraging a natural-language prompt and a few task demonstrations as input context.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"Inspired by their findings, we study few-shot learning in a more practical scenario, where we use smaller language models for which fine-tuning is computationally efficient.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":19,"sentence":"We present LM-BFF-better few-shot fine-tuning of language models-a suite of simple and complementary techniques for fine-tuning language models on a small number of annotated examples.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":19,"sentence":"Our approach includes (1) prompt-based fine-tuning together with a novel pipeline for automating prompt generation; and (2) a refined strategy for dynamically and selectively incorporating demonstrations into each context.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":19,"sentence":"Finally, we present a systematic evaluation for analyzing few-shot performance on a range of NLP tasks, including classification and regression.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":19,"sentence":"Our experiments demonstrate that our methods combine to dramatically outperform standard fine-tuning procedures in this low resource setting, achieving up to 30% absolute improvement, and 11% on average across all tasks.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":19,"sentence":"Our approach makes minimal assumptions on task resources and domain expertise, and hence constitutes a strong task-agnostic method for few-shot learning.","offset":6,"pro":0.8571428571428571,"labels":"MTD"},{"idx":0,"sentence":"The Universal Trigger (UniTrigger) is a recently-proposed powerful adversarial textual attack method.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"Utilizing a learning-based mechanism, UniTrigger generates a fixed phrase that, when added to any benign inputs, can drop the prediction accuracy of a textual neural network (NN) model to near zero on a target class.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":0,"sentence":"To defend against this attack that can cause significant harm, in this paper, we borrow the honeypot concept from the cybersecurity community and propose DARCY, a honeypot-based defense framework against UniTrigger.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":0,"sentence":"DARCY greedily searches and injects multiple trapdoors into an NN model to bait and catch potential attacks.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":0,"sentence":"Through comprehensive experiments across four public datasets, we show that DARCY detects UniTrigger's adversarial attacks with up to 99% TPR and less than 2% FPR in most cases, while maintaining the prediction accuracy (in F1) for clean inputs within a 1% margin.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":0,"sentence":"We also demonstrate that DARCY with multiple trapdoors is also robust to a diverse set of attack scenarios with attackers' varying levels of knowledge and skills.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":0,"sentence":"We release the source code of DARCY at: https://github.com/lethaiq/ACL2021-DARCY-HoneypotDefenseNLP.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":1,"sentence":"Detecting rumors on social media is a very critical task with significant implications to the economy, public health, etc. Previous works generally capture effective features from texts and the propagation structure.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"However, the uncertainty caused by unreliable relations in the propagation structure is common and inevitable due to wily rumor producers and the limited collection of spread data.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":1,"sentence":"Most approaches neglect it and may seriously limit the learning of features.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":1,"sentence":"Towards this issue, this paper makes the first attempt to explore propagation uncertainty for rumor detection.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":1,"sentence":"Specifically, we propose a novel Edge-enhanced Bayesian Graph Convolutional Network (EBGCN) to capture robust structural features.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":1,"sentence":"The model adaptively rethinks the reliability of latent relations by adopting a Bayesian approach.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":1,"sentence":"Besides, we design a new edge-wise consistency training framework to optimize the model by enforcing consistency on relations.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":1,"sentence":"Experiments on three public benchmark datasets demonstrate that the proposed model achieves better performance than baseline methods on both rumor detection and early rumor detection tasks.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":2,"sentence":"Multi-label text classification is one of the fundamental tasks in natural language processing.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"Previous studies have difficulties to distinguish similar labels well because they learn the same document representations for different labels, that is they do not explicitly extract label-specific semantic components from documents.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":2,"sentence":"Moreover, they do not fully explore the high-order interactions among these semantic components, which is very helpful to predict tail labels.","offset":2,"pro":0.4,"labels":"GAP"},{"idx":2,"sentence":"In this paper, we propose a novel label-specific dual graph neural network (LDGN), which incorporates category information to learn label-specific components from documents, and employs dual Graph Convolution Network (GCN) to model complete and adaptive interactions among these components based on the statistical label co-occurrence and dynamic reconstruction graph in a joint way.","offset":3,"pro":0.6,"labels":"PUR"},{"idx":2,"sentence":"Experimental results on three benchmark datasets demonstrate that LDGN significantly outperforms the state-of-the-art models, and also achieves better performance with respect to tail labels.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":3,"sentence":"Topic models have been widely used to learn text representations and gain insight into document corpora.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"To perform topic discovery, most existing neural models either take document bag-of-words (BoW) or sequence of tokens as input followed by variational inference and BoW reconstruction to learn topic-word distribution.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":3,"sentence":"However, leveraging topic-word distribution for learning better features during document encoding has not been explored much.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":3,"sentence":"To this end, we develop a framework TAN-NTM, which processes document as a sequence of tokens through a LSTM whose contextual outputs are attended in a topic-aware manner.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":3,"sentence":"We propose a novel attention mechanism which factors in topic-word distribution to enable the model to attend on relevant words that convey topic related cues.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":3,"sentence":"The output of topic attention module is then used to carry out variational inference.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":3,"sentence":"We perform extensive ablations and experiments resulting in asciitilde9-15 percentage improvement over score of existing SOTA topic models in NPMI coherence on several benchmark datasets - 20Newsgroups, Yelp Review Polarity and AGNews.","offset":6,"pro":0.75,"labels":"RST"},{"idx":3,"sentence":"Further, we show that our method learns better latent document-topic features compared to existing topic models through improvement on two downstream tasks: document classification and topic guided keyphrase generation.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":4,"sentence":"This paper proposes an approach to cross-language sentence selection in a low-resource setting.","offset":0,"pro":0,"labels":"PUR"},{"idx":4,"sentence":"It uses data augmentation and negative sampling techniques on noisy parallel sentence data to directly learn a cross-lingual embedding-based query relevance model.","offset":1,"pro":0.25,"labels":"MTD"},{"idx":4,"sentence":"Results show that this approach performs as well as or better than multiple state-of-the-art machine translation + monolingual retrieval systems trained on the same parallel data.","offset":2,"pro":0.5,"labels":"CLN"},{"idx":4,"sentence":"Moreover, when a rationale training secondary objective is applied to encourage the model to match word alignment hints from a phrase-based statistical machine translation model, consistent improvements are seen across three language pairs (English-Somali, English-Swahili and English-Tagalog) over a variety of state-of-the-art baselines.","offset":3,"pro":0.75,"labels":"CLN"},{"idx":5,"sentence":"Question answering (QA) systems for large document collections typically use pipelines that (i) retrieve possibly relevant documents, (ii) re-rank them, (iii) rank paragraphs or other snippets of the top-ranked documents, and (iv) select spans of the top-ranked snippets as exact answers.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"Pipelines are conceptually simple, but errors propagate from one component to the next, without later components being able to revise earlier decisions.","offset":1,"pro":0.09090909090909091,"labels":"BAC"},{"idx":5,"sentence":"We present an architecture for joint document and snippet ranking, the two middle stages, which leverages the intuition that relevant documents have good snippets and good snippets come from relevant documents.","offset":2,"pro":0.18181818181818182,"labels":"PUR"},{"idx":5,"sentence":"The architecture is general and can be used with any neural text relevance ranker.","offset":3,"pro":0.2727272727272727,"labels":"MTD"},{"idx":5,"sentence":"We experiment with two main instantiations of the architecture, based on POSIT-DRMM (PDRMM) and a BERT-based ranker.","offset":4,"pro":0.36363636363636365,"labels":"MTD"},{"idx":5,"sentence":"Experiments on biomedical data from BIOASQ show that our joint models vastly outperform the pipelines in snippet retrieval, the main goal for QA, with fewer trainable parameters, also remaining competitive in document retrieval.","offset":5,"pro":0.45454545454545453,"labels":"CLN"},{"idx":5,"sentence":"Furthermore, our joint PDRMM-based model is competitive with BERT-based models, despite using orders of magnitude fewer parameters.","offset":6,"pro":0.5454545454545454,"labels":"CLN"},{"idx":5,"sentence":"These claims are also supported by human evaluation on two test batches of BIOASQ.","offset":7,"pro":0.6363636363636364,"labels":"MTD"},{"idx":5,"sentence":"To test our key findings on another dataset, we modified the Natural Questions dataset so that it can also be used for document and snippet retrieval.","offset":8,"pro":0.7272727272727273,"labels":"MTD"},{"idx":5,"sentence":"Our joint PDRMM-based model again outperforms the corresponding pipeline in snippet retrieval on the modified Natural Questions dataset, even though it performs worse than the pipeline in document retrieval.","offset":9,"pro":0.8181818181818182,"labels":"RST"},{"idx":5,"sentence":"We make our code and the modified Natural Questions dataset publicly available.","offset":10,"pro":0.9090909090909091,"labels":"CTN"},{"idx":6,"sentence":"Aiming for a better integration of data-driven and linguistically-inspired approaches, we explore whether RST Nuclearity, assigning a binary assessment of importance between text segments, can be replaced by automatically generated, real-valued scores, in what we call a Weighted-RST framework.","offset":0,"pro":0,"labels":"PUR"},{"idx":6,"sentence":"In particular, we find that weighted discourse trees from auxiliary tasks can benefit key NLP downstream applications, compared to nuclearity-centered approaches.","offset":1,"pro":0.3333333333333333,"labels":"CLN"},{"idx":6,"sentence":"We further show that real-valued importance distributions partially and interestingly align with the assessment and uncertainty of human annotators.","offset":2,"pro":0.6666666666666666,"labels":"CLN"},{"idx":7,"sentence":"Atomic clauses are fundamental text units for understanding complex sentences.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"Identifying the atomic sentences within complex sentences is important for applications such as summarization, argument mining, discourse analysis, discourse parsing, and question answering.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":7,"sentence":"Previous work mainly relies on rule-based methods dependent on parsing.","offset":2,"pro":0.2,"labels":"GAP"},{"idx":7,"sentence":"We propose a new task to decompose each complex sentence into simple sentences derived from the tensed clauses in the source, and a novel problem formulation as a graph edit task.","offset":3,"pro":0.3,"labels":"PUR"},{"idx":7,"sentence":"Our neural model learns to Accept, Break, Copy or Drop elements of a graph that combines word adjacency and grammatical dependencies.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":7,"sentence":"The full processing pipeline includes modules for graph construction, graph editing, and sentence generation from the output graph.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":7,"sentence":"We introduce DeSSE, a new dataset designed to train and evaluate complex sentence decomposition, and MinWiki, a subset of MinWikiSplit.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":7,"sentence":"ABCD achieves comparable performance as two parsing baselines on MinWiki.","offset":7,"pro":0.7,"labels":"RST"},{"idx":7,"sentence":"On DeSSE, which has a more even balance of complex sentence types, our model achieves higher accuracy on the number of atomic sentences than an encoder-decoder baseline.","offset":8,"pro":0.8,"labels":"RST"},{"idx":7,"sentence":"Results include a detailed error analysis.","offset":9,"pro":0.9,"labels":"MTD"},{"idx":8,"sentence":"Many Question-Answering (QA) datasets contain unanswerable questions, but their treatment in QA systems remains primitive.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"Our analysis of the Natural Questions (Kwiatkowski et al. 2019) dataset reveals that a substantial portion of unanswerable questions (asciitilde21%) can be explained based on the presence of unverifiable presuppositions.","offset":1,"pro":0.2,"labels":"CLN"},{"idx":8,"sentence":"Through a user preference study, we demonstrate that the oracle behavior of our proposed system-which provides responses based on presupposition failure-is preferred over the oracle behavior of existing QA systems.","offset":2,"pro":0.4,"labels":"CLN"},{"idx":8,"sentence":"Then, we present a novel framework for implementing such a system in three steps: presupposition generation, presupposition verification, and explanation generation, reporting progress on each.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":8,"sentence":"Finally, we show that a simple modification of adding presuppositions and their verifiability to the input of a competitive end-to-end QA system yields modest gains in QA performance and unanswerability detection, demonstrating the promise of our approach.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":9,"sentence":"Text-level discourse rhetorical structure (DRS) parsing is known to be challenging due to the notorious lack of training data.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"Although recent top-down DRS parsers can better leverage global document context and have achieved certain success, the performance is still far from perfect.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":9,"sentence":"To our knowledge, all previous DRS parsers make local decisions for either bottom-up node composition or top-down split point ranking at each time step, and largely ignore DRS parsing from the global view point.","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":9,"sentence":"Obviously, it is not sufficient to build an entire DRS tree only through these local decisions.","offset":3,"pro":0.3333333333333333,"labels":"GAP"},{"idx":9,"sentence":"In this work, we present our insight on evaluating the pros and cons of the entire DRS tree for global optimization.","offset":4,"pro":0.4444444444444444,"labels":"PUR"},{"idx":9,"sentence":"Specifically, based on recent well-performing top-down frameworks, we introduce a novel method to transform both gold standard and predicted constituency trees into tree diagrams with two color channels.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":9,"sentence":"After that, we learn an adversarial bot between gold and fake tree diagrams to estimate the generated DRS trees from a global perspective.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":9,"sentence":"We perform experiments on both RST-DT and CDTB corpora and use the original Parseval for performance evaluation.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":9,"sentence":"The experimental results show that our parser can substantially improve the performance when compared with previous state-of-the-art parsers.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":10,"sentence":"Discourse relations among arguments reveal logical structures of a debate conversation.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"However, no prior work has explicitly studied how the sequence of discourse relations influence a claim's impact.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":10,"sentence":"This paper empirically shows that the discourse relations between two arguments along the context path are essential factors for identifying the persuasive power of an argument.","offset":2,"pro":0.4,"labels":"CLN"},{"idx":10,"sentence":"We further propose DisCOC to inject and fuse the sentence-level structural discourse information with contextualized features derived from large-scale language models.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":10,"sentence":"Experimental results and extensive analysis show that the attention and gate mechanisms that explicitly model contexts and texts can indeed help the argument impact classification task defined by Durmus et al. (2019), and discourse structures among the context path of the claim to be classified can further boost the performance.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":11,"sentence":"This paper proposes a sophisticated neural architecture to incorporate bilingual dictionaries into Neural Machine Translation (NMT) models.","offset":0,"pro":0,"labels":"PUR"},{"idx":11,"sentence":"By introducing three novel components: Pointer, Disambiguator, and Copier, our method PDC achieves the following merits inherently compared with previous efforts: (1) Pointer leverages the semantic information from bilingual dictionaries, for the first time, to better locate source words whose translation in dictionaries can potentially be used; (2) Disambiguator synthesizes contextual information from the source view and the target view, both of which contribute to distinguishing the proper translation of a specific source word from multiple candidates in dictionaries; (3) Copier systematically connects Pointer and Disambiguator based on a hierarchical copy mechanism seamlessly integrated with Transformer, thereby building an end-to-end architecture that could avoid error propagation problems in alternative pipe-line methods.","offset":1,"pro":0.3333333333333333,"labels":"MTD"},{"idx":11,"sentence":"The experimental results on Chinese-English and English-Japanese benchmarks demonstrate the PDC's overall superiority and effectiveness of each component.","offset":2,"pro":0.6666666666666666,"labels":"CLN"},{"idx":12,"sentence":"Existing work in multilingual pretraining has demonstrated the potential of cross-lingual transferability by training a unified Transformer encoder for multiple languages.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"However, much of this work only relies on the shared vocabulary and bilingual contexts to encourage the correlation across languages, which is loose and implicit for aligning the contextual representations between languages.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":12,"sentence":"In this paper, we plug a cross-attention module into the Transformer encoder to explicitly build the interdependence between languages.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":12,"sentence":"It can effectively avoid the degeneration of predicting masked words only conditioned on the context in its own language.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":12,"sentence":"More importantly, when fine-tuning on downstream tasks, the cross-attention module can be plugged in or out on-demand, thus naturally benefiting a wider range of cross-lingual tasks, from language understanding to generation.","offset":4,"pro":0.5714285714285714,"labels":"BAC"},{"idx":12,"sentence":"As a result, the proposed cross-lingual model delivers new state-of-the-art results on various cross-lingual understanding tasks of the XTREME benchmark, covering text classification, sequence labeling, question answering, and sentence retrieval.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":12,"sentence":"For cross-lingual generation tasks, it also outperforms all existing cross-lingual models and state-of-the-art Transformer variants on WMT14 English-to-German and English-to-French translation datasets, with gains of up to 1 2 BLEU.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":13,"sentence":"The sentence is a fundamental unit of text processing.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"Yet sentences in the wild are commonly encountered not in isolation, but unsegmented within larger paragraphs and documents.","offset":1,"pro":0.1,"labels":"GAP"},{"idx":13,"sentence":"Therefore, the first step in many NLP pipelines is sentence segmentation.","offset":2,"pro":0.2,"labels":"BAC"},{"idx":13,"sentence":"Despite its importance, this step is the subject of relatively little research.","offset":3,"pro":0.3,"labels":"GAP"},{"idx":13,"sentence":"There are no standard test sets or even methods for evaluation, leaving researchers and engineers without a clear footing for evaluating and selecting models for the task.","offset":4,"pro":0.4,"labels":"GAP"},{"idx":13,"sentence":"Existing tools have relatively small language coverage, and efforts to extend them to other languages are often ad hoc.","offset":5,"pro":0.5,"labels":"GAP"},{"idx":13,"sentence":"We introduce a modern context-based modeling approach that provides a solution to the problem of segmenting punctuated text in many languages, and show how it can be trained on noisily-annotated data.","offset":6,"pro":0.6,"labels":"PUR"},{"idx":13,"sentence":"We also establish a new 23-language multilingual evaluation set.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":13,"sentence":"Our approach exceeds high baselines set by existing methods on prior English corpora (WSJ and Brown corpora), and also performs well on average on our new evaluation set.","offset":8,"pro":0.8,"labels":"RST"},{"idx":13,"sentence":"We release our tool, ersatz, as open source.","offset":9,"pro":0.9,"labels":"CTN"},{"idx":14,"sentence":"A good translation should not only translate the original content semantically, but also incarnate personal traits of the original text.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"For a real-world neural machine translation (NMT) system, these user traits (e.g., topic preference, stylistic characteristics and expression habits) can be preserved in user behavior (e.g., historical inputs).","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":14,"sentence":"However, current NMT systems marginally consider the user behavior due to: 1) the difficulty of modeling user portraits in zero-shot scenarios, and 2) the lack of user-behavior annotated parallel dataset.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":14,"sentence":"To fill this gap, we introduce a novel framework called user-driven NMT.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":14,"sentence":"Specifically, a cache-based module and a user-driven contrastive learning method are proposed to offer NMT the ability to capture potential user traits from their historical inputs under a zero-shot learning fashion.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":14,"sentence":"Furthermore, we contribute the first Chinese-English parallel corpus annotated with user behavior called UDT-Corpus.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":14,"sentence":"Experimental results confirm that the proposed user-driven NMT can generate user-specific translations.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":15,"sentence":"Lexically constrained machine translation allows the user to manipulate the output sentence by enforcing the presence or absence of certain words and phrases.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"Although current approaches can enforce terms to appear in the translation, they often struggle to make the constraint word form agree with the rest of the generated output.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":15,"sentence":"Our manual analysis shows that 46% of the errors in the output of a baseline constrained model for English to Czech translation are related to agreement.","offset":2,"pro":0.2857142857142857,"labels":"RST"},{"idx":15,"sentence":"We investigate mechanisms to allow neural machine translation to infer the correct word inflection given lemmatized constraints.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":15,"sentence":"In particular, we focus on methods based on training the model with constraints provided as part of the input sequence.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":15,"sentence":"Our experiments on English-Czech language pair show that this approach improves translation of constrained terms in both automatic and manual evaluation by reducing errors in agreement.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":15,"sentence":"Our approach thus eliminates inflection errors, without introducing new errors or decreasing overall quality of the translation.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":16,"sentence":"Technical logbooks are a challenging and under-explored text type in automated event identification.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"These texts are typically short and written in non-standard yet technical language, posing challenges to off-the-shelf NLP pipelines.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":16,"sentence":"The granularity of issue types described in these datasets additionally leads to class imbalance, making it challenging for models to accurately predict which issue each logbook entry describes.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":16,"sentence":"In this paper we focus on the problem of technical issue classification by considering logbook datasets from the automotive, aviation, and facilities maintenance domains.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":16,"sentence":"We adapt a feedback strategy from computer vision for handling extreme class imbalance, which resamples the training data based on its error in the prediction process.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":16,"sentence":"Our experiments show that with statistical significance this feedback strategy provides the best results for four different neural network models trained across a suite of seven different technical logbook datasets from distinct technical domains.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":16,"sentence":"The feedback strategy is also generic and could be applied to any learning problem with substantial class imbalances.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":17,"sentence":"An automated system that could assist a judge in predicting the outcome of a case would help expedite the judicial process.","offset":0,"pro":0,"labels":"MTD"},{"idx":17,"sentence":"For such a system to be practically useful, predictions by the system should be explainable.","offset":1,"pro":0.1,"labels":"MTD"},{"idx":17,"sentence":"To promote research in developing such a system, we introduce ILDC (Indian Legal Documents Corpus).","offset":2,"pro":0.2,"labels":"PUR"},{"idx":17,"sentence":"ILDC is a large corpus of 35k Indian Supreme Court cases annotated with original court decisions.","offset":3,"pro":0.3,"labels":"MTD"},{"idx":17,"sentence":"A portion of the corpus (a separate test set) is annotated with gold standard explanations by legal experts.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":17,"sentence":"Based on ILDC, we propose the task of Court Judgment Prediction and Explanation (CJPE).","offset":5,"pro":0.5,"labels":"MTD"},{"idx":17,"sentence":"The task requires an automated system to predict an explainable outcome of a case.","offset":6,"pro":0.6,"labels":"BAC"},{"idx":17,"sentence":"We experiment with a battery of baseline models for case predictions and propose a hierarchical occlusion based model for explainability.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":17,"sentence":"Our best prediction model has an accuracy of 78% versus 94% for human legal experts, pointing towards the complexity of the prediction task.","offset":8,"pro":0.8,"labels":"RST"},{"idx":17,"sentence":"The analysis of explanations by the proposed algorithm reveals a significant difference in the point of view of the algorithm and legal experts for explaining the judgments, pointing towards scope for future research.","offset":9,"pro":0.9,"labels":"CLN"},{"idx":18,"sentence":"We present an annotation approach to capturing emotional and cognitive empathy in student-written peer reviews on business models in German.","offset":0,"pro":0,"labels":"MTD"},{"idx":18,"sentence":"We propose an annotation scheme that allows us to model emotional and cognitive empathy scores based on three types of review components.","offset":1,"pro":0.14285714285714285,"labels":"MTD"},{"idx":18,"sentence":"Also, we conducted an annotation study with three annotators based on 92 student essays to evaluate our annotation scheme.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":18,"sentence":"The obtained inter-rater agreement of α=0.79 for the components and the multi-{mbox{pi=0.41 for the empathy scores indicate that the proposed annotation scheme successfully guides annotators to a substantial to moderate agreement.","offset":3,"pro":0.42857142857142855,"labels":"RST"},{"idx":18,"sentence":"Moreover, we trained predictive models to detect the annotated empathy structures and embedded them in an adaptive writing support system for students to receive individual empathy feedback independent of an instructor, time, and location.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":18,"sentence":"We evaluated our tool in a peer learning exercise with 58 students and found promising results for perceived empathy skill learning, perceived feedback accuracy, and intention to use.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":18,"sentence":"Finally, we present our freely available corpus of 500 empathy-annotated, student-written peer reviews on business models and our annotation guidelines to encourage future research on the design and development of empathy support systems.","offset":6,"pro":0.8571428571428571,"labels":"MTD"},{"idx":19,"sentence":"The current state-of-the-art generative models for open-domain question answering (ODQA) have focused on generating direct answers from unstructured textual information.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"However, a large amount of world's knowledge is stored in structured databases, and need to be accessed using query languages such as SQL.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":19,"sentence":"Furthermore, query languages can answer questions that require complex reasoning, as well as offering full explainability.","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":19,"sentence":"In this paper, we propose a hybrid framework that takes both textual and tabular evidences as input and generates either direct answers or SQL queries depending on which form could better answer the question.","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":19,"sentence":"The generated SQL queries can then be executed on the associated databases to obtain the final answers.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":19,"sentence":"To the best of our knowledge, this is the first paper that applies Text2SQL to ODQA tasks.","offset":5,"pro":0.5555555555555556,"labels":"CTN"},{"idx":19,"sentence":"Empirically, we demonstrate that on several ODQA datasets, the hybrid methods consistently outperforms the baseline models that only takes homogeneous input by a large margin.","offset":6,"pro":0.6666666666666666,"labels":"CLN"},{"idx":19,"sentence":"Specifically we achieve the state-of-the-art performance on OpenSQuAD dataset using a T5-base model.","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":19,"sentence":"In a detailed analysis, we demonstrate that the being able to generate structural SQL queries can always bring gains, especially for those questions that requires complex reasoning.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":0,"sentence":"We demonstrate that the generated contexts substantially enrich the semantics of the queries and GAR with sparse representations (BM25) achieves comparable or better performance than state-of-the-art dense retrieval methods such as DPR.","offset":0,"pro":0,"labels":"CLN"},{"idx":0,"sentence":"We show that generating diverse contexts for a query is beneficial as fusing their results consistently yields better retrieval accuracy.","offset":1,"pro":0.2,"labels":"CLN"},{"idx":0,"sentence":"Moreover, as sparse and dense representations are often complementary, GAR can be easily combined with DPR to achieve even better performance.","offset":2,"pro":0.4,"labels":"CLN"},{"idx":0,"sentence":"GAR achieves state-of-the-art performance on Natural Questions and TriviaQA datasets under the extractive QA setup when equipped with an extractive reader, and consistently outperforms other retrieval methods when the same generative reader is used.","offset":3,"pro":0.6,"labels":"RST"},{"idx":0,"sentence":"We propose Generation-Augmented Retrieval (GAR) for answering open-domain questions, which augments a query through text generation of heuristically discovered relevant contexts without external resources as supervision.","offset":4,"pro":0.8,"labels":"PUR"},{"idx":1,"sentence":"While sophisticated neural-based models have achieved remarkable success in Visual Question Answering (VQA), these models tend to answer questions only according to superficial correlations between question and answer.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"Several recent approaches have been developed to address this language priors problem.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":1,"sentence":"However, most of them predict the correct answer according to one best output without checking the authenticity of answers.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":1,"sentence":"Besides, they only explore the interaction between image and question, ignoring the semantics of candidate answers.","offset":3,"pro":0.42857142857142855,"labels":"GAP"},{"idx":1,"sentence":"In this paper, we propose a select-and-rerank (SAR) progressive framework based on Visual Entailment.","offset":4,"pro":0.5714285714285714,"labels":"PUR"},{"idx":1,"sentence":"Specifically, we first select the candidate answers relevant to the question or the image, then we rerank the candidate answers by a visual entailment task, which verifies whether the image semantically entails the synthetic statement of the question and each candidate answer.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":1,"sentence":"Experimental results show the effectiveness of our proposed framework, which establishes a new state-of-the-art accuracy on VQA-CP v2 with a 7.55% improvement.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":2,"sentence":"Weakly supervised question answering usually has only the final answers as supervision signals while the correct solutions to derive the answers are not provided.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"Previous learning methods mostly filter out spurious solutions with heuristics or using model confidence, but do not explicitly exploit the semantic correlations between a question and its solution.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":2,"sentence":"In this paper, to alleviate the spurious solution problem, we propose to explicitly exploit such semantic correlations by maximizing the mutual information between question-answer pairs and predicted solutions.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":2,"sentence":"Extensive experiments on four question answering datasets show that our method significantly outperforms previous learning methods in terms of task performance and is more effective in training models to produce correct solutions.","offset":3,"pro":0.6,"labels":"CLN"},{"idx":2,"sentence":"This setting gives rise to the spurious solution problem: there may exist many spurious solutions that coincidentally derive the correct answer, but training on such solutions can hurt model performance (e.g., producing wrong solutions or answers).","offset":4,"pro":0.8,"labels":"BAC"},{"idx":3,"sentence":"Privacy plays a crucial role in preserving democratic ideals and personal autonomy.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"The dominant legal approach to privacy in many jurisdictions is the Notice and Choice paradigm, where privacy policies are the primary instrument used to convey information to users.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":3,"sentence":"However, privacy policies are long and complex documents that are difficult for users to read and comprehend.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":3,"sentence":"Our goal is to provide a roadmap for the development and use of language technologies to empower users to reclaim control over their privacy, limit privacy harms, and rally research efforts from the community towards addressing an issue with large social impact.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":3,"sentence":"We highlight many remaining opportunities to develop language technologies that are more precise or nuanced in the way in which they use the text of privacy policies.","offset":4,"pro":0.6666666666666666,"labels":"CTN"},{"idx":3,"sentence":"We discuss how language technologies can play an important role in addressing this information gap, reporting on initial progress towards helping three specific categories of stakeholders take advantage of digital privacy policies: consumers, enterprises, and regulators.","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":4,"sentence":"Open pit mines left many regions worldwide inhospitable or uninhabitable.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"Many sites are left behind in a hazardous or contaminated state, show remnants of waste, or have other restrictions imposed upon them, e.g., for the protection of human or nature.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":4,"sentence":"In this work we present and evaluate an automated workflow for supporting the post-mining management of former lignite open pit mines in the eastern part of Germany, where prior to any planned land reuse, aforementioned information has to be acquired to ensure the safety and validity of such an endeavor.","offset":2,"pro":0.2222222222222222,"labels":"PUR"},{"idx":4,"sentence":"Usually, this information is found in expert reports, either in the form of paper documents, or in the best case as digitized unstructured text-all of them in German language.","offset":3,"pro":0.3333333333333333,"labels":"BAC"},{"idx":4,"sentence":"However, due to the size and complexity of these documents, any inquiry is tedious and time-consuming, thereby slowing down or even obstructing the reuse of related areas.","offset":4,"pro":0.4444444444444444,"labels":"GAP"},{"idx":4,"sentence":"Since no training data is available, we employ active learning in order to perform multi-label sentence classification for two categories of restrictions and seven categories of topics.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":4,"sentence":"The final system integrates optical character recognition (OCR), active-learning-based text classification, and geographic information system visualization in order to effectively extract, query, and visualize this information for any area of interest.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":4,"sentence":"Active learning and text classification results are twofold: Whereas the restriction categories were reasonably accurate (greater0.85 F1), the seven topic-oriented categories seemed to be complex even for human annotators and achieved mediocre evaluation scores (less0.70 F1).","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":4,"sentence":"Such information has to be permanently managed in order to reuse those areas in the future.","offset":8,"pro":0.8888888888888888,"labels":"BAC"},{"idx":5,"sentence":"Questions of fairness, robustness, and transparency are paramount to address before deploying NLP systems.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"To address this, we argue for the need for reliability testing and contextualize it among existing work on improving accountability.","offset":1,"pro":0.25,"labels":"PUR"},{"idx":5,"sentence":"We show how adversarial attacks can be reframed for this goal, via a framework for developing reliability tests.","offset":2,"pro":0.5,"labels":"RST"},{"idx":5,"sentence":"Central to these concerns is the question of reliability: Can NLP systems reliably treat different demographics fairly and function correctly in diverse and noisy environments?","offset":3,"pro":0.75,"labels":"GAP"},{"idx":6,"sentence":"Mental health conditions remain underdiagnosed even in countries with common access to advanced medical care.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"The ability to accurately and efficiently predict mood from easily collectible data has several important implications for the early detection, intervention, and treatment of mental health disorders.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":6,"sentence":"One promising data source to help monitor human behavior is daily smartphone usage.","offset":2,"pro":0.25,"labels":"BAC"},{"idx":6,"sentence":"However, care must be taken to summarize behaviors without identifying the user through personal (e.g., personally identifiable information) or protected (e.g., race, gender) attributes.","offset":3,"pro":0.375,"labels":"GAP"},{"idx":6,"sentence":"In this paper, we study behavioral markers of daily mood using a recent dataset of mobile behaviors from adolescent populations at high risk of suicidal behaviors.","offset":4,"pro":0.5,"labels":"PUR"},{"idx":6,"sentence":"Using computational models, we find that language and multimodal representations of mobile typed text (spanning typed characters, words, keystroke timings, and app usage) are predictive of daily mood.","offset":5,"pro":0.625,"labels":"RST"},{"idx":6,"sentence":"However, we find that models trained to predict mood often also capture private user identities in their intermediate representations.","offset":6,"pro":0.75,"labels":"RST"},{"idx":6,"sentence":"To tackle this problem, we evaluate approaches that obfuscate user identity while remaining predictive. ","offset":7,"pro":0.875,"labels":"MTD"},{"idx":7,"sentence":"This position paper investigates the problem of automated text anonymisation, which is a prerequisite for secure sharing of documents containing sensitive information about individuals.","offset":0,"pro":0,"labels":"PUR"},{"idx":7,"sentence":"We summarise the key concepts behind text anonymisation and provide a review of current approaches.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":7,"sentence":"Based on a case study, we outline the benefits and limitations of these approaches and discuss a number of open challenges, such as (1) how to account for multiple types of semantic inferences, (2) how to strike a balance between disclosure risk and data utility and (3) how to evaluate the quality of the resulting anonymisation.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":7,"sentence":"We lay out a case for moving beyond sequence labelling models and incorporate explicit measures of disclosure risk into the text anonymisation process.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":7,"sentence":"Anonymisation methods have so far been developed in two fields with little mutual interaction, namely natural language processing and privacy-preserving data publishing.","offset":4,"pro":0.8,"labels":"BAC"},{"idx":8,"sentence":"Although parsing to Abstract Meaning Representation (AMR) has become very popular and AMR has been shown effective on the many sentence-level downstream tasks, little work has studied how to generate AMRs that can represent multi-sentence information.","offset":0,"pro":0,"labels":"GAP"},{"idx":8,"sentence":"Compared with the previous pipeline and rule-based approaches, our model alleviates error propagation and it is more robust for both in-domain and out-domain situations.","offset":1,"pro":0.25,"labels":"CLN"},{"idx":8,"sentence":"Besides, the document-level AMRs obtained by our model can significantly improve over the AMRs generated by a rule-based method (Liu et al., 2015) on text summarization.","offset":2,"pro":0.5,"labels":"CLN"},{"idx":8,"sentence":"We introduce the first end-to-end AMR coreference resolution model in order to build multi-sentence AMRs.","offset":3,"pro":0.75,"labels":"PUR"},{"idx":9,"sentence":"In this work, we use Gaussian models for density estimation at intermediate layers of three language models (BERT, RoBERTa, and XLNet), and evaluate our method on BLiMP, a grammaticality judgement benchmark.","offset":0,"pro":0,"labels":"PUR"},{"idx":9,"sentence":"In lower layers, surprisal is highly correlated to low token frequency, but this correlation diminishes in upper layers.","offset":1,"pro":0.16666666666666666,"labels":"RST"},{"idx":9,"sentence":"These results suggest that language models employ separate mechanisms to detect different types of linguistic anomalies.","offset":2,"pro":0.3333333333333333,"labels":"CLN"},{"idx":9,"sentence":"Transformer language models have shown remarkable ability in detecting when a word is anomalous in context,","offset":3,"pro":0.5,"labels":"BAC"},{"idx":9,"sentence":"but likelihood scores offer no information about the cause of the anomaly.","offset":4,"pro":0.6666666666666666,"labels":"GAP"},{"idx":9,"sentence":"Next, we gather datasets of morphosyntactic, semantic, and commonsense anomalies from psycholinguistic studies;","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":10,"sentence":"Most of the recent work on personality detection from online posts adopts multifarious deep neural networks to represent the posts and builds predictive models in a data-driven manner, without the exploitation of psycholinguistic knowledge that may unveil the connections between one's language use and his psychological traits.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"In this paper, we propose a psycholinguistic knowledge-based tripartite graph network, TrigNet, which consists of a tripartite graph network and a BERT-based graph initializer.","offset":1,"pro":0.125,"labels":"PUR"},{"idx":10,"sentence":"The graph network injects structural psycholinguistic knowledge in LIWC, a computerized instrument for psycholinguistic analysis, by constructing a heterogeneous tripartite graph.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":10,"sentence":"The initializer is employed to provide initial embeddings for the graph nodes.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":10,"sentence":"Extensive experiments on two datasets show that TrigNet outperforms the existing state-of-art model by 3.47 and 2.10 points in average F1.","offset":4,"pro":0.5,"labels":"RST"},{"idx":10,"sentence":"Moreover, the flow GAT reduces the FLOPS and Memory measures by 38% and 32%, respectively, in comparison to the original GAT in our setting.","offset":5,"pro":0.625,"labels":"RST"},{"idx":10,"sentence":"To reduce the computational cost in graph learning, we further propose a novel flow graph attention network (GAT) that only transmits messages between neighboring parties in the tripartite graph.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":10,"sentence":"Benefiting from the tripartite graph, TrigNet can aggregate post information from a psychological perspective, which is a novel way of exploiting domain knowledge.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":11,"sentence":"Correct natural language understanding requires computers to distinguish the literal and metaphorical senses of a word.","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"Recent neu- ral models achieve progress on verb metaphor detection by viewing it as sequence labeling.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":11,"sentence":"In this paper, we argue that it is appropriate to view this task as relation classification between a verb and its various contexts.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":11,"sentence":"We propose the Metaphor-relation BERT (Mr-BERT) model, which explicitly models the relation between a verb and its grammatical, sentential and semantic contexts.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":11,"sentence":"We evaluate our method on the VUA, MOH-X and TroFi datasets.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":11,"sentence":"Our method gets competitive results compared with state-of-the-art approaches.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":12,"sentence":"Pretraining and multitask learning are widely used to improve the speech translation performance.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"In this study, we are interested in training a speech translation model along with an auxiliary text translation task.","offset":1,"pro":0.1,"labels":"PUR"},{"idx":12,"sentence":"We conduct a detailed analysis to understand the impact of the auxiliary task on the primary task within the multitask learning framework.","offset":2,"pro":0.2,"labels":"MTD"},{"idx":12,"sentence":"Our analysis confirms that multitask learning tends to generate similar decoder representations from different modalities and preserve more information from the pretrained text translation modules.","offset":3,"pro":0.3,"labels":"CLN"},{"idx":12,"sentence":"We observe minimal negative transfer effect between the two tasks and sharing more parameters is helpful to transfer knowledge from the text task to the speech task.","offset":4,"pro":0.4,"labels":"RST"},{"idx":12,"sentence":"The analysis also reveals that the modality representation difference at the top decoder layers is still not negligible, and those layers are critical for the translation quality.","offset":5,"pro":0.5,"labels":"CLN"},{"idx":12,"sentence":"First, a parameter sharing and initialization strategy is proposed to enhance information sharing between the tasks.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":12,"sentence":"Second, a novel attention-based regularization is proposed for the encoders and pulls the representations from different modalities closer.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":12,"sentence":"Third, an online knowledge distillation is proposed to enhance the knowledge transfer from the text to the speech task.","offset":8,"pro":0.8,"labels":"MTD"},{"idx":12,"sentence":"Our experiments show that the proposed approach improves translation performance by more than 2 BLEU over a strong baseline and achieves state-of-the-art results on the MuST-C English-German, English-French and English-Spanish language pairs.","offset":9,"pro":0.9,"labels":"RST"},{"idx":13,"sentence":"Large pre-trained language models (PTLMs) have been shown to carry biases towards different social groups which leads to the reproduction of stereotypical and toxic content by major NLP systems.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"We propose a method based on logistic regression classifiers to probe English, French, and Arabic PTLMs and quantify the potentially harmful content that they convey with respect to a set of templates.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":13,"sentence":"The templates are prompted by a name of a social group followed by a cause-effect relation.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":13,"sentence":"We use PTLMs to predict masked tokens at the end of a sentence in order to examine how likely they enable toxicity towards specific communities.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":13,"sentence":"We shed the light on how such negative content can be triggered within unrelated and benign contexts based on evidence from a large-scale study, then we explain how to take advantage of our methodology to assess and mitigate the toxicity transmitted by PTLMs.","offset":4,"pro":0.8,"labels":"MTD"},{"idx":14,"sentence":"Technology for language generation has advanced rapidly, spurred by advancements in pre-training large models on massive amounts of data and the need for intelligent agents to communicate in a natural manner.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"To better understand these challenges, we present a survey on societal biases in language generation, focusing on how data and techniques contribute to biases and progress towards reducing biases.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":14,"sentence":"Motivated by a lack of studies on biases from decoding techniques, we also conduct experiments to quantify the effects of these techniques.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":14,"sentence":"By further discussing general trends and open challenges, we call to attention promising directions for research and the importance of fairness and inclusivity considerations for language generation applications.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":14,"sentence":"While techniques can effectively generate fluent text, they can also produce undesirable societal biases that can have a disproportionately negative impact on marginalized populations.","offset":4,"pro":0.6666666666666666,"labels":"BAC"},{"idx":14,"sentence":"Language generation presents unique challenges for biases in terms of direct user interaction and the structure of decoding techniques.","offset":5,"pro":0.8333333333333334,"labels":"GAP"},{"idx":15,"sentence":"Inspired by old and well-established ideas in machine learning, we explore a variety of non-linear reservoir layers interspersed with regular transformer layers, and show improvements in wall-clock compute time until convergence, as well as overall performance, on various machine translation and (masked) language modelling tasks.","offset":0,"pro":0,"labels":"MTD"},{"idx":16,"sentence":"Active Learning (AL) has been successfully applied to Deep Learning in order to drastically reduce the amount of data required to achieve high performance.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"However, these methods do not exploit the sequential nature of language and the heterogeneity of uncertainty within each instance, requiring the labelling of whole sentences.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":16,"sentence":"In this work, we overcome these limitations by allowing the AL algorithm to query subsequences within sentences, and propagate their labels to other sentences.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":16,"sentence":"We achieve highly efficient results on OntoNotes 5.0, only requiring 13% of the original training data, and CoNLL 2003, requiring only 27%.","offset":3,"pro":0.5,"labels":"RST"},{"idx":16,"sentence":"This is an improvement of 39% and 37% compared to querying full sentences.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":16,"sentence":"Previous works have shown that lightweight architectures for Named Entity Recognition (NER) can achieve optimal performance with only 25% of the original training data.","offset":5,"pro":0.8333333333333334,"labels":"BAC"},{"idx":17,"sentence":"In this paper, we detail the relationship between convolutions and self-attention in natural language tasks.","offset":0,"pro":0,"labels":"PUR"},{"idx":17,"sentence":"Specifically, we propose composite attention, which unites previous relative position encoding methods under a convolutional framework.","offset":1,"pro":0.3333333333333333,"labels":"MTD"},{"idx":17,"sentence":"We conduct experiments by training BERT with composite attention, finding that convolutions consistently improve performance on multiple downstream tasks, replacing absolute position embeddings.","offset":2,"pro":0.6666666666666666,"labels":"MTD"},{"idx":18,"sentence":"The rapid development of large pre-trained language models has greatly increased the demand for model compression techniques, among which quantization is a popular solution.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"In this paper, we propose BinaryBERT, which pushes BERT quantization to the limit by weight binarization.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":18,"sentence":"We find that a binary BERT is hard to be trained directly than a ternary counterpart due to its complex and irregular loss landscape.","offset":2,"pro":0.2857142857142857,"labels":"RST"},{"idx":18,"sentence":"Therefore, we propose ternary weight splitting, which initializes BinaryBERT by equivalently splitting from a half-sized ternary network.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":18,"sentence":"The binary model thus inherits the good performance of the ternary one, and can be further enhanced by fine-tuning the new architecture after splitting.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":18,"sentence":"Empirical results show that our BinaryBERT has only a slight performance drop compared with the full-precision model while being 24x smaller, achieving the state-of-the-art compression results on the GLUE and SQuAD benchmarks.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":18,"sentence":"Code will be released.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":19,"sentence":"In the era of pre-trained language models, Transformers are the de facto choice of model architectures.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"While recent research has shown promise in entirely convolutional, or CNN, architectures, they have not been explored using the pre-train-fine-tune paradigm.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":19,"sentence":"This paper investigates this research question and presents several interesting findings.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":19,"sentence":"Across an extensive set of experiments on 8 datasets/tasks, we find that CNN-based pre-trained models are competitive and outperform their Transformer counterpart in certain scenarios, albeit with caveats.","offset":3,"pro":0.5,"labels":"RST"},{"idx":19,"sentence":"We believe our research paves the way for a healthy amount of optimism in alternative architectures.","offset":4,"pro":0.6666666666666666,"labels":"CTN"},{"idx":19,"sentence":"Overall, the findings outlined in this paper suggest that conflating pre-training and architectural advances is misguided and that both advances should be considered independently.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":0,"sentence":"Distance based knowledge graph embedding methods show promising results on link prediction task, on which two topics have been widely studied: one is the ability to handle complex relations, such as N-to-1, 1-to-N and N-to-N, the other is to encode various relation patterns, such as symmetry/antisymmetry.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"However, the existing methods fail to solve these two problems at the same time, which leads to unsatisfactory results.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":0,"sentence":"To mitigate this problem, we propose PairRE, a model with paired vectors for each relation representation.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":0,"sentence":"The paired vectors enable an adaptive adjustment of the margin in loss function to fit for different complex relations.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":0,"sentence":"Besides, PairRE is capable of encoding three important relation patterns, symmetry/antisymmetry, inverse and composition.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":0,"sentence":"Given simple constraints on relation representations, PairRE can encode subrelation further.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":0,"sentence":"Experiments on link prediction benchmarks demonstrate the proposed key capabilities of PairRE.","offset":6,"pro":0.75,"labels":"RST"},{"idx":0,"sentence":"Moreover, We set a new state-of-the-art on two knowledge graph datasets of the challenging Open Graph Benchmark.","offset":7,"pro":0.875,"labels":"RST"},{"idx":1,"sentence":"Hierarchical text classification is an important yet challenging task due to the complex structure of the label hierarchy.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"Existing methods ignore the semantic relationship between text and labels, so they cannot make full use of the hierarchical information.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":1,"sentence":"To this end, we formulate the text-label semantics relationship as a semantic matching problem and thus propose a hierarchy-aware label semantics matching network (HiMatch).","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":1,"sentence":"First, we project text semantics and label semantics into a joint embedding space.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":1,"sentence":"We then introduce a joint embedding loss and a matching learning loss to model the matching relationship between the text semantics and the label semantics.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":1,"sentence":"Our model captures the text-label semantics matching relationship among coarse-grained labels and fine-grained labels in a hierarchy-aware manner.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":1,"sentence":"The experimental results on various benchmark datasets verify that our model achieves state-of-the-art results.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":2,"sentence":"Fine-tuning large pre-trained models with task-specific data has achieved great success in NLP.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"However, it has been demonstrated that the majority of information within the self-attention networks is redundant and not utilized effectively during the fine-tuning stage.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":2,"sentence":"This leads to inferior results when generalizing the obtained models to out-of-domain distributions.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":2,"sentence":"To this end, we propose a simple yet effective data augmentation technique, HiddenCut, to better regularize the model and encourage it to learn more generalizable features.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":2,"sentence":"Specifically, contiguous spans within the hidden space are dynamically and strategically dropped during training.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":2,"sentence":"Experiments show that our HiddenCut method outperforms the state-of-the-art augmentation methods on the GLUE benchmark, and consistently exhibits superior generalization performances on out-of-distribution and challenging counterexamples.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":2,"sentence":"We have publicly released our code at https://github.com/GT-SALT/HiddenCut.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":3,"sentence":"Generating open-domain conversational responses in the desired style usually suffers from the lack of parallel data in the style.","offset":0,"pro":0,"labels":"GAP"},{"idx":3,"sentence":"Meanwhile, using monolingual stylistic data to increase style intensity often leads to the expense of decreasing content relevance.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":3,"sentence":"In this paper, we propose to disentangle the content and style in latent space by diluting sentence-level information in style representations.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":3,"sentence":"Combining the desired style representation and a response content representation will then obtain a stylistic response.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":3,"sentence":"Our approach achieves a higher BERT-based style intensity score and comparable BLEU scores, compared with baselines.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":3,"sentence":"Human evaluation results show that our approach significantly improves style intensity and maintains content relevance.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":4,"sentence":"Understanding privacy policies is crucial for users as it empowers them to learn about the information that matters to them.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"Sentences written in a privacy policy document explain privacy practices, and the constituent text spans convey further specific information about that practice.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":4,"sentence":"We refer to predicting the privacy practice explained in a sentence as intent classification and identifying the text spans sharing specific information as slot filling.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":4,"sentence":"In this work, we propose PolicyIE, an English corpus consisting of 5,250 intent and 11,788 slot annotations spanning 31 privacy policies of websites and mobile applications.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":4,"sentence":"PolicyIE corpus is a challenging real-world benchmark with limited labeled examples reflecting the cost of collecting large-scale annotations from domain experts.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":4,"sentence":"We present two alternative neural approaches as baselines, (1) intent classification and slot filling as a joint sequence tagging and (2) modeling them as a sequence-to-sequence (Seq2Seq) learning task.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":4,"sentence":"The experiment results show that both approaches perform comparably in intent classification, while the Seq2Seq method outperforms the sequence tagging approach in slot filling by a large margin.","offset":6,"pro":0.75,"labels":"RST"},{"idx":4,"sentence":"We perform a detailed error analysis to reveal the challenges of the proposed corpus.","offset":7,"pro":0.875,"labels":"MTD"},{"idx":5,"sentence":"For task-oriented dialog systems to be maximally useful, it must be able to process conversations in a way that is (1) generalizable with a small number of training examples for new task domains, and (2) robust to user input in various styles, modalities, or domains.","offset":0,"pro":0,"labels":"GAP"},{"idx":5,"sentence":"In pursuit of these goals, we introduce the RADDLE benchmark, a collection of corpora and tools for evaluating the performance of models across a diverse set of domains.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":5,"sentence":"By including tasks with limited training data, RADDLE is designed to favor and encourage models with a strong generalization ability.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":5,"sentence":"RADDLE also includes a diagnostic checklist that facilitates detailed robustness analysis in aspects such as language variations, speech errors, unseen entities, and out-of-domain utterances.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":5,"sentence":"We evaluate recent state-of-the-art systems based on pre-training and fine-tuning, and find that grounded pre-training on heterogeneous dialog corpora performs better than training a separate model per domain.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":5,"sentence":"Adversarial training is also proposed to improve model robustness against noisy inputs.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":5,"sentence":"Overall, existing models are less than satisfactory in robustness evaluation, which suggests opportunities for future improvement.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":6,"sentence":"Although neural models have achieved competitive results in dialogue systems, they have shown limited ability in representing core semantics, such as ignoring important entities.","offset":0,"pro":0,"labels":"GAP"},{"idx":6,"sentence":"To this end, we exploit Abstract Meaning Representation (AMR) to help dialogue modeling.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":6,"sentence":"Compared with the textual input, AMR explicitly provides core semantic knowledge and reduces data sparsity.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":6,"sentence":"We develop an algorithm to construct dialogue-level AMR graphs from sentence-level AMRs and explore two ways to incorporate AMRs into dialogue systems.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":6,"sentence":"Experimental results on both dialogue understanding and response generation tasks show the superiority of our model.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":6,"sentence":"To our knowledge, we are the first to leverage a formal semantic representation into neural dialogue modeling.","offset":5,"pro":0.8333333333333334,"labels":"CTN"},{"idx":7,"sentence":"Recently, many studies are emerging towards building a retrieval-based dialogue system that is able to effectively leverage background knowledge (e.g., documents) when conversing with humans.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"However, it is non-trivial to collect large-scale dialogues that are naturally grounded on the background documents, which hinders the effective and adequate training of knowledge selection and response matching.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":7,"sentence":"To overcome the challenge, we consider decomposing the training of the knowledge-grounded response selection into three tasks including: 1) query-passage matching task; 2) query-dialogue history matching task; 3) multi-turn response matching task, and joint learning all these tasks in a unified pre-trained language model.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":7,"sentence":"The former two tasks could help the model in knowledge selection and comprehension, while the last task is designed for matching the proper response with the given query and background knowledge (dialogue history).","offset":3,"pro":0.5,"labels":"MTD"},{"idx":7,"sentence":"By this means, the model can be learned to select relevant knowledge and distinguish proper response, with the help of ad-hoc retrieval corpora and a large number of ungrounded multi-turn dialogues.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":7,"sentence":"Experimental results on two benchmarks of knowledge-grounded response selection indicate that our model can achieve comparable performance with several existing methods that rely on crowd-sourced data for training.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":8,"sentence":"Syntactic information, especially dependency trees, has been widely used by existing studies to improve relation extraction with better semantic guidance for analyzing the context information associated with the given entities.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"However, most existing studies suffer from the noise in the dependency trees, especially when they are automatically generated, so that intensively leveraging dependency information may introduce confusions to relation classification and necessary pruning is of great importance in this task.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":8,"sentence":"In this paper, we propose a dependency-driven approach for relation extraction with attentive graph convolutional networks (A-GCN).","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":8,"sentence":"In this approach, an attention mechanism upon graph convolutional networks is applied to different contextual words in the dependency tree obtained from an off-the-shelf dependency parser, to distinguish the importance of different word dependencies.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":8,"sentence":"Consider that dependency types among words also contain important contextual guidance, which is potentially helpful for relation extraction, we also include the type information in A-GCN modeling.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":8,"sentence":"Experimental results on two English benchmark datasets demonstrate the effectiveness of our A-GCN, which outperforms previous studies and achieves state-of-the-art performance on both datasets.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":9,"sentence":"Retrieval is a core component for open-domain NLP tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"In open-domain tasks, multiple entities can share a name, making disambiguation an inherent yet under-explored problem.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":9,"sentence":"We propose an evaluation benchmark for assessing the entity disambiguation capabilities of these retrievers, which we call Ambiguous Entity Retrieval (AmbER) sets.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":9,"sentence":"We define an AmbER set as a collection of entities that share a name along with queries about those entities.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":9,"sentence":"By covering the set of entities for polysemous names, AmbER sets act as a challenging test of entity disambiguation.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":9,"sentence":"We create AmbER sets for three popular open-domain tasks: fact checking, slot filling, and question answering, and evaluate a diverse set of retrievers.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":9,"sentence":"We find that the retrievers exhibit popularity bias, significantly under-performing on rarer entities that share a name, e.g., they are twice as likely to retrieve erroneous documents on queries for the less popular entity under the same name.","offset":6,"pro":0.75,"labels":"RST"},{"idx":9,"sentence":"These experiments on AmbER sets show their utility as an evaluation tool and highlight the weaknesses of popular retrieval systems.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":10,"sentence":"Leaderboards are widely used in NLP and push the field forward.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"While leaderboards are a straightforward ranking of NLP models, this simplicity can mask nuances in evaluation items (examples) and subjects (NLP models).","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":10,"sentence":"Rather than replace leaderboards, we advocate a re-imagining so that they better highlight if and where progress is made.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":10,"sentence":"Building on educational testing, we create a Bayesian leaderboard model where latent subject skill and latent item difficulty predict correct responses.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":10,"sentence":"Using this model, we analyze the ranking reliability of leaderboards.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":10,"sentence":"Afterwards, we show the model can guide what to annotate, identify annotation errors, detect overfitting, and identify informative examples.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":10,"sentence":"We conclude with recommendations for future benchmark tasks.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":11,"sentence":"Manual fact-checking does not scale well to serve the needs of the internet.","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"This issue is further compounded in non-English contexts.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":11,"sentence":"In this paper, we discuss claim matching as a possible solution to scale fact-checking.","offset":2,"pro":0.2,"labels":"PUR"},{"idx":11,"sentence":"We define claim matching as the task of identifying pairs of textual messages containing claims that can be served with one fact-check.","offset":3,"pro":0.3,"labels":"MTD"},{"idx":11,"sentence":"We construct a novel dataset of WhatsApp tipline and public group messages alongside fact-checked claims that are first annotated for containing claim-like statements and then matched with potentially similar items and annotated for claim matching.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":11,"sentence":"Our dataset contains content in high-resource (English, Hindi) and lower-resource (Bengali, Malayalam, Tamil) languages.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":11,"sentence":"We train our own embedding model using knowledge distillation and a high-quality teacher model in order to address the imbalance in embedding quality between the low- and high-resource languages in our dataset.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":11,"sentence":"We provide evaluations on the performance of our solution and compare with baselines and existing state-of-the-art multilingual embedding models, namely LASER and LaBSE.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":11,"sentence":"We demonstrate that our performance exceeds LASER and LaBSE in all settings.","offset":8,"pro":0.8,"labels":"RST"},{"idx":11,"sentence":"We release our annotated datasets, codebooks, and trained embedding model to allow for further research.","offset":9,"pro":0.9,"labels":"CTN"},{"idx":12,"sentence":"While pre-training techniques are working very well in natural language processing, how to pre-train a decoder and effectively use it for neural machine translation (NMT) still remains a tricky issue.","offset":0,"pro":0,"labels":"GAP"},{"idx":12,"sentence":"The main reason is that the cross-attention module between the encoder and decoder cannot be pre-trained, and the combined encoder-decoder model cannot work well in the fine-tuning stage because the inputs of the decoder cross-attention come from unknown encoder outputs.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":12,"sentence":"In this paper, we propose a better pre-training method for NMT by defining a semantic interface (SemFace) between the pre-trained encoder and the pre-trained decoder.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":12,"sentence":"Specifically, we propose two types of semantic interfaces, including CL-SemFace which regards cross-lingual embeddings as an interface, and VQ-SemFace which employs vector quantized embeddings to constrain the encoder outputs and decoder inputs in the same language-independent space.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":12,"sentence":"We conduct massive experiments on six supervised translation pairs and three unsupervised pairs.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":12,"sentence":"Experimental results demonstrate that our proposed SemFace can effectively connect the pre-trained encoder and decoder, and achieves significant improvement by 3.7 and 1.5 BLEU points on the two tasks respectively compared with previous pre-training-based NMT models.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":13,"sentence":"The discrepancy between maximum likelihood estimation (MLE) and task measures such as BLEU score has been studied before for autoregressive neural machine translation (NMT) and resulted in alternative training algorithms (Ranzato et al., 2016; Norouzi et al., 2016; Shen et al., 2016; Wu et al., 2018).","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"However, MLE training remains the de facto approach for autoregressive NMT because of its computational efficiency and stability.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":13,"sentence":"Despite this mismatch between the training objective and task measure, we notice that the samples drawn from an MLE-based trained NMT support the desired distribution - there are samples with much higher BLEU score comparing to the beam decoding output.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":13,"sentence":"To benefit from this observation, we train an energy-based model to mimic the behavior of the task measure (i.e., the energy-based model assigns lower energy to samples with higher BLEU score), which is resulted in a re-ranking algorithm based on the samples drawn from NMT: energy-based re-ranking (EBR).","offset":3,"pro":0.5,"labels":"PUR"},{"idx":13,"sentence":"We use both marginal energy models (over target sentence) and joint energy models (over both source and target sentences).","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":13,"sentence":"Our EBR with the joint energy model consistently improves the performance of the Transformer-based NMT: +3.7 BLEU points on IWSLT'14 German-English, +3.37 BELU points on Sinhala-English, +1.4 BLEU points on WMT'16 English-German tasks.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":14,"sentence":"In recent years, we have seen a colossal effort in pre-training multilingual text encoders using large-scale corpora in many languages to facilitate cross-lingual transfer learning.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"However, due to typological differences across languages, the cross-lingual transfer is challenging.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":14,"sentence":"Nevertheless, language syntax, e.g., syntactic dependencies, can bridge the typological gap.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":14,"sentence":"Previous works have shown that pre-trained multilingual encoders, such as mBERT (CITATION), capture language syntax, helping cross-lingual transfer.","offset":3,"pro":0.375,"labels":"BAC"},{"idx":14,"sentence":"This work shows that explicitly providing language syntax and training mBERT using an auxiliary objective to encode the universal dependency tree structure helps cross-lingual transfer.","offset":4,"pro":0.5,"labels":"PUR"},{"idx":14,"sentence":"We perform rigorous experiments on four NLP tasks, including text classification, question answering, named entity recognition, and task-oriented semantic parsing.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":14,"sentence":"The experiment results show that syntax-augmented mBERT improves cross-lingual transfer on popular benchmarks, such as PAWS-X and MLQA, by 1.4 and 1.6 points on average across all languages.","offset":6,"pro":0.75,"labels":"RST"},{"idx":14,"sentence":"In the generalized transfer setting, the performance boosted significantly, with 3.9 and 3.1 points on average in PAWS-X and MLQA.","offset":7,"pro":0.875,"labels":"RST"},{"idx":15,"sentence":"Pretrained multilingual models (PMMs) enable zero-shot learning via cross-lingual transfer, performing best for languages seen during pretraining.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"While methods exist to improve performance for unseen languages, they have almost exclusively been evaluated using amounts of raw text only available for a small fraction of the world's languages.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":15,"sentence":"In this paper, we evaluate the performance of existing methods to adapt PMMs to new languages using a resource available for close to 1600 languages: the New Testament.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":15,"sentence":"This is challenging for two reasons: (1) the small corpus size, and (2) the narrow domain.","offset":3,"pro":0.42857142857142855,"labels":"GAP"},{"idx":15,"sentence":"While performance drops for all approaches, we surprisingly still see gains of up to 17.69% accuracy for part-of-speech tagging and 6.29 F1 for NER on average over all languages as compared to XLM-R.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":15,"sentence":"Another unexpected finding is that continued pretraining, the simplest approach, performs best.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":15,"sentence":"Finally, we perform a case study to disentangle the effects of domain and size and to shed light on the influence of the finetuning source language.","offset":6,"pro":0.8571428571428571,"labels":"MTD"},{"idx":16,"sentence":"We study the problem of building entity tagging systems by using a few rules as weak supervision.","offset":0,"pro":0,"labels":"PUR"},{"idx":16,"sentence":"Previous methods mostly focus on disambiguating entity types based on contexts and expert-provided rules, while assuming entity spans are given.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":16,"sentence":"In this work, we propose a novel method TALLOR that bootstraps high-quality logical rules to train a neural tagger in a fully automated manner.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":16,"sentence":"Specifically, we introduce compound rules that are composed from simple rules to increase the precision of boundary detection and generate more diverse pseudo labels.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":16,"sentence":"We further design a dynamic label selection strategy to ensure pseudo label quality and therefore avoid overfitting the neural tagger.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":16,"sentence":"Experiments on three datasets demonstrate that our method outperforms other weakly supervised methods and even rivals a state-of-the-art distantly supervised tagger with a lexicon of over 2,000 terms when starting from only 20 simple rules.","offset":5,"pro":0.625,"labels":"RST"},{"idx":16,"sentence":"Our method can serve as a tool for rapidly building taggers in emerging domains and tasks.","offset":6,"pro":0.75,"labels":"CLN"},{"idx":16,"sentence":"Case studies show that learned rules can potentially explain the predicted entities.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":17,"sentence":"Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"However, fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":17,"sentence":"In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":17,"sentence":"Prefix-tuning draws inspiration from prompting for language models, allowing subsequent tokens to attend to this prefix as if it were virtual tokens.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":17,"sentence":"We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":17,"sentence":"We show that by learning only 0.1% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics that are unseen during training.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":18,"sentence":"Recently, the sequence-to-sequence models have made remarkable progress on the task of keyphrase generation (KG) by concatenating multiple keyphrases in a predefined order as a target sequence during training.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"However, the keyphrases are inherently an unordered set rather than an ordered sequence.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":18,"sentence":"Imposing a predefined order will introduce wrong bias during training, which can highly penalize shifts in the order between keyphrases.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":18,"sentence":"In this work, we propose a new training paradigm One2Set without predefining an order to concatenate the keyphrases.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":18,"sentence":"To fit this paradigm, we propose a novel model that utilizes a fixed set of learned control codes as conditions to generate a set of keyphrases in parallel.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":18,"sentence":"To solve the problem that there is no correspondence between each prediction and target during training, we propose a K-step label assignment mechanism via bipartite matching, which greatly increases the diversity and reduces the repetition rate of generated keyphrases.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":18,"sentence":"The experimental results on multiple benchmarks demonstrate that our approach significantly outperforms the state-of-the-art methods.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":19,"sentence":"Recent years have witnessed various types of generative models for natural language generation (NLG), especially RNNs or transformer based sequence-to-sequence models, as well as variational autoencoder (VAE) and generative adversarial network (GAN) based models.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"However, flow-based generative models, which achieve strong performance in image generation due to their invertibility and exact density estimation properties, have been less explored for NLG.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":19,"sentence":"In this paper, we propose a flow-based language generation model by adapting previous flow generative models to language generation via continuous input embeddings, adapted affine coupling structures, and a novel architecture for autoregressive text generation.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":19,"sentence":"We also apply our framework to Sequence-to-Sequence generation, including text- and video-based Question Generation (QG) and Neural Machine Translation (NMT), and data augmentation for Question Answering (QA).","offset":3,"pro":0.5,"labels":"PUR"},{"idx":19,"sentence":"We use our language flow model to provide extra input features for QG and NMT, which achieves improvements over the strong QG baselines on SQuAD and TVQA and NMT baseline on WMT16.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":19,"sentence":"We also augment QA data with new context by injecting noise to the latent features of the language flow and show this augmentation leads to a large performance improvement from strong baselines on SQuAD and TVQA.","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":0,"sentence":"A neural machine translation (NMT) system is expensive to train, especially with high-resource settings.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"As the NMT architectures become deeper and wider, this issue gets worse and worse.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":0,"sentence":"In this paper, we aim to improve the efficiency of training an NMT by introducing a novel norm-based curriculum learning method.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":0,"sentence":"We use the norm (aka length or module) of a word embedding as a measure of 1) the difficulty of the sentence, 2) the competence of the model, and 3) the weight of the sentence.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":0,"sentence":"It is easy to determine and contains learning-dependent features.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":0,"sentence":"The norm-based model competence makes NMT learn the curriculum in a fully automated way, while the norm-based sentence weight further enhances the learning of the vector representation of the NMT.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":0,"sentence":"Experimental results for the WMT'14 English-German and WMT'17 Chinese-English translation tasks demonstrate that the proposed method outperforms strong baselines in terms of BLEU score (+1.17/+1.56) and training speedup (2.22x/3.33x).","offset":6,"pro":0.75,"labels":"RST"},{"idx":0,"sentence":"The norm-based sentence difficulty takes the advantages of both linguistically motivated and model-based sentence difficulties. ","offset":7,"pro":0.875,"labels":"MTD"},{"idx":1,"sentence":"Simultaneous translation has many important application scenarios and attracts much attention from both academia and industry recently.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"Most existing frameworks, however, have difficulties in balancing between the translation quality and latency, i.e., the decoding policy is usually either too aggressive or too conservative.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":1,"sentence":"We propose an opportunistic decoding technique with timely correction ability, which always (over-)generates a certain mount of extra words at each step to keep the audience on track with the latest information.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":1,"sentence":"At the same time, it also corrects, in a timely fashion, the mistakes in the former overgenerated words when observing more source context to ensure high translation quality.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":1,"sentence":"Experiments show our technique achieves substantial reduction in latency and up to +3.1 increase in BLEU, with revision rate under 8% in Chinese-to-English and English-to-Chinese translation.","offset":4,"pro":0.8,"labels":"RST"},{"idx":2,"sentence":"We develop a formal hierarchy of the expressive capacity of RNN architectures.","offset":0,"pro":0,"labels":"MTD"},{"idx":2,"sentence":"The hierarchy is based on two formal properties: space complexity, which measures the RNN's memory, and rational recurrence, defined as whether the recurrent update can be described by a weighted finite-state machine.","offset":1,"pro":0.1111111111111111,"labels":"MTD"},{"idx":2,"sentence":"We place several RNN variants within this hierarchy.","offset":2,"pro":0.2222222222222222,"labels":"MTD"},{"idx":2,"sentence":"For example, we prove the LSTM is not rational, which formally separates it from the related QRNN (Bradbury et al., 2016).","offset":3,"pro":0.3333333333333333,"labels":"RST"},{"idx":2,"sentence":"We also show how these models' expressive capacity is expanded by stacking multiple layers or composing them with different pooling functions.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":2,"sentence":"Our results build on the theory of saturated RNNs (Merrill, 2019).","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":2,"sentence":"While formally extending these findings to unsaturated RNNs is left to future work, we hypothesize that the practical learnable capacity of unsaturated RNNs obeys a similar hierarchy.","offset":6,"pro":0.6666666666666666,"labels":"IMP"},{"idx":2,"sentence":"We provide empirical results to support this conjecture.","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":2,"sentence":"Experimental findings from training unsaturated networks on formal languages support this conjecture.","offset":8,"pro":0.8888888888888888,"labels":"RST"},{"idx":3,"sentence":"The formulation is derived based on the empirical observation that d^2 (x+y)/dx^2 is a typical impulse function, where (x,y)=(log r, log f). ","offset":0,"pro":0,"labels":"RST"},{"idx":3,"sentence":"The formulation is the power law when beta=0 and the Zipf-Mandelbrot law when alpha=0. ","offset":1,"pro":0.3333333333333333,"labels":"RST"},{"idx":3,"sentence":"We illustrate that alpha is related to the analytic features of syntax and beta+gamma to those of morphology in natural languages from an investigation of multilingual corpora. ","offset":2,"pro":0.6666666666666666,"labels":"RST"},{"idx":4,"sentence":"Many NLP tasks such as tagging and machine reading comprehension are faced with the severe data imbalance issue: negative examples significantly outnumber positive examples, and the huge number of easy-negative examples overwhelms the training.","offset":0,"pro":0,"labels":"GAP"},{"idx":4,"sentence":"In this paper, we propose to use dice loss in replacement of the standard cross-entropy objective for data-imbalanced NLP tasks.","offset":1,"pro":0.125,"labels":"PUR"},{"idx":4,"sentence":"Dice loss is based on the S{orensen--Dice coefficient or Tversky index , which attaches similar importance to false positives and false negatives, and is more immune to the data-imbalance issue.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":4,"sentence":"Theoretical analysis shows that this strategy narrows down the gap between the F1 score in evaluation and the dice loss in training.","offset":3,"pro":0.375,"labels":"RST"},{"idx":4,"sentence":"With the proposed training objective, we observe significant performance boost on a wide range of data imbalanced NLP tasks.","offset":4,"pro":0.5,"labels":"RST"},{"idx":4,"sentence":"Notably, we are able to achieve SOTA results on CTB5, CTB6 and UD1.4 for the part of speech tagging task; SOTA results on CoNLL03, OntoNotes5.0, MSRA and OntoNotes4.0 for the named entity recognition task; along with competitive results on the tasks of machine reading comprehension and paraphrase identification.","offset":5,"pro":0.625,"labels":"RST"},{"idx":4,"sentence":"The most commonly used cross entropy (CE) criteria is actually an accuracy-oriented objective, and thus creates a discrepancy between training and test: at training time, each training instance contributes equally to the objective function, while at test time F1 score concerns more about positive examples. ","offset":6,"pro":0.75,"labels":"GAP"},{"idx":4,"sentence":"To further alleviate the dominating influence from easy-negative examples in training, we propose to associate training examples with dynamically adjusted weights to deemphasize easy-negative examples. ","offset":7,"pro":0.875,"labels":"MTD"},{"idx":5,"sentence":"This paper is a theoretical contribution to the debate on the learnability of syntax from a corpus without explicit syntax-specific guidance.","offset":0,"pro":0,"labels":"PUR"},{"idx":5,"sentence":"Our approach originates in the observable structure of a corpus, which we use to define and isolate grammaticality (syntactic information) and meaning/pragmatics information.","offset":1,"pro":0.3333333333333333,"labels":"MTD"},{"idx":5,"sentence":"We describe the formal characteristics of an autonomous syntax and show that it becomes possible to search for syntax-based lexical categories with a simple optimization process, without any prior hypothesis on the form of the model.","offset":2,"pro":0.6666666666666666,"labels":"RST"},{"idx":6,"sentence":"We examine a methodology using neural language models (LMs) for analyzing the word order of language.","offset":0,"pro":0,"labels":"MTD"},{"idx":6,"sentence":"This LM-based method has the potential to overcome the difficulties existing methods face, such as the propagation of preprocessor errors in count-based methods.","offset":1,"pro":0.14285714285714285,"labels":"CTN"},{"idx":6,"sentence":"In this study, we explore whether the LM-based method is valid for analyzing the word order.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":6,"sentence":"As a case study, this study focuses on Japanese due to its complex and flexible word order.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":6,"sentence":"To validate the LM-based method, we test (i) parallels between LMs and human word order preference, and (ii) consistency of the results obtained using the LM-based method with previous linguistic studies.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":6,"sentence":"Through our experiments, we tentatively conclude that LMs display sufficient word order knowledge for usage as an analysis tool.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":6,"sentence":"Finally, using the LM-based method, we demonstrate the relationship between the canonical word order and topicalization, which had yet to be analyzed by large-scale experiments.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":7,"sentence":"This paper solves the fake news detection problem under a more realistic scenario on social media.","offset":0,"pro":0,"labels":"PUR"},{"idx":7,"sentence":"Given the source short-text tweet and the corresponding sequence of retweet users without text comments, we aim at predicting whether the source tweet is fake or not, and generating explanation by highlighting the evidences on suspicious retweeters and the words they concern.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":7,"sentence":"We develop a novel neural network-based model, Graph-aware Co-Attention Networks (GCAN), to achieve the goal.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":7,"sentence":"Extensive experiments conducted on real tweet datasets exhibit that GCAN can significantly outperform state-of-the-art methods by 16% in accuracy on average.","offset":3,"pro":0.6,"labels":"RST"},{"idx":7,"sentence":"In addition, the case studies also show that GCAN can produce reasonable explanations.","offset":4,"pro":0.8,"labels":"RST"},{"idx":8,"sentence":"Identifying controversial posts on social media is a fundamental task for mining public sentiment, assessing the influence of events, and alleviating the polarized views.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"However, existing methods fail to 1) effectively incorporate the semantic information from content-related posts; 2) preserve the structural information for reply relationship modeling; 3) properly handle posts from topics dissimilar to those in the training set.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":8,"sentence":"To overcome the first two limitations, we propose Topic-Post-Comment Graph Convolutional Network (TPC-GCN), which integrates the information from the graph structure and content of topics, posts, and comments for post-level controversy detection.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":8,"sentence":"As to the third limitation, we extend our model to Disentangled TPC-GCN (DTPC-GCN), to disentangle topic-related and topic-unrelated features and then fuse dynamically.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":8,"sentence":"Extensive experiments on two real-world datasets demonstrate that our models outperform existing methods.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":8,"sentence":"Analysis of the results and cases proves that our models can integrate both semantic and structural information with significant generalizability.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":9,"sentence":"Discovering the stances of media outlets and influential people on current, debatable topics is important for social statisticians and policy makers.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"Many supervised solutions exist for determining viewpoints, but manually annotating training data is costly.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":9,"sentence":"We evaluate the model by comparing its predictions to gold labels from the Media Bias/Fact Check website, achieving 82.6% accuracy.","offset":2,"pro":0.4,"labels":"RST"},{"idx":9,"sentence":"In this paper, we propose a cascaded method that uses unsupervised learning to ascertain the stance of Twitter users with respect to a polarizing topic by leveraging their retweet behavior;","offset":3,"pro":0.6,"labels":"PUR"},{"idx":9,"sentence":"then, it uses supervised learning based on user labels to characterize both the general political leaning of online media and of popular Twitter users, as well as their stance with respect to the target polarizing topic. ","offset":4,"pro":0.8,"labels":"MTD"},{"idx":10,"sentence":"The problem of comparing two bodies of text and searching for words that differ in their usage between them arises often in digital humanities and computational social science.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"This is commonly approached by training word embeddings on each corpus, aligning the vector spaces, and looking for words whose cosine distance in the aligned space is large.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":10,"sentence":"However, these methods often require extensive filtering of the vocabulary to perform well, and - as we show in this work - result in unstable, and hence less reliable, results.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":10,"sentence":"The method is simple, interpretable and stable.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":10,"sentence":"We demonstrate its effectiveness in 9 different setups, considering different corpus splitting criteria (age, gender and profession of tweet authors, time of tweet) and different languages (English, French and Hebrew).","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":10,"sentence":"We propose an alternative approach that does not use vector space alignment, and instead considers the neighbors of each word. ","offset":5,"pro":0.8333333333333334,"labels":"PUR"},{"idx":11,"sentence":"Emotion-controllable response generation is an attractive and valuable task that aims to make open-domain conversations more empathetic and engaging.","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"Existing methods mainly enhance the emotion expression by adding regularization terms to standard cross-entropy loss and thus influence the training process.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":11,"sentence":"However, due to the lack of further consideration of content consistency, the common problem of response generation tasks, safe response, is intensified.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":11,"sentence":"Besides, query emotions that can help model the relationship between query and response are simply ignored in previous models, which would further hurt the coherence.","offset":3,"pro":0.375,"labels":"GAP"},{"idx":11,"sentence":"To alleviate these problems, we propose a novel framework named Curriculum Dual Learning (CDL) which extends the emotion-controllable response generation to a dual task to generate emotional responses and emotional queries alternatively.","offset":4,"pro":0.5,"labels":"PUR"},{"idx":11,"sentence":"CDL utilizes two rewards focusing on emotion and content to improve the duality.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":11,"sentence":"Additionally, it applies curriculum learning to gradually generate high-quality responses based on the difficulties of expressing various emotions.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":11,"sentence":"Experimental results show that CDL significantly outperforms the baselines in terms of coherence, diversity, and relation to emotion factors.","offset":7,"pro":0.875,"labels":"RST"},{"idx":12,"sentence":"Recent works in dialogue state tracking (DST) focus on an open vocabulary-based setting to resolve scalability and generalization issues of the predefined ontology-based approaches.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"However, they are inefficient in that they predict the dialogue state at every turn from scratch.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":12,"sentence":"Here, we consider dialogue state as an explicit fixed-sized memory and propose a selectively overwriting mechanism for more efficient DST.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":12,"sentence":"This mechanism consists of two steps: (1) predicting state operation on each of the memory slots, and (2) overwriting the memory with new values, of which only a few are generated according to the predicted state operations.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":12,"sentence":"Our method decomposes DST into two sub-tasks and guides the decoder to focus only on one of the tasks, thus reducing the burden of the decoder.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":12,"sentence":"This enhances the effectiveness of training and DST performance.","offset":5,"pro":0.625,"labels":"RST"},{"idx":12,"sentence":"Our SOM-DST (Selectively Overwriting Memory for Dialogue State Tracking) model achieves state-of-the-art joint goal accuracy with 51.72% in MultiWOZ 2.0 and 53.01% in MultiWOZ 2.1 in an open vocabulary-based DST setting.","offset":6,"pro":0.75,"labels":"RST"},{"idx":12,"sentence":"In addition, we analyze the accuracy gaps between the current and the ground truth-given situations and suggest that it is a promising direction to improve state operation prediction to boost the DST performance.","offset":7,"pro":0.875,"labels":"RST"},{"idx":13,"sentence":"The traditional approach to build such a dialogue system is to take a pipelined modular architecture, where its modules are optimized individually.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"However, such an optimization scheme does not necessarily yield the overall performance improvement of the whole system.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":13,"sentence":"This scheme makes it difficult for goal-oriented dialogues where the system needs to integrate with external systems or to provide interpretable information about why the system generated a particular response.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":13,"sentence":"In this paper, we present an end-to-end neural architecture for dialogue systems that addresses both challenges above.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":13,"sentence":"In the human evaluation, our dialogue system achieved the success rate of 68.32%, the language understanding score of 4.149, and the response appropriateness score of 4.287, which ranked the system at the top position in the end-to-end multi-domain dialogue system task in the 8th dialogue systems technology challenge (DSTC8).","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":13,"sentence":"On the other hand, end-to-end dialogue systems with monolithic neural architecture are often trained only with input-output utterances, without taking into account the entire annotations available in the corpus. ","offset":5,"pro":0.8333333333333334,"labels":"GAP"},{"idx":14,"sentence":"Existing automatic evaluation metrics for open-domain dialogue response generation systems correlate poorly with human evaluation.","offset":0,"pro":0,"labels":"GAP"},{"idx":14,"sentence":"We focus on evaluating response generation systems via response selection.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":14,"sentence":"To evaluate systems properly via response selection, we propose a method to construct response selection test sets with well-chosen false candidates.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":14,"sentence":"Specifically, we propose to construct test sets filtering out some types of false candidates: (i) those unrelated to the ground-truth response and (ii) those acceptable as appropriate responses.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":14,"sentence":"Through experiments, we demonstrate that evaluating systems via response selection with the test set developed by our method correlates more strongly with human evaluation, compared with widely used automatic evaluation metrics such as BLEU.","offset":4,"pro":0.8,"labels":"RST"},{"idx":15,"sentence":"Off-topic spoken response detection, the task aiming at predicting whether a response is off-topic for the corresponding prompt, is important for an automated speaking assessment system.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"In many real-world educational applications, off-topic spoken response detectors are required to achieve high recall for off-topic responses not only on seen prompts but also on prompts that are unseen during training.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":15,"sentence":"In this paper, we propose a novel approach for off-topic spoken response detection with high off-topic recall on both seen and unseen prompts.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":15,"sentence":"We introduce a new model, Gated Convolutional Bidirectional Attention-based Model (GCBiA), which applies bi-attention mechanism and convolutions to extract topic words of prompts and key-phrases of responses, and introduces gated unit and residual connections between major layers to better represent the relevance of responses and prompts.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":15,"sentence":"Moreover, a new negative sampling method is proposed to augment training data.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":15,"sentence":"Experiment results demonstrate that our novel approach can achieve significant improvements in detecting off-topic responses with extremely high on-topic recall, for both seen and unseen prompts.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":16,"sentence":"Existing end-to-end dialog systems perform less effectively when data is scarce.","offset":0,"pro":0,"labels":"GAP"},{"idx":16,"sentence":"To obtain an acceptable success in real-life online services with only a handful of training examples, both fast adaptability and reliable performance are highly desirable for dialog systems.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":16,"sentence":"In this paper, we propose the Meta-Dialog System (MDS), which combines the advantages of both meta-learning approaches and human-machine collaboration.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":16,"sentence":"We evaluate our methods on a new extended-bAbI dataset and a transformed MultiWOZ dataset for low-resource goal-oriented dialog learning.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":16,"sentence":"Experimental results show that MDS significantly outperforms non-meta-learning baselines and can achieve more than 90% per-turn accuracies with only 10 dialogs on the extended-bAbI dataset.","offset":4,"pro":0.8,"labels":"RST"},{"idx":17,"sentence":"Neural-based context-aware models for slot tagging have achieved state-of-the-art performance.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"However, the presence of OOV(out-of-vocab) words significantly degrades the performance of neural-based models, especially in a few-shot scenario.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":17,"sentence":"In this paper, we propose a novel knowledge-enhanced slot tagging model to integrate contextual representation of input text and the large-scale lexical background knowledge.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":17,"sentence":"Besides, we use multi-level graph attention to explicitly model lexical relations.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":17,"sentence":"The experiments show that our proposed knowledge integration mechanism achieves consistent improvements across settings with different sizes of training data on two public benchmark datasets.","offset":4,"pro":0.8,"labels":"RST"},{"idx":18,"sentence":"Many studies have applied reinforcement learning to train a dialog policy and show great promise these years.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"One common approach is to employ a user simulator to obtain a large number of simulated user experiences for reinforcement learning algorithms.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":18,"sentence":"However, modeling a realistic user simulator is challenging.","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":18,"sentence":"A rule-based simulator requires heavy domain expertise for complex tasks, and a data-driven simulator requires considerable data and it is even unclear how to evaluate a simulator.","offset":3,"pro":0.3333333333333333,"labels":"GAP"},{"idx":18,"sentence":"To avoid explicitly building a user simulator beforehand, we propose Multi-Agent Dialog Policy Learning, which regards both the system and the user as the dialog agents.","offset":4,"pro":0.4444444444444444,"labels":"PUR"},{"idx":18,"sentence":"Two agents interact with each other and are jointly learned simultaneously.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":18,"sentence":"The method uses the actor-critic framework to facilitate pretraining and improve scalability.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":18,"sentence":"We also propose Hybrid Value Network for the role-aware reward decomposition to integrate role-specific domain knowledge of each agent in the task-oriented dialog.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":18,"sentence":"Results show that our method can successfully build a system policy and a user policy simultaneously, and two agents can achieve a high task success rate through conversational interaction.","offset":8,"pro":0.8888888888888888,"labels":"RST"},{"idx":19,"sentence":"Neural generative models have achieved promising performance on dialog generation tasks if given a huge data set.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"However, the lack of high-quality dialog data and the expensive data annotation process greatly limit their application in real world settings.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":19,"sentence":"We propose a paraphrase augmented response generation (PARG) framework that jointly trains a paraphrase model and a response generation model to improve the dialog generation performance.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":19,"sentence":"We also design a method to automatically construct paraphrase training data set based on dialog state and dialog act labels.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":19,"sentence":"PARG is applicable to various dialog generation models, such as TSCP (Lei et al., 2018) and DAMD (Zhang et al., 2019).","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":19,"sentence":"Experimental results show that the proposed framework improves these state-of-the-art dialog models further on CamRest676 and MultiWOZ.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":19,"sentence":"PARG also outperforms other data augmentation methods significantly in dialog generation tasks, especially under low resource settings.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":0,"sentence":"Neural conversation models are known to generate appropriate but non-informative responses in general.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"A scenario where informativeness can be significantly enhanced is Conversing by Reading (CbR), where conversations take place with respect to a given external document.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":0,"sentence":"In previous work, the external document is utilized by (1) creating a context-aware document memory that integrates information from the document and the conversational context, and then (2) generating responses referring to the memory.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":0,"sentence":"In this paper, we propose to create the document memory with some anticipated responses in mind.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":0,"sentence":"This is achieved using a teacher-student framework.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":0,"sentence":"The teacher is given the external document, the context, and the ground-truth response, and learns how to build a response-aware document memory from three sources of information.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":0,"sentence":"The student learns to construct a response-anticipated document memory from the first two sources, and teacher's insight on memory creation.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":0,"sentence":"Empirical results show that our model outperforms the previous state-of-the-art for the CbR task.","offset":7,"pro":0.875,"labels":"RST"},{"idx":1,"sentence":"Dialogue policy optimization often obtains feedback until task completion in task-oriented dialogue systems.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"This is insufficient for training intermediate dialogue turns since supervision signals (or rewards) are only provided at the end of dialogues.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":1,"sentence":"To address this issue, reward learning has been introduced to learn from state-action pairs of an optimal policy to provide turn-by-turn rewards.","offset":2,"pro":0.2222222222222222,"labels":"BAC"},{"idx":1,"sentence":"This approach requires complete state-action annotations of human-to-human dialogues (i.e., expert demonstrations), which is labor intensive.","offset":3,"pro":0.3333333333333333,"labels":"GAP"},{"idx":1,"sentence":"To overcome this limitation, we propose a novel reward learning approach for semi-supervised policy learning.","offset":4,"pro":0.4444444444444444,"labels":"PUR"},{"idx":1,"sentence":"The proposed approach learns a dynamics model as the reward function which models dialogue progress (i.e., state-action sequences) based on expert demonstrations, either with or without annotations.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":1,"sentence":"The dynamics model computes rewards by predicting whether the dialogue progress is consistent with expert demonstrations.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":1,"sentence":"We further propose to learn action embeddings for a better generalization of the reward function.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":1,"sentence":"The proposed approach outperforms competitive policy learning baselines on MultiWOZ, a benchmark multi-domain dataset.","offset":8,"pro":0.8888888888888888,"labels":"RST"},{"idx":2,"sentence":"In modular dialogue systems, natural language understanding (NLU) and natural language generation (NLG) are two critical components, where NLU extracts the semantics from the given texts and NLG is to construct corresponding natural language sentences based on the input semantic representations.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"However, the dual property between understanding and generation has been rarely explored.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":2,"sentence":"The prior work is the first attempt that utilized the duality between NLU and NLG to improve the performance via a dual supervised learning framework.","offset":2,"pro":0.3333333333333333,"labels":"BAC"},{"idx":2,"sentence":"However, the prior work still learned both components in a supervised manner; instead, this paper introduces a general learning framework to effectively exploit such duality, providing flexibility of incorporating both supervised and unsupervised learning algorithms to train language understanding and generation models in a joint fashion.","offset":3,"pro":0.5,"labels":"GAP"},{"idx":2,"sentence":"The benchmark experiments demonstrate that the proposed approach is capable of boosting the performance of both NLU and NLG.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":2,"sentence":"The source code is available at: https://github.com/MiuLab/DuaLUG.","offset":5,"pro":0.8333333333333334,"labels":"CTN"},{"idx":3,"sentence":"The lack of meaningful automatic evaluation metrics for dialog has impeded open-domain dialog research.","offset":0,"pro":0,"labels":"GAP"},{"idx":3,"sentence":"Standard language generation metrics have been shown to be ineffective for evaluating dialog models.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":3,"sentence":"To this end, this paper presents USR, an UnSupervised and Reference-free evaluation metric for dialog.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":3,"sentence":"USR is a reference-free metric that trains unsupervised models to measure several desirable qualities of dialog.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":3,"sentence":"USR is shown to strongly correlate with human judgment on both Topical-Chat (turn-level: 0.42, system-level: 1.0) and PersonaChat (turn-level: 0.48 and system-level: 1.0).","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":3,"sentence":"USR additionally produces interpretable measures for several desirable properties of dialog.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":4,"sentence":"Definition generation, which aims to automatically generate dictionary definitions for words, has recently been proposed to assist the construction of dictionaries and help people understand unfamiliar texts.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"However, previous works hardly consider explicitly modeling the components of definitions, leading to under-specific generation results.","offset":1,"pro":0.25,"labels":"GAP"},{"idx":4,"sentence":"In this paper, we propose ESD, namely Explicit Semantic Decomposition for definition Generation, which explicitly decomposes the meaning of words into semantic components, and models them with discrete latent variables for definition generation.","offset":2,"pro":0.5,"labels":"PUR"},{"idx":4,"sentence":"Experimental results show that achieves top results on WordNet and Oxford benchmarks, outperforming strong previous baselines.","offset":3,"pro":0.75,"labels":"RST"},{"idx":5,"sentence":"Neural language models are usually trained to match the distributional properties of large-scale corpora by minimizing the log loss.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"While straightforward to optimize, this approach forces the model to reproduce all variations in the dataset, including noisy and invalid references (e.g., misannotations and hallucinated facts).","offset":1,"pro":0.125,"labels":"MTD"},{"idx":5,"sentence":"Even a small fraction of noisy data can degrade the performance of log loss.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":5,"sentence":"As an alternative, prior work has shown that minimizing the distinguishability of generated samples is a principled and robust loss that can handle invalid references.","offset":3,"pro":0.375,"labels":"BAC"},{"idx":5,"sentence":"However, distinguishability has not been used in practice due to challenges in optimization and estimation.","offset":4,"pro":0.5,"labels":"GAP"},{"idx":5,"sentence":"We propose loss truncation: a simple and scalable procedure which adaptively removes high log loss examples as a way to optimize for distinguishability.","offset":5,"pro":0.625,"labels":"PUR"},{"idx":5,"sentence":"Empirically, we demonstrate that loss truncation outperforms existing baselines on distinguishability on a summarization task.","offset":6,"pro":0.75,"labels":"RST"},{"idx":5,"sentence":"Furthermore, we show that samples generated by the loss truncation model have factual accuracy ratings that exceed those of baselines and match human references.","offset":7,"pro":0.875,"labels":"RST"},{"idx":6,"sentence":"Efficient structure encoding for graphs with labeled edges is an important yet challenging point in many graph-based models.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"This work focuses on AMR-to-text generation - A graph-to-sequence task aiming to recover natural language from Abstract Meaning Representations (AMR).","offset":1,"pro":0.125,"labels":"PUR"},{"idx":6,"sentence":"Existing graph-to-sequence approaches generally utilize graph neural networks as their encoders, which have two limitations: 1) The message propagation process in AMR graphs is only guided by the first-order adjacency information.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":6,"sentence":"2) The relationships between labeled edges are not fully considered.","offset":3,"pro":0.375,"labels":"GAP"},{"idx":6,"sentence":"In this work, we propose a novel graph encoding framework which can effectively explore the edge relations.","offset":4,"pro":0.5,"labels":"PUR"},{"idx":6,"sentence":"We also adopt graph attention networks with higher-order neighborhood information to encode the rich structure in AMR graphs.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":6,"sentence":"Experiment results show that our approach obtains new state-of-the-art performance on English AMR benchmark datasets.","offset":6,"pro":0.75,"labels":"RST"},{"idx":6,"sentence":"The ablation analyses also demonstrate that both edge relations and higher-order information are beneficial to graph-to-sequence modeling.","offset":7,"pro":0.875,"labels":"RST"},{"idx":7,"sentence":"Neural text generation has made tremendous progress in various tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"One common characteristic of most of the tasks is that the texts are not restricted to some rigid formats when generating.","offset":1,"pro":0.08333333333333333,"labels":"BAC"},{"idx":7,"sentence":"However, we may confront some special text paradigms such as Lyrics (assume the music score is given), Sonnet, SongCi (classical Chinese poetry of the Song dynasty), etc. The typical characteristics of these texts are in three folds: (1) They must comply fully with the rigid predefined formats.","offset":2,"pro":0.16666666666666666,"labels":"GAP"},{"idx":7,"sentence":"(2) They must obey some rhyming schemes.","offset":3,"pro":0.25,"labels":"MTD"},{"idx":7,"sentence":"(3) Although they are restricted to some formats, the sentence integrity must be guaranteed.","offset":4,"pro":0.3333333333333333,"labels":"GAP"},{"idx":7,"sentence":"To the best of our knowledge, text generation based on the predefined rigid formats has not been well investigated.","offset":5,"pro":0.4166666666666667,"labels":"GAP"},{"idx":7,"sentence":"Therefore, we propose a simple and elegant framework named SongNet to tackle this problem.","offset":6,"pro":0.5,"labels":"PUR"},{"idx":7,"sentence":"The backbone of the framework is a Transformer-based auto-regressive language model.","offset":7,"pro":0.5833333333333334,"labels":"MTD"},{"idx":7,"sentence":"Sets of symbols are tailor-designed to improve the modeling performance especially on format, rhyme, and sentence integrity.","offset":8,"pro":0.6666666666666666,"labels":"MTD"},{"idx":7,"sentence":"We improve the attention mechanism to impel the model to capture some future information on the format.","offset":9,"pro":0.75,"labels":"MTD"},{"idx":7,"sentence":"A pre-training and fine-tuning framework is designed to further improve the generation quality.","offset":10,"pro":0.8333333333333334,"labels":"MTD"},{"idx":7,"sentence":"Extensive experiments conducted on two collected corpora demonstrate that our proposed framework generates significantly better results in terms of both automatic metrics and the human evaluation.","offset":11,"pro":0.9166666666666666,"labels":"RST"},{"idx":8,"sentence":"Question Generation (QG) is fundamentally a simple syntactic transformation; however, many aspects of semantics influence what questions are good to form.","offset":0,"pro":0,"labels":"GAP"},{"idx":8,"sentence":"We implement this observation by developing Syn-QG, a set of transparent syntactic rules leveraging universal dependencies, shallow semantic parsing, lexical resources, and custom rules which transform declarative sentences into question-answer pairs.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":8,"sentence":"We utilize PropBank argument descriptions and VerbNet state predicates to incorporate shallow semantic content, which helps generate questions of a descriptive nature and produce inferential and semantically richer questions than existing systems.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":8,"sentence":"In order to improve syntactic fluency and eliminate grammatically incorrect questions, we employ back-translation over the output of these syntactic rules.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":8,"sentence":"A set of crowd-sourced evaluations shows that our system can generate a larger number of highly grammatical and relevant questions than previous QG systems and that back-translation drastically improves grammaticality at a slight cost of generating irrelevant questions.","offset":4,"pro":0.8,"labels":"RST"},{"idx":9,"sentence":"Clustering short text streams is a challenging task due to its unique properties: infinite length, sparse data representation and cluster evolution.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"Existing approaches often exploit short text streams in a batch way.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":9,"sentence":"However, determine the optimal batch size is usually a difficult task since we have no priori knowledge when the topics evolve.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":9,"sentence":"In addition, traditional independent word representation in graphical model tends to cause term ambiguity problem in short text clustering.","offset":3,"pro":0.5,"labels":"GAP"},{"idx":9,"sentence":"Therefore, in this paper, we propose an Online Semantic-enhanced Dirichlet Model for short sext stream clustering, called OSDM, which integrates the word-occurance semantic information (i.e., context) into a new graphical model and clusters each arriving short text automatically in an online way.","offset":4,"pro":0.6666666666666666,"labels":"PUR"},{"idx":9,"sentence":"Extensive results have demonstrated that OSDM has better performance compared to many state-of-the-art algorithms on both synthetic and real-world data sets.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":10,"sentence":"Generative semantic hashing is a promising technique for large-scale information retrieval thanks to its fast retrieval speed and small memory footprint.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"For the tractability of training, existing generative-hashing methods mostly assume a factorized form for the posterior distribution, enforcing independence among the bits of hash codes.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":10,"sentence":"From the perspectives of both model representation and code space size, independence is always not the best assumption.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":10,"sentence":"In this paper, to introduce correlations among the bits of hash codes, we propose to employ the distribution of Boltzmann machine as the variational posterior.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":10,"sentence":"To address the intractability issue of training, we first develop an approximate method to reparameterize the distribution of a Boltzmann machine by augmenting it as a hierarchical concatenation of a Gaussian-like distribution and a Bernoulli distribution.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":10,"sentence":"Based on that, an asymptotically-exact lower bound is further derived for the evidence lower bound (ELBO).","offset":5,"pro":0.625,"labels":"MTD"},{"idx":10,"sentence":"With these novel techniques, the entire model can be optimized efficiently.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":10,"sentence":"Extensive experimental results demonstrate that by effectively modeling correlations among different bits within a hash code, our model can achieve significant performance gains.","offset":7,"pro":0.875,"labels":"RST"},{"idx":11,"sentence":"We propose a methodology to construct a term dictionary for text analytics through an interactive process between a human and a machine, which helps the creation of flexible dictionaries with precise granularity required in typical text analysis.","offset":0,"pro":0,"labels":"MTD"},{"idx":11,"sentence":"This paper introduces the first formulation of interactive dictionary construction to address this issue.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":11,"sentence":"To optimize the interaction, we propose a new algorithm that effectively captures an analyst's intention starting from only a small number of sample terms.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":11,"sentence":"Along with the algorithm, we also design an automatic evaluation framework that provides a systematic assessment of any interactive method for the dictionary creation task.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":11,"sentence":"Experiments using real scenario based corpora and dictionaries show that our algorithm outperforms baseline methods, and works even with a small number of interactions.","offset":4,"pro":0.8,"labels":"RST"},{"idx":12,"sentence":"This paper presents a tree-structured neural topic model, which has a topic distribution over a tree with an infinite number of branches.","offset":0,"pro":0,"labels":"PUR"},{"idx":12,"sentence":"Our model parameterizes an unbounded ancestral and fraternal topic distribution by applying doubly-recurrent neural networks.","offset":1,"pro":0.25,"labels":"MTD"},{"idx":12,"sentence":"With the help of autoencoding variational Bayes, our model improves data scalability and achieves competitive performance when inducing latent topics and tree structures, as compared to a prior tree-structured topic model (Blei et al., 2010).","offset":2,"pro":0.5,"labels":"RST"},{"idx":12,"sentence":"This work extends the tree-structured topic model such that it can be incorporated with neural models for downstream tasks.","offset":3,"pro":0.75,"labels":"PUR"},{"idx":13,"sentence":"We focus on the task of Frequently Asked Questions (FAQ) retrieval.","offset":0,"pro":0,"labels":"MTD"},{"idx":13,"sentence":"A given user query can be matched against the questions and/or the answers in the FAQ.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":13,"sentence":"We present a fully unsupervised method that exploits the FAQ pairs to train two BERT models.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":13,"sentence":"The two models match user queries to FAQ answers and questions, respectively.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":13,"sentence":"We alleviate the missing labeled data of the latter by automatically generating high-quality question paraphrases.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":13,"sentence":"We show that our model is on par and even outperforms supervised models on existing datasets.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":14,"sentence":"Humor plays an important role in human languages and it is essential to model humor when building intelligence systems.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"Among different forms of humor, puns perform wordplay for humorous effects by employing words with double entendre and high phonetic similarity.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":14,"sentence":"However, identifying and modeling puns are challenging as puns usually involved implicit semantic or phonological tricks.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":14,"sentence":"In this paper, we propose Pronunciation-attentive Contextualized Pun Recognition (PCPR) to perceive human humor, detect if a sentence contains puns and locate them in the sentence.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":14,"sentence":"PCPR derives contextualized representation for each word in a sentence by capturing the association between the surrounding context and its corresponding phonetic symbols.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":14,"sentence":"Extensive experiments are conducted on two benchmark datasets.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":14,"sentence":"Results demonstrate that the proposed approach significantly outperforms the state-of-the-art methods in pun detection and location tasks.","offset":6,"pro":0.75,"labels":"RST"},{"idx":14,"sentence":"In-depth analyses verify the effectiveness and robustness of PCPR.","offset":7,"pro":0.875,"labels":"RST"},{"idx":15,"sentence":"Even though BERT has achieved successful performance improvements in various supervised learning tasks, BERT is still limited by repetitive inferences on unsupervised tasks for the computation of contextual language representations.","offset":0,"pro":0,"labels":"GAP"},{"idx":15,"sentence":"To resolve this limitation, we propose a novel deep bidirectional language model called a Transformer-based Text Autoencoder (T-TA).","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":15,"sentence":"The T-TA computes contextual language representations without repetition and displays the benefits of a deep bidirectional architecture, such as that of BERT.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":15,"sentence":"In computation time experiments in a CPU environment, the proposed T-TA performs over six times faster than the BERT-like model on a reranking task and twelve times faster on a semantic similarity task.","offset":3,"pro":0.5,"labels":"RST"},{"idx":15,"sentence":"Furthermore, the T-TA shows competitive or even better accuracies than those of BERT on the above tasks.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":15,"sentence":"Code is available at https://github.com/joongbo/tta.","offset":5,"pro":0.8333333333333334,"labels":"CTN"},{"idx":16,"sentence":"Personalized news recommendation is a critical technology to improve users' online news reading experience.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"The core of news recommendation is accurate matching between user's interests and candidate news.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":16,"sentence":"The same user usually has diverse interests that are reflected in different news she has browsed.","offset":2,"pro":0.2,"labels":"BAC"},{"idx":16,"sentence":"Meanwhile, important semantic features of news are implied in text segments of different granularities.","offset":3,"pro":0.3,"labels":"BAC"},{"idx":16,"sentence":"Existing studies generally represent each user as a single vector and then match the candidate news vector, which may lose fine-grained information for recommendation.","offset":4,"pro":0.4,"labels":"GAP"},{"idx":16,"sentence":"In this paper, we propose FIM, a Fine-grained Interest Matching method for neural news recommendation.","offset":5,"pro":0.5,"labels":"PUR"},{"idx":16,"sentence":"Instead of aggregating user's all historical browsed news into a unified vector, we hierarchically construct multi-level representations for each news via stacked dilated convolutions.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":16,"sentence":"Then we perform fine-grained matching between segment pairs of each browsed news and the candidate news at each semantic level.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":16,"sentence":"High-order salient signals are then identified by resembling the hierarchy of image recognition for final click prediction.","offset":8,"pro":0.8,"labels":"MTD"},{"idx":16,"sentence":"Extensive experiments on a real-world dataset from MSN news validate the effectiveness of our model on news recommendation.","offset":9,"pro":0.9,"labels":"RST"},{"idx":17,"sentence":"Operational risk management is one of the biggest challenges nowadays faced by financial institutions.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"There are several major challenges of building a text classification system for automatic operational risk prediction, including imbalanced labeled/unlabeled data and lacking interpretability.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":17,"sentence":"To tackle these challenges, we present a semi-supervised text classification framework that integrates multi-head attention mechanism with Semi-supervised variational inference for Operational Risk Classification (SemiORC).","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":17,"sentence":"We empirically evaluate the framework on a real-world dataset.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":17,"sentence":"The results demonstrate that our method can better utilize unlabeled data and learn visually interpretable document representations.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":17,"sentence":"SemiORC also outperforms other baseline methods on operational risk classification.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":18,"sentence":"Identifying user geolocation in online social networks is an essential task in many location-based applications.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"Existing methods rely on the similarity of text and network structure, however, they suffer from a lack of interpretability on the corresponding results, which is crucial for understanding model behavior.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":18,"sentence":"In this work, we adopt influence functions to interpret the behavior of GNN-based models by identifying the importance of training users when predicting the locations of the testing users.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":18,"sentence":"This methodology helps with providing meaningful explanations on prediction results.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":18,"sentence":"Furthermore, it also initiates an attempt to uncover the so-called black-box GNN-based models by investigating the effect of individual nodes.","offset":4,"pro":0.8,"labels":"MTD"},{"idx":19,"sentence":"Language modeling is the technique to estimate the probability of a sequence of words.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"A bilingual language model is expected to model the sequential dependency for words across languages, which is difficult due to the inherent lack of suitable training data as well as diverse syntactic structure across languages.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":19,"sentence":"We propose a bilingual attention language model (BALM) that simultaneously performs language modeling objective with a quasi-translation objective to model both the monolingual as well as the cross-lingual sequential dependency.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":19,"sentence":"The attention mechanism learns the bilingual context from a parallel corpus.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":19,"sentence":"BALM achieves state-of-the-art performance on the SEAME code-switch database by reducing the perplexity of 20.5% over the best-reported result.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":19,"sentence":"We also apply BALM in bilingual lexicon induction, and language normalization tasks to validate the idea.","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":0,"sentence":"Chinese Spelling Check (CSC) is a task to detect and correct spelling errors in Chinese natural language.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"Existing methods have made attempts to incorporate the similarity knowledge between Chinese characters.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":0,"sentence":"However, they take the similarity knowledge as either an external input resource or just heuristic rules.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":0,"sentence":"This paper proposes to incorporate phonological and visual similarity knowledge into language models for CSC via a specialized graph convolutional network (SpellGCN).","offset":3,"pro":0.375,"labels":"PUR"},{"idx":0,"sentence":"The model builds a graph over the characters, and SpellGCN is learned to map this graph into a set of inter-dependent character classifiers.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":0,"sentence":"These classifiers are applied to the representations extracted by another network, such as BERT, enabling the whole network to be end-to-end trainable.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":0,"sentence":"Experiments are conducted on three human-annotated datasets.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":0,"sentence":"Our method achieves superior performance against previous models by a large margin.","offset":7,"pro":0.875,"labels":"RST"},{"idx":1,"sentence":"Spelling error correction is an important yet challenging task because a satisfactory solution of it essentially needs human-level language understanding ability.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"Without loss of generality we consider Chinese spelling error correction (CSC) in this paper.","offset":1,"pro":0.14285714285714285,"labels":"MTD"},{"idx":1,"sentence":"A state-of-the-art method for the task selects a character from a list of candidates for correction (including non-correction) at each position of the sentence on the basis of BERT, the language representation model.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":1,"sentence":"The accuracy of the method can be sub-optimal, however, because BERT does not have sufficient capability to detect whether there is an error at each position, apparently due to the way of pre-training it using mask language modeling.","offset":3,"pro":0.42857142857142855,"labels":"GAP"},{"idx":1,"sentence":"In this work, we propose a novel neural architecture to address the aforementioned issue, which consists of a network for error detection and a network for error correction based on BERT, with the former being connected to the latter with what we call soft-masking technique.","offset":4,"pro":0.5714285714285714,"labels":"PUR"},{"idx":1,"sentence":"Our method of using Soft-Masked BERT' is general, and it may be employed in other language detection-correction problems.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":1,"sentence":"Experimental results on two datasets, including one large dataset which we create and plan to release, demonstrate that the performance of our proposed method is significantly better than the baselines including the one solely based on BERT.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":2,"sentence":"Sentence representation (SR) is the most crucial and challenging task in Machine Reading Comprehension (MRC).","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"MRC systems typically only utilize the information contained in the sentence itself, while human beings can leverage their semantic knowledge.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":2,"sentence":"To bridge the gap, we proposed a novel Frame-based Sentence Representation (FSR) method, which employs frame semantic knowledge to facilitate sentence modelling.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":2,"sentence":"Specifically, different from existing methods that only model lexical units (LUs), Frame Representation Models, which utilize both LUs in frame and Frame-to-Frame (F-to-F) relations, are designed to model frames and sentences with attention schema.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":2,"sentence":"Our proposed FSR method is able to integrate multiple-frame semantic information to get much better sentence representations.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":2,"sentence":"Our extensive experimental results show that it performs better than state-of-the-art technologies on machine reading comprehension task.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":3,"sentence":"In this paper, we introduce a novel methodology to efficiently construct a corpus for question answering over structured data.","offset":0,"pro":0,"labels":"PUR"},{"idx":3,"sentence":"For this, we introduce an intermediate representation that is based on the logical query plan in a database, called Operation Trees (OT).","offset":1,"pro":0.14285714285714285,"labels":"MTD"},{"idx":3,"sentence":"This representation allows us to invert the annotation process without loosing flexibility in the types of queries that we generate.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":3,"sentence":"Furthermore, it allows for fine-grained alignment of the tokens to the operations.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":3,"sentence":"Thus, we randomly generate OTs from a context free grammar and annotators just have to write the appropriate question and assign the tokens.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":3,"sentence":"We compare our corpus OTTA (Operation Trees and Token Assignment), a large semantic parsing corpus for evaluating natural language interfaces to databases, to Spider and LC-QuaD 2.0 and show that our methodology more than triples the annotation speed while maintaining the complexity of the queries.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":3,"sentence":"Finally, we train a state-of-the-art semantic parsing model on our data and show that our dataset is a challenging dataset and that the token alignment can be leveraged to significantly increase the performance.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":4,"sentence":"Open-domain question answering can be formulated as a phrase retrieval problem, in which we can expect huge scalability and speed benefit but often suffer from low accuracy due to the limitation of existing phrase representation models.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"In this paper, we aim to improve the quality of each phrase embedding by augmenting it with a contextualized sparse representation (Sparc).","offset":1,"pro":0.2,"labels":"PUR"},{"idx":4,"sentence":"Unlike previous sparse vectors that are term-frequency-based (e.g., tf-idf) or directly learned (only few thousand dimensions), we leverage rectified self-attention to indirectly learn sparse vectors in n-gram vocabulary space.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":4,"sentence":"By augmenting the previous phrase retrieval model (Seo et al., 2019) with Sparc, we show 4%+ improvement in CuratedTREC and SQuAD-Open.","offset":3,"pro":0.6,"labels":"RST"},{"idx":4,"sentence":"Our CuratedTREC score is even better than the best known retrieve {& read model with at least 45x faster inference speed.","offset":4,"pro":0.8,"labels":"RST"},{"idx":5,"sentence":"Building general reading comprehension systems, capable of solving multiple datasets at the same time, is a recent aspirational goal in the research community.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"Prior work has focused on model architecture or generalization to held out datasets, and largely passed over the particulars of the multi-task learning set up.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":5,"sentence":"We show that a simple dynamic sampling strategy, selecting instances for training proportional to the multi-task model's current performance on a dataset relative to its single task performance, gives substantive gains over prior multi-task sampling strategies, mitigating the catastrophic forgetting that is common in multi-task learning.","offset":2,"pro":0.4,"labels":"RST"},{"idx":5,"sentence":"We also demonstrate that allowing instances of different tasks to be interleaved as much as possible between each epoch and batch has a clear benefit in multitask performance over forcing task homogeneity at the epoch or batch level.","offset":3,"pro":0.6,"labels":"RST"},{"idx":5,"sentence":"Our final model shows greatly increased performance over the best model on ORB, a recently-released multitask reading comprehension benchmark.","offset":4,"pro":0.8,"labels":"RST"},{"idx":6,"sentence":"Multilingual pre-trained models could leverage the training data from a rich source language (such as English) to improve performance on low resource languages.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"However, the transfer quality for multilingual Machine Reading Comprehension (MRC) is significantly worse than sentence classification tasks mainly due to the requirement of MRC to detect the word level answer boundary.","offset":1,"pro":0.25,"labels":"GAP"},{"idx":6,"sentence":"In this paper, we propose two auxiliary tasks in the fine-tuning stage to create additional phrase boundary supervision: (1) A mixed MRC task, which translates the question or passage to other languages and builds cross-lingual question-passage pairs; (2) A language-agnostic knowledge masking task by leveraging knowledge phrases mined from web.","offset":2,"pro":0.5,"labels":"PUR"},{"idx":6,"sentence":"Besides, extensive experiments on two cross-lingual MRC datasets show the effectiveness of our proposed approach.","offset":3,"pro":0.75,"labels":"RST"},{"idx":7,"sentence":"The goal of conversational machine reading is to answer user questions given a knowledge base text which may require asking clarification questions.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"Existing approaches are limited in their decision making due to struggles in extracting question-related rules and reasoning about them.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":7,"sentence":"In this paper, we present a new framework of conversational machine reading that comprises a novel Explicit Memory Tracker (EMT) to track whether conditions listed in the rule text have already been satisfied to make a decision.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":7,"sentence":"Moreover, our framework generates clarification questions by adopting a coarse-to-fine reasoning strategy, utilizing sentence-level entailment scores to weight token-level distributions.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":7,"sentence":"On the ShARC benchmark (blind, held-out) testset, EMT achieves new state-of-the-art results of 74.6% micro-averaged decision accuracy and 49.5 BLEU4.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":7,"sentence":"We also show that EMT is more interpretable by visualizing the entailment-oriented reasoning process as the conversation flows.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":7,"sentence":"Code and models are released at https://github.com/Yifan-Gao/explicit{_memory{_tracker.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":8,"sentence":"Large pre-trained language models (LMs) are known to encode substantial amounts of linguistic information.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"However, high-level reasoning skills, such as numerical reasoning, are difficult to learn from a language-modeling objective only.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":8,"sentence":"Consequently, existing models for numerical reasoning have used specialized architectures with limited flexibility.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":8,"sentence":"In this work, we show that numerical reasoning is amenable to automatic data generation, and thus one can inject this skill into pre-trained LMs, by generating large amounts of data, and training in a multi-task setup.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":8,"sentence":"We show that pre-training our model, GenBERT, on this data, dramatically improves performance on DROP (49.3 -greater 72.3 F1), reaching performance that matches state-of-the-art models of comparable size, while using a simple and general-purpose encoder-decoder architecture.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":8,"sentence":"Moreover, GenBERT generalizes well to math word problem datasets, while maintaining high performance on standard RC tasks.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":8,"sentence":"Our approach provides a general recipe for injecting skills into large pre-trained LMs, whenever the skill is amenable to automatic data augmentation.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":9,"sentence":"Despite recent progress in conversational question answering, most prior work does not focus on follow-up questions.","offset":0,"pro":0,"labels":"GAP"},{"idx":9,"sentence":"Practical conversational question answering systems often receive follow-up questions in an ongoing conversation, and it is crucial for a system to be able to determine whether a question is a follow-up question of the current conversation, for more effective answer finding subsequently.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":9,"sentence":"In this paper, we introduce a new follow-up question identification task.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":9,"sentence":"We propose a three-way attentive pooling network that determines the suitability of a follow-up question by capturing pair-wise interactions between the associated passage, the conversation history, and a candidate follow-up question.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":9,"sentence":"It enables the model to capture topic continuity and topic shift while scoring a particular candidate follow-up question.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":9,"sentence":"Experiments show that our proposed three-way attentive pooling network outperforms all baseline systems by significant margins.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":10,"sentence":"Previous work on answering complex questions from knowledge bases usually separately addresses two types of complexity: questions with constraints and questions with multiple hops of relations.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"In this paper, we handle both types of complexity at the same time.","offset":1,"pro":0.25,"labels":"PUR"},{"idx":10,"sentence":"Motivated by the observation that early incorporation of constraints into query graphs can more effectively prune the search space, we propose a modified staged query graph generation method with more flexible ways to generate query graphs.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":10,"sentence":"Our experiments clearly show that our method achieves the state of the art on three benchmark KBQA datasets.","offset":3,"pro":0.75,"labels":"RST"},{"idx":11,"sentence":"We present ASDiv (Academia Sinica Diverse MWP Dataset), a diverse (in terms of both language patterns and problem types) English math word problem (MWP) corpus for evaluating the capability of various MWP solvers.","offset":0,"pro":0,"labels":"MTD"},{"idx":11,"sentence":"Existing MWP corpora for studying AI progress remain limited either in language usage patterns or in problem types.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":11,"sentence":"We thus present a new English MWP corpus with 2,305 MWPs that cover more text patterns and most problem types taught in elementary school.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":11,"sentence":"Each MWP is annotated with its problem type and grade level (for indicating the level of difficulty).","offset":3,"pro":0.5,"labels":"MTD"},{"idx":11,"sentence":"Furthermore, we propose a metric to measure the lexicon usage diversity of a given MWP corpus, and demonstrate that ASDiv is more diverse than existing corpora.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":11,"sentence":"Experiments show that our proposed corpus reflects the true capability of MWP solvers more faithfully.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":12,"sentence":"Evaluating image captions is very challenging partially due to the fact that there are multiple correct captions for every single image.","offset":0,"pro":0,"labels":"GAP"},{"idx":12,"sentence":"Most of the existing one-to-one metrics operate by penalizing mismatches between reference and generative caption without considering the intrinsic variance between ground truth captions.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":12,"sentence":"It usually leads to over-penalization and thus a bad correlation to human judgment.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":12,"sentence":"Recently, the latest one-to-one metric BERTScore can achieve high human correlation in system-level tasks while some issues can be fixed for better performance.","offset":3,"pro":0.5,"labels":"BAC"},{"idx":12,"sentence":"In this paper, we propose a novel metric based on BERTScore that could handle such a challenge and extend BERTScore with a few new features appropriately for image captioning evaluation.","offset":4,"pro":0.6666666666666666,"labels":"PUR"},{"idx":12,"sentence":"The experimental results show that our metric achieves state-of-the-art human judgment correlation.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":13,"sentence":"Existing approaches to mapping-based cross-lingual word embeddings are based on the assumption that the source and target embedding spaces are structurally similar.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"The structures of embedding spaces largely depend on the co-occurrence statistics of each word, which the choice of context window determines.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":13,"sentence":"Despite this obvious connection between the context window and mapping-based cross-lingual embeddings, their relationship has been underexplored in prior work.","offset":2,"pro":0.4,"labels":"GAP"},{"idx":13,"sentence":"In this work, we provide a thorough evaluation, in various languages, domains, and tasks, of bilingual embeddings trained with different context windows.","offset":3,"pro":0.6,"labels":"PUR"},{"idx":13,"sentence":"The highlight of our findings is that increasing the size of both the source and target window sizes improves the performance of bilingual lexicon induction, especially the performance on frequent nouns.","offset":4,"pro":0.8,"labels":"RST"},{"idx":14,"sentence":"A major obstacle in Word Sense Disambiguation (WSD) is that word senses are not uniformly distributed, causing existing models to generally perform poorly on senses that are either rare or unseen during training.","offset":0,"pro":0,"labels":"GAP"},{"idx":14,"sentence":"We propose a bi-encoder model that independently embeds (1) the target word with its surrounding context and (2) the dictionary definition, or gloss, of each sense.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":14,"sentence":"The encoders are jointly optimized in the same representation space, so that sense disambiguation can be performed by finding the nearest sense embedding for each target word embedding.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":14,"sentence":"Our system outperforms previous state-of-the-art models on English all-words WSD; these gains predominantly come from improved performance on rare senses, leading to a 31.1% error reduction on less frequent senses over prior work.","offset":3,"pro":0.6,"labels":"RST"},{"idx":14,"sentence":"This demonstrates that rare senses can be more effectively disambiguated by modeling their definitions.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":15,"sentence":"In this paper, we demonstrate how code-switching patterns can be utilised to improve various downstream NLP applications.","offset":0,"pro":0,"labels":"PUR"},{"idx":15,"sentence":"In particular, we encode various switching features to improve humour, sarcasm and hate speech detection tasks.","offset":1,"pro":0.3333333333333333,"labels":"MTD"},{"idx":15,"sentence":"We believe that this simple linguistic observation can also be potentially helpful in improving other similar NLP applications.","offset":2,"pro":0.6666666666666666,"labels":"CTN"},{"idx":16,"sentence":"Recently, many methods discover effective evidence from reliable sources by appropriate neural networks for explainable claim verification, which has been widely recognized.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"However, in these methods, the discovery process of evidence is nontransparent and unexplained.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":16,"sentence":"Simultaneously, the discovered evidence is aimed at the interpretability of the whole sequence of claims but insufficient to focus on the false parts of claims.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":16,"sentence":"In this paper, we propose a Decision Tree-based Co-Attention model (DTCA) to discover evidence for explainable claim verification.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":16,"sentence":"Specifically, we first construct Decision Tree-based Evidence model (DTE) to select comments with high credibility as evidence in a transparent and interpretable way.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":16,"sentence":"Then we design Co-attention Self-attention networks (CaSa) to make the selected evidence interact with claims, which is for 1) training DTE to determine the optimal decision thresholds and obtain more powerful evidence; and 2) utilizing the evidence to find the false parts in the claim.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":16,"sentence":"Experiments on two public datasets, RumourEval and PHEME, demonstrate that DTCA not only provides explanations for the results of claim verification but also achieves the state-of-the-art performance, boosting the F1-score by more than 3.11%, 2.41%, respectively.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":17,"sentence":"We focus on the study of conversational recommendation in the context of multi-type dialogs, where the bots can proactively and naturally lead a conversation from a non-recommendation dialog (e.g., QA) to a recommendation dialog, taking into account user's interests and feedback.","offset":0,"pro":0,"labels":"PUR"},{"idx":17,"sentence":"To facilitate the study of this task, we create a human-to-human Chinese dialog dataset DuRecDial (about 10k dialogs, 156k utterances), where there are multiple sequential dialogs for a pair of a recommendation seeker (user) and a recommender (bot).","offset":1,"pro":0.2,"labels":"PUR"},{"idx":17,"sentence":"In each dialog, the recommender proactively leads a multi-type dialog to approach recommendation targets and then makes multiple recommendations with rich interaction behavior.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":17,"sentence":"This dataset allows us to systematically investigate different parts of the overall problem, e.g., how to naturally lead a dialog, how to interact with users for recommendation.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":17,"sentence":"Finally we establish baseline results on DuRecDial for future studies.","offset":4,"pro":0.8,"labels":"CTN"},{"idx":18,"sentence":"User intent classification plays a vital role in dialogue systems.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"Since user intent may frequently change over time in many realistic scenarios, unknown (new) intent detection has become an essential problem, where the study has just begun.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":18,"sentence":"This paper proposes a semantic-enhanced Gaussian mixture model (SEG) for unknown intent detection.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":18,"sentence":"In particular, we model utterance embeddings with a Gaussian mixture distribution and inject dynamic class semantic information into Gaussian means, which enables learning more class-concentrated embeddings that help to facilitate downstream outlier detection.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":18,"sentence":"Coupled with a density-based outlier detection algorithm, SEG achieves competitive results on three real task-oriented dialogue datasets in two languages for unknown intent detection.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":18,"sentence":"On top of that, we propose to integrate SEG as an unknown intent identifier into existing generalized zero-shot intent classification models to improve their performance.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":18,"sentence":"A case study on a state-of-the-art method, ReCapsNet, shows that SEG can push the classification performance to a significantly higher level.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":19,"sentence":"The curse of knowledge can impede communication between experts and laymen.","offset":0,"pro":0,"labels":"GAP"},{"idx":19,"sentence":"We propose a new task of expertise style transfer and contribute a manually annotated dataset with the goal of alleviating such cognitive biases.","offset":1,"pro":0.125,"labels":"PUR"},{"idx":19,"sentence":"Solving this task not only simplifies the professional language, but also improves the accuracy and expertise level of laymen descriptions using simple words.","offset":2,"pro":0.25,"labels":"RST"},{"idx":19,"sentence":"This is a challenging task, unaddressed in previous work, as it requires the models to have expert intelligence in order to modify text with a deep understanding of domain knowledge and structures.","offset":3,"pro":0.375,"labels":"GAP"},{"idx":19,"sentence":"We establish the benchmark performance of five state-of-the-art models for style transfer and text simplification.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":19,"sentence":"The results demonstrate a significant gap between machine and human performance.","offset":5,"pro":0.625,"labels":"RST"},{"idx":19,"sentence":"We also discuss the challenges of automatic evaluation, to provide insights into future research directions.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":19,"sentence":"The dataset is publicly available at https://srhthu.github.io/expertise-style-transfer/.","offset":7,"pro":0.875,"labels":"CTN"},{"idx":0,"sentence":"Text generation from a knowledge base aims to translate knowledge triples to natural language descriptions.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"Most existing methods ignore the faithfulness between a generated text description and the original table, leading to generated information that goes beyond the content of the table.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":0,"sentence":"In this paper, for the first time, we propose a novel Transformer-based generation framework to achieve the goal.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":0,"sentence":"The core techniques in our method to enforce faithfulness include a new table-text optimal-transport matching loss and a table-text embedding similarity loss based on the Transformer model.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":0,"sentence":"Furthermore, to evaluate faithfulness, we propose a new automatic metric specialized to the table-to-text generation problem.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":0,"sentence":"We also provide detailed analysis on each component of our model in our experiments.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":0,"sentence":"Automatic and human evaluations show that our framework can significantly outperform state-of-the-art by a large margin.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":1,"sentence":"This paper proposes Dynamic Memory Induction Networks (DMIN) for few-short text classification.","offset":0,"pro":0,"labels":"PUR"},{"idx":1,"sentence":"The model develops a dynamic routing mechanism over static memory, enabling it to better adapt to unseen classes, a critical capability for few-short classification.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":1,"sentence":"The model also expands the induction process with supervised learning weights and query information to enhance the generalization ability of meta-learning.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":1,"sentence":"The proposed model brings forward the state-of-the-art performance significantly by 2asciitilde4% improvement on the miniRCV1 and ODIC datasets.","offset":3,"pro":0.6,"labels":"RST"},{"idx":1,"sentence":"Detailed analysis is further performed to show how the proposed network achieves the new performance.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":2,"sentence":"Keyphrase generation (KG) aims to summarize the main ideas of a document into a set of keyphrases.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"A new setting is recently introduced into this problem, in which, given a document, the model needs to predict a set of keyphrases and simultaneously determine the appropriate number of keyphrases to produce.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":2,"sentence":"Previous work in this setting employs a sequential decoding process to generate keyphrases.","offset":2,"pro":0.2222222222222222,"labels":"BAC"},{"idx":2,"sentence":"However, such a decoding method ignores the intrinsic hierarchical compositionality existing in the keyphrase set of a document.","offset":3,"pro":0.3333333333333333,"labels":"GAP"},{"idx":2,"sentence":"Moreover, previous work tends to generate duplicated keyphrases, which wastes time and computing resources.","offset":4,"pro":0.4444444444444444,"labels":"GAP"},{"idx":2,"sentence":"To overcome these limitations, we propose an exclusive hierarchical decoding framework that includes a hierarchical decoding process and either a soft or a hard exclusion mechanism.","offset":5,"pro":0.5555555555555556,"labels":"PUR"},{"idx":2,"sentence":"The hierarchical decoding process is to explicitly model the hierarchical compositionality of a keyphrase set.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":2,"sentence":"Both the soft and the hard exclusion mechanisms keep track of previously-predicted keyphrases within a window size to enhance the diversity of the generated keyphrases.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":2,"sentence":"Extensive experiments on multiple KG benchmark datasets demonstrate the effectiveness of our method to generate less duplicated and more accurate keyphrases.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":3,"sentence":"Hierarchical text classification is an essential yet challenging subtask of multi-label text classification with a taxonomic hierarchy.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"Existing methods have difficulties in modeling the hierarchical label structure in a global view.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":3,"sentence":"Furthermore, they cannot make full use of the mutual interactions between the text feature space and the label space.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":3,"sentence":"In this paper, we formulate the hierarchy as a directed graph and introduce hierarchy-aware structure encoders for modeling label dependencies.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":3,"sentence":"Based on the hierarchy encoder, we propose a novel end-to-end hierarchy-aware global model (HiAGM) with two variants.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":3,"sentence":"A multi-label attention variant (HiAGM-LA) learns hierarchy-aware label embeddings through the hierarchy encoder and conducts inductive fusion of label-aware text features.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":3,"sentence":"A text feature propagation model (HiAGM-TP) is proposed as the deductive variant that directly feeds text features into hierarchy encoders.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":3,"sentence":"Compared with previous works, both HiAGM-LA and HiAGM-TP achieve significant and consistent improvements on three benchmark datasets.","offset":7,"pro":0.875,"labels":"RST"},{"idx":4,"sentence":"Sequence-to-sequence models have lead to significant progress in keyphrase generation, but it remains unknown whether they are reliable enough to be beneficial for document retrieval.","offset":0,"pro":0,"labels":"GAP"},{"idx":4,"sentence":"This study provides empirical evidence that such models can significantly improve retrieval performance, and introduces a new extrinsic evaluation framework that allows for a better understanding of the limitations of keyphrase generation models.","offset":1,"pro":0.25,"labels":"RST"},{"idx":4,"sentence":"Using this framework, we point out and discuss the difficulties encountered with supplementing documents with -not present in text- keyphrases, and generalizing models across domains.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":4,"sentence":"Our code is available at https://github.com/boudinfl/ir-using-kg","offset":3,"pro":0.75,"labels":"CTN"},{"idx":5,"sentence":"There has been little work on modeling the morphological well-formedness (MWF) of derivatives, a problem judged to be complex and difficult in linguistics.","offset":0,"pro":0,"labels":"GAP"},{"idx":5,"sentence":"We present a graph auto-encoder that learns embeddings capturing information about the compatibility of affixes and stems in derivation.","offset":1,"pro":0.3333333333333333,"labels":"MTD"},{"idx":5,"sentence":"The auto-encoder models MWF in English surprisingly well by combining syntactic and semantic information with associative information from the mental lexicon.","offset":2,"pro":0.6666666666666666,"labels":"CLN"},{"idx":6,"sentence":"We introduce the first treebank for a romanized user-generated content variety of Algerian, a North-African Arabic dialect known for its frequent usage of code-switching.","offset":0,"pro":0,"labels":"PUR"},{"idx":6,"sentence":"Made of 1500 sentences, fully annotated in morpho-syntax and Universal Dependency syntax, with full translation at both the word and the sentence levels, this treebank is made freely available.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":6,"sentence":"It is supplemented with 50k unlabeled sentences collected from Common Crawl and web-crawled data using intensive data-mining techniques.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":6,"sentence":"Preliminary experiments demonstrate its usefulness for POS tagging and dependency parsing.","offset":3,"pro":0.5,"labels":"RST"},{"idx":6,"sentence":"We believe that what we present in this paper is useful beyond the low-resource language community.","offset":4,"pro":0.6666666666666666,"labels":"CTN"},{"idx":6,"sentence":"This is the first time that enough unlabeled and annotated data is provided for an emerging user-generated content dialectal language with rich morphology and code switching, making it an challenging test-bed for most recent NLP approaches.","offset":5,"pro":0.8333333333333334,"labels":"CTN"},{"idx":7,"sentence":"This paper introduces the Webis Gmane Email Corpus 2019, the largest publicly available and fully preprocessed email corpus to date.","offset":0,"pro":0,"labels":"PUR"},{"idx":7,"sentence":"We crawled more than 153 million emails from 14,699 mailing lists and segmented them into semantically consistent components using a new neural segmentation model.","offset":1,"pro":0.25,"labels":"MTD"},{"idx":7,"sentence":"With 96% accuracy on 15 classes of email segments, our model achieves state-of-the-art performance while being more efficient to train than previous ones.","offset":2,"pro":0.5,"labels":"RST"},{"idx":7,"sentence":"All data, code, and trained models are made freely available alongside the paper.","offset":3,"pro":0.75,"labels":"CTN"},{"idx":8,"sentence":"The patterns in which the syntax of different languages converges and diverges are often used to inform work on cross-lingual transfer.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"Nevertheless, little empirical work has been done on quantifying the prevalence of different syntactic divergences across language pairs.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":8,"sentence":"We propose a framework for extracting divergence patterns for any language pair from a parallel corpus, building on Universal Dependencies.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":8,"sentence":"We show that our framework provides a detailed picture of cross-language divergences, generalizes previous approaches, and lends itself to full automation.","offset":3,"pro":0.5,"labels":"CLN"},{"idx":8,"sentence":"We further present a novel dataset, a manually word-aligned subset of the Parallel UD corpus in five languages, and use it to perform a detailed corpus study.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":8,"sentence":"We demonstrate the usefulness of the resulting analysis by showing that it can help account for performance patterns of a cross-lingual parser.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":9,"sentence":"Recently research has started focusing on avoiding undesired effects that come with content moderation, such as censorship and overblocking, when dealing with hatred online.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"The core idea is to directly intervene in the discussion with textual responses that are meant to counter the hate content and prevent it from further spreading.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":9,"sentence":"Accordingly, automation strategies, such as natural language generation, are beginning to be investigated.","offset":2,"pro":0.4,"labels":"BAC"},{"idx":9,"sentence":"Still, they suffer from the lack of sufficient amount of quality data and tend to produce generic/repetitive responses.","offset":3,"pro":0.6,"labels":"GAP"},{"idx":9,"sentence":"Being aware of the aforementioned limitations, we present a study on how to collect responses to hate effectively, employing large scale unsupervised language models such as GPT-2 for the generation of silver data, and the best annotation strategies/neural architectures that can be used for data filtering before expert validation/post-editing.","offset":4,"pro":0.8,"labels":"PUR"},{"idx":10,"sentence":"In recent years, a series of Transformer-based models unlocked major improvements in general natural language understanding (NLU) tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"Such a fast pace of research would not be possible without general NLU benchmarks, which allow for a fair comparison of the proposed methods.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":10,"sentence":"However, such benchmarks are available only for a handful of languages.","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":10,"sentence":"To alleviate this issue, we introduce a comprehensive multi-task benchmark for the Polish language understanding, accompanied by an online leaderboard.","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":10,"sentence":"It consists of a diverse set of tasks, adopted from existing datasets for named entity recognition, question-answering, textual entailment, and others.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":10,"sentence":"We also introduce a new sentiment analysis task for the e-commerce domain, named Allegro Reviews (AR).","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":10,"sentence":"To ensure a common evaluation scheme and promote models that generalize to different NLU tasks, the benchmark includes datasets from varying domains and applications.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":10,"sentence":"Additionally, we release HerBERT, a Transformer-based model trained specifically for the Polish language, which has the best average performance and obtains the best results for three out of nine tasks.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":10,"sentence":"Finally, we provide an extensive evaluation, including several standard baselines and recently proposed, multilingual Transformer-based models.","offset":8,"pro":0.8888888888888888,"labels":"MTD"},{"idx":11,"sentence":"Emotion lexicons describe the affective meaning of words and thus constitute a centerpiece for advanced sentiment and emotion analysis.","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"Yet, manually curated lexicons are only available for a handful of languages, leaving most languages of the world without such a precious resource for downstream applications.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":11,"sentence":"Even worse, their coverage is often limited both in terms of the lexical units they contain and the emotional variables they feature.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":11,"sentence":"In order to break this bottleneck, we here introduce a methodology for creating almost arbitrarily large emotion lexicons for any target language.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":11,"sentence":"Our approach requires nothing but a source language emotion lexicon, a bilingual word translation model, and a target language embedding model.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":11,"sentence":"Fulfilling these requirements for 91 languages, we are able to generate representationally rich high-coverage lexicons comprising eight emotional variables with more than 100k lexical entries each.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":11,"sentence":"We evaluated the automatically generated lexicons against human judgment from 26 datasets, spanning 12 typologically diverse languages, and found that our approach produces results in line with state-of-the-art monolingual approaches to lexicon creation and even surpasses human reliability for some languages and variables.","offset":6,"pro":0.75,"labels":"RST"},{"idx":11,"sentence":"Code and data are available at https://github.com/JULIELab/MEmoLon archived under DOI 10.5281/zenodo.3779901.","offset":7,"pro":0.875,"labels":"CTN"},{"idx":12,"sentence":"Reliably evaluating Machine Translation (MT) through automated metrics is a long-standing problem.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"One of the main challenges is the fact that multiple outputs can be equally valid.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":12,"sentence":"Attempts to minimise this issue include metrics that relax the matching of MT output and reference strings, and the use of multiple references.","offset":2,"pro":0.2857142857142857,"labels":"BAC"},{"idx":12,"sentence":"The latter has been shown to significantly improve the performance of evaluation metrics.","offset":3,"pro":0.42857142857142855,"labels":"BAC"},{"idx":12,"sentence":"However, collecting multiple references is expensive and in practice a single reference is generally used.","offset":4,"pro":0.5714285714285714,"labels":"GAP"},{"idx":12,"sentence":"In this paper, we propose an alternative approach: instead of modelling linguistic variation in human reference we exploit the MT model uncertainty to generate multiple diverse translations and use these: (i) as surrogates to reference translations; (ii) to obtain a quantification of translation variability to either complement existing metric scores or (iii) replace references altogether.","offset":5,"pro":0.7142857142857143,"labels":"PUR"},{"idx":12,"sentence":"We show that for a number of popular evaluation metrics our variability estimates lead to substantial improvements in correlation with human judgements of quality by up 15%.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":13,"sentence":"We propose approaches to Quality Estimation (QE) for Machine Translation that explore both text and visual modalities for Multimodal QE.","offset":0,"pro":0,"labels":"PUR"},{"idx":13,"sentence":"We compare various multimodality integration and fusion strategies.","offset":1,"pro":0.3333333333333333,"labels":"MTD"},{"idx":13,"sentence":"For both sentence-level and document-level predictions, we show that state-of-the-art neural and feature-based QE frameworks obtain better results when using the additional modality.","offset":2,"pro":0.6666666666666666,"labels":"CLN"},{"idx":14,"sentence":"Deep neural models have repeatedly proved excellent at memorizing surface patterns from large datasets for various ML and NLP benchmarks.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"They struggle to achieve human-like thinking, however, because they lack the skill of iterative reasoning upon knowledge.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":14,"sentence":"To expose this problem in a new light, we introduce a challenge on learning from small data, PuzzLing Machines, which consists of Rosetta Stone puzzles from Linguistic Olympiads for high school students.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":14,"sentence":"These puzzles are carefully designed to contain only the minimal amount of parallel text necessary to deduce the form of unseen expressions.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":14,"sentence":"Solving them does not require external information (e.g., knowledge bases, visual signals) or linguistic expertise, but meta-linguistic awareness and deductive skills.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":14,"sentence":"Our challenge contains around 100 puzzles covering a wide range of linguistic phenomena from 81 languages.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":14,"sentence":"We show that both simple statistical algorithms and state-of-the-art deep neural models perform inadequately on this challenge, as expected.","offset":6,"pro":0.75,"labels":"CLN"},{"idx":14,"sentence":"We hope that this benchmark, available at https://ukplab.github.io/PuzzLing-Machines/, inspires further efforts towards a new paradigm in NLP-one that is grounded in human-like reasoning and understanding.","offset":7,"pro":0.875,"labels":"CTN"},{"idx":15,"sentence":"This paper presents a new challenging information extraction task in the domain of materials science.","offset":0,"pro":0,"labels":"PUR"},{"idx":15,"sentence":"We develop an annotation scheme for marking information on experiments related to solid oxide fuel cells in scientific publications, such as involved materials and measurement conditions.","offset":1,"pro":0.14285714285714285,"labels":"MTD"},{"idx":15,"sentence":"With this paper, we publish our annotation guidelines, as well as our SOFC-Exp corpus consisting of 45 open-access scholarly articles annotated by domain experts.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":15,"sentence":"A corpus and an inter-annotator agreement study demonstrate the complexity of the suggested named entity recognition and slot filling tasks as well as high annotation quality.","offset":3,"pro":0.42857142857142855,"labels":"CLN"},{"idx":15,"sentence":"We also present strong neural-network based models for a variety of tasks that can be addressed on the basis of our new data set.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":15,"sentence":"On all tasks, using BERT embeddings leads to large performance gains, but with increasing task complexity, adding a recurrent neural network on top seems beneficial.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":15,"sentence":"Our models will serve as competitive baselines in future work, and analysis of their performance highlights difficult cases when modeling the data and suggests promising research directions.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":16,"sentence":"We introduce TECHQA, a domain-adaptation question answering dataset for the technical support domain.","offset":0,"pro":0,"labels":"MTD"},{"idx":16,"sentence":"The TECHQA corpus highlights two real-world issues from the automated customer support domain.","offset":1,"pro":0.14285714285714285,"labels":"CTN"},{"idx":16,"sentence":"First, it contains actual questions posed by users on a technical forum, rather than questions generated specifically for a competition or a task.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":16,"sentence":"Second, it has a real-world size - 600 training, 310 dev, and 490 evaluation question/answer pairs - thus reflecting the cost of creating large labeled datasets with actual data.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":16,"sentence":"Hence, TECHQA is meant to stimulate research in domain adaptation rather than as a resource to build QA systems from scratch.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":16,"sentence":"TECHQA was obtained by crawling the IBMDeveloper and DeveloperWorks forums for questions with accepted answers provided in an IBM Technote-a technical document that addresses a specific technical issue.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":16,"sentence":"We also release a collection of the 801,998 Technotes available on the web as of April 4, 2019 as a companion resource that can be used to learn representations of the IT domain language.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":17,"sentence":"We consider the distinction between intended and perceived sarcasm in the context of textual sarcasm detection.","offset":0,"pro":0,"labels":"PUR"},{"idx":17,"sentence":"The former occurs when an utterance is sarcastic from the perspective of its author, while the latter occurs when the utterance is interpreted as sarcastic by the audience.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":17,"sentence":"We show the limitations of previous labelling methods in capturing intended sarcasm and introduce the iSarcasm dataset of tweets labeled for sarcasm directly by their authors.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":17,"sentence":"Examining the state-of-the-art sarcasm detection models on our dataset showed low performance compared to previously studied datasets, which indicates that these datasets might be biased or obvious and sarcasm could be a phenomenon under-studied computationally thus far.","offset":3,"pro":0.6,"labels":"RST"},{"idx":17,"sentence":"By providing the iSarcasm dataset, we aim to encourage future NLP research to develop methods for detecting sarcasm in text as intended by the authors of the text, not as labeled under assumptions that we demonstrate to be sub-optimal.","offset":4,"pro":0.8,"labels":"CTN"},{"idx":18,"sentence":"We propose a new end-to-end model that treats AMR parsing as a series of dual decisions on the input sequence and the incrementally constructed graph.","offset":0,"pro":0,"labels":"MTD"},{"idx":18,"sentence":"At each time step, our model performs multiple rounds of attention, reasoning, and composition that aim to answer two critical questions: (1) which part of the input sequence to abstract; and (2) where in the output graph to construct the new concept.","offset":1,"pro":0.14285714285714285,"labels":"MTD"},{"idx":18,"sentence":"We show that the answers to these two questions are mutually causalities.","offset":2,"pro":0.2857142857142857,"labels":"RST"},{"idx":18,"sentence":"We design a model based on iterative inference that helps achieve better answers in both perspectives, leading to greatly improved parsing accuracy.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":18,"sentence":"Our experimental results significantly outperform all previously reported Smatch scores by large margins.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":18,"sentence":"Remarkably, without the help of any large-scale pre-trained language model (e.g., BERT), our model already surpasses previous state-of-the-art using BERT.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":18,"sentence":"With the help of BERT, we can push the state-of-the-art results to 80.2% on LDC2017T10 (AMR 2.0) and 75.4% on LDC2014T12 (AMR 1.0).","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":19,"sentence":"Multi-document summarization (MDS) aims to compress the content in large document collections into short summaries and has important applications in story clustering for newsfeeds, presentation of search results, and timeline generation.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"However, there is a lack of datasets that realistically address such use cases at a scale large enough for training supervised models for this task.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":19,"sentence":"This work presents a new dataset for MDS that is large both in the total number of document clusters and in the size of individual clusters.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":19,"sentence":"We build this dataset by leveraging the Wikipedia Current Events Portal (WCEP), which provides concise and neutral human-written summaries of news events, with links to external source articles.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":19,"sentence":"We also automatically extend these source articles by looking for related articles in the Common Crawl archive.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":19,"sentence":"We provide a quantitative analysis of the dataset and empirical results for several state-of-the-art MDS techniques.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":0,"sentence":"Cross-lingual summarization aims at summarizing a document in one language (e.g., Chinese) into another language (e.g., English).","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"In this paper, we propose a novel method inspired by the translation pattern in the process of obtaining a cross-lingual summary.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":0,"sentence":"We first attend to some words in the source text, then translate them into the target language, and summarize to get the final summary.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":0,"sentence":"Specifically, we first employ the encoder-decoder attention distribution to attend to the source words.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":0,"sentence":"Second, we present three strategies to acquire the translation probability, which helps obtain the translation candidates for each source word.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":0,"sentence":"Finally, each summary word is generated either from the neural distribution or from the translation candidates of source words.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":0,"sentence":"Experimental results on Chinese-to-English and English-to-Chinese summarization tasks have shown that our proposed method can significantly outperform the baselines, achieving comparable performance with the state-of-the-art.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":1,"sentence":"Previous work on automatic news timeline summarization (TLS) leaves an unclear picture about how this task can generally be approached and how well it is currently solved.","offset":0,"pro":0,"labels":"GAP"},{"idx":1,"sentence":"This is mostly due to the focus on individual subtasks, such as date selection and date summarization, and to the previous lack of appropriate evaluation metrics for the full TLS task.","offset":1,"pro":0.25,"labels":"GAP"},{"idx":1,"sentence":"For a more robust evaluation, we also present a new TLS dataset, which is larger and spans longer time periods than previous datasets. ","offset":2,"pro":0.5,"labels":"PUR"},{"idx":1,"sentence":"In this paper, we compare different TLS strategies using appropriate evaluation frameworks, ","offset":3,"pro":0.75,"labels":"MTD"},{"idx":2,"sentence":"Most studies on abstractive summarization report ROUGE scores between system and reference summaries.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"However, we have a concern about the truthfulness of generated summaries: whether all facts of a generated summary are mentioned in the source text.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":2,"sentence":"This paper explores improving the truthfulness in headline generation on two popular datasets.","offset":2,"pro":0.2222222222222222,"labels":"PUR"},{"idx":2,"sentence":"After confirming quite a few untruthful instances in the datasets, this study hypothesizes that removing untruthful instances from the supervision data may remedy the problem of the untruthful behaviors of the model.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":2,"sentence":"Building a binary classifier that predicts an entailment relation between an article and its headline, we filter out untruthful instances from the supervision data.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":2,"sentence":"Experimental results demonstrate that the headline generation model trained on filtered supervision data shows no clear difference in ROUGE scores but remarkable improvements in automatic and manual evaluations of the generated headlines.","offset":5,"pro":0.5555555555555556,"labels":"RST"},{"idx":2,"sentence":"Analyzing headlines generated by the state-of-the-art encoder-decoder model, we show that the model sometimes generates untruthful headlines. ","offset":6,"pro":0.6666666666666666,"labels":"RST"},{"idx":2,"sentence":"In order to quantify the truthfulness of article-headline pairs, we consider the textual entailment of whether an article entails its headline. ","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":2,"sentence":"We conjecture that one of the reasons lies in untruthful supervision data used for training the model. ","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":3,"sentence":"We propose SUPERT, which rates the quality of a summary by measuring its semantic similarity with a pseudo reference summary, i.e. selected salient sentences from the source documents, using contextualized embeddings and soft token alignment techniques.","offset":0,"pro":0,"labels":"MTD"},{"idx":3,"sentence":"Compared to the state-of-the-art unsupervised evaluation metrics, SUPERT correlates better with human ratings by 18- 39%.","offset":1,"pro":0.2,"labels":"RST"},{"idx":3,"sentence":"All source code is available at https://github.com/yg211/acl20-ref-free-eval.","offset":2,"pro":0.4,"labels":"CTN"},{"idx":3,"sentence":"We study unsupervised multi-document summarization evaluation metrics, which require neither human-written reference summaries nor human annotations (e.g. preferences, ratings, etc.).","offset":3,"pro":0.6,"labels":"PUR"},{"idx":3,"sentence":"Furthermore, we use SUPERT as rewards to guide a neural-based reinforcement learning summarizer, yielding favorable performance compared to the state-of-the-art unsupervised summarizers. ","offset":4,"pro":0.8,"labels":"RST"},{"idx":4,"sentence":"Copy module has been widely equipped in the recent abstractive summarization models, which facilitates the decoder to extract words from the source into the summary.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"Generally, the encoder-decoder attention is served as the copy distribution, while how to guarantee that important words in the source are copied remains a challenge.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":4,"sentence":"In this work, we propose a Transformer-based model to enhance the copy mechanism.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":4,"sentence":"Specifically, we identify the importance of each source word based on the degree centrality with a directed graph built by the self-attention layer in the Transformer.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":4,"sentence":"We use the centrality of each source word to guide the copy process explicitly.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":4,"sentence":"Experimental results show that the self-attention graph provides useful guidance for the copy distribution.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":4,"sentence":"Our proposed models significantly outperform the baseline methods on the CNN/Daily Mail dataset and the Gigaword dataset.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":5,"sentence":"Open Domain dialog system evaluation is one of the most important challenges in dialog research.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"Existing automatic evaluation metrics, such as BLEU are mostly reference-based.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":5,"sentence":"They calculate the difference between the generated response and a limited number of available references.","offset":2,"pro":0.2222222222222222,"labels":"BAC"},{"idx":5,"sentence":"Likert-score based self-reported user rating is widely adopted by social conversational systems, such as Amazon Alexa Prize chatbots.","offset":3,"pro":0.3333333333333333,"labels":"BAC"},{"idx":5,"sentence":"However, self-reported user rating suffers from bias and variance among different users.","offset":4,"pro":0.4444444444444444,"labels":"GAP"},{"idx":5,"sentence":"To alleviate this problem, we formulate dialog evaluation as a comparison task.","offset":5,"pro":0.5555555555555556,"labels":"PUR"},{"idx":5,"sentence":"We also propose an automatic evaluation model CMADE (Comparison Model for Automatic Dialog Evaluation) that automatically cleans self-reported user ratings as it trains on them.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":5,"sentence":"Specifically, we first use a self-supervised method to learn better dialog feature representation, and then use KNN and Shapley to remove confusing samples.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":5,"sentence":"Our experiments show that CMADE achieves 89.2% accuracy in the dialog comparison task.","offset":8,"pro":0.8888888888888888,"labels":"RST"},{"idx":6,"sentence":"Human conversations contain many types of information, e.g., knowledge, common sense, and language habits.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"In this paper, we propose a conversational word embedding method named PR-Embedding, which utilizes the conversation pairs lesspost, replygreater to learn word embedding.","offset":1,"pro":0.25,"labels":"PUR"},{"idx":6,"sentence":"Different from previous works, PR-Embedding uses the vectors from two different semantic spaces to represent the words in post and reply.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":6,"sentence":"We evaluate the method on single-turn and multi-turn response selection tasks for retrieval-based dialog systems.","offset":3,"pro":0.75,"labels":"MTD"},{"idx":7,"sentence":"In this paper, we explore the slot tagging with only a few labeled support sentences (a.k.a. few-shot).","offset":0,"pro":0,"labels":"PUR"},{"idx":7,"sentence":"Few-shot slot tagging faces a unique challenge compared to the other fewshot classification problems as it calls for modeling the dependencies between labels.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":7,"sentence":"But it is hard to apply previously learned label dependencies to an unseen domain, due to the discrepancy of label sets.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":7,"sentence":"In the few-shot setting, the emission score of CRF can be calculated as a word's similarity to the representation of each label.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":7,"sentence":"To calculate such similarity, we propose a Label-enhanced Task-Adaptive Projection Network (L-TapNet) based on the state-of-the-art few-shot classification model - TapNet, by leveraging label name semantics in representing labels.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":7,"sentence":"Experimental results show that our model significantly outperforms the strongest few-shot learning baseline by 14.64 F1 scores in the one-shot setting.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":7,"sentence":"To tackle this, we introduce a collapsed dependency transfer mechanism into the conditional random field (CRF) to transfer abstract label dependency patterns as transition scores. ","offset":6,"pro":0.8571428571428571,"labels":"MTD"},{"idx":8,"sentence":"Deep reinforcement learning is a promising approach to training a dialog manager, but current methods struggle with the large state and action spaces of multi-domain dialog systems.","offset":0,"pro":0,"labels":"GAP"},{"idx":8,"sentence":"We make progressively fewer assumptions about the data needed, using labeled, reduced-labeled, and even unlabeled data to train expert demonstrators.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":8,"sentence":"We introduce Reinforced Fine-tune Learning, an extension to DQfD, enabling us to overcome the domain gap between the datasets and the environment.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":8,"sentence":"Experiments in a challenging multi-domain dialog system framework validate our approaches, and get high success rates even when trained on out-of-domain data.","offset":3,"pro":0.6,"labels":"RST"},{"idx":8,"sentence":"Building upon Deep Q-learning from Demonstrations (DQfD), an algorithm that scores highly in difficult Atari games, we leverage dialog data to guide the agent to successfully respond to a user's requests. ","offset":4,"pro":0.8,"labels":"PUR"},{"idx":9,"sentence":"Non-task oriented dialogue systems have achieved great success in recent years due to largely accessible conversation data and the development of deep learning techniques.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"Given a context, current systems are able to yield a relevant and fluent response, but sometimes make logical mistakes because of weak reasoning capabilities.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":9,"sentence":"To facilitate the conversation reasoning research, we introduce MuTual, a novel dataset for Multi-Turn dialogue Reasoning, consisting of 8,860 manually annotated dialogues based on Chinese student English listening comprehension exams.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":9,"sentence":"Empirical results show that state-of-the-art methods only reach 71%, which is far behind human performance of 94%, indicating that there is ample room for improving reasoning ability.","offset":3,"pro":0.6,"labels":"RST"},{"idx":9,"sentence":"Compared to previous benchmarks for non-task oriented dialogue systems, MuTual is much more challenging since it requires a model that be able to handle various reasoning problems. ","offset":4,"pro":0.8,"labels":"CLN"},{"idx":10,"sentence":"Despite the continuing efforts to improve the engagingness and consistency of chit-chat dialogue systems, the majority of current work simply focus on mimicking human-like responses, leaving understudied the aspects of modeling understanding between interlocutors.","offset":0,"pro":0,"labels":"GAP"},{"idx":10,"sentence":"The research in cognitive science, instead, suggests that understanding is an essential signal for a high-quality chit-chat conversation.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":10,"sentence":"Motivated by this, we propose P{^{2 Bot, a transmitter-receiver based framework with the aim of explicitly modeling understanding.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":10,"sentence":"Specifically, P{^{2 Bot incorporates mutual persona perception to enhance the quality of personalized dialogue generation.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":10,"sentence":"Experiments on a large public dataset, Persona-Chat, demonstrate the effectiveness of our approach, with a considerable boost over the state-of-the-art baselines across both automatic metrics and human evaluations.","offset":4,"pro":0.8,"labels":"RST"},{"idx":11,"sentence":"Most previous studies on bridging anaphora resolution (Poesio et al., 2004; Hou et al., 2013b; Hou, 2018a) use the pairwise model to tackle the problem and assume that the gold mention information is given.","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"In this paper, we cast bridging anaphora resolution as question answering based on context.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":11,"sentence":"We present a question answering framework (BARQA) for this task, which leverages the power of transfer learning.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":11,"sentence":"Furthermore, we propose a novel method to generate a large amount of quasi-bridging training data.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":11,"sentence":"We show that our model pre-trained on this dataset and fine-tuned on a small amount of in-domain dataset achieves new state-of-the-art results for bridging anaphora resolution on two bridging corpora (ISNotes (Markert et al., 2012) and BASHI (Ro ̈siger, 2018)).","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":11,"sentence":"This allows us to find the antecedent for a given anaphor without knowing any gold mention information (except the anaphor itself). ","offset":5,"pro":0.8333333333333334,"labels":"CTN"},{"idx":12,"sentence":"Recent dialogue coherence models use the coherence features designed for monologue texts, e.g. nominal entities, to represent utterances and then explicitly augment them with dialogue-relevant features, e.g., dialogue act labels.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"It indicates two drawbacks, (a) semantics of utterances are limited to entity mentions, and (b) the performance of coherence models strongly relies on the quality of the input dialogue act labels.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":12,"sentence":"We address these issues by introducing a novel approach to dialogue coherence assessment.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":12,"sentence":"We use dialogue act prediction as an auxiliary task in a multi-task learning scenario to obtain informative utterance representations for coherence assessment.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":12,"sentence":"The results of our experiments show that our model substantially (more than 20 accuracy points) outperforms its strong competitors on the DailyDialogue corpus, and performs on par with them on the SwitchBoard corpus for ranking dialogues concerning their coherence.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":12,"sentence":"We release our source code.","offset":5,"pro":0.7142857142857143,"labels":"CTN"},{"idx":12,"sentence":"Our approach alleviates the need for explicit dialogue act labels during evaluation.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":13,"sentence":"We formulate the task as a Traveling Salesman Problem (TSP), and use a biaffine attention model to calculate the edge costs.","offset":0,"pro":0,"labels":"MTD"},{"idx":13,"sentence":"We facilitate the decoding by solving the TSP for each subtree and combining the solution into a projective tree.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":13,"sentence":"We then design a transition system as post-processing, inspired by non-projective transition-based parsing, to obtain non-projective sentences.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":13,"sentence":"Our proposed method outperforms the state-of-the-art linearizer while being 10 times faster in training and decoding.","offset":3,"pro":0.6,"labels":"RST"},{"idx":13,"sentence":"We propose a graph-based method to tackle the dependency tree linearization task. ","offset":4,"pro":0.8,"labels":"PUR"},{"idx":14,"sentence":"This paper proposes the problem of Deep Question Generation (DQG), which aims to generate complex questions that require reasoning over multiple pieces of information about the input passage.","offset":0,"pro":0,"labels":"PUR"},{"idx":14,"sentence":"In order to capture the global structure of the document and facilitate reasoning, we propose a novel framework that first constructs a semantic-level graph for the input document and then encodes the semantic graph by introducing an attention-based GGNN (Att-GGNN).","offset":1,"pro":0.2,"labels":"MTD"},{"idx":14,"sentence":"Afterward, we fuse the document-level and graph-level representations to perform joint training of content selection and question decoding.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":14,"sentence":"On the HotpotQA deep-question centric dataset, our model greatly improves performance over questions requiring reasoning over multiple facts, leading to state-of-the-art performance.","offset":3,"pro":0.6,"labels":"RST"},{"idx":14,"sentence":"The code is publicly available at https://github.com/WING-NUS/SG-Deep-Question-Generation.","offset":4,"pro":0.8,"labels":"CTN"},{"idx":15,"sentence":"Extracting relational triples from unstructured text is crucial for large-scale knowledge graph construction.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"However, few existing works excel in solving the overlapping triple problem where multiple relational triples in the same sentence share the same entities.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":15,"sentence":"In this work, we introduce a fresh perspective to revisit the relational triple extraction task and propose a novel cascade binary tagging framework (CasRel) derived from a principled problem formulation.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":15,"sentence":"Instead of treating relations as discrete labels as in previous works, our new framework models relations as functions that map subjects to objects in a sentence, which naturally handles the overlapping problem.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":15,"sentence":"Experiments show that the CasRel framework already outperforms state-of-the-art methods even when its encoder module uses a randomly initialized BERT encoder, showing the power of the new tagging framework.","offset":4,"pro":0.5,"labels":"RST"},{"idx":15,"sentence":"It enjoys further performance boost when employing a pre-trained BERT encoder, outperforming the strongest baseline by 17.5 and 30.2 absolute gain in F1-score on two public datasets NYT and WebNLG, respectively.","offset":5,"pro":0.625,"labels":"RST"},{"idx":15,"sentence":"The source code and data are released online.","offset":6,"pro":0.75,"labels":"CTN"},{"idx":15,"sentence":"In-depth analysis on different scenarios of overlapping triples shows that the method delivers consistent performance gain across all these scenarios.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":16,"sentence":"Information Extraction (IE) from scientific texts can be used to guide readers to the central information in scientific documents.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"But narrow IE systems extract only a fraction of the information captured, and Open IE systems do not perform well on the long and complex sentences encountered in scientific texts.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":16,"sentence":"In this work we combine the output of both types of systems to achieve Semi-Open Relation Extraction, a new task that we explore in the Biology domain.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":16,"sentence":"First, we present the Focused Open Biological Information Extraction (FOBIE) dataset and use FOBIE to train a state-of-the-art narrow scientific IE system to extract trade-off relations and arguments that are central to biology texts.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":16,"sentence":"We then run both the narrow IE system and a state-of-the-art Open IE system on a corpus of 10K open-access scientific biological texts.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":16,"sentence":"We show that a significant amount (65%) of erroneous and uninformative Open IE extractions can be filtered using narrow IE extractions.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":16,"sentence":"Furthermore, we show that the retained extractions are significantly more often informative to a reader.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":17,"sentence":"Sequence labeling systems should perform reliably not only under ideal conditions but also with corrupted inputs-as these systems often process user-generated text or follow an error-prone upstream component.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"To this end, we formulate the noisy sequence labeling problem, where the input may undergo an unknown noising process and propose two Noise-Aware Training (NAT) objectives that improve robustness of sequence labeling performed on perturbed input: Our data augmentation method trains a neural model using a mixture of clean and noisy samples, whereas our stability training algorithm encourages the model to create a noise-invariant latent representation.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":17,"sentence":"We employ a vanilla noise model at training time.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":17,"sentence":"For evaluation, we use both the original data and its variants perturbed with real OCR errors and misspellings.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":17,"sentence":"Extensive experiments on English and German named entity recognition benchmarks confirmed that NAT consistently improved robustness of popular sequence labeling models, preserving accuracy on the original input.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":17,"sentence":"We make our code and data publicly available for the research community.","offset":5,"pro":0.8333333333333334,"labels":"CTN"},{"idx":18,"sentence":"Named Entity Recognition (NER) performance often degrades rapidly when applied to target domains that differ from the texts observed during training.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"When in-domain labelled data is available, transfer learning techniques can be used to adapt existing NER models to the target domain.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":18,"sentence":"But what should one do when there is no hand-labelled data for the target domain?","offset":2,"pro":0.25,"labels":"GAP"},{"idx":18,"sentence":"This paper presents a simple but powerful approach to learn NER models in the absence of labelled data through weak supervision.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":18,"sentence":"The approach relies on a broad spectrum of labelling functions to automatically annotate texts from the target domain.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":18,"sentence":"These annotations are then merged together using a hidden Markov model which captures the varying accuracies and confusions of the labelling functions.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":18,"sentence":"A sequence labelling model can finally be trained on the basis of this unified annotation.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":18,"sentence":"We evaluate the approach on two English datasets (CoNLL 2003 and news articles from Reuters and Bloomberg) and demonstrate an improvement of about 7 percentage points in entity-level F1 scores compared to an out-of-domain neural NER model.","offset":7,"pro":0.875,"labels":"RST"},{"idx":19,"sentence":"Common methods encode the source sentence, conditioned on the entity mentions, before classifying the relation.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"However, the complexity of the task makes it difficult to understand how encoder architecture and supporting linguistic knowledge affect the features learned by the encoder.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":19,"sentence":"We find that the bias induced by the architecture and the inclusion of linguistic features are clearly expressed in the probing task performance.","offset":2,"pro":0.25,"labels":"RST"},{"idx":19,"sentence":"For example, adding contextualized word representations greatly increases performance on probing tasks with a focus on named entity and part-of-speech information, and yields better results in RE.","offset":3,"pro":0.375,"labels":"RST"},{"idx":19,"sentence":"In contrast, entity masking improves RE, but considerably lowers performance on entity type related probing tasks.","offset":4,"pro":0.5,"labels":"RST"},{"idx":19,"sentence":"Despite the recent progress, little is known about the features captured by state-of-the-art neural relation extraction (RE) models.","offset":5,"pro":0.625,"labels":"BAC"},{"idx":19,"sentence":"We introduce 14 probing tasks targeting linguistic properties relevant to RE,","offset":6,"pro":0.75,"labels":"MTD"},{"idx":19,"sentence":"and we use them to study representations learned by more than 40 different encoder architecture and linguistic feature combinations trained on two datasets, TACRED and SemEval 2010 Task 8. ","offset":7,"pro":0.875,"labels":"PUR"},{"idx":0,"sentence":"Document-level relation extraction requires integrating information within and across multiple sentences of a document and capturing complex interactions between inter-sentence entities.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"However, effective aggregation of relevant information in the document remains a challenging research question.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":0,"sentence":"Existing approaches construct static document-level graphs based on syntactic trees, co-references or heuristics from the unstructured text to model the dependencies.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":0,"sentence":"Unlike previous methods that may not be able to capture rich non-local interactions for inference, we propose a novel model that empowers the relational reasoning across sentences by automatically inducing the latent document-level graph.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":0,"sentence":"We further develop a refinement strategy, which enables the model to incrementally aggregate relevant information for multi-hop reasoning.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":0,"sentence":"Specifically, our model achieves an F1 score of 59.05 on a large-scale document-level dataset (DocRED), significantly improving over the previous results, and also yields new state-of-the-art results on the CDR and GDA dataset.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":0,"sentence":"Furthermore, extensive analyses show that the model is able to discover more accurate inter-sentence relations.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":1,"sentence":"TACRED is one of the largest, most widely used crowdsourced datasets in Relation Extraction (RE).","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"But, even with recent advances in unsupervised pre-training and knowledge enhanced neural RE, models still show a high error rate.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":1,"sentence":"In this paper, we investigate the questions: Have we reached a performance ceiling or is there still room for improvement?","offset":2,"pro":0.2222222222222222,"labels":"PUR"},{"idx":1,"sentence":"And how do crowd annotations, dataset, and models contribute to this error rate?","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":1,"sentence":"To answer these questions, we first validate the most challenging 5K examples in the development and test sets using trained annotators.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":1,"sentence":"We find that label errors account for 8% absolute F1 test error, and that more than 50% of the examples need to be relabeled.","offset":5,"pro":0.5555555555555556,"labels":"RST"},{"idx":1,"sentence":"On the relabeled test set the average F1 score of a large baseline model set improves from 62.1 to 70.1.","offset":6,"pro":0.6666666666666666,"labels":"RST"},{"idx":1,"sentence":"After validation, we analyze misclassifications on the challenging instances, categorize them into linguistically motivated error groups, and verify the resulting error hypotheses on three state-of-the-art RE models.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":1,"sentence":"We show that two groups of ambiguous relations are responsible for most of the remaining errors and that models may adopt shallow heuristics on the dataset when entities are not masked.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":2,"sentence":"In this paper, we propose a new task of machine translation (MT), which is based on no parallel sentences but can refer to a ground-truth bilingual dictionary.","offset":0,"pro":0,"labels":"PUR"},{"idx":2,"sentence":"Motivated by the ability of a monolingual speaker learning to translate via looking up the bilingual dictionary, we propose the task to see how much potential an MT system can attain using the bilingual dictionary and large scale monolingual corpora, while is independent on parallel sentences.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":2,"sentence":"We propose anchored training (AT) to tackle the task.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":2,"sentence":"AT uses the bilingual dictionary to establish anchoring points for closing the gap between source language and target language.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":2,"sentence":"Experiments on various language pairs show that our approaches are significantly better than various baselines, including dictionary-based word-by-word translation, dictionary-supervised cross-lingual word embedding transformation, and unsupervised MT.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":2,"sentence":"On distant language pairs that are hard for unsupervised MT to perform well, AT performs remarkably better, achieving performances comparable to supervised SMT trained on more than 4M parallel sentences.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":3,"sentence":"This paper explores data augmentation methods for training Neural Machine Translation to make use of similar translations, in a comparable way a human translator employs fuzzy matches.","offset":0,"pro":0,"labels":"PUR"},{"idx":3,"sentence":"In particular, we show how we can simply present the neural model with information of both source and target sides of the fuzzy matches, we also extend the similarity to include semantically related translations retrieved using sentence distributed representations.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":3,"sentence":"We show that translations based on fuzzy matching provide the model with copy information while translations based on embedding similarities tend to extend the translation context.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":3,"sentence":"Results indicate that the effect from both similar sentences are adding up to further boost accuracy, combine naturally with model fine-tuning and are providing dynamic adaptation for unseen translation pairs.","offset":3,"pro":0.5,"labels":"CLN"},{"idx":3,"sentence":"Tests on multiple data sets and domains show consistent accuracy improvements.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":3,"sentence":"To foster research around these techniques, we also release an Open-Source toolkit with efficient and flexible fuzzy-match implementation.","offset":5,"pro":0.8333333333333334,"labels":"CTN"},{"idx":4,"sentence":"We explore the suitability of self-attention models for character-level neural machine translation.","offset":0,"pro":0,"labels":"PUR"},{"idx":4,"sentence":"We test the standard transformer model, as well as a novel variant in which the encoder block combines information from nearby characters using convolutions.","offset":1,"pro":0.25,"labels":"MTD"},{"idx":4,"sentence":"We perform extensive experiments on WMT and UN datasets, testing both bilingual and multilingual translation to English using up to three input languages (French, Spanish, and Chinese).","offset":2,"pro":0.5,"labels":"MTD"},{"idx":4,"sentence":"Our transformer variant consistently outperforms the standard transformer at the character-level and converges faster while learning more robust character-level alignments.","offset":3,"pro":0.75,"labels":"RST"},{"idx":5,"sentence":"Word alignment was once a core unsupervised learning task in natural language processing because of its essential role in training statistical machine translation (MT) models.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"Although unnecessary for training neural MT models, word alignment still plays an important role in interactive applications of neural machine translation, such as annotation transfer and lexicon injection.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":5,"sentence":"While statistical MT methods have been replaced by neural approaches with superior performance, the twenty-year-old GIZA++ toolkit remains a key component of state-of-the-art word alignment systems.","offset":2,"pro":0.3333333333333333,"labels":"BAC"},{"idx":5,"sentence":"Prior work on neural word alignment has only been able to outperform GIZA++ by using its output during training.","offset":3,"pro":0.5,"labels":"GAP"},{"idx":5,"sentence":"We present the first end-to-end neural word alignment method that consistently outperforms GIZA++ on three data sets.","offset":4,"pro":0.6666666666666666,"labels":"PUR"},{"idx":5,"sentence":"Our approach repurposes a Transformer model trained for supervised translation to also serve as an unsupervised word alignment model in a manner that is tightly integrated and does not affect translation quality.","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":6,"sentence":"Most neural machine translation models only rely on pairs of parallel sentences, assuming syntactic information is automatically learned by an attention mechanism.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"In this work, we investigate different approaches to incorporate syntactic knowledge in the Transformer model and also propose a novel, parameter-free, dependency-aware self-attention mechanism that improves its translation quality, especially for long sentences and in low-resource scenarios.","offset":1,"pro":0.3333333333333333,"labels":"PUR"},{"idx":6,"sentence":"We show the efficacy of each approach on WMT English-German and English-Turkish, and WAT English-Japanese translation tasks.","offset":2,"pro":0.6666666666666666,"labels":"RST"},{"idx":7,"sentence":"Massively multilingual models for neural machine translation (NMT) are theoretically attractive, but often underperform bilingual models and deliver poor zero-shot translations.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"In this paper, we explore ways to improve them.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":7,"sentence":"We argue that multilingual NMT requires stronger modeling capacity to support language pairs with varying typological characteristics, and overcome this bottleneck via language-specific components and deepening NMT architectures.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":7,"sentence":"We identify the off-target translation issue (i.e. translating into a wrong target language) as the major source of the inferior zero-shot performance, and propose random online backtranslation to enforce the translation of unseen training language pairs.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":7,"sentence":"Experiments on OPUS-100 (a novel multilingual dataset with 100 languages) show that our approach substantially narrows the performance gap with bilingual models in both one-to-many and many-to-many settings, and improves zero-shot performance by asciitilde10 BLEU, approaching conventional pivot-based methods.","offset":4,"pro":0.8,"labels":"RST"},{"idx":8,"sentence":"The performance of neural machine translation systems is commonly evaluated in terms of BLEU.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"However, due to its reliance on target language properties and generation, the BLEU metric does not allow an assessment of which translation directions are more difficult to model.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":8,"sentence":"In this paper, we propose cross-mutual information (XMI): an asymmetric information-theoretic metric of machine translation difficulty that exploits the probabilistic nature of most neural machine translation models.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":8,"sentence":"XMI allows us to better evaluate the difficulty of translating text into the target language while controlling for the difficulty of the target-side generation component independent of the translation task.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":8,"sentence":"We then present the first systematic and controlled study of cross-lingual translation difficulties using modern neural translation systems.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":8,"sentence":"Code for replicating our experiments is available online at https://github.com/e-bug/nmt-difficulty.","offset":5,"pro":0.8333333333333334,"labels":"CTN"},{"idx":9,"sentence":"Multilingual neural machine translation (NMT) has led to impressive accuracy improvements in low-resource scenarios by sharing common linguistic information across languages.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"However, the traditional multilingual model fails to capture the diversity and specificity of different languages, resulting in inferior performance compared with individual models that are sufficiently trained.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":9,"sentence":"In this paper, we incorporate a language-aware interlingua into the Encoder-Decoder architecture.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":9,"sentence":"The interlingual network enables the model to learn a language-independent representation from the semantic spaces of different languages, while still allowing for language-specific specialization of a particular language-pair.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":9,"sentence":"Experiments show that our proposed method achieves remarkable improvements over state-of-the-art multilingual NMT baselines and produces comparable performance with strong individual models.","offset":4,"pro":0.8,"labels":"RST"},{"idx":10,"sentence":"Evaluation of cross-lingual encoders is usually performed either via zero-shot cross-lingual transfer in supervised downstream tasks or via unsupervised cross-lingual textual similarity.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"In this paper, we concern ourselves with reference-free machine translation (MT) evaluation where we directly compare source texts to (sometimes low-quality) system translations, which represents a natural adversarial setup for multilingual encoders.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":10,"sentence":"Reference-free evaluation holds the promise of web-scale comparison of MT systems.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":10,"sentence":"We systematically investigate a range of metrics based on state-of-the-art cross-lingual semantic representations obtained with pretrained M-BERT and LASER.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":10,"sentence":"We find that they perform poorly as semantic encoders for reference-free MT evaluation and identify their two key limitations, namely, (a) a semantic mismatch between representations of mutual translations and, more prominently, (b) the inability to punish translationese, i.e., low-quality literal translations.","offset":4,"pro":0.5714285714285714,"labels":"GAP"},{"idx":10,"sentence":"We propose two partial remedies: (1) post-hoc re-alignment of the vector spaces and (2) coupling of semantic-similarity based metrics with target-side language modeling.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":10,"sentence":"In segment-level MT evaluation, our best metric surpasses reference-based BLEU by 5.7 correlation points.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":11,"sentence":"We present a novel method to extract parallel sentences from two monolingual corpora, using neural machine translation.","offset":0,"pro":0,"labels":"PUR"},{"idx":11,"sentence":"Our method relies on translating sentences in one corpus, but constraining the decoding by a prefix tree built on the other corpus.","offset":1,"pro":0.25,"labels":"MTD"},{"idx":11,"sentence":"We argue that a neural machine translation system by itself can be a sentence similarity scorer and it efficiently approximates pairwise comparison with a modified beam search.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":11,"sentence":"When benchmarked on the BUCC shared task, our method achieves results comparable to other submissions.","offset":3,"pro":0.75,"labels":"RST"},{"idx":12,"sentence":"Position encoding (PE), an essential part of self-attention networks (SANs), is used to preserve the word order information for natural language processing tasks, generating fixed position indices for input sequences.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"However, in cross-lingual scenarios, machine translation, the PEs of source and target sentences are modeled independently.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":12,"sentence":"Due to word order divergences in different languages, modeling the cross-lingual positional relationships might help SANs tackle this problem.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":12,"sentence":"In this paper, we augment SANs with cross-lingual position representations to model the bilingually aware latent structure for the input sentence.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":12,"sentence":"Specifically, we utilize bracketing transduction grammar (BTG)-based reordering information to encourage SANs to learn bilingual diagonal alignments.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":12,"sentence":"Experimental results on WMT'14 EnglishRightarrowGerman, WAT'17 JapaneseRightarrowEnglish, and WMT'17 ChineseLeftrightarrowEnglish translation tasks demonstrate that our approach significantly and consistently improves translation quality over strong baselines.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":12,"sentence":"Extensive analyses confirm that the performance gains come from the cross-lingual information.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":13,"sentence":"The main goal of machine translation has been to convey the correct content.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"Stylistic considerations have been at best secondary.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":13,"sentence":"We show that as a consequence, the output of three commercial machine translation systems (Bing, DeepL, Google) make demographically diverse samples from five languages sound older and more male than the original.","offset":2,"pro":0.4,"labels":"RST"},{"idx":13,"sentence":"Our findings suggest that translation models reflect demographic bias in the training data.","offset":3,"pro":0.6,"labels":"CLN"},{"idx":13,"sentence":"This opens up interesting new research avenues in machine translation to take stylistic considerations into account.","offset":4,"pro":0.8,"labels":"IMP"},{"idx":14,"sentence":"Current advances in machine translation (MT) increase the need for translators to switch from traditional translation to post-editing (PE) of machine-translated text, a process that saves time and reduces errors.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"This affects the design of translation interfaces, as the task changes from mainly generating text to correcting errors within otherwise helpful translation proposals.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":14,"sentence":"Since this paradigm shift offers potential for modalities other than mouse and keyboard, we present MMPE, the first prototype to combine traditional input modes with pen, touch, and speech modalities for PE of MT.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":14,"sentence":"The results of an evaluation with professional translators suggest that pen and touch interaction are suitable for deletion and reordering tasks, while they are of limited use for longer insertions.","offset":3,"pro":0.5,"labels":"RST"},{"idx":14,"sentence":"On the other hand, speech and multi-modal combinations of select {& speech are considered suitable for replacements and insertions but offer less potential for deletion and reordering.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":14,"sentence":"Overall, participants were enthusiastic about the new modalities and saw them as good extensions to mouse {& keyboard, but not as a complete substitute.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":15,"sentence":"We use the multilingual OSCAR corpus, extracted from Common Crawl via language classification, filtering and cleaning, to train monolingual contextualized word embeddings (ELMo) for five mid-resource languages.","offset":0,"pro":0,"labels":"MTD"},{"idx":15,"sentence":"We then compare the performance of OSCAR-based and Wikipedia-based ELMo embeddings for these languages on the part-of-speech tagging and parsing tasks.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":15,"sentence":"We show that, despite the noise in the Common-Crawl-based OSCAR data, embeddings trained on OSCAR perform much better than monolingual embeddings trained on Wikipedia.","offset":2,"pro":0.4,"labels":"RST"},{"idx":15,"sentence":"They actually equal or improve the current state of the art in tagging and parsing for all five languages.","offset":3,"pro":0.6,"labels":"RST"},{"idx":15,"sentence":"In particular, they also improve over multilingual Wikipedia-based contextual embeddings (multilingual BERT), which almost always constitutes the previous state of the art, thereby showing that the benefit of a larger, more diverse corpus surpasses the cross-lingual benefit of multilingual embedding architectures.","offset":4,"pro":0.8,"labels":"RST"},{"idx":16,"sentence":"We present a new challenging stance detection dataset, called Will-They-Won't-They (WT--WT), which contains 51,284 tweets in English, making it by far the largest available dataset of the type.","offset":0,"pro":0,"labels":"PUR"},{"idx":16,"sentence":"Our experiments with a wide range of recent state-of-the-art stance detection systems show that the dataset poses a strong challenge to existing models in this domain.","offset":1,"pro":0.25,"labels":"RST"},{"idx":16,"sentence":"All the annotations are carried out by experts","offset":2,"pro":0.5,"labels":"MTD"},{"idx":16,"sentence":"therefore, the dataset constitutes a high-quality and reliable benchmark for future research in stance detection.","offset":3,"pro":0.75,"labels":"CLN"},{"idx":17,"sentence":"While state-of-the-art neural network models continue to achieve lower perplexity scores on language modeling benchmarks, it remains unknown whether optimizing for broad-coverage predictive performance leads to human-like syntactic knowledge.","offset":0,"pro":0,"labels":"GAP"},{"idx":17,"sentence":"Furthermore, existing work has not provided a clear picture about the model properties required to produce proper syntactic generalizations.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":17,"sentence":"We present a systematic evaluation of the syntactic knowledge of neural language models, testing 20 combinations of model types and data sizes on a set of 34 English-language syntactic test suites.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":17,"sentence":"We find substantial differences in syntactic generalization performance by model architecture, with sequential models underperforming other architectures.","offset":3,"pro":0.5,"labels":"RST"},{"idx":17,"sentence":"Factorially manipulating model architecture and training dataset size (1M-40M words), we find that variability in syntactic generalization performance is substantially greater by architecture than by dataset size for the corpora tested in our experiments.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":17,"sentence":"Our results also reveal a dissociation between perplexity and syntactic generalization performance.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":18,"sentence":"Can artificial neural networks learn to represent inflectional morphology and generalize to new words as human speakers do?","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"Kirov and Cotterell (2018) argue that the answer is yes: modern Encoder-Decoder (ED) architectures learn human-like behavior when inflecting English verbs, such as extending the regular past tense form /-(e)d/ to novel words.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":18,"sentence":"However, their work does not address the criticism raised by Marcus et al. (1995): that neural models may learn to extend not the regular, but the most frequent class - and thus fail on tasks like German number inflection, where infrequent suffixes like /-s/ can still be productively generalized.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":18,"sentence":"To investigate this question, we first collect a new dataset from German speakers (production and ratings of plural forms for novel nouns) that is designed to avoid sources of information unavailable to the ED model.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":18,"sentence":"The speaker data show high variability, and two suffixes evince regular' behavior, appearing more often with phonologically atypical inputs.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":18,"sentence":"Encoder-decoder models do generalize the most frequently produced plural class, but do not show human-like variability or regular' extension of these other plural markers.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":18,"sentence":"We conclude that modern neural models may still struggle with minority-class generalization.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":19,"sentence":"With the advent of powerful neural language models over the last few years, research attention has increasingly focused on what aspects of language they represent that make them so successful.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"Several testing methodologies have been developed to probe models' syntactic representations.","offset":1,"pro":0.25,"labels":"BAC"},{"idx":19,"sentence":"One popular method for determining a model's ability to induce syntactic structure trains a model on strings generated according to a template then tests the model's ability to distinguish such strings from superficially similar ones with different syntax.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":19,"sentence":"We illustrate a fundamental problem with this approach by reproducing positive results from a recent paper with two non-syntactic baseline language models: an n-gram model and an LSTM model trained on scrambled inputs.","offset":3,"pro":0.75,"labels":"CLN"},{"idx":0,"sentence":"This work revisits the task of training sequence tagging models with limited resources using transfer learning.","offset":0,"pro":0,"labels":"PUR"},{"idx":0,"sentence":"We investigate several proposed approaches introduced in recent works and suggest a new loss that relies on sentence reconstruction from normalized embeddings.","offset":1,"pro":0.25,"labels":"MTD"},{"idx":0,"sentence":"Specifically, our method demonstrates how by adding a decoding layer for sentence reconstruction, we can improve the performance of various baselines.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":0,"sentence":"We show improved results on the CoNLL02 NER and UD 1.2 POS datasets and demonstrate the power of the method for transfer learning with low-resources achieving 0.6 F1 score in Dutch using only one sample from it.","offset":3,"pro":0.75,"labels":"RST"},{"idx":1,"sentence":"Pretrained masked language models (MLMs) require finetuning for most NLP tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"We show that PLLs outperform scores from autoregressive language models like GPT-2 in a variety of tasks.","offset":1,"pro":0.125,"labels":"RST"},{"idx":1,"sentence":"By rescoring ASR and NMT hypotheses, RoBERTa reduces an end-to-end LibriSpeech model's WER by 30% relative and adds up to +1.7 BLEU on state-of-the-art baselines for low-resource translation pairs, with further gains from domain adaptation.","offset":2,"pro":0.25,"labels":"RST"},{"idx":1,"sentence":"We attribute this success to PLL's unsupervised expression of linguistic acceptability without a left-to-right bias, greatly improving on scores from GPT-2 (+10 points on island effects, NPI licensing in BLiMP).","offset":3,"pro":0.375,"labels":"RST"},{"idx":1,"sentence":"We release our library for language model scoring at https://github.com/awslabs/mlm-scoring.","offset":4,"pro":0.5,"labels":"CTN"},{"idx":1,"sentence":"In all, PLLs and their associated pseudo-perplexities (PPPLs) enable plug-and-play use of the growing number of pretrained MLMs; e.g., we use a single cross-lingual model to rescore translations in multiple languages. ","offset":5,"pro":0.625,"labels":"CLN"},{"idx":1,"sentence":"Instead, we evaluate MLMs out of the box via their pseudo-log-likelihood scores (PLLs), which are computed by masking tokens one by one. ","offset":6,"pro":0.75,"labels":"PUR"},{"idx":1,"sentence":"One can finetune MLMs to give scores without masking, enabling computation in a single inference pass. ","offset":7,"pro":0.875,"labels":"IMP"},{"idx":2,"sentence":"Distance-based knowledge graph embeddings have shown substantial improvement on the knowledge graph link prediction task, from TransE to the latest state-of-the-art RotatE.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"However, complex relations such as N-to-1, 1-to-N and N-to-N still remain challenging to predict.","offset":1,"pro":0.1,"labels":"GAP"},{"idx":2,"sentence":"In this work, we propose a novel distance-based approach for knowledge graph link prediction.","offset":2,"pro":0.2,"labels":"PUR"},{"idx":2,"sentence":"First, we extend the RotatE from 2D complex domain to high dimensional space with orthogonal transforms to model relations.","offset":3,"pro":0.3,"labels":"MTD"},{"idx":2,"sentence":"The orthogonal transform embedding for relations keeps the capability for modeling symmetric/anti-symmetric, inverse and compositional relations while achieves better modeling capacity.","offset":4,"pro":0.4,"labels":"RST"},{"idx":2,"sentence":"Second, the graph context is integrated into distance scoring functions directly.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":2,"sentence":"Specifically, graph context is explicitly modeled via two directed context representations.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":2,"sentence":"Each node embedding in knowledge graph is augmented with two context representations, which are computed from the neighboring outgoing and incoming nodes/edges respectively.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":2,"sentence":"The proposed approach improves prediction accuracy on the difficult N-to-1, 1-to-N and N-to-N cases.","offset":8,"pro":0.8,"labels":"RST"},{"idx":2,"sentence":"Our experimental results show that it achieves state-of-the-art results on two common benchmarks FB15k-237 and WNRR-18, especially on FB15k-237 which has many high in-degree nodes.","offset":9,"pro":0.9,"labels":"RST"},{"idx":3,"sentence":"Most classification models work by first predicting a posterior probability distribution over all classes and then selecting that class with the largest estimated probability.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"In many settings however, the quality of posterior probability itself (e.g., 65% chance having diabetes), gives more reliable information than the final predicted class alone.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":3,"sentence":"When these methods are shown to be poorly calibrated, most fixes to date have relied on posterior calibration, which rescales the predicted probabilities but often has little impact on final classifications.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":3,"sentence":"Our PosCal achieves about 2.5% of task performance gain and 16.1% of calibration error reduction on GLUE (Wang et al., 2018) compared to the baseline.","offset":3,"pro":0.375,"labels":"RST"},{"idx":3,"sentence":"We achieved the comparable task performance with 13.2% calibration error reduction on xSLUE (Kang and Hovy, 2019), but not outperforming the two-stage calibration baseline.","offset":4,"pro":0.5,"labels":"RST"},{"idx":3,"sentence":"Here we propose an end-to-end training procedure called posterior calibrated (PosCal) training that directly optimizes the objective while minimizing the difference between the predicted and empirical posterior probabilities.","offset":5,"pro":0.625,"labels":"PUR"},{"idx":3,"sentence":"PosCal training can be easily extendable to any types of classification tasks as a form of regularization term. ","offset":6,"pro":0.75,"labels":"CLN"},{"idx":3,"sentence":"Also, PosCal has the advantage that it incrementally tracks needed statistics for the calibration objective during the training process, making efficient use of large training sets. ","offset":7,"pro":0.875,"labels":"CLN"},{"idx":4,"sentence":"Text generation often requires high-precision output that obeys task-specific rules.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"This fine-grained control is difficult to enforce with off-the-shelf deep learning models.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":4,"sentence":"In this work, we consider augmenting neural generation models with discrete control states learned through a structured latent-variable approach.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":4,"sentence":"Under this formulation, task-specific knowledge can be encoded through a range of rich, posterior constraints that are effectively trained into the model.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":4,"sentence":"This approach allows users to ground internal model decisions based on prior knowledge, without sacrificing the representational power of neural generative models.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":4,"sentence":"Experiments consider applications of this approach for text generation.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":4,"sentence":"We find that this method improves over standard benchmarks, while also providing fine-grained control.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":5,"sentence":"Although pretrained Transformers such as BERT achieve high accuracy on in-distribution examples, do they generalize to new distributions?","offset":0,"pro":0,"labels":"GAP"},{"idx":5,"sentence":"Pretrained transformers are also more effective at detecting anomalous or OOD examples, while many previous models are frequently worse than chance.","offset":1,"pro":0.16666666666666666,"labels":"RST"},{"idx":5,"sentence":"We examine which factors affect robustness, finding that larger models are not necessarily more robust, distillation can be harmful, and more diverse pretraining data can enhance robustness.","offset":2,"pro":0.3333333333333333,"labels":"RST"},{"idx":5,"sentence":"Finally, we show where future work can improve OOD robustness.","offset":3,"pro":0.5,"labels":"RST"},{"idx":5,"sentence":"We systematically measure out-of-distribution (OOD) generalization for seven NLP datasets by constructing a new robustness benchmark with realistic distribution shifts. ","offset":4,"pro":0.6666666666666666,"labels":"PUR"},{"idx":5,"sentence":"We measure the generalization of previous models including bag-of-words models, ConvNets, and LSTMs, ","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":6,"sentence":"Despite excellent performance on many tasks, NLP systems are easily fooled by small adversarial perturbations of inputs.","offset":0,"pro":0,"labels":"GAP"},{"idx":6,"sentence":"Existing procedures to defend against such perturbations are either (i) heuristic in nature and susceptible to stronger attacks or (ii) provide guaranteed robustness to worst-case attacks, but are incompatible with state-of-the-art models like BERT.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":6,"sentence":"In this work, we introduce robust encodings (RobEn): a simple framework that confers guaranteed robustness, without making compromises on model architecture.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":6,"sentence":"The core component of RobEn is an encoding function, which maps sentences to a smaller, discrete space of encodings.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":6,"sentence":"We identify two desiderata to construct robust encoding functions: perturbations of a sentence should map to a small set of encodings (stability), and models using encodings should still perform well (fidelity).","offset":4,"pro":0.5,"labels":"MTD"},{"idx":6,"sentence":"We instantiate RobEn to defend against a large family of adversarial typos.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":6,"sentence":"Across six tasks from GLUE, our instantiation of RobEn paired with BERT achieves an average robust accuracy of 71.3% against all adversarial typos in the family considered, while previous work using a typo-corrector achieves only 35.3% accuracy against a simple greedy attack.","offset":6,"pro":0.75,"labels":"RST"},{"idx":6,"sentence":"Systems using these encodings as a bottleneck confer guaranteed robustness with standard training, and the same encodings can be used across multiple tasks. ","offset":7,"pro":0.875,"labels":"MTD"},{"idx":7,"sentence":"In natural language processing, a recently popular line of work explores how to best report the experimental results of neural networks.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"One exemplar publication, titled Show Your Work: Improved Reporting of Experimental Results (Dodge et al., 2019), advocates for reporting the expected validation effectiveness of the best-tuned model, with respect to the computational budget.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":7,"sentence":"In the present work, we critically examine this paper.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":7,"sentence":"As far as statistical generalizability is concerned, we find unspoken pitfalls and caveats with this approach.","offset":3,"pro":0.375,"labels":"RST"},{"idx":7,"sentence":"We analytically show that their estimator is biased and uses error-prone assumptions.","offset":4,"pro":0.5,"labels":"RST"},{"idx":7,"sentence":"We find that the estimator favors negative errors and yields poor bootstrapped confidence intervals.","offset":5,"pro":0.625,"labels":"RST"},{"idx":7,"sentence":"We derive an unbiased alternative and bolster our claims with empirical evidence from statistical simulation.","offset":6,"pro":0.75,"labels":"RST"},{"idx":7,"sentence":"Our codebase is at https://github.com/castorini/meanmax.","offset":7,"pro":0.875,"labels":"CTN"},{"idx":8,"sentence":"BERT (Bidirectional Encoder Representations from Transformers) and related pre-trained Transformers have provided large gains across many language understanding tasks, achieving a new state-of-the-art (SOTA).","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"In this paper we introduce a new pre-training task inspired by reading comprehension to better align the pre-training from memorization to understanding.","offset":1,"pro":0.125,"labels":"PUR"},{"idx":8,"sentence":"Span Selection PreTraining (SSPT) poses cloze-like training instances, but rather than draw the answer from the model's parameters, it is selected from a relevant passage.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":8,"sentence":"We find significant and consistent improvements over both BERT-BASE and BERT-LARGE on multiple Machine Reading Comprehension (MRC) datasets.","offset":3,"pro":0.375,"labels":"RST"},{"idx":8,"sentence":"Specifically, our proposed model has strong empirical evidence as it obtains SOTA results on Natural Questions, a new benchmark MRC dataset, outperforming BERT-LARGE by 3 F1 points on short answer prediction.","offset":4,"pro":0.5,"labels":"RST"},{"idx":8,"sentence":"We also show significant impact in HotpotQA, improving answer prediction F1 by 4 points and supporting fact prediction F1 by 1 point and outperforming the previous best system.","offset":5,"pro":0.625,"labels":"RST"},{"idx":8,"sentence":"Moreover, we show that our pre-training approach is particularly effective when training data is limited, improving the learning curve by a large amount.","offset":6,"pro":0.75,"labels":"RST"},{"idx":8,"sentence":"BERT is pretrained on two auxiliary tasks: Masked Language Model and Next Sentence Prediction. ","offset":7,"pro":0.875,"labels":"BAC"},{"idx":9,"sentence":"Sentence ordering is the task of arranging the sentences of a given text in the correct order.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"Recent work using deep neural networks for this task has framed it as a sequence prediction problem.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":9,"sentence":"In this paper, we propose a new framing of this task as a constraint solving problem and introduce a new technique to solve it.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":9,"sentence":"The results on both automatic and human metrics across four different datasets show that this new technique is better at capturing coherence in documents.","offset":3,"pro":0.6,"labels":"RST"},{"idx":9,"sentence":"Additionally, we propose a human evaluation for this task. ","offset":4,"pro":0.8,"labels":"PUR"},{"idx":10,"sentence":"Recently, NLP has seen a surge in the usage of large pre-trained models.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"This raises the question of whether downloading untrusted pre-trained weights can pose a security threat.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":10,"sentence":"In this paper, we show that it is possible to construct weight poisoning attacks where pre-trained weights are injected with vulnerabilities that expose backdoors after fine-tuning, enabling the attacker to manipulate the model prediction simply by injecting an arbitrary keyword.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":10,"sentence":"We show that by applying a regularization method which we call RIPPLe and an initialization procedure we call Embedding Surgery, such attacks are possible even with limited knowledge of the dataset and fine-tuning procedure.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":10,"sentence":"Our experiments on sentiment classification, toxicity detection, and spam detection show that this attack is widely applicable and poses a serious threat.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":10,"sentence":"Users download weights of models pre-trained on large datasets, then fine-tune the weights on a task of their choice. ","offset":5,"pro":0.7142857142857143,"labels":"BAC"},{"idx":10,"sentence":"Finally, we outline practical defenses against such attacks. ","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":11,"sentence":"Transformers have gradually become a key component for many state-of-the-art natural language representation models.","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"A recent Transformer based model- BERTachieved state-of-the-art results on various natural language processing tasks, including GLUE, SQuAD v1.1, and SQuAD v2.0.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":11,"sentence":"This model however is computationally prohibitive and has a huge number of parameters.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":11,"sentence":"In this work we revisit the architecture choices of BERT in efforts to obtain a lighter model.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":11,"sentence":"We focus on reducing the number of parameters yet our methods can be applied towards other objectives such FLOPs or latency.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":11,"sentence":"We show that much efficient light BERT models can be obtained by reducing algorithmically chosen correct architecture design dimensions rather than reducing the number of Transformer encoder layers.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":11,"sentence":"In particular, our schuBERT gives 6.6% higher average accuracy on GLUE and SQuAD datasets as compared to BERT with three encoder layers while having the same number of parameters.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":12,"sentence":"We propose to train a non-autoregressive machine translation model to minimize the energy defined by a pretrained autoregressive model.","offset":0,"pro":0,"labels":"PUR"},{"idx":12,"sentence":"In particular, we view our non-autoregressive translation system as an inference network (Tu and Gimpel, 2018) trained to minimize the autoregressive teacher energy.","offset":1,"pro":0.25,"labels":"MTD"},{"idx":12,"sentence":"Our approach, which we call ENGINE (ENerGy-based Inference NEtworks), achieves state-of-the-art non-autoregressive results on the IWSLT 2014 DE-EN and WMT 2016 RO-EN datasets, approaching the performance of autoregressive models.","offset":2,"pro":0.5,"labels":"RST"},{"idx":12,"sentence":"This contrasts with the popular approach of training a non-autoregressive model on a distilled corpus consisting of the beam-searched outputs of such a teacher model. ","offset":3,"pro":0.75,"labels":"MTD"},{"idx":13,"sentence":"Over the last few years two promising research directions in low-resource neural machine translation (NMT) have emerged.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"In this work, we join these two lines of research and demonstrate the efficacy of monolingual data with self-supervision in multilingual NMT.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":13,"sentence":"We offer three major results: (i) Using monolingual data significantly boosts the translation quality of low-resource languages in multilingual models. (ii) Self-supervision improves zero-shot translation quality in multilingual models. (iii) Leveraging monolingual data with self-supervision provides a viable path towards adding new languages to multilingual models, getting up to 33 BLEU on ro-en translation without any parallel data or back-translation.","offset":2,"pro":0.4,"labels":"RST"},{"idx":13,"sentence":"The first focuses on utilizing high-resource languages to improve the quality of low-resource languages via multilingual NMT. ","offset":3,"pro":0.6,"labels":"BAC"},{"idx":13,"sentence":"The second direction employs monolingual data with self-supervision to pre-train translation models, followed by fine-tuning on small amounts of supervised data. ","offset":4,"pro":0.8,"labels":"BAC"},{"idx":14,"sentence":"Back-translation is a widely used data augmentation technique which leverages target monolingual data.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"However, its effectiveness has been challenged since automatic metrics such as BLEU only show significant improvements for test examples where the source itself is a translation, or translationese.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":14,"sentence":"In this work, we show that this conjecture is not empirically supported and that back-translation improves translation quality of both naturally occurring text as well as translationese according to professional human translators.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":14,"sentence":"We provide empirical evidence to support the view that back-translation is preferred by humans because it produces more fluent outputs.","offset":3,"pro":0.42857142857142855,"labels":"RST"},{"idx":14,"sentence":"BLEU cannot capture human preferences because references are translationese when source sentences are natural text. ","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":14,"sentence":"We recommend complementing BLEU with a language model score to measure fluency. ","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":14,"sentence":"This is believed to be due to translationese inputs better matching the back-translated training data. ","offset":6,"pro":0.8571428571428571,"labels":"GAP"},{"idx":15,"sentence":"Adaptive policies are better than fixed policies for simultaneous translation, since they can flexibly balance the tradeoff between translation quality and latency based on the current context information.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"But previous methods on obtaining adaptive policies either rely on complicated training process, or underperform simple fixed policies.","offset":1,"pro":0.25,"labels":"GAP"},{"idx":15,"sentence":"Experiments on Chinese -greater English and German -greater English show that our adaptive policies can outperform fixed ones by up to 4 BLEU points for the same latency, and more surprisingly, it even surpasses the BLEU score of full-sentence translation in the greedy mode (and very close to beam mode), but with much lower latency.","offset":2,"pro":0.5,"labels":"RST"},{"idx":15,"sentence":"We design an algorithm to achieve adaptive policies via a simple heuristic composition of a set of fixed policies. ","offset":3,"pro":0.75,"labels":"PUR"},{"idx":16,"sentence":"Neural architectures are the current state of the art in Word Sense Disambiguation (WSD).","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"However, they make limited use of the vast amount of relational information encoded in Lexical Knowledge Bases (LKB).","offset":1,"pro":0.2,"labels":"GAP"},{"idx":16,"sentence":"We present Enhanced WSD Integrating Synset Embeddings and Relations (EWISER), a neural supervised architecture that is able to tap into this wealth of knowledge by embedding information from the LKB graph within the neural architecture, and to exploit pretrained synset embeddings, enabling the network to predict synsets that are not in the training set.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":16,"sentence":"As a result, we set a new state of the art on almost all the evaluation settings considered, also breaking through, for the first time, the 80% ceiling on the concatenation of all the standard all-words English WSD evaluation benchmarks.","offset":3,"pro":0.6,"labels":"RST"},{"idx":16,"sentence":"On multilingual all-words WSD, we report state-of-the-art results by training on nothing but English. ","offset":4,"pro":0.8,"labels":"CTN"},{"idx":17,"sentence":"Chinese NLP applications that rely on large text often contain huge amounts of vocabulary which are sparse in corpus.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"Glyph2Vec extracts visual features from word glyphs to expand current word embedding space for out-of-vocabulary word embedding, without the need of accessing any corpus, which is useful for improving Chinese NLP systems, especially for low-resource scenarios.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":17,"sentence":"Experiments across different applications show the significant effectiveness of our model.","offset":2,"pro":0.4,"labels":"RST"},{"idx":17,"sentence":"We show that characters' written form, Glyphs, in ideographic languages could carry rich semantics. ","offset":3,"pro":0.6,"labels":"BAC"},{"idx":17,"sentence":"We present a multi-modal model, Glyph2Vec, to tackle Chinese out-of-vocabulary word embedding problem. ","offset":4,"pro":0.8,"labels":"PUR"},{"idx":18,"sentence":"Our model induces a joint function-specific word vector space, where vectors of e.g. plausible SVO compositions lie close together.","offset":0,"pro":0,"labels":"MTD"},{"idx":18,"sentence":"We show the robustness and versatility of the proposed framework by reporting state-of-the-art results on the tasks of estimating selectional preference and event similarity.","offset":1,"pro":0.2,"labels":"RST"},{"idx":18,"sentence":"We present a neural framework for learning associations between interrelated groups of words such as the ones found in Subject-Verb-Object (SVO) structures. ","offset":2,"pro":0.4,"labels":"PUR"},{"idx":18,"sentence":"The results indicate that the combinations of representations learned with our task-independent model outperform task-specific architectures from prior work, while reducing the number of parameters by up to 95%. ","offset":3,"pro":0.6,"labels":"CLN"},{"idx":18,"sentence":"The model retains information about word group membership even in the joint space, and can thereby effectively be applied to a number of tasks reasoning over the SVO structure. ","offset":4,"pro":0.8,"labels":"MTD"},{"idx":19,"sentence":"While automatic term extraction is a well-researched area, computational approaches to distinguish between degrees of technicality are still understudied.","offset":0,"pro":0,"labels":"GAP"},{"idx":19,"sentence":"When defining a classification approach that combines general-language and domain-specific word embeddings, we go beyond previous work and align vector spaces to gain comparative embeddings.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":19,"sentence":"We suggest two novel models to exploit general- vs. domain-specific comparisons: a simple neural network model with pre-computed comparative-embedding information as input, and a multi-channel model computing the comparison internally.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":19,"sentence":"Both models outperform previous approaches, with the multi-channel model performing best.","offset":3,"pro":0.6,"labels":"RST"},{"idx":19,"sentence":"We semi-automatically create a German gold standard of technicality across four domains, and illustrate the impact of a web-crawled general-language corpus on technicality prediction. ","offset":4,"pro":0.8,"labels":"PUR"},{"idx":0,"sentence":"Metaphor is a linguistic device in which a concept is expressed by mentioning another.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"Identifying metaphorical expressions, therefore, requires a non-compositional understanding of semantics.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":0,"sentence":"Multiword Expressions (MWEs), on the other hand, are linguistic phenomena with varying degrees of semantic opacity and their identification poses a challenge to computational models.","offset":2,"pro":0.3333333333333333,"labels":"BAC"},{"idx":0,"sentence":"This work is the first attempt at analysing the interplay of metaphor and MWEs processing through the design of a neural architecture whereby classification of metaphors is enhanced by informing the model of the presence of MWEs.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":0,"sentence":"To the best of our knowledge, this is the first MWE-aware metaphor identification system paving the way for further experiments on the complex interactions of these phenomena.","offset":4,"pro":0.6666666666666666,"labels":"CTN"},{"idx":0,"sentence":"The results and analyses show that this proposed architecture reach state-of-the-art on two different established metaphor datasets.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":1,"sentence":"Multilingual representations embed words from many languages into a single semantic space such that words with similar meanings are close to each other regardless of the language.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"These embeddings have been widely used in various settings, such as cross-lingual transfer, where a natural language processing (NLP) model trained on one language is deployed to another language.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":1,"sentence":"While the cross-lingual transfer techniques are powerful, they carry gender bias from the source to target languages.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":1,"sentence":"In this paper, we study gender bias in multilingual embeddings and how it affects transfer learning for NLP applications.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":1,"sentence":"We create a multilingual dataset for bias analysis and propose several ways for quantifying bias in multilingual representations from both the intrinsic and extrinsic perspectives.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":1,"sentence":"Experimental results show that the magnitude of bias in the multilingual representations changes differently when we align the embeddings to different target spaces and that the alignment direction can also have an influence on the bias in transfer learning.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":1,"sentence":"We further provide recommendations for using the multilingual word representations for downstream tasks.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":2,"sentence":"As part of growing NLP capabilities, coupled with an awareness of the ethical dimensions of research, questions have been raised about whether particular datasets and tasks should be deemed off-limits for NLP research.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"We examine this question with respect to a paper on automatic legal sentencing from EMNLP 2019 which was a source of some debate, in asking whether the paper should have been allowed to be published, who should have been charged with making such a decision, and on what basis.","offset":1,"pro":0.3333333333333333,"labels":"PUR"},{"idx":2,"sentence":"We focus in particular on the role of data statements in ethically assessing research, but also discuss the topic of dual use, and examine the outcomes of similar debates in other scientific disciplines.","offset":2,"pro":0.6666666666666666,"labels":"MTD"},{"idx":3,"sentence":"Most NLP datasets are not annotated with protected attributes such as gender, making it difficult to measure classification bias using standard measures of fairness (e.g., equal opportunity).","offset":0,"pro":0,"labels":"GAP"},{"idx":3,"sentence":"However, manually annotating a large dataset with a protected attribute is slow and expensive.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":3,"sentence":"Instead of annotating all the examples, can we annotate a subset of them and use that sample to estimate the bias?","offset":2,"pro":0.2222222222222222,"labels":"BAC"},{"idx":3,"sentence":"While it is possible to do so, the smaller this annotated sample is, the less certain we are that the estimate is close to the true bias.","offset":3,"pro":0.3333333333333333,"labels":"GAP"},{"idx":3,"sentence":"In this work, we propose using Bernstein bounds to represent this uncertainty about the bias estimate as a confidence interval.","offset":4,"pro":0.4444444444444444,"labels":"PUR"},{"idx":3,"sentence":"We provide empirical evidence that a 95% confidence interval derived this way consistently bounds the true bias.","offset":5,"pro":0.5555555555555556,"labels":"RST"},{"idx":3,"sentence":"In quantifying this uncertainty, our method, which we call Bernstein-bounded unfairness, helps prevent classifiers from being deemed biased or unbiased when there is insufficient evidence to make either claim.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":3,"sentence":"Our findings suggest that the datasets currently used to measure specific biases are too small to conclusively identify bias except in the most egregious cases.","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":3,"sentence":"For example, consider a co-reference resolution system that is 5% more accurate on gender-stereotypical sentences - to claim it is biased with 95% confidence, we need a bias-specific dataset that is 3.8 times larger than WinoBias, the largest available.","offset":8,"pro":0.8888888888888888,"labels":"MTD"},{"idx":4,"sentence":"Training on only perfect Standard English corpora predisposes pre-trained neural networks to discriminate against minorities from non-standard linguistic backgrounds (e.g., African American Vernacular English, Colloquial Singapore English, etc.).","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"We perturb the inflectional morphology of words to craft plausible and semantically similar adversarial examples that expose these biases in popular NLP models, e.g., BERT and Transformer, and show that adversarially fine-tuning them for a single epoch significantly improves robustness without sacrificing performance on clean data.","offset":1,"pro":0.5,"labels":"MTD"},{"idx":5,"sentence":"Advanced machine learning techniques have boosted the performance of natural language processing.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"Nevertheless, recent studies, e.g., (CITATION) show that these techniques inadvertently capture the societal bias hidden in the corpus and further amplify it.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":5,"sentence":"However, their analysis is conducted only on models' top predictions.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":5,"sentence":"In this paper, we investigate the gender bias amplification issue from the distribution perspective and demonstrate that the bias is amplified in the view of predicted probability distribution over labels.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":5,"sentence":"We further propose a bias mitigation approach based on posterior regularization.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":5,"sentence":"With little performance loss, our method can almost remove the bias amplification in the distribution.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":5,"sentence":"Our study sheds the light on understanding the bias amplification.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":6,"sentence":"Recent developments in Neural Relation Extraction (NRE) have made significant strides towards Automated Knowledge Base Construction.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"While much attention has been dedicated towards improvements in accuracy, there have been no attempts in the literature to evaluate social biases exhibited in NRE systems.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":6,"sentence":"In this paper, we create WikiGenderBias, a distantly supervised dataset composed of over 45,000 sentences including a 10% human annotated test set for the purpose of analyzing gender bias in relation extraction systems.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":6,"sentence":"We find that when extracting spouse-of and hypernym (i.e., occupation) relations, an NRE system performs differently when the gender of the target entity is different.","offset":3,"pro":0.375,"labels":"RST"},{"idx":6,"sentence":"However, such disparity does not appear when extracting relations such as birthDate or birthPlace.","offset":4,"pro":0.5,"labels":"RST"},{"idx":6,"sentence":"We also analyze how existing bias mitigation techniques, such as name anonymization, word embedding debiasing, and data augmentation affect the NRE system in terms of maintaining the test performance and reducing biases.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":6,"sentence":"Unfortunately, due to NRE models rely heavily on surface level cues, we find that existing bias mitigation approaches have a negative effect on NRE.","offset":6,"pro":0.75,"labels":"GAP"},{"idx":6,"sentence":"Our analysis lays groundwork for future quantifying and mitigating bias in NRE.","offset":7,"pro":0.875,"labels":"CTN"},{"idx":7,"sentence":"We propose a deep and interpretable probabilistic generative model to analyze glyph shapes in printed Early Modern documents.","offset":0,"pro":0,"labels":"MTD"},{"idx":7,"sentence":"We focus on clustering extracted glyph images into underlying templates in the presence of multiple confounding sources of variance.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":7,"sentence":"Our approach introduces a neural editor model that first generates well-understood printing phenomena like spatial perturbations from template parameters via interpertable latent variables, and then modifies the result by generating a non-interpretable latent vector responsible for inking variations, jitter, noise from the archiving process, and other unforeseen phenomena associated with Early Modern printing.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":7,"sentence":"Critically, by introducing an inference network whose input is restricted to the visual residual between the observation and the interpretably-modified template, we are able to control and isolate what the vector-valued latent variable captures.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":7,"sentence":"We show that our approach outperforms rigid interpretable clustering baselines (c.f.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":7,"sentence":"Ocular) and overly-flexible deep generative models (VAE) alike on the task of completely unsupervised discovery of typefaces in mixed-fonts documents.","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":8,"sentence":"Pooling is an important technique for learning text representations in many neural NLP models.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"In conventional pooling methods such as average, max and attentive pooling, text representations are weighted summations of the L1 or L{mbox{infty norm of input features.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":8,"sentence":"However, their pooling norms are always fixed and may not be optimal for learning accurate text representations in different tasks.","offset":2,"pro":0.2,"labels":"GAP"},{"idx":8,"sentence":"In addition, in many popular pooling methods such as max and attentive pooling some features may be over-emphasized, while other useful ones are not fully exploited.","offset":3,"pro":0.3,"labels":"GAP"},{"idx":8,"sentence":"In this paper, we propose an Attentive Pooling with Learnable Norms (APLN) approach for text representation.","offset":4,"pro":0.4,"labels":"PUR"},{"idx":8,"sentence":"Different from existing pooling methods that use a fixed pooling norm, we propose to learn the norm in an end-to-end manner to automatically find the optimal ones for text representation in different tasks.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":8,"sentence":"In addition, we propose two methods to ensure the numerical stability of the model training.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":8,"sentence":"The first one is scale limiting, which re-scales the input to ensure non-negativity and alleviate the risk of exponential explosion.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":8,"sentence":"The second one is re-formulation, which decomposes the exponent operation to avoid computing the real-valued powers of the input and further accelerate the pooling operation.","offset":8,"pro":0.8,"labels":"MTD"},{"idx":8,"sentence":"Experimental results on four benchmark datasets show that our approach can effectively improve the performance of attentive pooling.","offset":9,"pro":0.9,"labels":"RST"},{"idx":9,"sentence":"Multi-task learning (MTL) and transfer learning (TL) are techniques to overcome the issue of data scarcity when training state-of-the-art neural networks.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"However, finding beneficial auxiliary datasets for MTL or TL is a time- and resource-consuming trial-and-error approach.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":9,"sentence":"We propose new methods to automatically assess the similarity of sequence tagging datasets to identify beneficial auxiliary data for MTL or TL setups.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":9,"sentence":"Our methods can compute the similarity between any two sequence tagging datasets, they do not need to be annotated with the same tagset or multiple labels in parallel.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":9,"sentence":"Additionally, our methods take tokens and their labels into account, which is more robust than only using either of them as an information source, as conducted in prior work.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":9,"sentence":"We empirically show that our similarity measures correlate with the change in test score of neural networks that use the auxiliary dataset for MTL to increase the main task performance.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":9,"sentence":"We provide an efficient, open-source implementation.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":10,"sentence":"Self-attention networks (SANs) with selective mechanism has produced substantial improvements in various NLP tasks by concentrating on a subset of input words.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"However, the underlying reasons for their strong performance have not been well explained.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":10,"sentence":"In this paper, we bridge the gap by assessing the strengths of selective SANs (SSANs), which are implemented with a flexible and universal Gumbel-Softmax.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":10,"sentence":"Experimental results on several representative NLP tasks, including natural language inference, semantic role labelling, and machine translation, show that SSANs consistently outperform the standard SANs.","offset":3,"pro":0.5,"labels":"RST"},{"idx":10,"sentence":"Through well-designed probing experiments, we empirically validate that the improvement of SSANs can be attributed in part to mitigating two commonly-cited weaknesses of SANs: word order encoding and structure modeling.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":10,"sentence":"Specifically, the selective mechanism improves SANs by paying more attention to content words that contribute to the meaning of the sentence.","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":11,"sentence":"Multilayer transformer networks consist of interleaved self-attention and feedforward sublayers.","offset":0,"pro":0,"labels":"MTD"},{"idx":11,"sentence":"Could ordering the sublayers in a different pattern lead to better performance?","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":11,"sentence":"We generate randomly ordered transformers and train them with the language modeling objective.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":11,"sentence":"We observe that some of these models are able to achieve better performance than the interleaved baseline, and that those successful variants tend to have more self-attention at the bottom and more feedforward sublayers at the top.","offset":3,"pro":0.42857142857142855,"labels":"RST"},{"idx":11,"sentence":"We propose a new transformer pattern that adheres to this property, the sandwich transformer, and show that it improves perplexity on multiple word-level and character-level language modeling benchmarks, at no cost in parameters, memory, or training time.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":11,"sentence":"However, the sandwich reordering pattern does not guarantee performance gains across every task, as we demonstrate on machine translation models.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":11,"sentence":"Instead, we suggest that further exploration of task-specific sublayer reorderings is needed in order to unlock additional gains.","offset":6,"pro":0.8571428571428571,"labels":"PUR"},{"idx":12,"sentence":"Model ensemble techniques often increase task performance in neural networks; however, they require increased time, memory, and management effort.","offset":0,"pro":0,"labels":"GAP"},{"idx":12,"sentence":"In this study, we propose a novel method that replicates the effects of a model ensemble with a single model.","offset":1,"pro":0.25,"labels":"PUR"},{"idx":12,"sentence":"Our approach creates K-virtual models within a single parameter space using K-distinct pseudo-tags and K-distinct vectors.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":12,"sentence":"Experiments on text classification and sequence labeling tasks on several datasets demonstrate that our method emulates or outperforms a traditional model ensemble with 1/K-times fewer parameters.","offset":3,"pro":0.75,"labels":"RST"},{"idx":13,"sentence":"Zero-shot learning has been a tough problem since no labeled data is available for unseen classes during training, especially for classes with low similarity.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"In this situation, transferring from seen classes to unseen classes is extremely hard.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":13,"sentence":"To tackle this problem, in this paper we propose a self-training based method to efficiently leverage unlabeled data.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":13,"sentence":"Traditional self-training methods use fixed heuristics to select instances from unlabeled data, whose performance varies among different datasets.","offset":3,"pro":0.5,"labels":"BAC"},{"idx":13,"sentence":"We propose a reinforcement learning framework to learn data selection strategy automatically and provide more reliable selection.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":13,"sentence":"Experimental results on both benchmarks and a real-world e-commerce dataset show that our approach significantly outperforms previous methods in zero-shot text classification","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":14,"sentence":"Multi-modal neural machine translation (NMT) aims to translate source sentences into a target language paired with images.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"However, dominant multi-modal NMT models do not fully exploit fine-grained semantic correspondences between semantic units of different modalities, which have potential to refine multi-modal representation learning.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":14,"sentence":"To deal with this issue, in this paper, we propose a novel graph-based multi-modal fusion encoder for NMT.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":14,"sentence":"Specifically, we first represent the input sentence and image using a unified multi-modal graph, which captures various semantic relationships between multi-modal semantic units (words and visual objects).","offset":3,"pro":0.375,"labels":"MTD"},{"idx":14,"sentence":"We then stack multiple graph-based multi-modal fusion layers that iteratively perform semantic interactions to learn node representations.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":14,"sentence":"Finally, these representations provide an attention-based context vector for the decoder.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":14,"sentence":"We evaluate our proposed encoder on the Multi30K datasets.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":14,"sentence":"Experimental results and in-depth analysis show the superiority of our multi-modal NMT model.","offset":7,"pro":0.875,"labels":"RST"},{"idx":15,"sentence":"Recently unsupervised Bilingual Lexicon Induction(BLI) without any parallel corpus has attracted much research interest.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"One of the crucial parts in methods for the BLI task is the matching procedure.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":15,"sentence":"Previous works impose a too strong constraint on the matching and lead to many counterintuitive translation pairings.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":15,"sentence":"Thus We propose a relaxed matching procedure to find a more precise matching between two languages.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":15,"sentence":"We also find that aligning source and target language embedding space bidirectionally will bring significant improvement.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":15,"sentence":"We follow the previous iterative framework to conduct experiments.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":15,"sentence":"Results on standard benchmark demonstrate the effectiveness of our proposed method, which substantially outperforms previous unsupervised methods.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":16,"sentence":"This paper introduces Dynamic Programming Encoding (DPE), a new segmentation algorithm for tokenizing sentences into subword units.","offset":0,"pro":0,"labels":"PUR"},{"idx":16,"sentence":"We view the subword segmentation of output sentences as a latent variable that should be marginalized out for learning and inference.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":16,"sentence":"A mixed character-subword transformer is proposed, which enables exact log marginal likelihood estimation and exact MAP inference to find target segmentations with maximum posterior probability.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":16,"sentence":"DPE uses a lightweight mixed character-subword transformer as a means of pre-processing parallel data to segment output sentences using dynamic programming.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":16,"sentence":"Empirical results on machine translation suggest that DPE is effective for segmenting output sentences and can be combined with BPE dropout for stochastic segmentation of source sentences.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":16,"sentence":"DPE achieves an average improvement of 0.9 BLEU over BPE (Sennrich et al., 2016) and an average improvement of 0.55 BLEU over BPE dropout (Provilkov et al., 2019) on several WMT datasets including English less=greater (German, Romanian, Estonian, Finnish, Hungarian).","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":17,"sentence":"We propose a novel manifold based geometric approach for learning unsupervised alignment of word embeddings between the source and the target languages.","offset":0,"pro":0,"labels":"PUR"},{"idx":17,"sentence":"Our approach formulates the alignment learning problem as a domain adaptation problem over the manifold of doubly stochastic matrices.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":17,"sentence":"This viewpoint arises from the aim to align the second order information of the two language spaces.","offset":2,"pro":0.3333333333333333,"labels":"BAC"},{"idx":17,"sentence":"The rich geometry of the doubly stochastic manifold allows to employ efficient Riemannian conjugate gradient algorithm for the proposed formulation.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":17,"sentence":"Empirically, the proposed approach outperforms state-of-the-art optimal transport based approach on the bilingual lexicon induction task across several language pairs.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":17,"sentence":"The performance improvement is more significant for distant language pairs.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":18,"sentence":"Non-autoregressive neural machine translation (NAT) predicts the entire target sequence simultaneously and significantly accelerates inference process.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"However, NAT discards the dependency information in a sentence, and thus inevitably suffers from the multi-modality problem: the target tokens may be provided by different possible translations, often causing token repetitions or missing.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":18,"sentence":"To alleviate this problem, we propose a novel semi-autoregressive model RecoverSAT in this work, which generates a translation as a sequence of segments.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":18,"sentence":"The segments are generated simultaneously while each segment is predicted token-by-token.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":18,"sentence":"By dynamically determining segment length and deleting repetitive segments, RecoverSAT is capable of recovering from repetitive and missing token errors.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":18,"sentence":"Experimental results on three widely-used benchmark datasets show that our proposed model achieves more than 4 times speedup while maintaining comparable performance compared with the corresponding autoregressive model.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":19,"sentence":"Confidence calibration, which aims to make model predictions equal to the true correctness measures, is important for neural machine translation (NMT) because it is able to offer useful indicators of translation errors in the generated output.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"While prior studies have shown that NMT models trained with label smoothing are well-calibrated on the ground-truth training data, we find that miscalibration still remains a severe challenge for NMT during inference due to the discrepancy between training and inference.","offset":1,"pro":0.25,"labels":"GAP"},{"idx":19,"sentence":"By carefully designing experiments on three language pairs, our work provides in-depth analyses of the correlation between calibration and translation performance as well as linguistic properties of miscalibration and reports a number of interesting findings that might help humans better analyze, understand and improve NMT models.","offset":2,"pro":0.5,"labels":"RST"},{"idx":19,"sentence":"Based on these observations, we further propose a new graduated label smoothing method that can improve both inference calibration and translation performance.","offset":3,"pro":0.75,"labels":"MTD"},{"idx":0,"sentence":"To achieve the long-term goal of machines being able to engage humans in conversation, our models should captivate the interest of their speaking partners.","offset":0,"pro":0,"labels":"PUR"},{"idx":0,"sentence":"Communication grounded in images, whereby a dialogue is conducted based on a given photo, is a setup naturally appealing to humans (Hu et al., 2014).","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":0,"sentence":"In this work we study large-scale architectures and datasets for this goal.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":0,"sentence":"We test a set of neural architectures using state-of-the-art image and text representations, considering various ways to fuse the components.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":0,"sentence":"To test such models, we collect a dataset of grounded human-human conversations, where speakers are asked to play roles given a provided emotional mood or style, as the use of such traits is also a key factor in engagingness (Guo et al., 2019).","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":0,"sentence":"Our dataset, Image-Chat, consists of 202k dialogues over 202k images using 215 possible style traits.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":0,"sentence":"Automatic metrics and human evaluations of engagingness show the efficacy of our approach; in particular, we obtain state-of-the-art performance on the existing IGC task, and our best performing model is almost on par with humans on the Image-Chat test set (preferred 47.7% of the time).","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":1,"sentence":"Evaluating the quality of a dialogue interaction between two agents is a difficult task, especially in open-domain chit-chat style dialogue.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"Here, we propose an unreferenced automated evaluation metric that uses large pre-trained language models to extract latent representations of utterances, and leverages the temporal transitions that exist between them.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":1,"sentence":"We show that our model achieves higher correlation with human annotations in an online setting, while not requiring true responses for comparison during inference.","offset":2,"pro":0.4,"labels":"RST"},{"idx":1,"sentence":"There have been recent efforts to develop automatic dialogue evaluation metrics","offset":3,"pro":0.6,"labels":"BAC"},{"idx":1,"sentence":" but most of them do not generalize to unseen datasets and/or need a human-generated reference response during inference, making it infeasible for online evaluatio","offset":4,"pro":0.8,"labels":"GAP"},{"idx":2,"sentence":"The timings of spoken response offsets in human dialogue have been shown to vary based on contextual elements of the dialogue.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"We propose neural models that simulate the distributions of these response offsets, taking into account the response turn as well as the preceding turn.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":2,"sentence":"The models are designed to be integrated into the pipeline of an incremental spoken dialogue system (SDS).","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":2,"sentence":"We evaluate our models using offline experiments as well as human listening tests.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":2,"sentence":"We show that human listeners consider certain response timings to be more natural based on the dialogue context.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":2,"sentence":"The introduction of these models into SDS pipelines could increase the perceived naturalness of interactions.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":3,"sentence":"We introduce dodecaDialogue: a set of 12 tasks that measures if a conversational agent can communicate engagingly with personality and empathy, ask questions, answer questions by utilizing knowledge resources, discuss topics and situations, and perceive and converse about images.","offset":0,"pro":0,"labels":"MTD"},{"idx":3,"sentence":"By multi-tasking on such a broad large-scale set of data, we hope to both move towards and measure progress in producing a single unified agent that can perceive, reason and converse with humans in an open-domain setting.","offset":1,"pro":0.25,"labels":"CTN"},{"idx":3,"sentence":"We show that such multi-tasking improves over a BERT pre-trained baseline, largely due to multi-tasking with very large dialogue datasets in a similar domain, and that the multi-tasking in general provides gains to both text and image-based tasks using several metrics in both the fine-tune and task transfer settings.","offset":2,"pro":0.5,"labels":"RST"},{"idx":3,"sentence":"We obtain state-of-the-art results on many of the tasks, providing a strong baseline for this challenge.","offset":3,"pro":0.75,"labels":"CLN"},{"idx":4,"sentence":"In the last few years, a number of successful approaches have emerged that are able to adequately model various aspects of natural language.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"In particular, language models based on neural networks have improved the state of the art with regard to predictive language modeling, while topic models are successful at capturing clear-cut, semantic dimensions.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":4,"sentence":"In this paper, we will explore how these approaches can be adapted and combined to model the linguistic and literary aspects needed for poetry generation.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":4,"sentence":"The system is exclusively trained on standard, non-poetic text, and its output is constrained in order to confer a poetic character to the generated verse.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":4,"sentence":"The framework is applied to the generation of poems in both English and French, and is equally evaluated for both languages.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":4,"sentence":"Even though it only uses standard, non-poetic text as input, the system yields state of the art results for poetry generation.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":5,"sentence":"Generating sequential natural language descriptions from graph-structured data (e.g., knowledge graph) is challenging, partly because of the structural differences between the input graph and the output text.","offset":0,"pro":0,"labels":"GAP"},{"idx":5,"sentence":"Hence, popular sequence-to-sequence models, which require serialized input, are not a natural fit for this task.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":5,"sentence":"Graph neural networks, on the other hand, can better encode the input graph but broaden the structural gap between the encoder and decoder, making faithful generation difficult.","offset":2,"pro":0.4,"labels":"GAP"},{"idx":5,"sentence":"To narrow this gap, we propose DualEnc, a dual encoding model that can not only incorporate the graph structure, but can also cater to the linear structure of the output text.","offset":3,"pro":0.6,"labels":"PUR"},{"idx":5,"sentence":"Empirical comparisons with strong single-encoder baselines demonstrate that dual encoding can significantly improve the quality of the generated text.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":6,"sentence":"We present a simple approach for text infilling, the task of predicting missing spans of text at any position in a document.","offset":0,"pro":0,"labels":"PUR"},{"idx":6,"sentence":"While infilling could enable rich functionality especially for writing assistance tools, more attention has been devoted to language modeling-a special case of infilling where text is predicted at the end of a document.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":6,"sentence":"In this paper, we aim to extend the capabilities of language models (LMs) to the more general task of infilling.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":6,"sentence":"To this end, we train (or fine tune) off-the-shelf LMs on sequences containing the concatenation of artificially-masked text and the text which was masked.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":6,"sentence":"We show that this approach, which we call infilling by language modeling, can enable LMs to infill entire sentences effectively on three different domains: short stories, scientific abstracts, and lyrics.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":6,"sentence":"Furthermore, we show that humans have difficulty identifying sentences infilled by our approach as machine-generated in the domain of short stories.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":7,"sentence":"Missing sentence generation (or sentence in-filling) fosters a wide range of applications in natural language generation, such as document auto-completion and meeting note expansion.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"This task asks the model to generate intermediate missing sentences that can syntactically and semantically bridge the surrounding context.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":7,"sentence":"Solving the sentence infilling task requires techniques in natural language processing ranging from understanding to discourse-level planning to generation.","offset":2,"pro":0.4,"labels":"BAC"},{"idx":7,"sentence":"In this paper, we propose a framework to decouple the challenge and address these three aspects respectively, leveraging the power of existing large-scale pre-trained models such as BERT and GPT-2.","offset":3,"pro":0.6,"labels":"PUR"},{"idx":7,"sentence":"We empirically demonstrate the effectiveness of our model in learning a sentence representation for generation and further generating a missing sentence that fits the context.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":8,"sentence":"Auto-regressive text generation models usually focus on local fluency, and may cause inconsistent semantic meaning in long text generation.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"Further, automatically generating words with similar semantics is challenging, and hand-crafted linguistic rules are difficult to apply.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":8,"sentence":"We consider a text planning scheme and present a model-based imitation-learning approach to alleviate the aforementioned issues.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":8,"sentence":"Specifically, we propose a novel guider network to focus on the generative process over a longer horizon, which can assist next-word prediction and provide intermediate rewards for generator optimization.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":8,"sentence":"Extensive experiments demonstrate that the proposed method leads to improved performance.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":9,"sentence":"Retrieve-and-edit seq2seq methods typically retrieve an output from the training set and learn a model to edit it to produce the final output.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"We propose to extend this framework with a simple and effective post-generation ranking approach.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":9,"sentence":"Our framework (i) retrieves several potentially relevant outputs for each input, (ii) edits each candidate independently, and (iii) re-ranks the edited candidates to select the final output.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":9,"sentence":"We use a standard editing model with simple task-specific re-ranking approaches, and we show empirically that this approach outperforms existing, significantly more complex methodologies.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":9,"sentence":"Experiments on two machine translation (MT) datasets show new state-of-art results.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":9,"sentence":"We also achieve near state-of-art performance on the Gigaword summarization dataset, where our analyses show that there is significant room for performance improvement with better candidate output selection in future work.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":10,"sentence":"Learning to follow instructions is of fundamental importance to autonomous agents for vision-and-language navigation (VLN).","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"In this paper, we study how an agent can navigate long paths when learning from a corpus that consists of shorter ones.","offset":1,"pro":0.09090909090909091,"labels":"PUR"},{"idx":10,"sentence":"We show that existing state-of-the-art agents do not generalize well.","offset":2,"pro":0.18181818181818182,"labels":"RST"},{"idx":10,"sentence":"To this end, we propose BabyWalk, a new VLN agent that is learned to navigate by decomposing long instructions into shorter ones (BabySteps) and completing them sequentially.","offset":3,"pro":0.2727272727272727,"labels":"PUR"},{"idx":10,"sentence":"A special design memory buffer is used by the agent to turn its past experiences into contexts for future steps.","offset":4,"pro":0.36363636363636365,"labels":"MTD"},{"idx":10,"sentence":"The learning process is composed of two phases.","offset":5,"pro":0.45454545454545453,"labels":"MTD"},{"idx":10,"sentence":"In the first phase, the agent uses imitation learning from demonstration to accomplish BabySteps.","offset":6,"pro":0.5454545454545454,"labels":"MTD"},{"idx":10,"sentence":"In the second phase, the agent uses curriculum-based reinforcement learning to maximize rewards on navigation tasks with increasingly longer instructions.","offset":7,"pro":0.6363636363636364,"labels":"MTD"},{"idx":10,"sentence":"We create two new benchmark datasets (of long navigation tasks) and use them in conjunction with existing ones to examine BabyWalk's generalization ability.","offset":8,"pro":0.7272727272727273,"labels":"MTD"},{"idx":10,"sentence":"Empirical results show that BabyWalk achieves state-of-the-art results on several metrics, in particular, is able to follow long instructions better.","offset":9,"pro":0.8181818181818182,"labels":"RST"},{"idx":10,"sentence":"The codes and the datasets are released on our project page: https://github.com/Sha-Lab/babywalk.","offset":10,"pro":0.9090909090909091,"labels":"CTN"},{"idx":11,"sentence":"We introduce a new task, MultiMedia Event Extraction, which aims to extract events and their arguments from multimedia documents.","offset":0,"pro":0,"labels":"PUR"},{"idx":11,"sentence":"We develop the first benchmark and collect a dataset of 245 multimedia news articles with extensively annotated events and arguments.","offset":1,"pro":0.14285714285714285,"labels":"MTD"},{"idx":11,"sentence":"We propose a novel method, Weakly Aligned Structured Embedding (WASE), that encodes structured representations of semantic information from textual and visual data into a common embedding space.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":11,"sentence":"The structures are aligned across modalities by employing a weakly supervised training strategy, which enables exploiting available resources without explicit cross-media annotation.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":11,"sentence":"Compared to uni-modal state-of-the-art methods, our approach achieves 4.0% and 9.8% absolute F-score gains on text event argument role labeling and visual event extraction.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":11,"sentence":"Compared to state-of-the-art multimedia unstructured representations, we achieve 8.3% and 5.0% absolute F-score gains on multimedia event extraction and argument role labeling, respectively.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":11,"sentence":"By utilizing images, we extract 21.4% more event mentions than traditional text-only methods.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":12,"sentence":"We apply a generative segmental model of task structure, guided by narration, to action segmentation in video.","offset":0,"pro":0,"labels":"MTD"},{"idx":12,"sentence":"We focus on unsupervised and weakly-supervised settings where no action labels are known during training.","offset":1,"pro":0.25,"labels":"MTD"},{"idx":12,"sentence":"Despite its simplicity, our model performs competitively with previous work on a dataset of naturalistic instructional videos.","offset":2,"pro":0.5,"labels":"RST"},{"idx":12,"sentence":"Our model allows us to vary the sources of supervision used in training, and we find that both task structure and narrative language provide large benefits in segmentation quality.","offset":3,"pro":0.75,"labels":"RST"},{"idx":13,"sentence":"The Minecraft Collaborative Building Task is a two-player game in which an Architect (A) instructs a Builder (B) to construct a target structure in a simulated Blocks World Environment.","offset":0,"pro":0,"labels":"MTD"},{"idx":13,"sentence":"We define the subtask of predicting correct action sequences (block placements and removals) in a given game contex","offset":1,"pro":0.3333333333333333,"labels":"MTD"},{"idx":13,"sentence":"and show that capturing B's past actions as well as B's perspective leads to a significant improvement in performance on this challenging language understanding problem","offset":2,"pro":0.6666666666666666,"labels":"RST"},{"idx":14,"sentence":"Generating multi-sentence descriptions for videos is one of the most challenging captioning tasks due to its high requirements for not only visual relevance but also discourse-based coherence across the sentences in the paragraph.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"Towards this goal, we propose a new approach called Memory-Augmented Recurrent Transformer (MART), which uses a memory module to augment the transformer architecture.","offset":1,"pro":0.25,"labels":"PUR"},{"idx":14,"sentence":"The memory module generates a highly summarized memory state from the video segments and the sentence history so as to help better prediction of the next sentence (w.r.t. coreference and repetition aspects), thus encouraging coherent paragraph generation.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":14,"sentence":"Extensive experiments, human evaluations, and qualitative analyses on two popular datasets ActivityNet Captions and YouCookII show that MART generates more coherent and less repetitive paragraph captions than baseline methods, while maintaining relevance to the input video events.","offset":3,"pro":0.75,"labels":"RST"},{"idx":15,"sentence":"Visual features are a promising signal for learning bootstrap textual models.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"However, blackbox learning models make it difficult to isolate the specific contribution of visual components.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":15,"sentence":"In this analysis, we consider the case study of the Visually Grounded Neural Syntax Learner (Shi et al., 2019), a recent approach for learning syntax from a visual training signal.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":15,"sentence":"By constructing simplified versions of the model, we isolate the core factors that yield the model's strong performance.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":15,"sentence":"Contrary to what the model might be capable of learning, we find significantly less expressive versions produce similar predictions and perform just as well, or even better.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":15,"sentence":"We also find that a simple lexical signal of noun concreteness plays the main role in the model's predictions as opposed to more complex syntactic reasoning.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":16,"sentence":"Variational Autoencoder (VAE) is widely used as a generative model to approximate a model's posterior on latent variables by combining the amortized variational inference and deep neural networks.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"However, when paired with strong autoregressive decoders, VAE often converges to a degenerated local optimum known as posterior collapse.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":16,"sentence":"Previous approaches consider the Kullback-Leibler divergence (KL) individual for each datapoint.","offset":2,"pro":0.25,"labels":"BAC"},{"idx":16,"sentence":"We propose to let the KL follow a distribution across the whole dataset, and analyze that it is sufficient to prevent posterior collapse by keeping the expectation of the KL's distribution positive.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":16,"sentence":"Then we propose Batch Normalized-VAE (BN-VAE), a simple but effective approach to set a lower bound of the expectation by regularizing the distribution of the approximate posterior's parameters.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":16,"sentence":"Without introducing any new model component or modifying the objective, our approach can avoid the posterior collapse effectively and efficiently.","offset":5,"pro":0.625,"labels":"RST"},{"idx":16,"sentence":"We further show that the proposed BN-VAE can be extended to conditional VAE (CVAE).","offset":6,"pro":0.75,"labels":"RST"},{"idx":16,"sentence":"Empirically, our approach surpasses strong autoregressive baselines on language modeling, text classification and dialogue generation, and rivals more complex approaches while keeping almost the same training time as VAE.","offset":7,"pro":0.875,"labels":"RST"},{"idx":17,"sentence":"We study the settings for which deep contextual embeddings (e.g., BERT) give large improvements in performance relative to classic pretrained embeddings (e.g., GloVe), and an even simpler baseline-random word embeddings-focusing on the impact of the training set size and the linguistic properties of the task.","offset":0,"pro":0,"labels":"MTD"},{"idx":17,"sentence":"Surprisingly, we find that both of these simpler baselines can match contextual embeddings on industry-scale data, and often perform within 5 to 10% accuracy (absolute) on benchmark tasks.","offset":1,"pro":0.3333333333333333,"labels":"RST"},{"idx":17,"sentence":"Furthermore, we identify properties of data for which contextual embeddings give particularly large gains: language containing complex structure, ambiguous word usage, and words unseen in training.","offset":2,"pro":0.6666666666666666,"labels":"RST"},{"idx":18,"sentence":"We study the potential for interaction in natural language classification.","offset":0,"pro":0,"labels":"MTD"},{"idx":18,"sentence":"We add a limited form of interaction for intent classification, where users provide an initial query using natural language, and the system asks for additional information using binary or multi-choice questions.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":18,"sentence":"At each turn, our system decides between asking the most informative question or making the final classification pre-diction.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":18,"sentence":"The simplicity of the model allows for bootstrapping of the system without interaction data, instead relying on simple crowd-sourcing tasks.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":18,"sentence":"We evaluate our approach on two domains, showing the benefit of interaction and the advantage of learning to balance between asking additional questions and making the final prediction.","offset":4,"pro":0.8,"labels":"RST"},{"idx":19,"sentence":"Knowledge graph (KG) representation learning techniques that learn continuous embeddings of entities and relations in the KG have become popular in many AI applications.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"With a large KG, the embeddings consume a large amount of storage and memory.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":19,"sentence":"This is problematic and prohibits the deployment of these techniques in many real world settings.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":19,"sentence":"Thus, we propose an approach that compresses the KG embedding layer by representing each entity in the KG as a vector of discrete codes and then composes the embeddings from these codes.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":19,"sentence":"The approach can be trained end-to-end with simple modifications to any existing KG embedding technique.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":19,"sentence":"We evaluate the approach on various standard KG embedding evaluations and show that it achieves 50-1000x compression of embeddings with a minor loss in performance.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":19,"sentence":"The compressed embeddings also retain the ability to perform various reasoning tasks such as KG inference.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":0,"sentence":"Extracting lexico-semantic relations as graph-structured taxonomies, also known as taxonomy construction, has been beneficial in a variety of NLP applications.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"Recently Graph Neural Network (GNN) has shown to be powerful in successfully tackling many tasks.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":0,"sentence":"However, there has been no attempt to exploit GNN to create taxonomies.","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":0,"sentence":"In this paper, we propose Graph2Taxo, a GNN-based cross-domain transfer framework for the taxonomy construction task.","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":0,"sentence":"Our main contribution is to learn the latent features of taxonomy construction from existing domains to guide the structure learning of an unseen domain.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":0,"sentence":"We also propose a novel method of directed acyclic graph (DAG) generation for taxonomy construction.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":0,"sentence":"Specifically, our proposed Graph2Taxo uses a noisy graph constructed from automatically extracted noisy hyponym hypernym candidate pairs, and a set of taxonomies for some known domains for training.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":0,"sentence":"The learned model is then used to generate taxonomy for a new unknown domain given a set of terms for that domain.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":0,"sentence":"Experiments on benchmark datasets from science and environment domains show that our approach attains significant improvements correspondingly over the state of the art.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":1,"sentence":"Pretraining NLP models with variants of Masked Language Model (MLM) objectives has recently led to a significant improvements on many tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"This paper examines the benefits of pretrained models as a function of the number of training samples used in the downstream task.","offset":1,"pro":0.25,"labels":"PUR"},{"idx":1,"sentence":"On several text classification tasks, we show that as the number of training examples grow into the millions, the accuracy gap between finetuning BERT-based model and training vanilla LSTM from scratch narrows to within 1%.","offset":2,"pro":0.5,"labels":"RST"},{"idx":1,"sentence":"Our findings indicate that MLM-based models might reach a diminishing return point as the supervised data size increases significantly.","offset":3,"pro":0.75,"labels":"RST"},{"idx":2,"sentence":"Cross-lingual word embeddings (CLWE) are often evaluated on bilingual lexicon induction (BLI).","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"Recent CLWE methods use linear projections, which underfit the training dictionary, to generalize on BLI.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":2,"sentence":"However, underfitting can hinder generalization to other downstream tasks that rely on words from the training dictionary.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":2,"sentence":"We address this limitation by retrofitting CLWE to the training dictionary, which pulls training translation pairs closer in the embedding space and overfits the training dictionary.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":2,"sentence":"This simple post-processing step often improves accuracy on two downstream tasks, despite lowering BLI test accuracy.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":2,"sentence":"We also retrofit to both the training dictionary and a synthetic dictionary induced from CLWE, which sometimes generalizes even better on downstream tasks.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":2,"sentence":"Our results confirm the importance of fully exploiting training dictionary in downstream tasks and explains why BLI is a flawed CLWE evaluation.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":3,"sentence":"Deep and large pre-trained language models are the state-of-the-art for various natural language processing tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"However, the huge size of these models could be a deterrent to using them in practice.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":3,"sentence":"Some recent works use knowledge distillation to compress these huge models into shallow ones.","offset":2,"pro":0.2857142857142857,"labels":"BAC"},{"idx":3,"sentence":"In this work we study knowledge distillation with a focus on multilingual Named Entity Recognition (NER).","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":3,"sentence":"In particular, we study several distillation strategies and propose a stage-wise optimization scheme leveraging teacher internal representations, that is agnostic of teacher architecture, and show that it outperforms strategies employed in prior works.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":3,"sentence":"Additionally, we investigate the role of several factors like the amount of unlabeled data, annotation resources, model architecture and inference latency to name a few.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":3,"sentence":"We show that our approach leads to massive compression of teacher models like mBERT by upto 35x in terms of parameters and 51x in terms of latency for batch inference while retaining 95% of its F1-score for NER over 41 languages.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":4,"sentence":"Authorship attribution aims to identify the author of a text based on the stylometric analysis.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"Authorship obfuscation, on the other hand, aims to protect against authorship attribution by modifying a text's style.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":4,"sentence":"In this paper, we evaluate the stealthiness of state-of-the-art authorship obfuscation methods under an adversarial threat model.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":4,"sentence":"An obfuscator is stealthy to the extent an adversary finds it challenging to detect whether or not a text modified by the obfuscator is obfuscated - a decision that is key to the adversary interested in authorship attribution.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":4,"sentence":"We show that the existing authorship obfuscation methods are not stealthy as their obfuscated texts can be identified with an average F1 score of 0.87.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":4,"sentence":"The reason for the lack of stealthiness is that these obfuscators degrade text smoothness, as ascertained by neural language models, in a detectable manner.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":4,"sentence":"Our results highlight the need to develop stealthy authorship obfuscation methods that can better protect the identity of an author seeking anonymity.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":5,"sentence":"Large-scale pre-trained language models such as BERT have brought significant improvements to NLP applications.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"However, they are also notorious for being slow in inference, which makes them difficult to deploy in real-time applications.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":5,"sentence":"We propose a simple but effective method, DeeBERT, to accelerate BERT inference.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":5,"sentence":"Our approach allows samples to exit earlier without passing through the entire model.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":5,"sentence":"Experiments show that DeeBERT is able to save up to asciitilde40% inference time with minimal degradation in model quality.","offset":4,"pro":0.5,"labels":"RST"},{"idx":5,"sentence":"Further analyses show different behaviors in the BERT transformer layers and also reveal their redundancy.","offset":5,"pro":0.625,"labels":"RST"},{"idx":5,"sentence":"Our work provides new ideas to efficiently apply deep transformer-based models to downstream tasks.","offset":6,"pro":0.75,"labels":"CTN"},{"idx":5,"sentence":"Code is available at https://github.com/castorini/DeeBERT.","offset":7,"pro":0.875,"labels":"CTN"},{"idx":6,"sentence":"In hierarchical text classification, we perform a sequence of inference steps to predict the category of a document from top to bottom of a given class taxonomy.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"Most of the studies have focused on developing novels neural network architectures to deal with the hierarchical structure, but we prefer to look for efficient ways to strengthen a baseline model.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":6,"sentence":"We first define the task as a sequence-to-sequence problem.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":6,"sentence":"Afterwards, we propose an auxiliary synthetic task of bottom-up-classification.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":6,"sentence":"Then, from external dictionaries, we retrieve textual definitions for the classes of all the hierarchy's layers, and map them into the word vector space.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":6,"sentence":"We use the class-definition embeddings as an additional input to condition the prediction of the next layer and in an adapted beam search.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":6,"sentence":"Whereas the modified search did not provide large gains, the combination of the auxiliary task and the additional input of class-definitions significantly enhance the classification accuracy.","offset":6,"pro":0.75,"labels":"RST"},{"idx":6,"sentence":"With our efficient approaches, we outperform previous studies, using a drastically reduced number of parameters, in two well-known English datasets.","offset":7,"pro":0.875,"labels":"RST"},{"idx":7,"sentence":"We address the task of automatically grading the language proficiency of spontaneous speech based on textual features from automatic speech recognition transcripts.","offset":0,"pro":0,"labels":"PUR"},{"idx":7,"sentence":"Motivated by recent advances in multi-task learning, we develop neural networks trained in a multi-task fashion that learn to predict the proficiency level of non-native English speakers by taking advantage of inductive transfer between the main task (grading) and auxiliary prediction tasks: morpho-syntactic labeling, language modeling, and native language identification (L1).","offset":1,"pro":0.2,"labels":"MTD"},{"idx":7,"sentence":"We encode the transcriptions with both bi-directional recurrent neural networks and with bi-directional representations from transformers, compare against a feature-rich baseline, and analyse performance at different proficiency levels and with transcriptions of varying error rates.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":7,"sentence":"Our best performance comes from a transformer encoder with L1 prediction as an auxiliary task.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":7,"sentence":"We discuss areas for improvement and potential applications for text-only speech scoring.","offset":4,"pro":0.8,"labels":"MTD"},{"idx":8,"sentence":"Representation learning is a critical ingredient for natural language processing systems.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":8,"sentence":"For applications on scientific documents, such as classification and recommendation, accurate embeddings of documents are a necessity.","offset":2,"pro":0.2857142857142857,"labels":"BAC"},{"idx":8,"sentence":"We propose SPECTER, a new method to generate document-level embedding of scientific papers based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":8,"sentence":"Unlike existing pretrained language models, Specter can be easily applied to downstream applications without task-specific fine-tuning.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":8,"sentence":"Additionally, to encourage further research on document-level models, we introduce SciDocs, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":8,"sentence":"We show that Specter outperforms a variety of competitive baselines on the benchmark.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":9,"sentence":"We propose a method for program generation based on semantic scaffolds, lightweight structures representing the high-level semantic and syntactic composition of a program.","offset":0,"pro":0,"labels":"PUR"},{"idx":9,"sentence":"By first searching over plausible scaffolds then using these as constraints for a beam search over programs, we achieve better coverage of the search space when compared with existing techniques.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":9,"sentence":"We apply our hierarchical search method to the SPoC dataset for pseudocode-to-code generation, in which we are given line-level natural language pseudocode annotations and aim to produce a program satisfying execution-based test cases.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":9,"sentence":"By using semantic scaffolds during inference, we achieve a 10% absolute improvement in top-100 accuracy over the previous state-of-the-art.","offset":3,"pro":0.6,"labels":"RST"},{"idx":9,"sentence":"Additionally, we require only 11 candidates to reach the top-3000 performance of the previous best approach when tested against unseen problems, demonstrating a substantial improvement in efficiency.","offset":4,"pro":0.8,"labels":"RST"},{"idx":10,"sentence":"Open Information Extraction systems extract (subject text, relation text, object text) triples from raw text.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"Some triples are textual versions of facts, i.e., non-canonicalized mentions of entities and relations.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":10,"sentence":"In this paper, we investigate whether it is possible to infer new facts directly from the open knowledge graph without any canonicalization or any supervision from curated knowledge.","offset":2,"pro":0.2222222222222222,"labels":"PUR"},{"idx":10,"sentence":"For this purpose, we propose the open link prediction task,i.e., predicting test facts by completing (subject text, relation text, ?) questions.","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":10,"sentence":"An evaluation in such a setup raises the question if a correct prediction is actually a new fact that was induced by reasoning over the open knowledge graph or if it can be trivially explained.","offset":4,"pro":0.4444444444444444,"labels":"GAP"},{"idx":10,"sentence":"For example, facts can appear in different paraphrased textual variants, which can lead to test leakage.","offset":5,"pro":0.5555555555555556,"labels":"GAP"},{"idx":10,"sentence":"To this end, we propose an evaluation protocol and a methodology for creating the open link prediction benchmark OlpBench.","offset":6,"pro":0.6666666666666666,"labels":"PUR"},{"idx":10,"sentence":"We performed experiments with a prototypical knowledge graph embedding model for openlink prediction.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":10,"sentence":"While the task is very challenging, our results suggests that it is possible to predict genuinely new facts, which can not be trivially explained.","offset":8,"pro":0.8888888888888888,"labels":"RST"},{"idx":11,"sentence":"In this paper, we observe that semi-structured tabulated text is ubiquitous; understanding them requires not only comprehending the meaning of text fragments, but also implicit relationships between them.","offset":0,"pro":0,"labels":"PUR"},{"idx":11,"sentence":"We argue that such data can prove as a testing ground for understanding how we reason about information.","offset":1,"pro":0.2,"labels":"CLN"},{"idx":11,"sentence":"To study this, we introduce a new dataset called INFOTABS, comprising of human-written textual hypotheses based on premises that are tables extracted from Wikipedia info-boxes.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":11,"sentence":"Our analysis shows that the semi-structured, multi-domain and heterogeneous nature of the premises admits complex, multi-faceted reasoning.","offset":3,"pro":0.6,"labels":"CLN"},{"idx":11,"sentence":"Experiments reveal that, while human annotators agree on the relationships between a table-hypothesis pair, several standard modeling strategies are unsuccessful at the task, suggesting that reasoning about tables can pose a difficult modeling challenge.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":12,"sentence":"Existing machine reading comprehension (MRC) models do not scale effectively to real-world applications like web-level information retrieval and question answering (QA).","offset":0,"pro":0,"labels":"GAP"},{"idx":12,"sentence":"We argue that this stems from the nature of MRC datasets: most of these are static environments wherein the supporting documents and all necessary information are fully observed.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":12,"sentence":"In this paper, we propose a simple method that reframes existing MRC datasets as interactive, partially observable environments.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":12,"sentence":"Specifically, we occlude the majority of a document's text and add context-sensitive commands that reveal glimpses of the hidden text to a model.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":12,"sentence":"We repurpose SQuAD and NewsQA as an initial case study, and then show how the interactive corpora can be used to train a model that seeks relevant information through sequential decision making.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":12,"sentence":"We believe that this setting can contribute in scaling models to web-level QA scenarios.","offset":5,"pro":0.8333333333333334,"labels":"CTN"},{"idx":13,"sentence":"Pretrained neural models such as BERT, when fine-tuned to perform natural language inference (NLI), often show high accuracy on standard datasets, but display a surprising lack of sensitivity to word order on controlled challenge sets.","offset":0,"pro":0,"labels":"GAP"},{"idx":13,"sentence":"We hypothesize that this issue is not primarily caused by the pretrained model's limitations, but rather by the paucity of crowdsourced NLI examples that might convey the importance of syntactic structure at the fine-tuning stage.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":13,"sentence":"We explore several methods to augment standard training sets with syntactically informative examples, generated by applying syntactic transformations to sentences from the MNLI corpus.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":13,"sentence":"The best-performing augmentation method, subject/object inversion, improved BERT's accuracy on controlled examples that diagnose sensitivity to word order from 0.28 to 0.73, without affecting performance on the MNLI test set.","offset":3,"pro":0.6,"labels":"RST"},{"idx":13,"sentence":"This improvement generalized beyond the particular construction used for data augmentation, suggesting that augmentation causes BERT to recruit abstract syntactic representations.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":14,"sentence":"Training objectives based on predictive coding have recently been shown to be very effective at learning meaningful representations from unlabeled speech.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"One example is Autoregressive Predictive Coding (Chung et al., 2019), which trains an autoregressive RNN to generate an unseen future frame given a context such as recent past frames.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":14,"sentence":"The basic hypothesis of these approaches is that hidden states that can accurately predict future frames are a useful representation for many downstream tasks.","offset":2,"pro":0.3333333333333333,"labels":"BAC"},{"idx":14,"sentence":"In this paper we extend this hypothesis and aim to enrich the information encoded in the hidden states by training the model to make more accurate future predictions.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":14,"sentence":"We propose an auxiliary objective that serves as a regularization to improve generalization of the future frame prediction task.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":14,"sentence":"Experimental results on phonetic classification, speech recognition, and speech translation not only support the hypothesis, but also demonstrate the effectiveness of our approach in learning representations that contain richer phonetic content.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":15,"sentence":"Recent Transformer-based contextual word representations, including BERT and XLNet, have shown state-of-the-art performance in multiple disciplines within NLP.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"Fine-tuning the trained contextual models on task-specific datasets has been the key to achieving superior performance downstream.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":15,"sentence":"While fine-tuning these pre-trained models is straightforward for lexical applications (applications with only language modality), it is not trivial for multimodal language (a growing area in NLP focused on modeling face-to-face communication).","offset":2,"pro":0.2,"labels":"GAP"},{"idx":15,"sentence":"More specifically, this is due to the fact that pre-trained models don't have the necessary components to accept two extra modalities of vision and acoustic.","offset":3,"pro":0.3,"labels":"GAP"},{"idx":15,"sentence":"In this paper, we proposed an attachment to BERT and XLNet called Multimodal Adaptation Gate (MAG).","offset":4,"pro":0.4,"labels":"PUR"},{"idx":15,"sentence":"MAG allows BERT and XLNet to accept multimodal nonverbal data during fine-tuning.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":15,"sentence":"It does so by generating a shift to internal representation of BERT and XLNet; a shift that is conditioned on the visual and acoustic modalities.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":15,"sentence":"In our experiments, we study the commonly used CMU-MOSI and CMU-MOSEI datasets for multimodal sentiment analysis.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":15,"sentence":"Fine-tuning MAG-BERT and MAG-XLNet significantly boosts the sentiment analysis performance over previous baselines as well as language-only fine-tuning of BERT and XLNet.","offset":8,"pro":0.8,"labels":"RST"},{"idx":15,"sentence":"On the CMU-MOSI dataset, MAG-XLNet achieves human-level multimodal sentiment analysis performance for the first time in the NLP community.","offset":9,"pro":0.9,"labels":"RST"},{"idx":16,"sentence":"We address a challenging and practical task of labeling questions in speech in real time during telephone calls to emergency medical services in English, which embeds within a broader decision support system for emergency call-takers.","offset":0,"pro":0,"labels":"PUR"},{"idx":16,"sentence":"We propose a novel multimodal approach to real-time sequence labeling in speech.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":16,"sentence":"Our model treats speech and its own textual representation as two separate modalities or views, as it jointly learns from streamed audio and its noisy transcription into text via automatic speech recognition.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":16,"sentence":"Our results show significant gains of jointly learning from the two modalities when compared to text or audio only, under adverse noise and limited volume of training data.","offset":3,"pro":0.6,"labels":"RST"},{"idx":16,"sentence":"The results generalize to medical symptoms detection where we observe a similar pattern of improvements with multimodal learning.","offset":4,"pro":0.8,"labels":"RST"},{"idx":17,"sentence":"This paper presents an audio visual automatic speech recognition (AV-ASR) system using a Transformer-based architecture.","offset":0,"pro":0,"labels":"PUR"},{"idx":17,"sentence":"We particularly focus on the scene context provided by the visual information, to ground the ASR.","offset":1,"pro":0.14285714285714285,"labels":"MTD"},{"idx":17,"sentence":"We extract representations for audio features in the encoder layers of the transformer and fuse video features using an additional crossmodal multihead attention layer.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":17,"sentence":"Additionally, we incorporate a multitask training criterion for multiresolution ASR, where we train the model to generate both character and subword level transcriptions.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":17,"sentence":"Experimental results on the How2 dataset, indicate that multiresolution training can speed up convergence by around 50% and relatively improves word error rate (WER) performance by upto 18% over subword prediction models.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":17,"sentence":"Further, incorporating visual information improves performance with relative gains upto 3.76% over audio only models.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":17,"sentence":"Our results are comparable to state-of-the-art Listen, Attend and Spell-based architectures.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":18,"sentence":"End-to-end models for speech translation (ST) more tightly couple speech recognition (ASR) and machine translation (MT) than a traditional cascade of separate ASR and MT models, with simpler model architectures and the potential for reduced error propagation.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"Their performance is often assumed to be superior, though in many conditions this is not yet the case.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":18,"sentence":"We compare cascaded and end-to-end models across high, medium, and low-resource conditions, and show that cascades remain stronger baselines.","offset":2,"pro":0.4,"labels":"RST"},{"idx":18,"sentence":"Further, we introduce two methods to incorporate phone features into ST models.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":18,"sentence":"We show that these features improve both architectures, closing the gap between end-to-end models and cascades, and outperforming previous academic work - by up to 9 BLEU on our low-resource setting.","offset":4,"pro":0.8,"labels":"RST"},{"idx":19,"sentence":"Effective dialogue involves grounding, the process of establishing mutual knowledge that is essential for communication between people.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"Modern dialogue systems are not explicitly trained to build common ground, and therefore overlook this important aspect of communication.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":19,"sentence":"Improvisational theater (improv) intrinsically contains a high proportion of dialogue focused on building common ground, and makes use of the yes-and principle, a strong grounding speech act, to establish coherence and an actionable objective reality.","offset":2,"pro":0.4,"labels":"BAC"},{"idx":19,"sentence":"We collect a corpus of more than 26,000 yes-and turns, transcribing them from improv dialogues and extracting them from larger, but more sparsely populated movie script dialogue corpora, via a bootstrapped classifier.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":19,"sentence":"We fine-tune chit-chat dialogue systems with our corpus to encourage more grounded, relevant conversation and confirm these findings with human evaluations.","offset":4,"pro":0.8,"labels":"MTD"},{"idx":0,"sentence":"Can artificial neural networks learn to represent inflectional morphology and generalize to new words as human speakers do?","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"Kirov and Cotterell (2018) argue that the answer is yes: modern Encoder-Decoder (ED) architectures learn human-like behavior when inflecting English verbs, such as extending the regular past tense form /-(e)d/ to novel words.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":0,"sentence":"However, their work does not address the criticism raised by Marcus et al. (1995): that neural models may learn to extend not the regular, but the most frequent class - and thus fail on tasks like German number inflection, where infrequent suffixes like /-s/ can still be productively generalized.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":0,"sentence":"The speaker data show high variability, and two suffixes evince regular' behavior, appearing more often with phonologically atypical inputs.","offset":3,"pro":0.42857142857142855,"labels":"RST"},{"idx":0,"sentence":"Encoder-decoder models do generalize the most frequently produced plural class, but do not show human-like variability or regular' extension of these other plural markers.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":0,"sentence":"We conclude that modern neural models may still struggle with minority-class generalization.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":0,"sentence":"To investigate this question, we first collect a new dataset from German speakers (production and ratings of plural forms for novel nouns) that is designed to avoid sources of information unavailable to the ED model.","offset":6,"pro":0.8571428571428571,"labels":"MTD"},{"idx":1,"sentence":"With the advent of powerful neural language models over the last few years, research attention has increasingly focused on what aspects of language they represent that make them so successful.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"Several testing methodologies have been developed to probe models' syntactic representations.","offset":1,"pro":0.25,"labels":"BAC"},{"idx":1,"sentence":"One popular method for determining a model's ability to induce syntactic structure trains a model on strings generated according to a template then tests the model's ability to distinguish such strings from superficially similar ones with different syntax.","offset":2,"pro":0.5,"labels":"BAC"},{"idx":1,"sentence":"We illustrate a fundamental problem with this approach by reproducing positive results from a recent paper with two non-syntactic baseline language models: an n-gram model and an LSTM model trained on scrambled inputs.","offset":3,"pro":0.75,"labels":"PUR"},{"idx":2,"sentence":"Suspense is a crucial ingredient of narrative fiction, engaging readers and making stories compelling.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"While there is a vast theoretical literature on suspense, it is computationally not well understood.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":2,"sentence":"Both can be computed either directly over story representations or over their probability distributions.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":2,"sentence":"We propose a hierarchical language model that encodes stories and computes surprise and uncertainty reduction.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":2,"sentence":"Evaluating against short stories annotated with human suspense judgements, we find that uncertainty reduction over representations is the best predictor, resulting in near human accuracy.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":2,"sentence":"We also show that uncertainty reduction can be used to predict suspenseful events in movie synopses.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":2,"sentence":"We compare two ways for modelling suspense: surprise, a backward-looking measure of how unexpected the current state is given the story so far; and uncertainty reduction, a forward-looking measure of how unexpected the continuation of the story is.","offset":6,"pro":0.8571428571428571,"labels":"PUR"},{"idx":3,"sentence":"Predicting reading time has been a subject of much previous work, focusing on how different words affect human processing, measured by reading time.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"However, previous work has dealt with a limited number of participants as well as word level only predictions (i.e. predicting the time to read a single word).","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":3,"sentence":"We seek to extend these works by examining whether or not document level predictions are effective, given additional information such as subject matter, font characteristics, and readability metrics.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":3,"sentence":"We perform a novel experiment to examine how different features of text contribute to the time it takes to read, distributing and collecting data from over a thousand participants.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":3,"sentence":"We then employ a large number of machine learning methods to predict a user's reading time.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":3,"sentence":"We find that despite extensive research showing that word level reading time can be most effectively predicted by neural networks, larger scale text can be easily and most accurately predicted by one factor, the number of words.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":4,"sentence":"Natural language understanding (NLU) and natural language generation (NLG) are two fundamental and related tasks in building task-oriented dialogue systems with opposite objectives: NLU tackles the transformation from natural language to formal representations, whereas NLG does the reverse.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"A key to success in either task is parallel training data which is expensive to obtain at a large scale.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":4,"sentence":"In this work, we propose a generative model which couples NLU and NLG through a shared latent variable.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":4,"sentence":"Our model achieves state-of-the-art performance on two dialogue datasets with both flat and tree-structured formal representations.","offset":3,"pro":0.5,"labels":"RST"},{"idx":4,"sentence":"We also show that the model can be trained in a semi-supervised fashion by utilising unlabelled data to boost its performance.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":4,"sentence":"This approach allows us to explore both spaces of natural language and formal representations, and facilitates information sharing through the latent space to eventually benefit NLU and NLG. ","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":5,"sentence":"Recent advancements in neural language modelling make it possible to rapidly generate vast amounts of human-sounding text.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"The capabilities of humans and automatic discriminators to detect machine-generated text have been a large source of research interest, but humans and machines rely on different cues to make their decisions.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":5,"sentence":"We also show that though both human and automatic detector performance improve with longer excerpt length, even multi-sentence excerpts can fool expert human raters over 30% of the time.","offset":2,"pro":0.3333333333333333,"labels":"RST"},{"idx":5,"sentence":"Our findings reveal the importance of using both human and automatic detectors to assess the humanness of text generation systems.","offset":3,"pro":0.5,"labels":"CLN"},{"idx":5,"sentence":"Here, we perform careful benchmarking and analysis of three popular sampling-based decoding strategies-top-{_k{_, nucleus sampling, and untruncated random sampling-and show that improvements in decoding methods have primarily optimized for fooling humans.","offset":4,"pro":0.6666666666666666,"labels":"PUR"},{"idx":5,"sentence":"This comes at the expense of introducing statistical abnormalities that make detection easy for automatic systems. ","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":6,"sentence":"Many multi-domain neural machine translation (NMT) models achieve knowledge transfer by enforcing one encoder to learn shared embedding across domains.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"However, this design lacks adaptation to individual domains.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":6,"sentence":"To overcome this limitation, we propose a novel multi-domain NMT model using individual modules for each domain, on which we apply word-level, adaptive and layer-wise domain mixing.","offset":2,"pro":0.2222222222222222,"labels":"PUR"},{"idx":6,"sentence":"We first observe that words in a sentence are often related to multiple domains.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":6,"sentence":"Hence, we assume each word has a domain proportion, which indicates its domain preference.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":6,"sentence":"Then word representations are obtained by mixing their embedding in individual domains based on their domain proportions.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":6,"sentence":"We show this can be achieved by carefully designing multi-head dot-product attention modules for different domains, and eventually taking weighted averages of their parameters by word-level layer-wise domain proportions.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":6,"sentence":"Through this, we can achieve effective domain knowledge sharing and capture fine-grained domain-specific knowledge as well.","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":6,"sentence":"Our experiments show that our proposed model outperforms existing ones in several NMT tasks.","offset":8,"pro":0.8888888888888888,"labels":"RST"},{"idx":7,"sentence":"To address the challenge of policy learning in open-domain multi-turn conversation, we propose to represent prior information about dialog transitions as a graph and learn a graph grounded dialog policy, aimed at fostering a more coherent and controllable dialog.","offset":0,"pro":0,"labels":"PUR"},{"idx":7,"sentence":"To this end, we first construct a conversational graph (CG) from dialog corpora, in which there are vertices to represent what to say and how to say, and edges to represent natural transition between a message (the last utterance in a dialog context) and its response.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":7,"sentence":"We then present a novel CG grounded policy learning framework that conducts dialog flow planning by graph traversal, which learns to identify a what-vertex and a how-vertex from the CG at each turn to guide response generation.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":7,"sentence":"In this way, we effectively leverage the CG to facilitate policy learning as follows: (1) it enables more effective long-term reward design, (2) it provides high-quality candidate actions, and (3) it gives us more control over the policy.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":7,"sentence":"Results on two benchmark corpora demonstrate the effectiveness of this framework.","offset":4,"pro":0.8,"labels":"RST"},{"idx":8,"sentence":"Abstract Meaning Representations (AMRs) are broad-coverage sentence-level semantic graphs.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"Existing approaches to generating text from AMR have focused on training sequence-to-sequence or graph-to-sequence models on AMR annotated data only.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":8,"sentence":"In this paper, we propose an alternative approach that combines a strong pre-trained language model with cycle consistency-based re-scoring.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":8,"sentence":"Despite the simplicity of the approach, our experimental results show these models outperform all previous techniques on the English LDC2017T10 dataset, including the recent use of transformer architectures.","offset":3,"pro":0.6,"labels":"RST"},{"idx":8,"sentence":"In addition to the standard evaluation metrics, we provide human evaluation experiments that further substantiate the strength of our approach.","offset":4,"pro":0.8,"labels":"RST"},{"idx":9,"sentence":"We formulate the novel task of automatically updating an existing natural language comment based on changes in the body of code it accompanies.","offset":0,"pro":0,"labels":"PUR"},{"idx":9,"sentence":"We propose an approach that learns to correlate changes across two distinct language representations, to generate a sequence of edits that are applied to the existing comment to reflect the source code modifications.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":9,"sentence":"We train and evaluate our model using a dataset that we collected from commit histories of open-source software projects, with each example consisting of a concurrent update to a method and its corresponding comment.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":9,"sentence":"We compare our approach against multiple baselines using both automatic metrics and human evaluation.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":9,"sentence":"Results reflect the challenge of this task and that our model outperforms baselines with respect to making edits.","offset":4,"pro":0.8,"labels":"RST"},{"idx":10,"sentence":"This paper introduces a new task of politeness transfer which involves converting non-polite sentences to polite sentences while preserving the meaning.","offset":0,"pro":0,"labels":"PUR"},{"idx":10,"sentence":"We design a tag and generate pipeline that identifies stylistic attributes and subsequently generates a sentence in the target style while preserving most of the source content.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":10,"sentence":"For politeness as well as five other transfer tasks, our model outperforms the state-of-the-art methods on automatic metrics for content preservation, with a comparable or better performance on style transfer accuracy.","offset":2,"pro":0.3333333333333333,"labels":"RST"},{"idx":10,"sentence":"Additionally, our model surpasses existing methods on human evaluations for grammaticality, meaning preservation and transfer accuracy across all the six style transfer tasks.","offset":3,"pro":0.5,"labels":"RST"},{"idx":10,"sentence":"The data and code is located at https://github.com/tag-and-generate.","offset":4,"pro":0.6666666666666666,"labels":"CTN"},{"idx":10,"sentence":"We also provide a dataset of more than 1.39 instances automatically labeled for politeness to encourage benchmark evaluations on this new task.","offset":5,"pro":0.8333333333333334,"labels":"PUR"},{"idx":11,"sentence":"Subword segmentation is widely used to address the open vocabulary problem in machine translation.","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"The dominant approach to subword segmentation is Byte Pair Encoding (BPE), which keeps the most frequent words intact while splitting the rare ones into multiple tokens.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":11,"sentence":"While multiple segmentations are possible even with the same vocabulary, BPE splits words into unique sequences; this may prevent a model from better learning the compositionality of words and being robust to segmentation errors.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":11,"sentence":"So far, the only way to overcome this BPE imperfection, its deterministic nature, was to create another subword segmentation algorithm (Kudo, 2018).","offset":3,"pro":0.375,"labels":"BAC"},{"idx":11,"sentence":"We introduce BPE-dropout - simple and effective subword regularization method based on and compatible with conventional BPE.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":11,"sentence":"It stochastically corrupts the segmentation procedure of BPE, which leads to producing multiple segmentations within the same fixed BPE framework.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":11,"sentence":"Using BPE-dropout during training and the standard BPE during inference improves translation quality up to 2.3 BLEU compared to BPE and up to 0.9 BLEU compared to the previous subword regularization.","offset":6,"pro":0.75,"labels":"RST"},{"idx":11,"sentence":"In contrast, we show that BPE itself incorporates the ability to produce multiple segmentations of the same word. ","offset":7,"pro":0.875,"labels":"PUR"},{"idx":12,"sentence":"Non-autoregressive (NAR) neural machine translation is usually done via knowledge distillation from an autoregressive (AR) model.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"On top of a strong NAR baseline, our experimental results on the WMT14 En-De and WMT16 En-Ro news translation tasks confirm that monolingual data augmentation consistently improves the performance of the NAR model to approach the teacher AR model's performance, yields comparable or better results than the best non-iterative NAR methods in the literature and helps reduce overfitting in the training process.","offset":1,"pro":0.3333333333333333,"labels":"RST"},{"idx":12,"sentence":"Under this framework, we leverage large monolingual corpora to improve the NAR model's performance, with the goal of transferring the AR model's generalization ability while preventing overfitting. ","offset":2,"pro":0.6666666666666666,"labels":"PUR"},{"idx":13,"sentence":"Sequence-to-sequence (seq2seq) network is a well-established model for text summarization task.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"In this paper, we approach the content selection problem for clinical abstractive summarization by augmenting salient ontological terms into the summarizer.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":13,"sentence":"Our experiments on two publicly available clinical data sets (107,372 reports of MIMIC-CXR, and 3,366 reports of OpenI) show that our model statistically significantly boosts state-of-the-art results in terms of ROUGE metrics (with improvements: 2.9% RG-1, 2.5% RG-2, 1.9% RG-L), in the healthcare domain where any range of improvement impacts patients' welfare.","offset":2,"pro":0.4,"labels":"RST"},{"idx":13,"sentence":"It can learn to produce readable content;","offset":3,"pro":0.6,"labels":"BAC"},{"idx":13,"sentence":"however, it falls short in effectively identifying key regions of the source.","offset":4,"pro":0.8,"labels":"GAP"},{"idx":14,"sentence":"It is well known that the standard likelihood training and approximate decoding objectives in neural text generation models lead to less human-like responses for open-ended tasks such as language modeling and story generation.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"In this paper we have analyzed limitations of these models for abstractive document summarization and found that these models are highly prone to hallucinate content that is unfaithful to the input document.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":14,"sentence":"We conducted a large scale human evaluation of several neural abstractive summarization systems to better understand the types of hallucinations they produce.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":14,"sentence":"Our human annotators found substantial amounts of hallucinated content in all model generated summaries.","offset":3,"pro":0.5,"labels":"RST"},{"idx":14,"sentence":"However, our analysis does show that pretrained models are better summarizers not only in terms of raw metrics, i.e., ROUGE, but also in generating faithful and factual summaries as evaluated by humans.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":14,"sentence":"Furthermore, we show that textual entailment measures better correlate with faithfulness than standard metrics, potentially leading the way to automatic evaluation metrics as well as training and decoding criteria.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":15,"sentence":"Most general-purpose extractive summarization models are trained on news articles, which are short and present all important information upfront.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"When summarizing long narratives, which have complex structure and present information piecemeal, simple position heuristics are not sufficient.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":15,"sentence":"In this paper, we propose to explicitly incorporate the underlying structure of narratives into general unsupervised and supervised extractive summarization models.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":15,"sentence":"We formalize narrative structure in terms of key narrative events (turning points) and treat it as latent in order to summarize screenplays (i.e., extract an optimal sequence of scenes).","offset":3,"pro":0.5,"labels":"MTD"},{"idx":15,"sentence":"Experimental results on the CSI corpus of TV screenplays, which we augment with scene-level summarization labels, show that latent turning points correlate with important aspects of a CSI episode and improve summarization performance over general extractive algorithms leading to more complete and diverse summaries.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":15,"sentence":"As a result, such models are biased on position and often perform a smart selection of sentences from the beginning of the document.","offset":5,"pro":0.8333333333333334,"labels":"BAC"},{"idx":16,"sentence":"The supervised training of high-capacity models on large datasets containing hundreds of thousands of document-summary pairs is critical to the recent success of deep learning techniques for abstractive summarization.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"Unfortunately, in most domains (other than news) such training data is not available and cannot be easily sourced.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":16,"sentence":"In this paper we enable the use of supervised learning for the setting where there are only documents available (e.g., product or business reviews) without ground truth summaries.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":16,"sentence":"We create a synthetic dataset from a corpus of user reviews by sampling a review, pretending it is a summary, and generating noisy versions thereof which we treat as pseudo-review input.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":16,"sentence":"We introduce several linguistically motivated noise generation functions and a summarization model which learns to denoise the input and generate the original review.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":16,"sentence":"At test time, the model accepts genuine reviews and generates a summary containing salient opinions, treating those that do not reach consensus as noise.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":16,"sentence":"Extensive automatic and human evaluation shows that our model brings substantial improvements over both abstractive and extractive baselines.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":17,"sentence":"In recent years there has been a burgeoning interest in the use of computational methods to distinguish between elicited speech samples produced by patients with dementia, and those from healthy controls.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"The difference between perplexity estimates from two neural language models (LMs) - one trained on transcripts of speech produced by healthy participants and one trained on those with dementia - as a single feature for diagnostic classification of unseen transcripts has been shown to produce state-of-the-art performance.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":17,"sentence":"However, little is known about why this approach is effective, and on account of the lack of case/control matching in the most widely-used evaluation set of transcripts (DementiaBank), it is unclear if these approaches are truly diagnostic, or are sensitive to other variables.","offset":2,"pro":0.4,"labels":"GAP"},{"idx":17,"sentence":"In this paper, we interrogate neural LMs trained on participants with and without dementia by using synthetic narratives previously developed to simulate progressive semantic dementia by manipulating lexical frequency.","offset":3,"pro":0.6,"labels":"PUR"},{"idx":17,"sentence":"We find that perplexity of neural LMs is strongly and differentially associated with lexical frequency, and that using a mixture model resulting from interpolating control and dementia LMs improves upon the current state-of-the-art for models trained on transcript text exclusively.","offset":4,"pro":0.8,"labels":"RST"},{"idx":18,"sentence":"Recently, there has been much interest in the question of whether deep natural language understanding (NLU) models exhibit systematicity, generalizing such that units like words make consistent contributions to the meaning of the sentences in which they appear.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"There is accumulating evidence that neural models do not learn systematically.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":18,"sentence":"We also identify ways in which network architectures can generalize non-systematically, and discuss why such forms of generalization may be unsatisfying.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":18,"sentence":"As a case study, we perform a series of experiments in the setting of natural language inference (NLI).","offset":3,"pro":0.5,"labels":"MTD"},{"idx":18,"sentence":"We provide evidence that current state-of-the-art NLU systems do not generalize systematically, despite overall high performance.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":18,"sentence":"We examine the notion of systematicity from a linguistic perspective, defining a set of probing tasks and a set of metrics to measure systematic behaviour.","offset":5,"pro":0.8333333333333334,"labels":"PUR"},{"idx":19,"sentence":"To facilitate this, we collect and release Hippocorpus, a dataset of 7,000 stories about imagined and recalled events.","offset":0,"pro":0,"labels":"MTD"},{"idx":19,"sentence":"We introduce a measure of narrative flow and use this to examine the narratives for imagined and recalled events.","offset":1,"pro":0.14285714285714285,"labels":"MTD"},{"idx":19,"sentence":"Additionally, we measure the differential recruitment of knowledge attributed to semantic memory versus episodic memory (Tulving, 1972) for imagined and recalled storytelling by comparing the frequency of descriptions of general commonsense events with more specific realis events.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":19,"sentence":"Our analyses show that imagined stories have a substantially more linear narrative flow, compared to recalled stories in which adjacent sentences are more disconnected.","offset":3,"pro":0.42857142857142855,"labels":"RST"},{"idx":19,"sentence":"Finally, our measures reveal the effect of narrativization of memories in stories (e.g., stories about frequently recalled memories flow more linearly; Bartlett, 1932).","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":19,"sentence":"In addition, while recalled stories rely more on autobiographical events based on episodic memory, imagined stories express more commonsense knowledge based on semantic memory.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":19,"sentence":"Our findings highlight the potential of using NLP tools to study the traces of human cognition in language. ","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":0,"sentence":"A standard approach to evaluating language models analyzes how models assign probabilities to valid versus invalid syntactic constructions (i.e. is a grammatical sentence more probable than an ungrammatical sentence).","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"Our work uses ambiguous relative clause attachment to extend such evaluations to cases of multiple simultaneous valid interpretations, where stark grammaticality differences are absent.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":0,"sentence":"We compare model performance in English and Spanish to show that non-linguistic biases in RNN LMs advantageously overlap with syntactic structure in English but not Spanish.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":0,"sentence":"Thus, English models may appear to acquire human-like syntactic preferences, while models trained on Spanish fail to acquire comparable human-like preferences.","offset":3,"pro":0.6,"labels":"RST"},{"idx":0,"sentence":"We conclude by relating these results to broader concerns about the relationship between comprehension (i.e. typical language model use cases) and production (which generates the training data for language models), suggesting that necessary linguistic biases are not present in the training signal at all.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":1,"sentence":"Recent work has found evidence that natural languages are shaped by pressures for efficient communication - e.g. the more contextually predictable a word is, the fewer speech sounds or syllables it has (Piantadosi et al. 2011).","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"Research on the degree to which speech and language are shaped by pressures for effective communication - robustness in the face of noise and uncertainty - has been more equivocal.","offset":1,"pro":0.25,"labels":"GAP"},{"idx":1,"sentence":"We develop a measure of contextual confusability during word recognition based on psychoacoustic data.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":1,"sentence":"Applying this measure to naturalistic speech corpora, we find evidence suggesting that speakers alter their productions to make contextually more confusable words easier to understand.","offset":3,"pro":0.75,"labels":"RST"},{"idx":2,"sentence":"We take up the scientific question of what determines the preferred order of adjectives in English, in phrases such as big blue box where multiple adjectives modify a following noun.","offset":0,"pro":0,"labels":"PUR"},{"idx":2,"sentence":"We implement and test four quantitative theories, all of which are theoretically motivated in terms of efficiency in human language production and comprehension.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":2,"sentence":"The four theories we test are subjectivity (Scontras et al., 2017), information locality (Futrell, 2019), integration cost (Dyer, 2017), and information gain, which we introduce.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":2,"sentence":"We evaluate theories based on their ability to predict orders of unseen adjectives in hand-parsed and automatically-parsed dependency treebanks.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":2,"sentence":"We find that subjectivity, information locality, and information gain are all strong predictors, with some evidence for a two-factor account, where subjectivity and information gain reflect a factor involving semantics, and information locality reflects collocational preferences.","offset":4,"pro":0.8,"labels":"RST"},{"idx":3,"sentence":"This paper discusses the importance of uncovering uncertainty in end-to-end dialog tasks and presents our experimental results on uncertainty classification on the processed Ubuntu Dialog Corpus.","offset":0,"pro":0,"labels":"PUR"},{"idx":3,"sentence":"We show that instead of retraining models for this specific purpose, we can capture the original retrieval model's underlying confidence concerning the best prediction using trivial additional computation.","offset":1,"pro":0.5,"labels":"RST"},{"idx":4,"sentence":"Being engaging, knowledgeable, and empathetic are all desirable general qualities in a conversational agent.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"Previous work has introduced tasks and datasets that aim to help agents to learn those qualities in isolation and gauge how well they can express them.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":4,"sentence":"But rather than being specialized in one single quality, a good open-domain conversational agent should be able to seamlessly blend them all into one cohesive conversational flow.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":4,"sentence":"In this work, we investigate several ways to combine models trained towards isolated capabilities, ranging from simple model aggregation schemes that require minimal additional training, to various forms of multi-task training that encompass several skills at all training stages.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":4,"sentence":"We further propose a new dataset, BlendedSkillTalk, to analyze how these capabilities would mesh together in a natural conversation, and compare the performance of different architectures and training schemes.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":4,"sentence":"Our experiments show that multi-tasking over several tasks that focus on particular capabilities results in better blended conversation performance compared to models trained on a single skill, and that both unified or two-stage approaches perform well if they are constructed to avoid unwanted bias in skill selection or are fine-tuned on our new task.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":5,"sentence":"Human conversations naturally evolve around related concepts and hop to distant concepts.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"This paper presents a new conversation generation model, ConceptFlow, which leverages commonsense knowledge graphs to explicitly model conversation flows.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":5,"sentence":"By grounding conversations to the concept space, ConceptFlow represents the potential conversation flow as traverses in the concept space along commonsense relations.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":5,"sentence":"The traverse is guided by graph attentions in the concept graph, moving towards more meaningful directions in the concept space, in order to generate more semantic and informative responses.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":5,"sentence":"Experiments on Reddit conversations demonstrate ConceptFlow's effectiveness over previous knowledge-aware conversation models and GPT-2 based models while using 70% fewer parameters, confirming the advantage of explicit modeling conversation structures.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":5,"sentence":"All source codes of this work are available at https://github.com/thunlp/ConceptFlow.","offset":5,"pro":0.8333333333333334,"labels":"CTN"},{"idx":6,"sentence":"Although deep learning models have brought tremendous advancements to the field of open-domain dialogue response generation, recent research results have revealed that the trained models have undesirable generation behaviors, such as malicious responses and generic (boring) responses.","offset":0,"pro":0,"labels":"GAP"},{"idx":6,"sentence":"In this work, we propose a framework named Negative Training to minimize such behaviors.","offset":1,"pro":0.25,"labels":"PUR"},{"idx":6,"sentence":"Given a trained model, the framework will first find generated samples that exhibit the undesirable behavior, and then use them to feed negative training signals for fine-tuning the model.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":6,"sentence":"Our experiments show that negative training can significantly reduce the hit rate of malicious responses, or discourage frequent responses and improve response diversity.","offset":3,"pro":0.75,"labels":"RST"},{"idx":7,"sentence":"The Natural Language Understanding (NLU) component in task oriented dialog systems processes a user's request and converts it into structured information that can be consumed by downstream components such as the Dialog State Tracker (DST).","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"This information is typically represented as a semantic frame that captures the intent and slot-labels provided by the user.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":7,"sentence":"We first show that such a shallow representation is insufficient for complex dialog scenarios, because it does not capture the recursive nature inherent in many domains.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":7,"sentence":"We propose a recursive, hierarchical frame-based representation and show how to learn it from data.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":7,"sentence":"We formulate the frame generation task as a template-based tree decoding task, where the decoder recursively generates a template and then fills slot values into the template.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":7,"sentence":"We extend local tree-based loss functions with terms that provide global supervision and show how to optimize them end-to-end.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":7,"sentence":"We achieve a small improvement on the widely used ATIS dataset and a much larger improvement on a more complex dataset we describe here.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":8,"sentence":"We study the task of semantic parse correction with natural language feedback.","offset":0,"pro":0,"labels":"CLN"},{"idx":8,"sentence":"Given a natural language utterance, most semantic parsing systems pose the problem as one-shot translation where the utterance is mapped to a corresponding logical form.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":8,"sentence":"In this paper, we investigate a more interactive scenario where humans can further interact with the system by providing free-form natural language feedback to correct the system when it generates an inaccurate interpretation of an initial utterance.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":8,"sentence":"We focus on natural language to SQL systems and construct, SPLASH, a dataset of utterances, incorrect SQL interpretations and the corresponding natural language feedback.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":8,"sentence":"We compare various reference models for the correction task and show that incorporating such a rich form of feedback can significantly improve the overall semantic parsing accuracy while retaining the flexibility of natural language interaction.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":8,"sentence":"While we estimated human correction accuracy is 81.5%, our best model achieves only 25.1%, which leaves a large gap for improvement in future research.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":8,"sentence":"SPLASH is publicly available at https://aka.ms/Splash{_dataset.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":9,"sentence":"We address the problem of calibrating prediction confidence for output entities of interest in natural language processing (NLP) applications.","offset":0,"pro":0,"labels":"PUR"},{"idx":9,"sentence":"It is important that NLP applications such as named entity recognition and question answering produce calibrated confidence scores for their predictions, especially if the applications are to be deployed in a safety-critical domain such as healthcare.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":9,"sentence":"However the output space of such structured prediction models are often too large to directly adapt binary or multi-class calibration methods.","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":9,"sentence":"In this study, we propose a general calibration scheme for output entities of interest in neural network based structured prediction models.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":9,"sentence":"Our proposed method can be used with any binary class calibration scheme and a neural network model.","offset":4,"pro":0.4444444444444444,"labels":"RST"},{"idx":9,"sentence":"Additionally, we show that our calibration method can also be used as an uncertainty-aware, entity-specific decoding step to improve the performance of the underlying model at no additional training cost or data requirements.","offset":5,"pro":0.5555555555555556,"labels":"RST"},{"idx":9,"sentence":"We show that our method outperforms current calibration techniques for Named Entity Recognition, Part-of-speech tagging and Question Answering systems.","offset":6,"pro":0.6666666666666666,"labels":"RST"},{"idx":9,"sentence":"We also observe an improvement in model performance from our decoding step across several tasks and benchmark datasets.","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":9,"sentence":"Our method improves the calibration and model performance on out-of-domain test scenarios as well.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":10,"sentence":"Imitation learning algorithms provide state-of-the-art results on many structured prediction tasks by learning near-optimal search policies.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"Such algorithms assume training-time access to an expert that can provide the optimal action at any queried state; unfortunately, the number of such queries is often prohibitive, frequently rendering these approaches impractical.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":10,"sentence":"To combat this query complexity, we consider an active learning setting in which the learning algorithm has additional access to a much cheaper noisy heuristic that provides noisy guidance.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":10,"sentence":"Our algorithm, LEAQI, learns a difference classifier that predicts when the expert is likely to disagree with the heuristic, and queries the expert only when necessary.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":10,"sentence":"We apply LEAQI to three sequence labelling tasks, demonstrating significantly fewer queries to the expert and comparable (or better) accuracies over a passive approach.","offset":4,"pro":0.8,"labels":"RST"},{"idx":11,"sentence":"Suppose we want to specify the inductive bias that married couples typically go on honeymoons for the task of extracting pairs of spouses from text.","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"In this paper, we allow model developers to specify these types of inductive biases as natural language explanations.","offset":1,"pro":0.25,"labels":"PUR"},{"idx":11,"sentence":"We use BERT fine-tuned on MultiNLI to interpret these explanations with respect to the input sentence, producing explanation-guided representations of the input.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":11,"sentence":"Across three relation extraction tasks, our method, ExpBERT, matches a BERT baseline but with 3-20x less labeled data and improves on the baseline by 3-10 F1 points with the same amount of labeled data.","offset":3,"pro":0.75,"labels":"RST"},{"idx":12,"sentence":"Recent Transformer-based architectures, e.g., BERT, provide impressive results in many Natural Language Processing tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"However, most of the adopted benchmarks are made of (sometimes hundreds of) thousands of examples.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":12,"sentence":"In many real scenarios, obtaining high- quality annotated data is expensive and time consuming; in contrast, unlabeled examples characterizing the target task can be, in general, easily collected.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":12,"sentence":"One promising method to enable semi-supervised learning has been proposed in image processing, based on Semi- Supervised Generative Adversarial Networks.","offset":3,"pro":0.5,"labels":"CLN"},{"idx":12,"sentence":"In this paper, we propose GAN-BERT that ex- tends the fine-tuning of BERT-like architectures with unlabeled data in a generative adversarial setting.","offset":4,"pro":0.6666666666666666,"labels":"PUR"},{"idx":12,"sentence":"Experimental results show that the requirement for annotated examples can be drastically reduced (up to only 50-100 annotated examples), still obtaining good performances in several sentence classification tasks.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":13,"sentence":"Natural language processing covers a wide variety of tasks predicting syntax, semantics, and information content, and usually each type of output is generated with specially designed architectures.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"In this paper, we provide the simple insight that a great variety of tasks can be represented in a single unified format consisting of labeling spans and relations between spans, thus a single task-independent model can be used across different tasks.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":13,"sentence":"We perform extensive experiments to test this insight on 10 disparate tasks spanning dependency parsing (syntax), semantic role labeling (semantics), relation extraction (information content), aspect based sentiment analysis (sentiment), and many others, achieving performance comparable to state-of-the-art specialized models.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":13,"sentence":"We further demonstrate benefits of multi-task learning, and also show that the proposed method makes it easy to analyze differences and similarities in how the model handles different tasks.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":13,"sentence":"Finally, we convert these datasets into a unified format to build a benchmark, which provides a holistic testbed for evaluating future models for generalized natural language analysis.","offset":4,"pro":0.8,"labels":"MTD"},{"idx":14,"sentence":"Sequence labeling is a fundamental task for a range of natural language processing problems.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"When used in practice, its performance is largely influenced by the annotation quality and quantity, and meanwhile, obtaining ground truth labels is often costly.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":14,"sentence":"In many cases, ground truth labels do not exist, but noisy annotations or annotations from different domains are accessible.","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":14,"sentence":"In this paper, we propose a novel framework Consensus Network (ConNet) that can be trained on annotations from multiple sources (e.g., crowd annotation, cross-domain data).","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":14,"sentence":"It learns individual representation for every source and dynamically aggregates source-specific knowledge by a context-aware attention module.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":14,"sentence":"Finally, it leads to a model reflecting the agreement (consensus) among multiple sources.","offset":5,"pro":0.5555555555555556,"labels":"RST"},{"idx":14,"sentence":"We evaluate the proposed framework in two practical settings of multi-source learning: learning with crowd annotations and unsupervised cross-domain model adaptation.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":14,"sentence":"Extensive experimental results show that our model achieves significant improvements over existing methods in both settings.","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":14,"sentence":"We also demonstrate that the method can apply to various tasks and cope with different encoders.","offset":8,"pro":0.8888888888888888,"labels":"RST"},{"idx":15,"sentence":"This paper presents MixText, a semi-supervised learning method for text classification, which uses our newly designed data augmentation method called TMix.","offset":0,"pro":0,"labels":"PUR"},{"idx":15,"sentence":"TMix creates a large amount of augmented training samples by interpolating text in hidden space.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":15,"sentence":"Moreover, we leverage recent advances in data augmentation to guess low-entropy labels for unlabeled data, hence making them as easy to use as labeled data.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":15,"sentence":"By mixing labeled, unlabeled and augmented data, MixText significantly outperformed current pre-trained and fined-tuned models and other state-of-the-art semi-supervised learning methods on several text classification benchmarks.","offset":3,"pro":0.5,"labels":"RST"},{"idx":15,"sentence":"The improvement is especially prominent when supervision is extremely limited.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":15,"sentence":"We have publicly released our code at https://github.com/GT-SALT/MixText.","offset":5,"pro":0.8333333333333334,"labels":"CTN"},{"idx":16,"sentence":"Natural Language Processing (NLP) has recently achieved great success by using huge pre-trained models with hundreds of millions of parameters.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"However, these models suffer from heavy model sizes and high latency such that they cannot be deployed to resource-limited mobile devices.","offset":1,"pro":0.1,"labels":"GAP"},{"idx":16,"sentence":"In this paper, we propose MobileBERT for compressing and accelerating the popular BERT model.","offset":2,"pro":0.2,"labels":"PUR"},{"idx":16,"sentence":"Like the original BERT, MobileBERT is task-agnostic, that is, it can be generically applied to various downstream NLP tasks via simple fine-tuning.","offset":3,"pro":0.3,"labels":"MTD"},{"idx":16,"sentence":"Basically, MobileBERT is a thin version of BERT{_LARGE, while equipped with bottleneck structures and a carefully designed balance between self-attentions and feed-forward networks.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":16,"sentence":"To train MobileBERT, we first train a specially designed teacher model, an inverted-bottleneck incorporated BERT{_LARGE model.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":16,"sentence":"Then, we conduct knowledge transfer from this teacher to MobileBERT.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":16,"sentence":"Empirical studies show that MobileBERT is 4.3x smaller and 5.5x faster than BERT{_BASE while achieving competitive results on well-known benchmarks.","offset":7,"pro":0.7,"labels":"RST"},{"idx":16,"sentence":"On the natural language inference tasks of GLUE, MobileBERT achieves a GLUE score of 77.7 (0.6 lower than BERT{_BASE), and 62 ms latency on a Pixel 4 phone.","offset":8,"pro":0.8,"labels":"RST"},{"idx":16,"sentence":"On the SQuAD v1.1/v2.0 question answering task, MobileBERT achieves a dev F1 score of 90.0/79.2 (1.5/2.1 higher than BERT{_BASE).","offset":9,"pro":0.9,"labels":"RST"},{"idx":17,"sentence":"Language models that use additional latent structures (e.g., syntax trees, coreference chains, knowledge graph links) provide several advantages over traditional language models.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"However, likelihood-based evaluation of these models is often intractable as it requires marginalizing over the latent space.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":17,"sentence":"Existing works avoid this issue by using importance sampling.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":17,"sentence":"Although this approach has asymptotic guarantees, analysis is rarely conducted on the effect of decisions such as sample size and choice of proposal distribution on the reported estimates.","offset":3,"pro":0.5,"labels":"GAP"},{"idx":17,"sentence":"In this paper, we carry out this analysis for three models: RNNG, EntityNLM, and KGLM.","offset":4,"pro":0.6666666666666666,"labels":"PUR"},{"idx":17,"sentence":"In addition, we elucidate subtle differences in how importance sampling is applied in these works that can have substantial effects on the final estimates, as well as provide theoretical results which reinforce the validity of this technique.","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":18,"sentence":"Transfer learning has fundamentally changed the landscape of natural language processing (NLP).","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"Many state-of-the-art models are first pre-trained on a large text corpus and then fine-tuned on downstream tasks.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":18,"sentence":"However, due to limited data resources from downstream tasks and the extremely high complexity of pre-trained models, aggressive fine-tuning often causes the fine-tuned model to overfit the training data of downstream tasks and fail to generalize to unseen data.","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":18,"sentence":"To address such an issue in a principled manner, we propose a new learning framework for robust and efficient fine-tuning for pre-trained models to attain better generalization performance.","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":18,"sentence":"The proposed framework contains two important ingredients: 1.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":18,"sentence":"Smoothness-inducing regularization, which effectively manages the complexity of the model; 2.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":18,"sentence":"Bregman proximal point optimization, which is an instance of trust-region methods and can prevent aggressive updating.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":18,"sentence":"Our experiments show that the proposed framework achieves new state-of-the-art performance on a number of NLP tasks including GLUE, SNLI, SciTail and ANLI.","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":18,"sentence":"Moreover, it also outperforms the state-of-the-art T5 model, which is the largest pre-trained model containing 11 billion parameters, on GLUE.","offset":8,"pro":0.8888888888888888,"labels":"RST"},{"idx":19,"sentence":"Neural Network Language Models (NNLMs) generate probability distributions by applying a softmax function to a distance metric formed by taking the dot product of a prediction vector with all word vectors in a high-dimensional embedding space.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"The dot-product distance metric forms part of the inductive bias of NNLMs.","offset":1,"pro":0.25,"labels":"BAC"},{"idx":19,"sentence":"Although NNLMs optimize well with this inductive bias, we show that this results in a sub-optimal ordering of the embedding space that structurally impoverishes some words at the expense of others when assigning probability.","offset":2,"pro":0.5,"labels":"RST"},{"idx":19,"sentence":"We present numerical, theoretical and empirical analyses which show that words on the interior of the convex hull in the embedding space have their probability bounded by the probabilities of the words on the hull.","offset":3,"pro":0.75,"labels":"CLN"},{"idx":0,"sentence":"Recent work has shown that neural rerankers can improve results for dependency parsing over the top k trees produced by a base parser.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"However, all neural rerankers so far have been evaluated on English and Chinese only, both languages with a configurational word order and poor morphology.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":0,"sentence":"In the paper, we re-assess the potential of successful neural reranking models from the literature on English and on two morphologically rich(er) languages, German and Czech.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":0,"sentence":"We show that the GCN not only outperforms previous models on English but is the only model that is able to improve results over the baselines on German and Czech.","offset":3,"pro":0.5,"labels":"RST"},{"idx":0,"sentence":"We explain the differences in reranking performance based on an analysis of a) the gold tree ratio and b) the variety in the k-best lists.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":0,"sentence":"In addition, we introduce a new variation of a discriminative reranker based on graph convolutional networks (GCNs). ","offset":5,"pro":0.8333333333333334,"labels":"PUR"},{"idx":1,"sentence":"With the recent proliferation of the use of text classifications, researchers have found that there are certain unintended biases in text classification datasets.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"For example, texts containing some demographic identity-terms (e.g., gay, black) are more likely to be abusive in existing abusive language detection datasets.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":1,"sentence":"As a result, models trained with these datasets may consider sentences like She makes me happy to be gay as abusive simply because of the word gay.","offset":2,"pro":0.3333333333333333,"labels":"BAC"},{"idx":1,"sentence":"In this paper, we formalize the unintended biases in text classification datasets as a kind of selection bias from the non-discrimination distribution to the discrimination distribution.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":1,"sentence":"Based on this formalization, we further propose a model-agnostic debiasing training framework by recovering the non-discrimination distribution using instance weighting, which does not require any extra resources or annotations apart from a pre-defined set of demographic identity-terms.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":1,"sentence":"Experiments demonstrate that our method can effectively alleviate the impacts of the unintended biases without significantly hurting models' generalization ability.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":2,"sentence":"Given the fast development of analysis techniques for NLP and speech processing systems, few systematic studies have been conducted to compare the strengths and weaknesses of each method.","offset":0,"pro":0,"labels":"GAP"},{"idx":2,"sentence":"As a step in this direction we study the case of representations of phonology in neural network models of spoken language.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":2,"sentence":"We use two commonly applied analytical techniques, diagnostic classifiers and representational similarity analysis, to quantify to what extent neural activation patterns encode phonemes and phoneme sequences.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":2,"sentence":"We manipulate two factors that can affect the outcome of analysis.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":2,"sentence":"First, we investigate the role of learning by comparing neural activations extracted from trained versus randomly-initialized models.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":2,"sentence":"Second, we examine the temporal scope of the activations by probing both local activations corresponding to a few milliseconds of the speech signal, and global activations pooled over the whole utterance.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":2,"sentence":"We conclude that reporting analysis results with randomly initialized models is crucial, and that global-scope methods tend to yield more consistent and interpretable results and we recommend their use as a complement to local-scope diagnostic methods.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":3,"sentence":"To increase trust in artificial intelligence systems, a promising research direction consists of designing neural models capable of generating natural language explanations for their predictions.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"In this work, we show that such models are nonetheless prone to generating mutually inconsistent explanations, such as Because there is a dog in the image. and Because there is no dog in the [same] image., exposing flaws in either the decision-making process of the model or in the generation of the explanations.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":3,"sentence":"We introduce a simple yet effective adversarial framework for sanity checking models against the generation of inconsistent natural language explanations.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":3,"sentence":"Moreover, as part of the framework, we address the problem of adversarial attacks with full target sequences, a scenario that was not previously addressed in sequence-to-sequence attacks.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":3,"sentence":"Finally, we apply our framework on a state-of-the-art neural natural language inference model that provides natural language explanations for its predictions.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":3,"sentence":"Our framework shows that this model is capable of generating a significant number of inconsistent explanations.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":4,"sentence":"The effectiveness of such probing tasks is taken as evidence that the pre-trained model encodes linguistic knowledge.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"However, this approach of evaluating a language model is undermined by the uncertainty of the amount of knowledge that is learned by the probe itself.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":4,"sentence":"Complementary to those works, we propose a parameter-free probing technique for analyzing pre-trained language models (e.g., BERT).","offset":2,"pro":0.25,"labels":"PUR"},{"idx":4,"sentence":"Our method does not require direct supervision from the probing tasks, nor do we introduce additional parameters to the probing process.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":4,"sentence":"Our experiments on BERT show that syntactic trees recovered from BERT using our method are significantly better than linguistically-uninformed baselines.","offset":4,"pro":0.5,"labels":"RST"},{"idx":4,"sentence":"By introducing a small set of additional parameters, a probe learns to solve specific linguistic tasks (e.g., dependency parsing) in a supervised manner using feature representations (e.g., contextualized embeddings). ","offset":5,"pro":0.625,"labels":"BAC"},{"idx":4,"sentence":"We further feed the empirically induced dependency structures into a downstream sentiment classification task","offset":6,"pro":0.75,"labels":"MTD"},{"idx":4,"sentence":"and find its improvement compatible with or even superior to a human-designed dependency schema. ","offset":7,"pro":0.875,"labels":"RST"},{"idx":5,"sentence":"Language models keep track of complex information about the preceding context - including, e.g., syntactic relations in a sentence.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"We investigate whether they also capture information beneficial for resolving pronominal anaphora in English.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":5,"sentence":"We analyze two state of the art models with LSTM and Transformer architectures, via probe tasks and analysis on a coreference annotated corpus.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":5,"sentence":"The Transformer outperforms the LSTM in all analyses.","offset":3,"pro":0.5,"labels":"RST"},{"idx":5,"sentence":"Our results suggest that language models are more successful at learning grammatical constraints than they are at learning truly referential information, in the sense of capturing the fact that we use language to refer to entities in the world.","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":5,"sentence":"However, we find traces of the latter aspect, too. ","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":6,"sentence":"In the Transformer model, self-attention combines information from attended embeddings into the representation of the focal embedding in the next layer.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"Thus, across layers of the Transformer, information originating from different tokens gets increasingly mixed.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":6,"sentence":"This makes attention weights unreliable as explanations probes.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":6,"sentence":"In this paper, we consider the problem of quantifying this flow of information through self-attention.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":6,"sentence":"We propose two methods for approximating the attention to input tokens given attention weights, attention rollout and attention flow, as post hoc methods when we use attention weights as the relative relevance of the input tokens.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":6,"sentence":"We show that these methods give complementary views on the flow of information, and compared to raw attention, both yield higher correlations with importance scores of input tokens obtained using an ablation method and input gradients.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":7,"sentence":"With the growing popularity of deep-learning based NLP models, comes a need for interpretable systems.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"But what is interpretability, and what constitutes a high-quality interpretation?","offset":1,"pro":0.125,"labels":"GAP"},{"idx":7,"sentence":"In this opinion piece we reflect on the current state of interpretability evaluation research.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":7,"sentence":"We call for more clearly differentiating between different desired criteria an interpretation should satisfy, and focus on the faithfulness criteria.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":7,"sentence":"We survey the literature with respect to faithfulness evaluation, and arrange the current approaches around three assumptions, providing an explicit form to how faithfulness is defined by the community.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":7,"sentence":"We provide concrete guidelines on how evaluation of interpretation methods should and should not be conducted.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":7,"sentence":"Finally, we claim that the current binary definition for faithfulness sets a potentially unrealistic bar for being considered faithful.","offset":6,"pro":0.75,"labels":"CLN"},{"idx":7,"sentence":"We call for discarding the binary notion of faithfulness in favor of a more graded one, which we believe will be of greater practical utility.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":8,"sentence":"Recent studies on interpretability of attention distributions have led to notions of faithful and plausible explanations for a model's predictions.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"Attention distributions can be considered a faithful explanation if a higher attention weight implies a greater impact on the model's prediction.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":8,"sentence":"They can be considered a plausible explanation if they provide a human-understandable justification for the model's predictions.","offset":2,"pro":0.2,"labels":"BAC"},{"idx":8,"sentence":"In this work, we first explain why current attention mechanisms in LSTM based encoders can neither provide a faithful nor a plausible explanation of the model's predictions.","offset":3,"pro":0.3,"labels":"PUR"},{"idx":8,"sentence":"We observe that in LSTM based encoders the hidden representations at different time-steps are very similar to each other (high conicity) and attention weights in these situations do not carry much meaning because even a random permutation of the attention weights does not affect the model's predictions.","offset":4,"pro":0.4,"labels":"RST"},{"idx":8,"sentence":"Based on experiments on a wide variety of tasks and datasets, we observe attention distributions often attribute the model's predictions to unimportant words such as punctuation and fail to offer a plausible explanation for the predictions.","offset":5,"pro":0.5,"labels":"RST"},{"idx":8,"sentence":"We show that the resulting attention distributions offer more transparency as they (i) provide a more precise importance ranking of the hidden states (ii) are better indicative of words important for the model's predictions (iii) correlate better with gradient-based attribution methods.","offset":6,"pro":0.6,"labels":"CLN"},{"idx":8,"sentence":"Human evaluations indicate that the attention distributions learned by our model offer a plausible explanation of the model's predictions.","offset":7,"pro":0.7,"labels":"CLN"},{"idx":8,"sentence":"Our code has been made publicly available at https://github.com/akashkm99/Interpretable-Attention","offset":8,"pro":0.8,"labels":"CTN"},{"idx":8,"sentence":"To make attention mechanisms more faithful and plausible, we propose a modified LSTM cell with a diversity-driven training objective that ensures that the hidden representations learned at different time steps are diverse. ","offset":9,"pro":0.9,"labels":"MTD"},{"idx":9,"sentence":"Multi-task Learning methods have achieved great progress in text classification.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"However, existing methods assume that multi-task text classification problems are convex multiobjective optimization problems, which is unrealistic in real-world applications.","offset":1,"pro":0.25,"labels":"GAP"},{"idx":9,"sentence":"To address this issue, this paper presents a novel Tchebycheff procedure to optimize the multi-task classification problems without convex assumption.","offset":2,"pro":0.5,"labels":"PUR"},{"idx":9,"sentence":"The extensive experiments back up our theoretical analysis and validate the superiority of our proposals.","offset":3,"pro":0.75,"labels":"CLN"},{"idx":10,"sentence":"This paper studies strategies to model word formation in NMT using rich linguistic information, namely a word segmentation approach that goes beyond splitting into substrings by considering fusional morphology.","offset":0,"pro":0,"labels":"PUR"},{"idx":10,"sentence":"Our linguistically sound segmentation is combined with a method for target-side inflection to accommodate modeling word formation.","offset":1,"pro":0.3333333333333333,"labels":"MTD"},{"idx":10,"sentence":"The best system variants employ source-side morphological analysis and model complex target-side words, improving over a standard system.","offset":2,"pro":0.6666666666666666,"labels":"MTD"},{"idx":11,"sentence":"Existing approaches to active learning maximize the system performance by sampling unlabeled instances for annotation that yield the most efficient training.","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"However, when active learning is integrated with an end-user application, this can lead to frustration for participating users, as they spend time labeling instances that they would not otherwise be interested in reading.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":11,"sentence":"In this paper, we propose a new active learning approach that jointly optimizes the seemingly counteracting objectives of the active learning system (training efficiently) and the user (receiving useful instances).","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":11,"sentence":"We study our approach in an educational application, which particularly benefits from this technique as the system needs to rapidly learn to predict the appropriateness of an exercise to a particular user, while the users should receive only exercises that match their skills.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":11,"sentence":"We evaluate multiple learning strategies and user types with data from real users","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":11,"sentence":"and find that our joint approach better satisfies both objectives when alternative methods lead to many unsuitable exercises for end users. ","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":12,"sentence":"This paper investigates how to effectively incorporate a pre-trained masked language model (MLM), such as BERT, into an encoder-decoder (EncDec) model for grammatical error correction (GEC).","offset":0,"pro":0,"labels":"PUR"},{"idx":12,"sentence":"The answer to this question is not as straightforward as one might expect because the previous common methods for incorporating a MLM into an EncDec model have potential drawbacks when applied to GEC.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":12,"sentence":"For example, the distribution of the inputs to a GEC model can be considerably different (erroneous, clumsy, etc.) from that of the corpora used for pre-training MLMs; however, this issue is not addressed in the previous methods.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":12,"sentence":"The best-performing model achieves state-of-the-art performances on the BEA-2019 and CoNLL-2014 benchmarks.","offset":3,"pro":0.5,"labels":"RST"},{"idx":12,"sentence":"Our code is publicly available at: https://github.com/kanekomasahiro/bert-gec.","offset":4,"pro":0.6666666666666666,"labels":"CTN"},{"idx":12,"sentence":"Our experiments show that our proposed method, where we first fine-tune a MLM with a given GEC corpus and then use the output of the fine-tuned MLM as additional features in the GEC model, maximizes the benefit of the MLM. ","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":13,"sentence":"With the explosion of news information, personalized news recommendation has become very important for users to quickly find their interested contents.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"Most existing methods usually learn the representations of users and news from news contents for recommendation.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":13,"sentence":"However, they seldom consider high-order connectivity underlying the user-news interactions.","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":13,"sentence":"Moreover, existing methods failed to disentangle a user's latent preference factors which cause her clicks on different news.","offset":3,"pro":0.3333333333333333,"labels":"GAP"},{"idx":13,"sentence":"In this paper, we model the user-news interactions as a bipartite graph and propose a novel Graph Neural News Recommendation model with Unsupervised Preference Disentanglement, named GNUD.","offset":4,"pro":0.4444444444444444,"labels":"PUR"},{"idx":13,"sentence":"Our model can encode high-order relationships into user and news representations by information propagation along the graph.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":13,"sentence":"Furthermore, the learned representations are disentangled with latent preference factors by a neighborhood routing algorithm, which can enhance expressiveness and interpretability.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":13,"sentence":"A preference regularizer is also designed to force each disentangled subspace to independently reflect an isolated preference, improving the quality of the disentangled representations.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":13,"sentence":"Experimental results on real-world news datasets demonstrate that our proposed model can effectively improve the performance of news recommendation and outperform state-of-the-art news recommendation methods. ","offset":8,"pro":0.8888888888888888,"labels":"RST"},{"idx":14,"sentence":"In this paper, we study the problem of identifying the principals and accessories from the fact description with multiple defendants in a criminal case.","offset":0,"pro":0,"labels":"PUR"},{"idx":14,"sentence":"We treat the fact descriptions as narrative texts and the defendants as roles over the narrative story.","offset":1,"pro":0.25,"labels":"MTD"},{"idx":14,"sentence":"Experimental results on a real-world dataset demonstrate the behavior analysis can effectively model the defendants' impacts in a complex case.","offset":2,"pro":0.5,"labels":"CLN"},{"idx":14,"sentence":"We propose to model the defendants with behavioral semantic information and statistical characteristics, then learning the importances of defendants within a learning-to-rank framework. ","offset":3,"pro":0.75,"labels":"MTD"},{"idx":15,"sentence":"The rise of online communication platforms has been accompanied by some undesirable effects, such as the proliferation of aggressive and abusive behaviour online.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"Aiming to tackle this problem, the natural language processing (NLP) community has experimented with a range of techniques for abuse detection.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":15,"sentence":"While achieving substantial success, these methods have so far only focused on modelling the linguistic properties of the comments and the online communities of users, disregarding the emotional state of the users and how this might affect their language.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":15,"sentence":"The latter is, however, inextricably linked to abusive behaviour.","offset":3,"pro":0.5,"labels":"GAP"},{"idx":15,"sentence":"In this paper, we present the first joint model of emotion and abusive language detection, experimenting in a multi-task learning framework that allows one task to inform the other.","offset":4,"pro":0.6666666666666666,"labels":"PUR"},{"idx":15,"sentence":"Our results demonstrate that incorporating affective features leads to significant improvements in abuse detection performance across datasets.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":16,"sentence":"The key to effortless end-user programming is natural language.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"We examine how to teach intelligent systems new functions, expressed in natural language.","offset":1,"pro":0.1111111111111111,"labels":"PUR"},{"idx":16,"sentence":"As a first step, we collected 3168 samples of teaching efforts in plain English.","offset":2,"pro":0.2222222222222222,"labels":"MTD"},{"idx":16,"sentence":"Then we built fuSE, a novel system that translates English function descriptions into code.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":16,"sentence":"Our approach is three-tiered and each task is evaluated separately.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":16,"sentence":"We first classify whether an intent to teach new functionality is present in the utterance (accuracy: 97.7% using BERT).","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":16,"sentence":"Then we analyze the linguistic structure and construct a semantic model (accuracy: 97.6% using a BiLSTM).","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":16,"sentence":"Finally, we synthesize the signature of the method, map the intermediate steps (instructions in the method body) to API calls and inject control structures (F1: 67.0% with information retrieval and knowledge-based methods).","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":16,"sentence":"In an end-to-end evaluation on an unseen dataset fuSE synthesized 84.6% of the method signatures and 79.2% of the API calls correctly.","offset":8,"pro":0.8888888888888888,"labels":"RST"},{"idx":17,"sentence":"Moderation is crucial to promoting healthy online discussions.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"Although several toxicity' detection datasets and models have been published, most of them ignore the context of the posts, implicitly assuming that comments may be judged independently.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":17,"sentence":"We investigate this assumption by focusing on two questions: (a) does context affect the human judgement, and (b) does conditioning on context improve performance of toxicity detection systems?","offset":2,"pro":0.2222222222222222,"labels":"PUR"},{"idx":17,"sentence":"We experiment with Wikipedia conversations, limiting the notion of context to the previous post in the thread and the discussion title.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":17,"sentence":"We find that context can both amplify or mitigate the perceived toxicity of posts.","offset":4,"pro":0.4444444444444444,"labels":"RST"},{"idx":17,"sentence":"Moreover, a small but significant subset of manually labeled posts (5% in one of our experiments) end up having the opposite toxicity labels if the annotators are not provided with context.","offset":5,"pro":0.5555555555555556,"labels":"RST"},{"idx":17,"sentence":"Surprisingly, we also find no evidence that context actually improves the performance of toxicity classifiers, having tried a range of classifiers and mechanisms to make them context aware.","offset":6,"pro":0.6666666666666666,"labels":"RST"},{"idx":17,"sentence":"We make our code and data publicly available.","offset":7,"pro":0.7777777777777778,"labels":"CTN"},{"idx":17,"sentence":"This points to the need for larger datasets of comments annotated in context.","offset":8,"pro":0.8888888888888888,"labels":"IMP"},{"idx":18,"sentence":"Abstract Meaning Representations (AMRs) capture sentence-level semantics structural representations to broad-coverage natural sentences.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"We investigate parsing AMR with explicit dependency structures and interpretable latent structures.","offset":1,"pro":0.25,"labels":"PUR"},{"idx":18,"sentence":"We generate the latent soft structure without additional annotations, and fuse both dependency and latent structure via an extended graph neural networks.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":18,"sentence":"The fused structural information helps our experiments results to achieve the best reported results on both AMR 2.0 (77.5% Smatch F1 on LDC2017T10) and AMR 1.0 ((71.8% Smatch F1 on LDC2014T12).","offset":3,"pro":0.75,"labels":"RST"},{"idx":19,"sentence":"Answering natural language questions over tables is usually seen as a semantic parsing task.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"To alleviate the collection cost of full logical forms, one popular approach focuses on weak supervision consisting of denotations instead of logical forms.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":19,"sentence":"However, training semantic parsers from weak supervision poses difficulties, and in addition, the generated logical forms are only used as an intermediate step prior to retrieving the denotation.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":19,"sentence":"In this paper, we present TaPas, an approach to question answering over tables without generating logical forms.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":19,"sentence":"TaPas trains from weak supervision, and predicts the denotation by selecting table cells and optionally applying a corresponding aggregation operator to such selection.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":19,"sentence":"TaPas extends BERT's architecture to encode tables as input, initializes from an effective joint pre-training of text segments and tables crawled from Wikipedia, and is trained end-to-end.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":19,"sentence":"We experiment with three different semantic parsing datasets, and find that TaPas outperforms or rivals semantic parsing models by improving state-of-the-art accuracy on SQA from 55.1 to 67.2 and performing on par with the state-of-the-art on WikiSQL and WikiTQ, but with a simpler model architecture.","offset":6,"pro":0.75,"labels":"RST"},{"idx":19,"sentence":"We additionally find that transfer learning, which is trivial in our setting, from WikiSQL to WikiTQ, yields 48.7 accuracy, 4.2 points above the state-of-the-art.","offset":7,"pro":0.875,"labels":"RST"},{"idx":0,"sentence":"Machine translation (MT) has benefited from using synthetic training data originating from translating monolingual corpora, a technique known as backtranslation.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"Combining backtranslated data from different sources has led to better results than when using such data in isolation.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":0,"sentence":"In this work we analyse the impact that data translated with rule-based, phrase-based statistical and neural MT systems has on new MT systems.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":0,"sentence":"We use a real-world low-resource use-case (Basque-to-Spanish in the clinical domain) as well as a high-resource language pair (German-to-English) to test different scenarios with backtranslation and employ data selection to optimise the synthetic corpora.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":0,"sentence":"We exploit different data selection strategies in order to reduce the amount of data used, while at the same time maintaining high-quality MT systems.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":0,"sentence":"We further tune the data selection method by taking into account the quality of the MT systems used for backtranslation and lexical diversity of the resulting corpora.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":0,"sentence":"Our experiments show that incorporating backtranslated data from different sources can be beneficial, and that availing of data selection can yield improved performance.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":1,"sentence":"Sparse models require less memory for storage and enable a faster inference by reducing the necessary number of FLOPs.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"This is relevant both for time-critical and on-device computations using neural networks.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":1,"sentence":"The stabilized lottery ticket hypothesis states that networks can be pruned after none or few training iterations, using a mask computed based on the unpruned converged model.","offset":2,"pro":0.4,"labels":"BAC"},{"idx":1,"sentence":"On the transformer architecture and the WMT 2014 English-to-German and English-to-French tasks, we show that stabilized lottery ticket pruning performs similar to magnitude pruning for sparsity levels of up to 85%, and propose a new combination of pruning techniques that outperforms all other techniques for even higher levels of sparsity.","offset":3,"pro":0.6,"labels":"RST"},{"idx":1,"sentence":"Furthermore, we confirm that the parameter's initial sign and not its specific value is the primary factor for successful training, and show that magnitude pruning cannot be used to find winning lottery tickets.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":2,"sentence":"Neural models have achieved great success on machine reading comprehension (MRC), many of which typically consist of two components: an evidence extractor and an answer predictor.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"The former seeks the most relevant information from a reference text, while the latter is to locate or generate answers from the extracted evidence.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":2,"sentence":"Despite the importance of evidence labels for training the evidence extractor, they are not cheaply accessible, particularly in many non-extractive MRC tasks such as YES/NO question answering and multi-choice MRC.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":2,"sentence":"To address this problem, we present a Self-Training method (STM), which supervises the evidence extractor with auto-generated evidence labels in an iterative process.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":2,"sentence":"At each iteration, a base MRC model is trained with golden answers and noisy evidence labels.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":2,"sentence":"The trained model will predict pseudo evidence labels as extra supervision in the next iteration.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":2,"sentence":"We evaluate STM on seven datasets over three MRC tasks.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":2,"sentence":"Experimental results demonstrate the improvement on existing MRC models, and we also analyze how and why such a self-training method works in MRC.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":3,"sentence":"While the recent tree-based neural models have demonstrated promising results in generating solution expression for the math word problem (MWP), most of these models do not capture the relationships and order information among the quantities well.","offset":0,"pro":0,"labels":"GAP"},{"idx":3,"sentence":"This results in poor quantity representations and incorrect solution expressions.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":3,"sentence":"In this paper, we propose Graph2Tree, a novel deep learning architecture that combines the merits of the graph-based encoder and tree-based decoder to generate better solution expressions.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":3,"sentence":"Included in our Graph2Tree framework are two graphs, namely the Quantity Cell Graph and Quantity Comparison Graph, which are designed to address limitations of existing methods by effectively representing the relationships and order information among the quantities in MWPs.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":3,"sentence":"We conduct extensive experiments on two available datasets.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":3,"sentence":"Our experiment results show that Graph2Tree outperforms the state-of-the-art baselines on two benchmark datasets significantly.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":3,"sentence":"We also discuss case studies and empirically examine Graph2Tree's effectiveness in translating the MWP text into solution expressions.","offset":6,"pro":0.8571428571428571,"labels":"MTD"},{"idx":4,"sentence":"In Ordinal Classification tasks, items have to be assigned to classes that have a relative ordering, such as positive, neutral, negative in sentiment analysis.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"Remarkably, the most popular evaluation metrics for ordinal classification tasks either ignore relevant information (for instance, precision/recall on each of the classes ignores their relative ordering) or assume additional information (for instance, Mean Average Error assumes absolute distances between classes).","offset":1,"pro":0.2,"labels":"GAP"},{"idx":4,"sentence":"In this paper we propose a new metric for Ordinal Classification, Closeness Evaluation Measure, that is rooted on Measurement Theory and Information Theory.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":4,"sentence":"Our theoretical analysis and experimental results over both synthetic data and data from NLP shared tasks indicate that the proposed metric captures quality aspects from different traditional tasks simultaneously.","offset":3,"pro":0.6,"labels":"CLN"},{"idx":4,"sentence":"In addition, it generalizes some popular classification (nominal scale) and error minimization (interval scale) metrics, depending on the measurement scale in which it is instantiated.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":5,"sentence":"Distributed representations of words have been an indispensable component for natural language processing (NLP) tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"However, the large memory footprint of word embeddings makes it challenging to deploy NLP models to memory-constrained devices (e.g., self-driving cars, mobile devices).","offset":1,"pro":0.09090909090909091,"labels":"GAP"},{"idx":5,"sentence":"In this paper, we propose a novel method to adaptively compress word embeddings.","offset":2,"pro":0.18181818181818182,"labels":"PUR"},{"idx":5,"sentence":"We fundamentally follow a code-book approach that represents words as discrete codes such as (8, 5, 2, 4).","offset":3,"pro":0.2727272727272727,"labels":"MTD"},{"idx":5,"sentence":"However, unlike prior works that assign the same length of codes to all words, we adaptively assign different lengths of codes to each word by learning downstream tasks.","offset":4,"pro":0.36363636363636365,"labels":"MTD"},{"idx":5,"sentence":"The proposed method works in two steps.","offset":5,"pro":0.45454545454545453,"labels":"MTD"},{"idx":5,"sentence":"First, each word directly learns to select its code length in an end-to-end manner by applying the Gumbel-softmax tricks.","offset":6,"pro":0.5454545454545454,"labels":"MTD"},{"idx":5,"sentence":"After selecting the code length, each word learns discrete codes through a neural network with a binary constraint.","offset":7,"pro":0.6363636363636364,"labels":"MTD"},{"idx":5,"sentence":"To showcase the general applicability of the proposed method, we evaluate the performance on four different downstream tasks.","offset":8,"pro":0.7272727272727273,"labels":"MTD"},{"idx":5,"sentence":"Comprehensive evaluation results clearly show that our method is effective and makes the highly compressed word embeddings without hurting the task accuracy.","offset":9,"pro":0.8181818181818182,"labels":"CLN"},{"idx":5,"sentence":"Moreover, we show that our model assigns word to each code-book by considering the significance of tasks.","offset":10,"pro":0.9090909090909091,"labels":"RST"},{"idx":6,"sentence":"This paper presents the first unsupervised approach to lexical semantic change that makes use of contextualised word representations.","offset":0,"pro":0,"labels":"PUR"},{"idx":6,"sentence":"We propose a novel method that exploits the BERT neural language model to obtain representations of word usages, clusters these representations into usage types, and measures change along time with three proposed metrics.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":6,"sentence":"We create a new evaluation dataset and show that the model representations and the detected semantic shifts are positively correlated with human judgements.","offset":2,"pro":0.4,"labels":"RST"},{"idx":6,"sentence":"Our extensive qualitative analysis demonstrates that our method captures a variety of synchronic and diachronic linguistic phenomena.","offset":3,"pro":0.6,"labels":"CLN"},{"idx":6,"sentence":"We expect our work to inspire further research in this direction.","offset":4,"pro":0.8,"labels":"IMP"},{"idx":7,"sentence":"Document clustering requires a deep understanding of the complex structure of long-text; in particular, the intra-sentential (local) and inter-sentential features (global).","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"Existing representation learning models do not fully capture these features.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":7,"sentence":"To address this, we present a novel graph-based representation for document clustering that builds a graph autoencoder (GAE) on a Keyword Correlation Graph.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":7,"sentence":"The graph is constructed with topical keywords as nodes and multiple local and global features as edges.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":7,"sentence":"A GAE is employed to aggregate the two sets of features by learning a latent representation which can jointly reconstruct them.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":7,"sentence":"Clustering is then performed on the learned representations, using vector dimensions as features for inducing document classes.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":7,"sentence":"Extensive experiments on two datasets show that the features learned by our approach can achieve better clustering performance than other existing features, including term frequency-inverse document frequency and average embedding.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":8,"sentence":"Functional Distributional Semantics provides a linguistically interpretable framework for distributional semantics, by representing the meaning of a word as a function (a binary classifier), instead of a vector.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"However, the large number of latent variables means that inference is computationally expensive, and training a model is therefore slow to converge.","offset":1,"pro":0.25,"labels":"GAP"},{"idx":8,"sentence":"In this paper, I introduce the Pixie Autoencoder, which augments the generative model of Functional Distributional Semantics with a graph-convolutional neural network to perform amortised variational inference.","offset":2,"pro":0.5,"labels":"PUR"},{"idx":8,"sentence":"This allows the model to be trained more effectively, achieving better results on two tasks (semantic similarity in context and semantic composition), and outperforming BERT, a large pre-trained language model.","offset":3,"pro":0.75,"labels":"CLN"},{"idx":9,"sentence":"abstract = {Pretraining deep language models has led to large performance gains in NLP.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"Despite this success, Schick and Sch{\"utze (2020) recently showed that these models struggle to understand rare words.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":9,"sentence":"For static word embeddings, this problem has been addressed by separately learning representations for rare words.","offset":2,"pro":0.3333333333333333,"labels":"BAC"},{"idx":9,"sentence":"In this work, we transfer this idea to pretrained language models: We introduce BERTRAM, a powerful architecture based on BERT that is capable of inferring high-quality embeddings for rare words that are suitable as input representations for deep language models.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":9,"sentence":"This is achieved by enabling the surface form and contexts of a word to interact with each other in a deep architecture.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":9,"sentence":"Integrating BERTRAM into BERT leads to large performance increases due to improved representations of rare and medium frequency words on both a rare word probing task and three downstream tasks.,","offset":5,"pro":0.8333333333333334,"labels":"BAC"},{"idx":10,"sentence":"Knowing the Most Frequent Sense (MFS) of a word has been proved to help Word Sense Disambiguation (WSD) models significantly.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"However, the scarcity of sense-annotated data makes it difficult to induce a reliable and high-coverage distribution of the meanings in a language vocabulary.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":10,"sentence":"To address this issue, in this paper we present CluBERT, an automatic and multilingual approach for inducing the distributions of word senses from a corpus of raw sentences.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":10,"sentence":"Our experiments show that CluBERT learns distributions over English senses that are of higher quality than those extracted by alternative approaches.","offset":3,"pro":0.42857142857142855,"labels":"RST"},{"idx":10,"sentence":"When used to induce the MFS of a lemma, CluBERT attains state-of-the-art results on the English Word Sense Disambiguation tasks and helps to improve the disambiguation performance of two off-the-shelf WSD models.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":10,"sentence":"Moreover, our distributions also prove to be effective in other languages, beating all their alternatives for computing the MFS on the multilingual WSD tasks.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":10,"sentence":"We release our sense distributions in five different languages at https://github.com/SapienzaNLP/clubert.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":11,"sentence":"Cross-domain sentiment classification aims to address the lack of massive amounts of labeled data.","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"It demands to predict sentiment polarity on a target domain utilizing a classifier learned from a source domain.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":11,"sentence":"In this paper, we investigate how to efficiently apply the pre-training language model BERT on the unsupervised domain adaptation.","offset":2,"pro":0.2222222222222222,"labels":"PUR"},{"idx":11,"sentence":"Due to the pre-training task and corpus, BERT is task-agnostic, which lacks domain awareness and can not distinguish the characteristic of source and target domain when transferring knowledge.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":11,"sentence":"To tackle these problems, we design a post-training procedure, which contains the target domain masked language model task and a novel domain-distinguish pre-training task.","offset":4,"pro":0.4444444444444444,"labels":"PUR"},{"idx":11,"sentence":"The post-training procedure will encourage BERT to be domain-aware and distill the domain-specific features in a self-supervised way.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":11,"sentence":"Based on this, we could then conduct the adversarial training to derive the enhanced domain-invariant features.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":11,"sentence":"Extensive experiments on Amazon dataset show that our model outperforms state-of-the-art methods by a large margin.","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":11,"sentence":"The ablation study demonstrates that the remarkable improvement is not only from BERT but also from our method.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":12,"sentence":"Generating a concise summary from a large collection of arguments on a given topic is an intriguing yet understudied problem.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"We propose to represent such summaries as a small set of talking points, termed key points, each scored according to its salience.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":12,"sentence":"We show, by analyzing a large dataset of crowd-contributed arguments, that a small number of key points per topic is typically sufficient for covering the vast majority of the arguments.","offset":2,"pro":0.3333333333333333,"labels":"RST"},{"idx":12,"sentence":"Furthermore, we found that a domain expert can often predict these key points in advance.","offset":3,"pro":0.5,"labels":"RST"},{"idx":12,"sentence":"We study the task of argument-to-key point mapping, and introduce a novel large-scale dataset for this task.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":12,"sentence":"We report empirical results for an extensive set of experiments with this dataset, showing promising performance.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":13,"sentence":"Understanding emotion expressed in language has a wide range of applications, from building empathetic chatbots to detecting harmful online behavior.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"Advancement in this area can be improved using large-scale datasets with a fine-grained typology, adaptable to multiple downstream tasks.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":13,"sentence":"We introduce GoEmotions, the largest manually annotated dataset of 58k English Reddit comments, labeled for 27 emotion categories or Neutral.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":13,"sentence":"We demonstrate the high quality of the annotations via Principal Preserved Component Analysis.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":13,"sentence":"We conduct transfer learning experiments with existing emotion benchmarks to show that our dataset generalizes well to other domains and different emotion taxonomies.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":13,"sentence":"Our BERT-based model achieves an average F1-score of .46 across our proposed taxonomy, leaving much room for improvement.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":14,"sentence":"In a context of offensive content mediation on social media now regulated by European laws, it is important not only to be able to automatically detect sexist content but also to identify if a message with a sexist content is really sexist or is a story of sexism experienced by a woman.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"We propose: (1) a new characterization of sexist content inspired by speech acts theory and discourse analysis studies, (2) the first French dataset annotated for sexism detection, and (3) a set of deep learning experiments trained on top of a combination of several tweet's vectorial representations (word embeddings, linguistic features, and various generalization strategies).","offset":1,"pro":0.3333333333333333,"labels":"MTD"},{"idx":14,"sentence":"Our results are encouraging and constitute a first step towards offensive content moderation.","offset":2,"pro":0.6666666666666666,"labels":"CLN"},{"idx":15,"sentence":"Recently, sentiment analysis has seen remarkable advance with the help of pre-training approaches.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"However, sentiment knowledge, such as sentiment words and aspect-sentiment pairs, is ignored in the process of pre-training, despite the fact that they are widely used in traditional sentiment analysis approaches.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":15,"sentence":"In this paper, we introduce Sentiment Knowledge Enhanced Pre-training (SKEP) in order to learn a unified sentiment representation for multiple sentiment analysis tasks.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":15,"sentence":"With the help of automatically-mined knowledge, SKEP conducts sentiment masking and constructs three sentiment knowledge prediction objectives, so as to embed sentiment information at the word, polarity and aspect level into pre-trained sentiment representation.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":15,"sentence":"In particular, the prediction of aspect-sentiment pairs is converted into multi-label classification, aiming to capture the dependency between words in a pair.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":15,"sentence":"Experiments on three kinds of sentiment tasks show that SKEP significantly outperforms strong pre-training baseline, and achieves new state-of-the-art results on most of the test datasets.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":15,"sentence":"We release our code at https://github.com/baidu/Senta.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":16,"sentence":"Recent work on the interpretability of deep neural language models has concluded that many properties of natural language syntax are encoded in their representational spaces.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"However, such studies often suffer from limited scope by focusing on a single language and a single linguistic formalism.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":16,"sentence":"In this study, we aim to investigate the extent to which the semblance of syntactic structure captured by language models adheres to a surface-syntactic or deep syntactic style of analysis, and whether the patterns are consistent across different languages.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":16,"sentence":"We apply a probe for extracting directed dependency trees to BERT and ELMo models trained on 13 different languages, probing for two different syntactic annotation styles: Universal Dependencies (UD), prioritizing deep syntactic relations, and Surface-Syntactic Universal Dependencies (SUD), focusing on surface structure.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":16,"sentence":"We find that both models exhibit a preference for UD over SUD - with interesting variations across languages and layers - and that the strength of this preference is correlated with differences in tree shape.","offset":4,"pro":0.8,"labels":"RST"},{"idx":17,"sentence":"Sequence-to-sequence constituent parsing requires a linearization to represent trees as sequences.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"Top-down tree linearizations, which can be based on brackets or shift-reduce actions, have achieved the best accuracy to date.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":17,"sentence":"In this paper, we show that these results can be improved by using an in-order linearization instead.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":17,"sentence":"Based on this observation, we implement an enriched in-order shift-reduce linearization inspired by Vinyals et al. (2015)'s approach, achieving the best accuracy to date on the English PTB dataset among fully-supervised single-model sequence-to-sequence constituent parsers.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":17,"sentence":"Finally, we apply deterministic attention mechanisms to match the speed of state-of-the-art transition-based parsers, thus showing that sequence-to-sequence models can match them, not only in accuracy, but also in speed.","offset":4,"pro":0.8,"labels":"MTD"},{"idx":18,"sentence":"A key problem in processing graph-based meaning representations is graph parsing, i.e. computing all possible derivations of a given graph according to a (competence) grammar.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"We demonstrate, for the first time, that exact graph parsing can be efficient for large graphs and with large Hyperedge Replacement Grammars (HRGs).","offset":1,"pro":0.25,"labels":"PUR"},{"idx":18,"sentence":"The advance is achieved by exploiting locality as terminal edge-adjacency in HRG rules.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":18,"sentence":"In particular, we highlight the importance of 1) a terminal edge-first parsing strategy, 2) a categorization of a subclass of HRG, i.e. what we call Weakly Regular Graph Grammar, and 3) distributing argument-structures to both lexical and phrasal rules.","offset":3,"pro":0.75,"labels":"MTD"},{"idx":19,"sentence":"Incremental syntactic parsing has been an active research area both for cognitive scientists trying to model human sentence processing and for NLP researchers attempting to combine incremental parsing with language modelling for ASR and MT.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"Most effort has been directed at designing the right transition mechanism, but less has been done to answer the question of what a probabilistic model for those transition parsers should look like.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":19,"sentence":"A very incremental transition mechanism of a recently proposed CCG parser when trained in straightforward locally normalised discriminative fashion produces very bad results on English CCGbank.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":19,"sentence":"We identify three biases as the causes of this problem: label bias, exposure bias and imbalanced probabilities bias.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":19,"sentence":"While known techniques for tackling these biases improve results, they still do not make the parser state of the art.","offset":4,"pro":0.5714285714285714,"labels":"GAP"},{"idx":19,"sentence":"Instead, we tackle all of these three biases at the same time using an improved version of beam search optimisation that minimises all beam search violations instead of minimising only the biggest violation.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":19,"sentence":"The new incremental parser gives better results than all previously published incremental CCG parsers, and outperforms even some widely used non-incremental CCG parsers.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":0,"sentence":"Neural machine translation systems tend to fail on less decent inputs despite its significant efficacy, which may significantly harm the credibility of these systems-fathoming how and when neural-based systems fail in such cases is critical for industrial maintenance.","offset":0,"pro":0,"labels":"GAP"},{"idx":0,"sentence":"Instead of collecting and analyzing bad cases using limited handcrafted error features, here we investigate this issue by generating adversarial examples via a new paradigm based on reinforcement learning.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":0,"sentence":"Our paradigm could expose pitfalls for a given performance metric, e.g., BLEU, and could target any given neural machine translation architecture.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":0,"sentence":"We conduct experiments of adversarial attacks on two mainstream neural machine translation architectures, RNN-search, and Transformer.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":0,"sentence":"The results show that our method efficiently produces stable attacks with meaning-preserving adversarial examples.","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":0,"sentence":"We also present a qualitative and quantitative analysis for the preference pattern of the attack, demonstrating its capability of pitfall exposure.","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":1,"sentence":"The commonly used framework for unsupervised machine translation builds initial translation models of both translation directions, and then performs iterative back-translation to jointly boost their translation performance.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"The initialization stage is very important since bad initialization may wrongly squeeze the search space, and too much noise introduced in this stage may hurt the final performance.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":1,"sentence":"In this paper, we propose a novel retrieval and rewriting based method to better initialize unsupervised translation models.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":1,"sentence":"We first retrieve semantically comparable sentences from monolingual corpora of two languages and then rewrite the target side to minimize the semantic gap between the source and retrieved targets with a designed rewriting model.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":1,"sentence":"The rewritten sentence pairs are used to initialize SMT models which are used to generate pseudo data for two NMT models, followed by the iterative back-translation.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":1,"sentence":"Experiments show that our method can build better initial unsupervised translation models and improve the final translation performance by over 4 BLEU scores.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":1,"sentence":"Our code is released at https://github.com/Imagist-Shuo/RRforUNMT.git.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":2,"sentence":"Most of the existing models for document-level machine translation adopt dual-encoder structures.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"The representation of the source sentences and the document-level contexts are modeled with two separate encoders.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":2,"sentence":"Although these models can make use of the document-level contexts, they do not fully model the interaction between the contexts and the source sentences, and can not directly adapt to the recent pre-training models (e.g., BERT) which encodes multiple sentences with a single encoder.","offset":2,"pro":0.4,"labels":"GAP"},{"idx":2,"sentence":"In this work, we propose a simple and effective unified encoder that can outperform the baseline models of dual-encoder models in terms of BLEU and METEOR scores.","offset":3,"pro":0.6,"labels":"PUR"},{"idx":2,"sentence":"Moreover, the pre-training models can further boost the performance of our proposed model.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":3,"sentence":"In encoder-decoder neural models, multiple encoders are in general used to represent the contextual information in addition to the individual sentence.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"In this paper, we investigate multi-encoder approaches in document-level neural machine translation (NMT).","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":3,"sentence":"Surprisingly, we find that the context encoder does not only encode the surrounding sentences but also behaves as a noise generator.","offset":2,"pro":0.2857142857142857,"labels":"RST"},{"idx":3,"sentence":"This makes us rethink the real benefits of multi-encoder in context-aware translation - some of the improvements come from robust training.","offset":3,"pro":0.42857142857142855,"labels":"CLN"},{"idx":3,"sentence":"We compare several methods that introduce noise and/or well-tuned dropout setup into the training of these encoders.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":3,"sentence":"Experimental results show that noisy training plays an important role in multi-encoder-based NMT, especially when the training data is small.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":3,"sentence":"Also, we establish a new state-of-the-art on IWSLT Fr-En task by careful use of noise generation and dropout methods.","offset":6,"pro":0.8571428571428571,"labels":"PUR"},{"idx":4,"sentence":"The choice of hyper-parameters affects the performance of neural models.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"While much previous research (Sutskever et al., 2013; Duchi et al., 2011; Kingma and Ba, 2015) focuses on accelerating convergence and reducing the effects of the learning rate, comparatively few papers concentrate on the effect of batch size.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":4,"sentence":"In this paper, we analyze how increasing batch size affects gradient direction, and propose to evaluate the stability of gradients with their angle change.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":4,"sentence":"Based on our observations, the angle change of gradient direction first tends to stabilize (i.e. gradually decrease) while accumulating mini-batches, and then starts to fluctuate.","offset":3,"pro":0.375,"labels":"RST"},{"idx":4,"sentence":"We propose to automatically and dynamically determine batch sizes by accumulating gradients of mini-batches and performing an optimization step at just the time when the direction of gradients starts to fluctuate.","offset":4,"pro":0.5,"labels":"PUR"},{"idx":4,"sentence":"To improve the efficiency of our approach for large models, we propose a sampling approach to select gradients of parameters sensitive to the batch size.","offset":5,"pro":0.625,"labels":"PUR"},{"idx":4,"sentence":"Our approach dynamically determines proper and efficient batch sizes during training.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":4,"sentence":"In our experiments on the WMT 14 English to German and English to French tasks, our approach improves the Transformer with a fixed 25k batch size by +0.73 and +0.82 BLEU respectively.","offset":7,"pro":0.875,"labels":"RST"},{"idx":5,"sentence":"Unsupervised neural machine translation (UNMT) has recently achieved remarkable results for several language pairs.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"However, it can only translate between a single language pair and cannot produce translation results for multiple language pairs at the same time.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":5,"sentence":"That is, research on multilingual UNMT has been limited.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":5,"sentence":"In this paper, we empirically introduce a simple method to translate between thirteen languages using a single encoder and a single decoder, making use of multilingual data to improve UNMT for all language pairs.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":5,"sentence":"On the basis of the empirical findings, we propose two knowledge distillation methods to further enhance multilingual UNMT performance.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":5,"sentence":"Our experiments on a dataset with English translated to and from twelve other languages (including three language families and six language branches) show remarkable results, surpassing strong unsupervised individual baselines while achieving promising performance between non-English language pairs in zero-shot translation scenarios and alleviating poor performance in low-resource language pairs.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":6,"sentence":"This paper proposes a simple and effective algorithm for incorporating lexical constraints in neural machine translation.","offset":0,"pro":0,"labels":"PUR"},{"idx":6,"sentence":"Previous work either required re-training existing models with the lexical constraints or incorporating them during beam search decoding with significantly higher computational overheads.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":6,"sentence":"Leveraging the flexibility and speed of a recently proposed Levenshtein Transformer model (Gu et al., 2019), our method injects terminology constraints at inference time without any impact on decoding speed.","offset":2,"pro":0.4,"labels":"RST"},{"idx":6,"sentence":"Our method does not require any modification to the training procedure and can be easily applied at runtime with custom dictionaries.","offset":3,"pro":0.6,"labels":"RST"},{"idx":6,"sentence":"Experiments on English-German WMT datasets show that our approach improves an unconstrained baseline and previous approaches.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":7,"sentence":"The standard training algorithm in neural machine translation (NMT) suffers from exposure bias, and alternative algorithms have been proposed to mitigate this.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"However, the practical impact of exposure bias is under debate.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":7,"sentence":"In this paper, we link exposure bias to another well-known problem in NMT, namely the tendency to generate hallucinations under domain shift.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":7,"sentence":"In experiments on three datasets with multiple test domains, we show that exposure bias is partially to blame for hallucinations, and that training with Minimum Risk Training, which avoids exposure bias, can mitigate this.","offset":3,"pro":0.5,"labels":"RST"},{"idx":7,"sentence":"Our analysis explains why exposure bias is more problematic under domain shift, and also links exposure bias to the beam search problem, i.e. performance deterioration with increasing beam size.","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":7,"sentence":"Our results provide a new justification for methods that reduce exposure bias: even if they do not increase performance on in-domain test sets, they can increase model robustness to domain shift.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":8,"sentence":"We propose an automatic evaluation method of machine translation that uses source language sentences regarded as additional pseudo references.","offset":0,"pro":0,"labels":"PUR"},{"idx":8,"sentence":"The proposed method evaluates a translation hypothesis in a regression model.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":8,"sentence":"The model takes the paired source, reference, and hypothesis sentence all together as an input.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":8,"sentence":"A pretrained large scale cross-lingual language model encodes the input to sentence-pair vectors, and the model predicts a human evaluation score with those vectors.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":8,"sentence":"Our experiments show that our proposed method using Cross-lingual Language Model (XLM) trained with a translation language modeling (TLM) objective achieves a higher correlation with human judgments than a baseline method that uses only hypothesis and reference sentences.","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":8,"sentence":"Additionally, using source sentences in our proposed method is confirmed to improve the evaluation performance.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":9,"sentence":"This paper presents the problem of conversational plotting agents that carry out plotting actions from natural language instructions.","offset":0,"pro":0,"labels":"PUR"},{"idx":9,"sentence":"To facilitate the development of such agents, we introduce ChartDialogs, a new multi-turn dialog dataset, covering a popular plotting library, matplotlib.","offset":1,"pro":0.25,"labels":"PUR"},{"idx":9,"sentence":"The dataset contains over 15,000 dialog turns from 3,200 dialogs covering the majority of matplotlib plot types.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":9,"sentence":"Extensive experiments show the best-performing method achieving 61% plotting accuracy, demonstrating that the dataset presents a non-trivial challenge for future research on this task.","offset":3,"pro":0.75,"labels":"RST"},{"idx":10,"sentence":"Code-switching is the use of more than one language in the same conversation or utterance.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"Recently, multilingual contextual embedding models, trained on multiple monolingual corpora, have shown promising results on cross-lingual and multilingual tasks.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":10,"sentence":"We present an evaluation benchmark, GLUECoS, for code-switched languages, that spans several NLP tasks in English-Hindi and English-Spanish.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":10,"sentence":"Specifically, our evaluation benchmark includes Language Identification from text, POS tagging, Named Entity Recognition, Sentiment Analysis, Question Answering and a new task for code-switching, Natural Language Inference.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":10,"sentence":"We present results on all these tasks using cross-lingual word embedding models and multilingual models.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":10,"sentence":"In addition, we fine-tune multilingual models on artificially generated code-switched data.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":10,"sentence":"Although multilingual models perform significantly better than cross-lingual models, our results show that in most tasks, across both language pairs, multilingual models fine-tuned on code-switched data perform best, showing that multilingual models can be further optimized for code-switching tasks.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":11,"sentence":"Recently, large-scale datasets have vastly facilitated the development in nearly all domains of Natural Language Processing.","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"However, there is currently no cross-task dataset in NLP, which hinders the development of multi-task learning.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":11,"sentence":"We propose MATINF, the first jointly labeled large-scale dataset for classification, question answering and summarization.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":11,"sentence":"MATINF contains 1.07 million question-answer pairs with human-labeled categories and user-generated question descriptions.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":11,"sentence":"Based on such rich information, MATINF is applicable for three major NLP tasks, including classification, question answering, and summarization.","offset":4,"pro":0.5714285714285714,"labels":"CTN"},{"idx":11,"sentence":"We benchmark existing methods and a novel multi-task baseline over MATINF to inspire further research.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":11,"sentence":"Our comprehensive comparison and experiments over MATINF and other datasets demonstrate the merits held by MATINF.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":12,"sentence":"News recommendation is an important technique for personalized news service.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"Compared with product and movie recommendations which have been comprehensively studied, the research on news recommendation is much more limited, mainly due to the lack of a high-quality benchmark dataset.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":12,"sentence":"In this paper, we present a large-scale dataset named MIND for news recommendation.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":12,"sentence":"Constructed from the user click logs of Microsoft News, MIND contains 1 million users and more than 160k English news articles, each of which has rich textual content such as title, abstract and body.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":12,"sentence":"We demonstrate MIND a good testbed for news recommendation through a comparative study of several state-of-the-art news recommendation methods which are originally developed on different proprietary datasets.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":12,"sentence":"Our results show the performance of news recommendation highly relies on the quality of news content understanding and user interest modeling.","offset":5,"pro":0.625,"labels":"CLN"},{"idx":12,"sentence":"Many natural language processing techniques such as effective text representation methods and pre-trained language models can effectively improve the performance of news recommendation.","offset":6,"pro":0.75,"labels":"BAC"},{"idx":12,"sentence":"The MIND dataset will be available at https://msnews.github.io.","offset":7,"pro":0.875,"labels":"CTN"},{"idx":13,"sentence":"The recent proliferation of fake news has triggered a number of responses, most notably the emergence of several manual fact-checking initiatives.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"As a result and over time, a large number of fact-checked claims have been accumulated, which increases the likelihood that a new claim in social media or a new statement by a politician might have already been fact-checked by some trusted fact-checking organization, as viral claims often come back after a while in social media, and politicians like to repeat their favorite statements, true or false, over and over again.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":13,"sentence":"As manual fact-checking is very time-consuming (and fully automatic fact-checking has credibility issues), it is important to try to save this effort and to avoid wasting time on claims that have already been fact-checked.","offset":2,"pro":0.25,"labels":"BAC"},{"idx":13,"sentence":"Interestingly, despite the importance of the task, it has been largely ignored by the research community so far.","offset":3,"pro":0.375,"labels":"GAP"},{"idx":13,"sentence":"Here, we aim to bridge this gap.","offset":4,"pro":0.5,"labels":"PUR"},{"idx":13,"sentence":"In particular, we formulate the task and we discuss how it relates to, but also differs from, previous work.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":13,"sentence":"We further create a specialized dataset, which we release to the research community.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":13,"sentence":"Finally, we present learning-to-rank experiments that demonstrate sizable improvements over state-of-the-art retrieval and textual similarity approaches.","offset":7,"pro":0.875,"labels":"MTD"},{"idx":14,"sentence":"Open-domain dialogue generation has gained increasing attention in Natural Language Processing.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"Its evaluation requires a holistic means.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":14,"sentence":"Human ratings are deemed as the gold standard.","offset":2,"pro":0.25,"labels":"BAC"},{"idx":14,"sentence":"As human evaluation is inefficient and costly, an automated substitute is highly desirable.","offset":3,"pro":0.375,"labels":"BAC"},{"idx":14,"sentence":"In this paper, we propose holistic evaluation metrics that capture different aspects of open-domain dialogues.","offset":4,"pro":0.5,"labels":"PUR"},{"idx":14,"sentence":"Our metrics consist of (1) GPT-2 based context coherence between sentences in a dialogue, (2) GPT-2 based fluency in phrasing, (3) n-gram based diversity in responses to augmented queries, and (4) textual-entailment-inference based logical self-consistency.","offset":5,"pro":0.625,"labels":"PUR"},{"idx":14,"sentence":"The empirical validity of our metrics is demonstrated by strong correlations with human judgments.","offset":6,"pro":0.75,"labels":"CLN"},{"idx":14,"sentence":"We open source the code and relevant materials.","offset":7,"pro":0.875,"labels":"CTN"},{"idx":15,"sentence":"The hypernymy detection task has been addressed under various frameworks.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"Previously, the design of unsupervised hypernymy scores has been extensively studied.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":15,"sentence":"In contrast, supervised classifiers, especially distributional models, leverage the global contexts of terms to make predictions, but are more likely to suffer from lexical memorization.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":15,"sentence":"In this work, we revisit supervised distributional models for hypernymy detection.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":15,"sentence":"Rather than taking embeddings of two terms as classification inputs, we introduce a representation learning framework named Bidirectional Residual Relation Embeddings (BiRRE).","offset":4,"pro":0.5,"labels":"PUR"},{"idx":15,"sentence":"In this model, a term pair is represented by a BiRRE vector as features for hypernymy classification, which models the possibility of a term being mapped to another in the embedding space by hypernymy relations.","offset":5,"pro":0.625,"labels":"PUR"},{"idx":15,"sentence":"A Latent Projection Model with Negative Regularization (LPMNR) is proposed to simulate how hypernyms and hyponyms are generated by neural language models, and to generate BiRRE vectors based on bidirectional residuals of projections.","offset":6,"pro":0.75,"labels":"PUR"},{"idx":15,"sentence":"Experiments verify BiRRE outperforms strong baselines over various evaluation frameworks.","offset":7,"pro":0.875,"labels":"RST"},{"idx":16,"sentence":"Biomedical named entities often play important roles in many biomedical text mining tools.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"However, due to the incompleteness of provided synonyms and numerous variations in their surface forms, normalization of biomedical entities is very challenging.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":16,"sentence":"In this paper, we focus on learning representations of biomedical entities solely based on the synonyms of entities.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":16,"sentence":"To learn from the incomplete synonyms, we use a model-based candidate selection and maximize the marginal likelihood of the synonyms present in top candidates.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":16,"sentence":"Our model-based candidates are iteratively updated to contain more difficult negative samples as our model evolves.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":16,"sentence":"In this way, we avoid the explicit pre-selection of negative samples from more than 400K candidates.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":16,"sentence":"On four biomedical entity normalization datasets having three different entity types (disease, chemical, adverse reaction), our model BioSyn consistently outperforms previous state-of-the-art models almost reaching the upper bound on each dataset.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":17,"sentence":"Hypernymy detection, a.k.a, lexical entailment, is a fundamental sub-task of many natural language understanding tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"Previous explorations mostly focus on monolingual hypernymy detection on high-resource languages, e.g., English, but few investigate the low-resource scenarios.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":17,"sentence":"This paper addresses the problem of low-resource hypernymy detection by combining high-resource languages.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":17,"sentence":"We extensively compare three joint training paradigms and for the first time propose applying meta learning to relieve the low-resource issue.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":17,"sentence":"Experiments demonstrate the superiority of our method among the three settings, which substantially improves the performance of extremely low-resource languages by preventing over-fitting on small datasets.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":18,"sentence":"This paper presents an investigation on the distribution of word vectors belonging to a certain word class in a pre-trained word vector space.","offset":0,"pro":0,"labels":"PUR"},{"idx":18,"sentence":"To this end, we made several assumptions about the distribution, modeled the distribution accordingly, and validated each assumption by comparing the goodness of each model.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":18,"sentence":"Specifically, we considered two types of word classes - the semantic class of direct objects of a verb and the semantic class in a thesaurus - and tried to build models that properly estimate how likely it is that a word in the vector space is a member of a given word class.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":18,"sentence":"Our results on selectional preference and WordNet datasets show that the centroid-based model will fail to achieve good enough performance, the geometry of the distribution and the existence of subgroups will have limited impact, and also the negative instances need to be considered for adequate modeling of the distribution.","offset":3,"pro":0.6,"labels":"CLN"},{"idx":18,"sentence":"We further investigated the relationship between the scores calculated by each model and the degree of membership and found that discriminative learning-based models are best in finding the boundaries of a class, while models based on the offset between positive and negative instances perform best in determining the degree of membership.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":19,"sentence":"In the literature, existing studies always consider Aspect Sentiment Classification (ASC) as an independent sentence-level classification problem aspect by aspect, which largely ignore the document-level sentiment preference information, though obviously such information is crucial for alleviating the information deficiency problem in ASC.","offset":0,"pro":0,"labels":"GAP"},{"idx":19,"sentence":"In this paper, we explore two kinds of sentiment preference information inside a document, i.e., contextual sentiment consistency w.r.t. the same aspect (namely intra-aspect sentiment consistency) and contextual sentiment tendency w.r.t. all the related aspects (namely inter-aspect sentiment tendency).","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":19,"sentence":"On the basis, we propose a Cooperative Graph Attention Networks (CoGAN) approach for cooperatively learning the aspect-related sentence representation.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":19,"sentence":"Specifically, two graph attention networks are leveraged to model above two kinds of document-level sentiment preference information respectively, followed by an interactive mechanism to integrate the two-fold preference.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":19,"sentence":"Detailed evaluation demonstrates the great advantage of the proposed approach to ASC over the state-of-the-art baselines.","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":19,"sentence":"This justifies the importance of the document-level sentiment preference information to ASC and the effectiveness of our approach capturing such information.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":0,"sentence":"The current aspect extraction methods suffer from boundary errors.","offset":0,"pro":0,"labels":"GAP"},{"idx":0,"sentence":"In general, these errors lead to a relatively minor difference between the extracted aspects and the ground-truth.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":0,"sentence":"However, they hurt the performance severely.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":0,"sentence":"In this paper, we propose to utilize a pointer network for repositioning the boundaries.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":0,"sentence":"Recycling mechanism is used, which enables the training data to be collected without manual intervention.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":0,"sentence":"We conduct the experiments on the benchmark datasets SE14 of laptop and SE14-16 of restaurant.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":0,"sentence":"Experimental results show that our method achieves substantial improvements over the baseline, and outperforms state-of-the-art methods.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":1,"sentence":"Aspect-based sentiment analysis (ABSA) involves three subtasks, i.e., aspect term extraction, opinion term extraction, and aspect-level sentiment classification.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"Most existing studies focused on one of these subtasks only.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":1,"sentence":"Several recent researches made successful attempts to solve the complete ABSA problem with a unified framework.","offset":2,"pro":0.25,"labels":"BAC"},{"idx":1,"sentence":"However, the interactive relations among three subtasks are still under-exploited.","offset":3,"pro":0.375,"labels":"GAP"},{"idx":1,"sentence":"We argue that such relations encode collaborative signals between different subtasks.","offset":4,"pro":0.5,"labels":"BAC"},{"idx":1,"sentence":"For example, when the opinion term is delicious, the aspect term must be food rather than place.","offset":5,"pro":0.625,"labels":"BAC"},{"idx":1,"sentence":"In order to fully exploit these relations, we propose a Relation-Aware Collaborative Learning (RACL) framework which allows the subtasks to work coordinately via the multi-task learning and relation propagation mechanisms in a stacked multi-layer network.","offset":6,"pro":0.75,"labels":"PUR"},{"idx":1,"sentence":"Extensive experiments on three real-world datasets demonstrate that RACL significantly outperforms the state-of-the-art methods for the complete ABSA task.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":2,"sentence":"We propose SentiBERT, a variant of BERT that effectively captures compositional sentiment semantics.","offset":0,"pro":0,"labels":"PUR"},{"idx":2,"sentence":"The model incorporates contextualized representation with binary constituency parse tree to capture semantic composition.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":2,"sentence":"Comprehensive experiments demonstrate that SentiBERT achieves competitive performance on phrase-level sentiment classification.","offset":2,"pro":0.3333333333333333,"labels":"RST"},{"idx":2,"sentence":"We further demonstrate that the sentiment composition learned from the phrase-level annotations on SST can be transferred to other sentiment analysis tasks as well as related tasks, such as emotion classification tasks.","offset":3,"pro":0.5,"labels":"CLN"},{"idx":2,"sentence":"Moreover, we conduct ablation studies and design visualization methods to understand SentiBERT.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":2,"sentence":"We show that SentiBERT is better than baseline approaches in capturing negation and the contrastive relation and model the compositional sentiment semantics.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":3,"sentence":"Emotion-cause pair extraction aims to extract all potential pairs of emotions and corresponding causes from unannotated emotion text.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"Most existing methods are pipelined framework, which identifies emotions and extracts causes separately, leading to a drawback of error propagation.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":3,"sentence":"Towards this issue, we propose a transition-based model to transform the task into a procedure of parsing-like directed graph construction.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":3,"sentence":"The proposed model incrementally generates the directed graph with labeled edges based on a sequence of actions, from which we can recognize emotions with the corresponding causes simultaneously, thereby optimizing separate subtasks jointly and maximizing mutual benefits of tasks interdependently.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":3,"sentence":"Experimental results show that our approach achieves the best performance, outperforming the state-of-the-art methods by 6.71% (pless0.01) in F1 measure.","offset":4,"pro":0.8,"labels":"RST"},{"idx":4,"sentence":"Previous studies in multimodal sentiment analysis have used limited datasets, which only contain unified multimodal annotations.","offset":0,"pro":0,"labels":"GAP"},{"idx":4,"sentence":"However, the unified annotations do not always reflect the independent sentiment of single modalities and limit the model to capture the difference between modalities.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":4,"sentence":"In this paper, we introduce a Chinese single- and multi-modal sentiment analysis dataset, CH-SIMS, which contains 2,281 refined video segments in the wild with both multimodal and independent unimodal annotations.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":4,"sentence":"It allows researchers to study the interaction between modalities or use independent unimodal annotations for unimodal sentiment analysis.Furthermore, we propose a multi-task learning framework based on late fusion as the baseline.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":4,"sentence":"Extensive experiments on the CH-SIMS show that our methods achieve state-of-the-art performance and learn more distinctive unimodal representations.","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":4,"sentence":"The full dataset and codes are available for use at https://github.com/thuiar/MMSA.","offset":5,"pro":0.8333333333333334,"labels":"CTN"},{"idx":5,"sentence":"End-to-end speech translation poses a heavy burden on the encoder because it has to transcribe, understand, and learn cross-lingual semantics simultaneously.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"To obtain a powerful encoder, traditional methods pre-train it on ASR data to capture speech features.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":5,"sentence":"However, we argue that pre-training the encoder only through simple speech recognition is not enough, and high-level linguistic knowledge should be considered.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":5,"sentence":"Inspired by this, we propose a curriculum pre-training method that includes an elementary course for transcription learning and two advanced courses for understanding the utterance and mapping words in two languages.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":5,"sentence":"The difficulty of these courses is gradually increasing.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":5,"sentence":"Experiments show that our curriculum pre-training method leads to significant improvements on En-De and En-Fr speech translation benchmarks.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":6,"sentence":"In this work, we present a detailed analysis of how accent information is reflected in the internal representation of speech in an end-to-end automatic speech recognition (ASR) system.","offset":0,"pro":0,"labels":"PUR"},{"idx":6,"sentence":"We use a state-of-the-art end-to-end ASR system, comprising convolutional and recurrent layers, that is trained on a large amount of US-accented English speech and evaluate the model on speech samples from seven different English accents.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":6,"sentence":"We examine the effects of accent on the internal representation using three main probing techniques: a) Gradient-based explanation methods, b) Information-theoretic measures, and c) Outputs of accent and phone classifiers.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":6,"sentence":"We find different accents exhibiting similar trends irrespective of the probing technique used.","offset":3,"pro":0.6,"labels":"CLN"},{"idx":6,"sentence":"We also find that most accent information is encoded within the first recurrent layer, which is suggestive of how one could adapt such an end-to-end model to learn representations that are invariant to accents.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":7,"sentence":"Self-attentive neural syntactic parsers using contextualized word embeddings (e.g. ELMo or BERT) currently produce state-of-the-art results in joint parsing and disfluency detection in speech transcripts.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"Since the contextualized word embeddings are pre-trained on a large amount of unlabeled data, using additional unlabeled data to train a neural model might seem redundant.","offset":1,"pro":0.25,"labels":"GAP"},{"idx":7,"sentence":"However, we show that self-training - a semi-supervised technique for incorporating unlabeled data - sets a new state-of-the-art for the self-attentive parser on disfluency detection, demonstrating that self-training provides benefits orthogonal to the pre-trained contextualized word representations.","offset":2,"pro":0.5,"labels":"CLN"},{"idx":7,"sentence":"We also show that ensembling self-trained parsers provides further gains for disfluency detection.","offset":3,"pro":0.75,"labels":"CLN"},{"idx":8,"sentence":"Pre-trained language models have achieved huge improvement on many NLP tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"However, these methods are usually designed for written text, so they do not consider the properties of spoken language.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":8,"sentence":"Therefore, this paper aims at generalizing the idea of language model pre-training to lattices generated by recognition systems.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":8,"sentence":"We propose a framework that trains neural lattice language models to provide contextualized representations for spoken language understanding tasks.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":8,"sentence":"The proposed two-stage pre-training approach reduces the demands of speech data and has better efficiency.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":8,"sentence":"Experiments on intent detection and dialogue act recognition datasets demonstrate that our proposed method consistently outperforms strong baselines when evaluated on spoken inputs.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":8,"sentence":"The code is available at https://github.com/MiuLab/Lattice-ELMo.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":9,"sentence":"An increasing number of people in the world today speak a mixed-language as a result of being multilingual.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"However, building a speech recognition system for code-switching remains difficult due to the availability of limited resources and the expense and significant effort required to collect mixed-language data.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":9,"sentence":"We therefore propose a new learning method, meta-transfer learning, to transfer learn on a code-switched speech recognition system in a low-resource setting by judiciously extracting information from high-resource monolingual datasets.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":9,"sentence":"Our model learns to recognize individual languages, and transfer them so as to better recognize mixed-language speech by conditioning the optimization on the code-switching data.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":9,"sentence":"Based on experimental results, our model outperforms existing baselines on speech recognition and language modeling tasks, and is faster to converge.","offset":4,"pro":0.8,"labels":"RST"},{"idx":10,"sentence":"Sarcasm is a sophisticated linguistic phenomenon to express the opposite of what one really means.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"With the rapid growth of social media, multimodal sarcastic tweets are widely posted on various social platforms.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":10,"sentence":"In multimodal context, sarcasm is no longer a pure linguistic phenomenon, and due to the nature of social media short text, the opposite is more often manifested via cross-modality expressions.","offset":2,"pro":0.25,"labels":"BAC"},{"idx":10,"sentence":"Thus traditional text-based methods are insufficient to detect multimodal sarcasm.","offset":3,"pro":0.375,"labels":"GAP"},{"idx":10,"sentence":"To reason with multimodal sarcastic tweets, in this paper, we propose a novel method for modeling cross-modality contrast in the associated context.","offset":4,"pro":0.5,"labels":"PUR"},{"idx":10,"sentence":"Our method models both cross-modality contrast and semantic association by constructing the Decomposition and Relation Network (namely D{&R Net).","offset":5,"pro":0.625,"labels":"MTD"},{"idx":10,"sentence":"The decomposition network represents the commonality and discrepancy between image and text, and the relation network models the semantic association in cross-modality context.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":10,"sentence":"Experimental results on a public dataset demonstrate the effectiveness of our model in multimodal sarcasm detection.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":11,"sentence":"In this work, we develop SimulSpeech, an end-to-end simultaneous speech to text translation system which translates speech in source language to text in target language concurrently.","offset":0,"pro":0,"labels":"PUR"},{"idx":11,"sentence":"SimulSpeech consists of a speech encoder, a speech segmenter and a text decoder, where 1) the segmenter builds upon the encoder and leverages a connectionist temporal classification (CTC) loss to split the input streaming speech in real time, 2) the encoder-decoder attention adopts a wait-k strategy for simultaneous translation.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":11,"sentence":"SimulSpeech is more challenging than previous cascaded systems (with simultaneous automatic speech recognition (ASR) and simultaneous neural machine translation (NMT)).","offset":2,"pro":0.4,"labels":"MTD"},{"idx":11,"sentence":"We introduce two novel knowledge distillation methods to ensure the performance: 1) Attention-level knowledge distillation transfers the knowledge from the multiplication of the attention matrices of simultaneous NMT and ASR models to help the training of the attention mechanism in SimulSpeech; 2) Data-level knowledge distillation transfers the knowledge from the full-sentence NMT model and also reduces the complexity of data distribution to help on the optimization of SimulSpeech.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":11,"sentence":"Experiments on MuST-C English-Spanish and English-German spoken language translation datasets show that SimulSpeech achieves reasonable BLEU scores and lower delay compared to full-sentence end-to-end speech to text translation (without simultaneous translation), and better performance than the two-stage cascaded simultaneous translation model in terms of BLEU scores and translation delay.","offset":4,"pro":0.8,"labels":"RST"},{"idx":12,"sentence":"Spoken language understanding tasks usually rely on pipelines involving complex processing blocks such as voice activity detection, speaker diarization and Automatic speech recognition (ASR).","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"We propose a novel framework for predicting utterance level labels directly from speech features, thus removing the dependency on first generating transcripts, and transcription free behavioral coding.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":12,"sentence":"Our classifier uses a pretrained Speech-2-Vector encoder as bottleneck to generate word-level representations from speech features.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":12,"sentence":"This pretrained encoder learns to encode speech features for a word using an objective similar to Word2Vec.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":12,"sentence":"Our proposed approach just uses speech features and word segmentation information for predicting spoken utterance-level target labels.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":12,"sentence":"We show that our model achieves competitive results to other state-of-the-art approaches which use transcribed text for the task of predicting psychotherapy-relevant behavior codes.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":13,"sentence":"Opinion prediction on Twitter is challenging due to the transient nature of tweet content and neighbourhood context.","offset":0,"pro":0,"labels":"GAP"},{"idx":13,"sentence":"In this paper, we model users' tweet posting behaviour as a temporal point process to jointly predict the posting time and the stance label of the next tweet given a user's historical tweet sequence and tweets posted by their neighbours.","offset":1,"pro":0.25,"labels":"PUR"},{"idx":13,"sentence":"We design a topic-driven attention mechanism to capture the dynamic topic shifts in the neighbourhood context.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":13,"sentence":"Experimental results show that the proposed model predicts both the posting time and the stance labels of future tweets more accurately compared to a number of competitive baselines.","offset":3,"pro":0.75,"labels":"RST"},{"idx":14,"sentence":"Trust is implicit in many online text conversations-striking up new friendships, or asking for tech support.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"But trust can be betrayed through deception.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":14,"sentence":"We study the language and dynamics of deception in the negotiation-based game Diplomacy, where seven players compete for world domination by forging and breaking alliances with each other.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":14,"sentence":"Our study with players from the Diplomacy community gathers 17,289 messages annotated by the sender for their intended truthfulness and by the receiver for their perceived truthfulness.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":14,"sentence":"Unlike existing datasets, this captures deception in long-lasting relationships, where the interlocutors strategically combine truth with lies to advance objectives.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":14,"sentence":"A model that uses power dynamics and conversational contexts can predict when a lie occurs nearly as well as human players.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":15,"sentence":"Generative feature matching network (GFMN) is an approach for training state-of-the-art implicit generative models for images by performing moment matching on features from pre-trained neural networks.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"In this paper, we present new GFMN formulations that are effective for sequential data.","offset":1,"pro":0.25,"labels":"PUR"},{"idx":15,"sentence":"Our experimental results show the effectiveness of the proposed method, SeqGFMN, for three distinct generation tasks in English: unconditional text generation, class-conditional text generation, and unsupervised text style transfer.","offset":2,"pro":0.5,"labels":"CLN"},{"idx":15,"sentence":"SeqGFMN is stable to train and outperforms various adversarial approaches for text generation and text style transfer.","offset":3,"pro":0.75,"labels":"CLN"},{"idx":16,"sentence":"A number of researchers have recently questioned the necessity of increasingly complex neural network (NN) architectures.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"In particular, several recent papers have shown that simpler, properly tuned models are at least competitive across several NLP tasks.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":16,"sentence":"In this work, we show that this is also the case for text generation from structured and unstructured data.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":16,"sentence":"We consider neural table-to-text generation and neural question generation (NQG) tasks for text generation from structured and unstructured data, respectively.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":16,"sentence":"Table-to-text generation aims to generate a description based on a given table, and NQG is the task of generating a question from a given passage where the generated question can be answered by a certain sub-span of the passage using NN models.","offset":4,"pro":0.5714285714285714,"labels":"BAC"},{"idx":16,"sentence":"Experimental results demonstrate that a basic attention-based seq2seq model trained with the exponential moving average technique achieves the state of the art in both tasks.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":16,"sentence":"Code is available at https://github.com/h-shahidi/2birds-gen.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":17,"sentence":"This paper presents the Bayesian Hierarchical Words Representation (BHWR) learning algorithm.","offset":0,"pro":0,"labels":"PUR"},{"idx":17,"sentence":"BHWR facilitates Variational Bayes word representation learning combined with semantic taxonomy modeling via hierarchical priors.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":17,"sentence":"By propagating relevant information between related words, BHWR utilizes the taxonomy to improve the quality of such representations.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":17,"sentence":"Evaluation of several linguistic datasets demonstrates the advantages of BHWR over suitable alternatives that facilitate Bayesian modeling with or without semantic priors.","offset":3,"pro":0.6,"labels":"CLN"},{"idx":17,"sentence":"Finally, we further show that BHWR produces better representations for rare words.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":18,"sentence":"Fine-tuning of pre-trained transformer models has become the standard approach for solving common NLP tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"Most of the existing approaches rely on a randomly initialized classifier on top of such networks.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":18,"sentence":"We argue that this fine-tuning procedure is sub-optimal as the pre-trained model has no prior on the specific classifier labels, while it might have already learned an intrinsic textual representation of the task.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":18,"sentence":"In this paper, we introduce a new scoring method that casts a plausibility ranking task in a full-text format and leverages the masked language modeling head tuned during the pre-training phase.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":18,"sentence":"We study commonsense reasoning tasks where the model must rank a set of hypotheses given a premise, focusing on the COPA, Swag, HellaSwag and CommonsenseQA datasets.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":18,"sentence":"By exploiting our scoring method without fine-tuning, we are able to produce strong baselines (e.g. 80% test accuracy on COPA) that are comparable to supervised approaches.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":18,"sentence":"Moreover, when fine-tuning directly on the proposed scoring function, we show that our method provides a much more stable training phase across random restarts (e.g x10 standard deviation reduction on COPA test accuracy) and requires less annotated data than the standard classifier approach to reach equivalent performances.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":19,"sentence":"In recent years, knowledge graph embedding becomes a pretty hot research topic of artificial intelligence and plays increasingly vital roles in various downstream applications, such as recommendation and question answering.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"However, existing methods for knowledge graph embedding can not make a proper trade-off between the model complexity and the model expressiveness, which makes them still far from satisfactory.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":19,"sentence":"To mitigate this problem, we propose a lightweight modeling framework that can achieve highly competitive relational expressiveness without increasing the model complexity.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":19,"sentence":"Our framework focuses on the design of scoring functions and highlights two critical characteristics: 1) facilitating sufficient feature interactions; 2) preserving both symmetry and antisymmetry properties of relations.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":19,"sentence":"It is noteworthy that owing to the general and elegant design of scoring functions, our framework can incorporate many famous existing methods as special cases.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":19,"sentence":"Moreover, extensive experiments on public benchmarks demonstrate the efficiency and effectiveness of our framework.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":19,"sentence":"Source codes and data can be found at url{https://github.com/Wentao-Xu/SEEK.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":0,"sentence":"We propose a Semi-supervIsed GeNerative Active Learning (SIGNAL) model to address the imbalance, efficiency, and text camouflage problems of Chinese text spam detection task.","offset":0,"pro":0,"labels":"PUR"},{"idx":0,"sentence":"A self-diversity criterion is proposed for measuring the worthiness of a candidate for annotation.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":0,"sentence":"A semi-supervised variational autoencoder with masked attention learning approach and a character variation graph-enhanced augmentation procedure are proposed for data augmentation.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":0,"sentence":"The preliminary experiment demonstrates the proposed SIGNAL model is not only sensitive to spam sample selection, but also can improve the performance of a series of conventional active learning models for Chinese spam detection task.","offset":3,"pro":0.6,"labels":"CLN"},{"idx":0,"sentence":"To the best of our knowledge, this is the first work to integrate active learning and semi-supervised generative learning for text spam detection.","offset":4,"pro":0.8,"labels":"CTN"},{"idx":1,"sentence":"Legal Judgement Prediction (LJP) is the task of automatically predicting a law case's judgment results given a text describing the case's facts, which has great prospects in judicial assistance systems and handy services for the public.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"In practice, confusing charges are often presented, because law cases applicable to similar law articles are easily misjudged.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":1,"sentence":"To address this issue, existing work relies heavily on domain experts, which hinders its application in different law systems.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":1,"sentence":"In this paper, we present an end-to-end model, LADAN, to solve the task of LJP.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":1,"sentence":"To distinguish confusing charges, we propose a novel graph neural network, GDL, to automatically learn subtle differences between confusing law articles, and also design a novel attention mechanism that fully exploits the learned differences to attentively extract effective discriminative features from fact descriptions.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":1,"sentence":"Experiments conducted on real-world datasets demonstrate the superiority of our LADAN.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":2,"sentence":"Writing a good job posting is a critical step in the recruiting process, but the task is often more difficult than many people think.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"It is challenging to specify the level of education, experience, relevant skills per the company information and job description.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":2,"sentence":"To this end, we propose a novel task of Job Posting Generation (JPG) which is cast as a conditional text generation problem to generate job requirements according to the job descriptions.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":2,"sentence":"To deal with this task, we devise a data-driven global Skill-Aware Multi-Attention generation model, named SAMA.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":2,"sentence":"Specifically, to model the complex mapping relationships between input and output, we design a hierarchical decoder that we first label the job description with multiple skills, then we generate a complete text guided by the skill labels.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":2,"sentence":"At the same time, to exploit the prior knowledge about the skills, we further construct a skill knowledge graph to capture the global prior knowledge of skills and refine the generated results.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":2,"sentence":"The proposed approach is evaluated on real-world job posting data.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":2,"sentence":"Experimental results clearly demonstrate the effectiveness of the proposed method.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":3,"sentence":"The International Classification of Diseases (ICD) provides a standardized way for classifying diseases, which endows each disease with a unique code.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"ICD coding aims to assign proper ICD codes to a medical record.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":3,"sentence":"Since manual coding is very laborious and prone to errors, many methods have been proposed for the automatic ICD coding task.","offset":2,"pro":0.25,"labels":"BAC"},{"idx":3,"sentence":"However, most of existing methods independently predict each code, ignoring two important characteristics: Code Hierarchy and Code Co-occurrence.","offset":3,"pro":0.375,"labels":"GAP"},{"idx":3,"sentence":"In this paper, we propose a Hyperbolic and Co-graph Representation method (HyperCore) to address the above problem.","offset":4,"pro":0.5,"labels":"PUR"},{"idx":3,"sentence":"Specifically, we propose a hyperbolic representation method to leverage the code hierarchy.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":3,"sentence":"Moreover, we propose a graph convolutional network to utilize the code co-occurrence.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":3,"sentence":"Experimental results on two widely used datasets demonstrate that our proposed model outperforms previous state-of-the-art methods.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":4,"sentence":"Although deep neural networks are effective at extracting high-level features, classification methods usually encode an input into a vector representation via simple feature aggregation operations (e.g. pooling).","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"In this case, one vector can not sufficiently capture its salient and discriminative content.","offset":1,"pro":0.1,"labels":"GAP"},{"idx":4,"sentence":"Thus, we propose Hyperbolic Capsule Networks (HyperCaps) for Multi-Label Classification (MLC), which have two merits.","offset":2,"pro":0.2,"labels":"PUR"},{"idx":4,"sentence":"First, hyperbolic capsules are designed to capture fine-grained document information for each label, which has the ability to characterize complicated structures among labels and documents.","offset":3,"pro":0.3,"labels":"MTD"},{"idx":4,"sentence":"Second, Hyperbolic Dynamic Routing (HDR) is introduced to aggregate hyperbolic capsules in a label-aware manner, so that the label-level discriminative information can be preserved along the depth of neural networks.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":4,"sentence":"To efficiently handle large-scale MLC datasets, we additionally present a new routing method to adaptively adjust the capsule number during routing.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":4,"sentence":"Extensive experiments are conducted on four benchmark datasets.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":4,"sentence":"Compared with the state-of-the-art methods, HyperCaps significantly improves the performance of MLC especially on tail labels.","offset":7,"pro":0.7,"labels":"CLN"},{"idx":4,"sentence":"Such operations limit the performance.","offset":8,"pro":0.8,"labels":"GAP"},{"idx":4,"sentence":"For instance, a multi-label document may contain several concepts.","offset":9,"pro":0.9,"labels":"BAC"},{"idx":5,"sentence":"Technical support problems are often long and complex.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"They typically contain user descriptions of the problem, the setup, and steps for attempted resolution.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":5,"sentence":"Often they also contain various non-natural language text elements like outputs of commands, snippets of code, error messages or stack traces.","offset":2,"pro":0.2,"labels":"BAC"},{"idx":5,"sentence":"These elements contain potentially crucial information for problem resolution.","offset":3,"pro":0.3,"labels":"BAC"},{"idx":5,"sentence":"However, they cannot be correctly parsed by tools designed for natural language.","offset":4,"pro":0.4,"labels":"GAP"},{"idx":5,"sentence":"In this paper, we address the problem of segmentation for technical support questions.","offset":5,"pro":0.5,"labels":"PUR"},{"idx":5,"sentence":"We formulate the problem as a sequence labelling task, and study the performance of state of the art approaches.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":5,"sentence":"We compare this against an intuitive contextual sentence-level classification baseline, and a state of the art supervised text-segmentation approach.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":5,"sentence":"We also introduce a novel component of combining contextual embeddings from multiple language models pre-trained on different data sources, which achieves a marked improvement over using embeddings from a single pre-trained language model.","offset":8,"pro":0.8,"labels":"MTD"},{"idx":5,"sentence":"Finally, we also demonstrate the usefulness of such segmentation with improvements on the downstream task of answer retrieval. ","offset":9,"pro":0.9,"labels":"MTD"},{"idx":6,"sentence":"Therefore, we present MOOCCube, a large-scale data repository of over 700 MOOC courses, 100k concepts, 8 million student behaviors with an external resource.","offset":0,"pro":0,"labels":"PUR"},{"idx":6,"sentence":"Moreover, we conduct a prerequisite discovery task as an example application to show the potential of MOOCCube in facilitating relevant research.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":6,"sentence":"The data repository is now available at http://moocdata.cn/data/MOOCCube.","offset":2,"pro":0.4,"labels":"CTN"},{"idx":6,"sentence":"The prosperity of Massive Open Online Courses (MOOCs) provides fodder for many NLP and AI research for education applications, e.g., course concept extraction, prerequisite relation discovery, etc.","offset":3,"pro":0.6,"labels":"BAC"},{"idx":6,"sentence":"However, the publicly available datasets of MOOC are limited in size with few types of data, which hinders advanced models and novel attempts in related topics.","offset":4,"pro":0.8,"labels":"GAP"},{"idx":7,"sentence":"The automatic text-based diagnosis remains a challenging task for clinical use because it requires appropriate balance between accuracy and interpretability.","offset":0,"pro":0,"labels":"GAP"},{"idx":7,"sentence":"In this paper, we attempt to propose a solution by introducing a novel framework that stacks Bayesian Network Ensembles on top of Entity-Aware Convolutional Neural Networks (CNN) towards building an accurate yet interpretable diagnosis system.","offset":1,"pro":0.25,"labels":"PUR"},{"idx":7,"sentence":"The proposed framework takes advantage of the high accuracy and generality of deep neural networks as well as the interpretability of Bayesian Networks, which is critical for AI-empowered healthcare.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":7,"sentence":"The evaluation conducted on the real Electronic Medical Record (EMR) documents from hospitals and annotated by professional doctors proves that, the proposed framework outperforms the previous automatic diagnosis methods in accuracy performance and the diagnosis explanation of the framework is reasonable. ","offset":3,"pro":0.75,"labels":"CLN"},{"idx":8,"sentence":"News editorials argue about political issues in order to challenge or reinforce the stance of readers with different ideologies.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"Previous research has investigated such persuasive effects for argumentative content.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":8,"sentence":"In contrast, this paper studies how important the style of news editorials is to achieve persuasion.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":8,"sentence":"To this end, we first compare content- and style-oriented classifiers on editorials from the liberal NYTimes with ideology-specific effect annotations.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":8,"sentence":"We find that conservative readers are resistant to NYTimes style, but on liberals, style even has more impact than content.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":8,"sentence":"Focusing on liberals, we then cluster the leads, bodies, and endings of editorials, in order to learn about writing style patterns of effective argumentation.","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":9,"sentence":"In recent years, a new interesting task, called emotion-cause pair extraction (ECPE), has emerged in the area of text emotion analysis.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"It aims at extracting the potential pairs of emotions and their corresponding causes in a document.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":9,"sentence":"To solve this task, the existing research employed a two-step framework, which first extracts individual emotion set and cause set, and then pair the corresponding emotions and causes.","offset":2,"pro":0.25,"labels":"BAC"},{"idx":9,"sentence":"However, such a pipeline of two steps contains some inherent flaws: 1) the modeling does not aim at extracting the final emotion-cause pair directly; 2) the errors from the first step will affect the performance of the second step.","offset":3,"pro":0.375,"labels":"GAP"},{"idx":9,"sentence":"To address these shortcomings, in this paper we propose a new end-to-end approach, called ECPE-Two-Dimensional (ECPE-2D), to represent the emotion-cause pairs by a 2D representation scheme.","offset":4,"pro":0.5,"labels":"PUR"},{"idx":9,"sentence":"A 2D transformer module and two variants, window-constrained and cross-road 2D transformers, are further proposed to model the interactions of different emotion-cause pairs.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":9,"sentence":"The 2D representation, interaction, and prediction are integrated into a joint framework.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":9,"sentence":"In addition to the advantages of joint modeling, the experimental results on the benchmark emotion cause corpus show that our approach improves the F1 score of the state-of-the-art from 61.28% to 68.89%.","offset":7,"pro":0.875,"labels":"RST"},{"idx":10,"sentence":"Emotion-cause pair extraction aims to extract all emotion clauses coupled with their cause clauses from a given document.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"Previous work employs two-step approaches, in which the first step extracts emotion clauses and cause clauses separately, and the second step trains a classifier to filter out negative pairs.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":10,"sentence":"However, such pipeline-style system for emotion-cause pair extraction is suboptimal because it suffers from error propagation and the two steps may not adapt to each other well.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":10,"sentence":"In this paper, we tackle emotion-cause pair extraction from a ranking perspective, i.e., ranking clause pair candidates in a document, and propose a one-step neural approach which emphasizes inter-clause modeling to perform end-to-end extraction.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":10,"sentence":"It models the interrelations between the clauses in a document to learn clause representations with graph attention, and enhances clause pair representations with kernel-based relative position embedding for effective ranking.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":10,"sentence":"Experimental results show that our approach significantly outperforms the current two-step systems, especially in the condition of extracting multiple pairs in one document.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":11,"sentence":"We present a simple but effective method for aspect identification in sentiment analysis.","offset":0,"pro":0,"labels":"PUR"},{"idx":11,"sentence":"Our unsupervised method only requires word embeddings and a POS tagger, and is therefore straightforward to apply to new domains and languages.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":11,"sentence":"Previous work relied on syntactic features and complex neural models.","offset":2,"pro":0.3333333333333333,"labels":"BAC"},{"idx":11,"sentence":"We show that given the simplicity of current benchmark datasets for aspect extraction, such complex models are not needed.","offset":3,"pro":0.5,"labels":"CLN"},{"idx":11,"sentence":"The code to reproduce the experiments reported in this paper is available at https://github.com/clips/cat.","offset":4,"pro":0.6666666666666666,"labels":"CTN"},{"idx":11,"sentence":"We introduce Contrastive Attention (CAt), a novel single-head attention mechanism based on an RBF kernel, which gives a considerable boost in performance and makes the model interpretable.","offset":5,"pro":0.8333333333333334,"labels":"PUR"},{"idx":12,"sentence":"Stance detection is an important task, which aims to classify the attitude of an opinionated text towards a given target.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"Remarkable success has been achieved when sufficient labeled training data is available.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":12,"sentence":"However, annotating sufficient data is labor-intensive, which establishes significant barriers for generalizing the stance classifier to the data with new targets.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":12,"sentence":"In this paper, we proposed a Semantic-Emotion Knowledge Transferring (SEKT) model for cross-target stance detection, which uses the external knowledge (semantic and emotion lexicons) as a bridge to enable knowledge transfer across different targets.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":12,"sentence":"Specifically, a semantic-emotion heterogeneous graph is constructed from external semantic and emotion lexicons, which is then fed into a graph convolutional network to learn multi-hop semantic connections between words and emotion tags.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":12,"sentence":"Then, the learned semantic-emotion graph representation, which serves as prior knowledge bridging the gap between the source and target domains, is fully integrated into the bidirectional long short-term memory (BiLSTM) stance classifier by adding a novel knowledge-aware memory unit to the BiLSTM cell.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":12,"sentence":"Extensive experiments on a large real-world dataset demonstrate the superiority of SEKT against the state-of-the-art baseline methods.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":13,"sentence":"Cross-domain sentiment analysis has received significant attention in recent years, prompted by the need to combat the domain gap between different applications that make use of sentiment analysis.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"In this paper, we take a novel perspective on this task by exploring the role of external commonsense knowledge.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":13,"sentence":"We introduce a new framework, KinGDOM, which utilizes the ConceptNet knowledge graph to enrich the semantics of a document by providing both domain-specific and domain-general background concepts.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":13,"sentence":"These concepts are learned by training a graph convolutional autoencoder that leverages inter-domain concepts in a domain-invariant manner.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":13,"sentence":"Conditioning a popular domain-adversarial baseline method with these learned concepts helps improve its performance over state-of-the-art approaches, demonstrating the efficacy of our proposed framework. ","offset":4,"pro":0.8,"labels":"CLN"},{"idx":14,"sentence":"The aspect-based sentiment analysis (ABSA) consists of two conceptual tasks, namely an aspect extraction and an aspect sentiment classification.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"Previous works in ABSA tasks did not fully leverage the importance of syntactical information.","offset":1,"pro":0.1,"labels":"GAP"},{"idx":14,"sentence":"Hence, the aspect extraction model often failed to detect the boundaries of multi-word aspect terms.","offset":2,"pro":0.2,"labels":"GAP"},{"idx":14,"sentence":"On the other hand, the aspect sentiment classifier was unable to account for the syntactical correlation between aspect terms and the context words.","offset":3,"pro":0.3,"labels":"GAP"},{"idx":14,"sentence":"This paper explores the grammatical aspect of the sentence and employs the self-attention mechanism for syntactical learning.","offset":4,"pro":0.4,"labels":"PUR"},{"idx":14,"sentence":"We combine part-of-speech embeddings, dependency-based embeddings and contextualized embeddings (e.g. BERT, RoBERTa) to enhance the performance of the aspect extractor.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":14,"sentence":"We also propose the syntactic relative distance to de-emphasize the adverse effects of unrelated words, having weak syntactic connection with the aspect terms.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":14,"sentence":"This increases the accuracy of the aspect sentiment classifier.","offset":7,"pro":0.7,"labels":"RST"},{"idx":14,"sentence":"Our solutions outperform the state-of-the-art models on SemEval-2014 dataset in both two subtasks.","offset":8,"pro":0.8,"labels":"RST"},{"idx":14,"sentence":"Rather than considering the tasks separately, we build an end-to-end ABSA solution. ","offset":9,"pro":0.9,"labels":"MTD"},{"idx":15,"sentence":"The main barrier to progress in the task of Formality Style Transfer is the inadequacy of training data.","offset":0,"pro":0,"labels":"GAP"},{"idx":15,"sentence":"In this paper, we study how to augment parallel data and propose novel and simple data augmentation methods for this task to obtain useful sentence pairs with easily accessible models and systems.","offset":1,"pro":0.3333333333333333,"labels":"PUR"},{"idx":15,"sentence":"Experiments demonstrate that our augmented parallel data largely helps improve formality style transfer when it is used to pre-train the model, leading to the state-of-the-art results in the GYAFC benchmark dataset.","offset":2,"pro":0.6666666666666666,"labels":"RST"},{"idx":16,"sentence":"Aspect-based sentiment analysis aims to determine the sentiment polarity towards a specific aspect in online reviews.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"Most recent efforts adopt attention-based neural network models to implicitly connect aspects with opinion words.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":16,"sentence":"However, due to the complexity of language and the existence of multiple aspects in a single sentence, these models often confuse the connections.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":16,"sentence":"In this paper, we address this problem by means of effective encoding of syntax information.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":16,"sentence":"Firstly, we define a unified aspect-oriented dependency tree structure rooted at a target aspect by reshaping and pruning an ordinary dependency parse tree.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":16,"sentence":"Then, we propose a relational graph attention network (R-GAT) to encode the new tree structure for sentiment prediction.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":16,"sentence":"Extensive experiments are conducted on the SemEval 2014 and Twitter datasets, and the experimental results confirm that the connections between aspects and opinion words can be better established with our approach, and the performance of the graph attention network (GAT) is significantly improved as a consequence. ","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":17,"sentence":"Aspect terms extraction and opinion terms extraction are two key problems of fine-grained Aspect Based Sentiment Analysis (ABSA).","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"The aspect-opinion pairs can provide a global profile about a product or service for consumers and opinion mining systems.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":17,"sentence":"However, traditional methods can not directly output aspect-opinion pairs without given aspect terms or opinion terms.","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":17,"sentence":"Although some recent co-extraction methods have been proposed to extract both terms jointly, they fail to extract them as pairs.","offset":3,"pro":0.3333333333333333,"labels":"GAP"},{"idx":17,"sentence":"To this end, this paper proposes an end-to-end method to solve the task of Pair-wise Aspect and Opinion Terms Extraction (PAOTE).","offset":4,"pro":0.4444444444444444,"labels":"PUR"},{"idx":17,"sentence":"Furthermore, this paper treats the problem from a perspective of joint term and relation extraction rather than under the sequence tagging formulation performed in most prior works.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":17,"sentence":"We propose a multi-task learning framework based on shared spans, where the terms are extracted under the supervision of span boundaries.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":17,"sentence":"Meanwhile, the pair-wise relations are jointly identified using the span representations.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":17,"sentence":"Extensive experiments show that our model consistently outperforms state-of-the-art methods. ","offset":8,"pro":0.8888888888888888,"labels":"RST"},{"idx":18,"sentence":"Opinion role labeling (ORL) is a fine-grained opinion analysis task and aims to answer who expressed what kind of sentiment towards what?.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"Due to the scarcity of labeled data, ORL remains challenging for data-driven methods.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":18,"sentence":"In this work, we try to enhance neural ORL models with syntactic knowledge by comparing and integrating different representations.","offset":2,"pro":0.2222222222222222,"labels":"PUR"},{"idx":18,"sentence":"We also propose dependency graph convolutional networks (DEPGCN) to encode parser information at different processing levels.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":18,"sentence":"In order to compensate for parser inaccuracy and reduce error propagation, we introduce multi-task learning (MTL) to train the parser and the ORL model simultaneously.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":18,"sentence":"We verify our methods on the benchmark MPQA corpus.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":18,"sentence":"The experimental results show that syntactic information is highly valuable for ORL, and our final MTL model effectively boosts the F1 score by 9.29 over the syntax-agnostic baseline.","offset":6,"pro":0.6666666666666666,"labels":"RST"},{"idx":18,"sentence":"In addition, we find that the contributions from syntactic knowledge do not fully overlap with contextualized word representations (BERT).","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":18,"sentence":"Our best model achieves 4.34 higher F1 score than the current state-ofthe-art.","offset":8,"pro":0.8888888888888888,"labels":"RST"},{"idx":19,"sentence":"State-of-the-art argument mining studies have advanced the techniques for predicting argument structures.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"However, the technology for capturing non-tree-structured arguments is still in its infancy.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":19,"sentence":"In this paper, we focus on non-tree argument mining with a neural model.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":19,"sentence":"We jointly predict proposition types and edges between propositions.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":19,"sentence":"Our proposed model incorporates (i) task-specific parameterization (TSP) that effectively encodes a sequence of propositions and (ii) a proposition-level biaffine attention (PLBA) that can predict a non-tree argument consisting of edges.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":19,"sentence":"Experimental results show that both TSP and PLBA boost edge prediction performance compared to baselines.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":0,"sentence":"We propose a novel linearization of a constituent tree, together with a new locally normalized model.","offset":0,"pro":0,"labels":"PUR"},{"idx":0,"sentence":"For each split point in a sentence, our model computes the normalizer on all spans ending with that split point, and then predicts a tree span from them.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":0,"sentence":"Compared with global models, our model is fast and parallelizable.","offset":2,"pro":0.4,"labels":"RST"},{"idx":0,"sentence":"Different from previous local models, our linearization method is tied on the spans directly and considers more local features when performing span prediction, which is more interpretable and effective.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":0,"sentence":"Experiments on PTB (95.8 F1) and CTB (92.4 F1) show that our model significantly outperforms existing local models and efficiently achieves competitive results with global models.","offset":4,"pro":0.8,"labels":"RST"},{"idx":1,"sentence":"Unsupervised constituency parsing aims to learn a constituency parser from a training corpus without parse tree annotations.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"While many methods have been proposed to tackle the problem, including statistical and neural methods, their experimental results are often not directly comparable due to discrepancies in datasets, data preprocessing, lexicalization, and evaluation metrics.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":1,"sentence":"In this paper, we first examine experimental settings used in previous work and propose to standardize the settings for better comparability between methods.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":1,"sentence":"We then empirically compare several existing methods, including decade-old and newly proposed ones, under the standardized settings on English and Japanese, two languages with different branching tendencies.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":1,"sentence":"We find that recent models do not show a clear advantage over decade-old models in our experiments.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":1,"sentence":"We hope our work can provide new insights into existing methods and facilitate future empirical evaluation of unsupervised constituency parsing.","offset":5,"pro":0.8333333333333334,"labels":"IMP"},{"idx":2,"sentence":"We propose a novel constituency parsing model that casts the parsing problem into a series of pointing tasks.","offset":0,"pro":0,"labels":"PUR"},{"idx":2,"sentence":"Specifically, our model estimates the likelihood of a span being a legitimate tree constituent via the pointing score corresponding to the boundary words of the span.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":2,"sentence":"Our parsing model supports efficient top-down decoding and our learning objective is able to enforce structural consistency without resorting to the expensive CKY inference.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":2,"sentence":"The experiments on the standard English Penn Treebank parsing task show that our method achieves 92.78 F1 without using pre-trained models, which is higher than all the existing methods with similar time complexity.","offset":3,"pro":0.5,"labels":"RST"},{"idx":2,"sentence":"Using pre-trained BERT, our model achieves 95.48 F1, which is competitive with the state-of-the-art while being faster.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":2,"sentence":"Our approach also establishes new state-of-the-art in Basque and Swedish in the SPMRL shared tasks on multilingual constituency parsing.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":3,"sentence":"In the deep learning (DL) era, parsing models are extremely simplified with little hurt on performance, thanks to the remarkable capability of multi-layer BiLSTMs in context representation.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"As the most popular graph-based dependency parser due to its high efficiency and performance, the biaffine parser directly scores single dependencies under the arc-factorization assumption, and adopts a very simple local token-wise cross-entropy training loss.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":3,"sentence":"This paper for the first time presents a second-order TreeCRF extension to the biaffine parser.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":3,"sentence":"For a long time, the complexity and inefficiency of the inside-outside algorithm hinder the popularity of TreeCRF.","offset":3,"pro":0.42857142857142855,"labels":"GAP"},{"idx":3,"sentence":"To address this issue, we propose an effective way to batchify the inside and Viterbi algorithms for direct large matrix operation on GPUs, and to avoid the complex outside algorithm via efficient back-propagation.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":3,"sentence":"Experiments and analysis on 27 datasets from 13 languages clearly show that techniques developed before the DL era, such as structural learning (global TreeCRF loss) and high-order modeling are still useful, and can further boost parsing performance over the state-of-the-art biaffine parser, especially for partially annotated training data.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":3,"sentence":"We release our code at https://github.com/yzhangcs/crfpar.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":4,"sentence":"Sequence-based neural networks show significant sensitivity to syntactic structure, but they still perform less well on syntactic tasks than tree-based networks.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"Such tree-based networks can be provided with a constituency parse, a dependency parse, or both.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":4,"sentence":"We evaluate which of these two representational schemes more effectively introduces biases for syntactic structure that increase performance on the subject-verb agreement prediction task.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":4,"sentence":"We find that a constituency-based network generalizes more robustly than a dependency-based one, and that combining the two types of structure does not yield further improvement.","offset":3,"pro":0.6,"labels":"RST"},{"idx":4,"sentence":"Finally, we show that the syntactic robustness of sequential models can be substantially improved by fine-tuning on a small amount of constructed data, suggesting that data augmentation is a viable alternative to explicit constituency structure for imparting the syntactic biases that sequential models are lacking.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":5,"sentence":"Multilingual sequence labeling is a task of predicting label sequences using a single unified model for multiple languages.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"Compared with relying on multiple monolingual models, using a multilingual model has the benefit of a smaller model size, easier in online serving, and generalizability to low-resource languages.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":5,"sentence":"However, current multilingual models still underperform individual monolingual models significantly due to model capacity limitations.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":5,"sentence":"In this paper, we propose to reduce the gap between monolingual models and the unified multilingual model by distilling the structural knowledge of several monolingual models (teachers) to the unified multilingual model (student).","offset":3,"pro":0.5,"labels":"PUR"},{"idx":5,"sentence":"We propose two novel KD methods based on structure-level information: (1) approximately minimizes the distance between the student's and the teachers' structure-level probability distributions, (2) aggregates the structure-level knowledge to local distributions and minimizes the distance between two local probability distributions.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":5,"sentence":"Our experiments on 4 multilingual tasks with 25 datasets show that our approaches outperform several strong baselines and have stronger zero-shot generalizability than both the baseline model and teacher models.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":6,"sentence":"Trending topics in social media content evolve over time, and it is therefore crucial to understand social media users and their interpersonal communications in a dynamic manner.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"Here we study dynamic online conversation recommendation, to help users engage in conversations that satisfy their evolving interests.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":6,"sentence":"While most prior work assumes static user interests, our model is able to capture the temporal aspects of user interests, and further handle future conversations that are unseen during training time.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":6,"sentence":"Concretely, we propose a neural architecture to exploit changes of user interactions and interests over time, to predict which discussions they are likely to enter.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":6,"sentence":"We conduct experiments on large-scale collections of Reddit conversations, and results on three subreddits show that our model significantly outperforms state-of-the-art models that make a static assumption of user interests.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":6,"sentence":"We further evaluate on handling cold start, and observe consistently better performance by our model when considering various degrees of sparsity of user's chatting history and conversation contexts.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":6,"sentence":"Lastly, analyses on our model outputs indicate user interest change, explaining the advantage and efficacy of our approach.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":7,"sentence":"In this paper, we study Multimodal Named Entity Recognition (MNER) for social media posts.","offset":0,"pro":0,"labels":"PUR"},{"idx":7,"sentence":"Existing approaches for MNER mainly suffer from two drawbacks: (1) despite generating word-aware visual representations, their word representations are insensitive to the visual context; (2) most of them ignore the bias brought by the visual context.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":7,"sentence":"To tackle the first issue, we propose a multimodal interaction module to obtain both image-aware word representations and word-aware visual representations.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":7,"sentence":"To alleviate the visual bias, we further propose to leverage purely text-based entity span detection as an auxiliary module, and design a Unified Multimodal Transformer to guide the final predictions with the entity span predictions.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":7,"sentence":"Experiments show that our unified approach achieves the new state-of-the-art performance on two benchmark datasets.","offset":4,"pro":0.8,"labels":"RST"},{"idx":8,"sentence":"Previous works that integrated news articles to better process stock prices used a variety of neural networks to predict price movements.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"The textual and price information were both encoded in the neural network, and it is therefore difficult to apply this approach in situations other than the original framework of the notoriously hard problem of price prediction.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":8,"sentence":"In contrast, this paper presents a method to encode the influence of news articles through a vector representation of stocks called a stock embedding.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":8,"sentence":"The stock embedding is acquired with a deep learning framework using both news articles and price history.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":8,"sentence":"Because the embedding takes the operational form of a vector, it is applicable to other financial problems besides price prediction.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":8,"sentence":"As one example application, we show the results of portfolio optimization using Reuters {& Bloomberg headlines, producing a capital gain 2.8 times larger than that obtained with a baseline method using only stock price data.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":8,"sentence":"This suggests that the proposed stock embedding can leverage textual financial semantics to solve financial prediction problems.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":9,"sentence":"Predicting the political bias and the factuality of reporting of entire news outlets are critical elements of media profiling, which is an understudied but an increasingly important research direction.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"The present level of proliferation of fake, biased, and propagandistic content online has made it impossible to fact-check every single suspicious claim, either manually or automatically.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":9,"sentence":"Thus, it has been proposed to profile entire news outlets and to look for those that are likely to publish fake or biased content.","offset":2,"pro":0.25,"labels":"BAC"},{"idx":9,"sentence":"This makes it possible to detect likely fake news the moment they are published, by simply checking the reliability of their source.","offset":3,"pro":0.375,"labels":"BAC"},{"idx":9,"sentence":"From a practical perspective, political bias and factuality of reporting have a linguistic aspect but also a social context.","offset":4,"pro":0.5,"labels":"BAC"},{"idx":9,"sentence":"Here, we study the impact of both, namely (i) what was written (i.e., what was published by the target medium, and how it describes itself in Twitter) vs. (ii) who reads it (i.e., analyzing the target medium's audience on social media).","offset":5,"pro":0.625,"labels":"PUR"},{"idx":9,"sentence":"We further study (iii) what was written about the target medium (in Wikipedia).","offset":6,"pro":0.75,"labels":"PUR"},{"idx":9,"sentence":"The evaluation results show that what was written matters most, and we further show that putting all information sources together yields huge improvements over the current state-of-the-art.","offset":7,"pro":0.875,"labels":"RST"},{"idx":10,"sentence":"We explore the utilities of explicit negative examples in training neural language models.","offset":0,"pro":0,"labels":"PUR"},{"idx":10,"sentence":"Negative examples here are incorrect words in a sentence, such as barks in *The dogs barks.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":10,"sentence":"Neural language models are commonly trained only on positive examples, a set of sentences in the training data, but recent studies suggest that the models trained in this way are not capable of robustly handling complex syntactic constructions, such as long-distance agreement.","offset":2,"pro":0.2,"labels":"GAP"},{"idx":10,"sentence":"In this paper, we first demonstrate that appropriately using negative examples about particular constructions (e.g., subject-verb agreement) will boost the model's robustness on them in English, with a negligible loss of perplexity.","offset":3,"pro":0.3,"labels":"MTD"},{"idx":10,"sentence":"The key to our success is an additional margin loss between the log-likelihoods of a correct word and an incorrect word.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":10,"sentence":"We then provide a detailed analysis of the trained models.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":10,"sentence":"One of our findings is the difficulty of object-relative clauses for RNNs.","offset":6,"pro":0.6,"labels":"RST"},{"idx":10,"sentence":"We find that even with our direct learning signals the models still suffer from resolving agreement across an object-relative clause.","offset":7,"pro":0.7,"labels":"RST"},{"idx":10,"sentence":"Augmentation of training sentences involving the constructions somewhat helps, but the accuracy still does not reach the level of subject-relative clauses.","offset":8,"pro":0.8,"labels":"RST"},{"idx":10,"sentence":"Although not directly cognitively appealing, our method can be a tool to analyze the true architectural limitation of neural models on challenging linguistic constructions.","offset":9,"pro":0.9,"labels":"CLN"},{"idx":11,"sentence":"We conduct a thorough study to diagnose the behaviors of pre-trained language encoders (ELMo, BERT, and RoBERTa) when confronted with natural grammatical errors.","offset":0,"pro":0,"labels":"PUR"},{"idx":11,"sentence":"Specifically, we collect real grammatical errors from non-native speakers and conduct adversarial attacks to simulate these errors on clean text data.","offset":1,"pro":0.125,"labels":"MTD"},{"idx":11,"sentence":"We use this approach to facilitate debugging models on downstream applications.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":11,"sentence":"Results confirm that the performance of all tested models is affected but the degree of impact varies.","offset":3,"pro":0.375,"labels":"RST"},{"idx":11,"sentence":"To interpret model behaviors, we further design a linguistic acceptability task to reveal their abilities in identifying ungrammatical sentences and the position of errors.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":11,"sentence":"We find that fixed contextual encoders with a simple classifier trained on the prediction of sentence correctness are able to locate error positions.","offset":5,"pro":0.625,"labels":"RST"},{"idx":11,"sentence":"We also design a cloze test for BERT and discover that BERT captures the interaction between errors and specific tokens in context.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":11,"sentence":"Our results shed light on understanding the robustness and behaviors of language encoders against grammatical errors.","offset":7,"pro":0.875,"labels":"IMP"},{"idx":12,"sentence":"Sentence encoders based on the transformer architecture have shown promising results on various natural language tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"The main impetus lies in the pre-trained neural language models that capture long-range dependencies among words, owing to multi-head attention that is unique in the architecture.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":12,"sentence":"However, little is known for how linguistic properties are processed, represented, and utilized for downstream tasks among hundreds of attention heads inside the pre-trained transformer-based model.","offset":2,"pro":0.4,"labels":"GAP"},{"idx":12,"sentence":"For the initial goal of examining the roles of attention heads in handling a set of linguistic features, we conducted a set of experiments with ten probing tasks and three downstream tasks on four pre-trained transformer families (GPT, GPT2, BERT, and ELECTRA).","offset":3,"pro":0.6,"labels":"PUR"},{"idx":12,"sentence":"Meaningful insights are shown through the lens of heat map visualization and utilized to propose a relatively simple sentence representation method that takes advantage of most influential attention heads, resulting in additional performance improvements on the downstream tasks.","offset":4,"pro":0.8,"labels":"MTD"},{"idx":13,"sentence":"Attention has been proven successful in many natural language processing (NLP) tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"Recently, many researchers started to investigate the interpretability of attention on NLP tasks.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":13,"sentence":"Many existing approaches focused on examining whether the local attention weights could reflect the importance of input representations.","offset":2,"pro":0.3333333333333333,"labels":"BAC"},{"idx":13,"sentence":"In this work, we present a study on understanding the internal mechanism of attention by looking into the gradient update process, checking its behavior when approaching a local minimum during training.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":13,"sentence":"We propose to analyze for each word token the following two quantities: its polarity score and its attention score, where the latter is a global assessment on the token's significance.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":13,"sentence":"We discuss conditions under which the attention mechanism may become more (or less) interpretable, and show how the interplay between the two quantities can contribute towards model performance.","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":14,"sentence":"Knowledge graph embedding methods often suffer from a limitation of memorizing valid triples to predict new ones for triple classification and search personalization problems.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"To this end, we introduce a novel embedding model, named R-MeN, that explores a relational memory network to encode potential dependencies in relationship triples.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":14,"sentence":"R-MeN considers each triple as a sequence of 3 input vectors that recurrently interact with a memory using a transformer self-attention mechanism.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":14,"sentence":"Thus R-MeN encodes new information from interactions between the memory and each input vector to return a corresponding vector.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":14,"sentence":"Consequently, R-MeN feeds these 3 returned vectors to a convolutional neural network-based decoder to produce a scalar score for the triple.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":14,"sentence":"Experimental results show that our proposed R-MeN obtains state-of-the-art results on SEARCH17 for the search personalization task, and on WN11 and FB13 for the triple classification task.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":15,"sentence":"It has been a common approach to pre-train a language model on a large corpus and fine-tune it on task-specific data.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"In practice, we observe that fine-tuning a pre-trained model on a small dataset may lead to over- and/or under-estimate problem.","offset":1,"pro":0.25,"labels":"GAP"},{"idx":15,"sentence":"In this paper, we propose MC-Tailor, a novel method to alleviate the above issue in text generation tasks by truncating and transferring the probability mass from over-estimated regions to under-estimated ones.","offset":2,"pro":0.5,"labels":"PUR"},{"idx":15,"sentence":"Experiments on a variety of text generation datasets show that MC-Tailor consistently and significantly outperforms the fine-tuning approach.","offset":3,"pro":0.75,"labels":"RST"},{"idx":16,"sentence":"Most Chinese pre-trained models take character as the basic unit and learn representation according to character's external contexts, ignoring the semantics expressed in the word, which is the smallest meaningful utterance in Chinese.","offset":0,"pro":0,"labels":"GAP"},{"idx":16,"sentence":"Hence, we propose a novel word-aligned attention to exploit explicit word information, which is complementary to various character-based Chinese pre-trained language models.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":16,"sentence":"Specifically, we devise a pooling mechanism to align the character-level attention to the word level and propose to alleviate the potential issue of segmentation error propagation by multi-source information fusion.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":16,"sentence":"As a result, word and character information are explicitly integrated at the fine-tuning procedure.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":16,"sentence":"Experimental results on five Chinese NLP benchmark tasks demonstrate that our method achieves significant improvements against BERT, ERNIE and BERT-wwm.","offset":4,"pro":0.8,"labels":"RST"},{"idx":17,"sentence":"Variational autoencoders (VAEs) combine latent variables with amortized variational inference, whose optimization usually converges into a trivial local optimum termed posterior collapse, especially in text modeling.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"By tracking the optimization dynamics, we observe the encoder-decoder incompatibility that leads to poor parameterizations of the data manifold.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":17,"sentence":"We argue that the trivial local optimum may be avoided by improving the encoder and decoder parameterizations since the posterior network is part of a transition map between them.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":17,"sentence":"To this end, we propose Coupled-VAE, which couples a VAE model with a deterministic autoencoder with the same structure and improves the encoder and decoder parameterizations via encoder weight sharing and decoder signal matching.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":17,"sentence":"We apply the proposed Coupled-VAE approach to various VAE models with different regularization, posterior family, decoder structure, and optimization strategy.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":17,"sentence":"Experiments on benchmark datasets (i.e., PTB, Yelp, and Yahoo) show consistently improved results in terms of probability estimation and richness of the latent space.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":17,"sentence":"We also generalize our method to conditional language modeling and propose Coupled-CVAE, which largely improves the diversity of dialogue generation on the Switchboard dataset.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":18,"sentence":"State-of-the-art NLP models can often be fooled by human-unaware transformations such as synonymous word substitution.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"For security reasons, it is of critical importance to develop models with certified robustness that can provably guarantee that the prediction is can not be altered by any possible synonymous word substitution.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":18,"sentence":"In this work, we propose a certified robust method based on a new randomized smoothing technique, which constructs a stochastic ensemble by applying random word substitutions on the input sentences, and leverage the statistical properties of the ensemble to provably certify the robustness.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":18,"sentence":"Our method is simple and structure-free in that it only requires the black-box queries of the model outputs, and hence can be applied to any pre-trained models (such as BERT) and any types of models (world-level or subword-level).","offset":3,"pro":0.5,"labels":"MTD"},{"idx":18,"sentence":"Our method significantly outperforms recent state-of-the-art methods for certified robustness on both IMDB and Amazon text classification tasks.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":18,"sentence":"To the best of our knowledge, we are the first work to achieve certified robustness on large systems such as BERT with practically meaningful certified accuracy.","offset":5,"pro":0.8333333333333334,"labels":"CTN"},{"idx":19,"sentence":"Unsupervised bilingual lexicon induction is the task of inducing word translations from monolingual corpora of two languages.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"Recent methods are mostly based on unsupervised cross-lingual word embeddings, the key to which is to find initial solutions of word translations, followed by the learning and refinement of mappings between the embedding spaces of two languages.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":19,"sentence":"However, previous methods find initial solutions just based on word-level information, which may be (1) limited and inaccurate, and (2) prone to contain some noise introduced by the insufficiently pre-trained embeddings of some words.","offset":2,"pro":0.2,"labels":"GAP"},{"idx":19,"sentence":"To deal with those issues, in this paper, we propose a novel graph-based paradigm to induce bilingual lexicons in a coarse-to-fine way.","offset":3,"pro":0.3,"labels":"PUR"},{"idx":19,"sentence":"We first build a graph for each language with its vertices representing different words.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":19,"sentence":"Then we extract word cliques from the graphs and map the cliques of two languages.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":19,"sentence":"Based on that, we induce the initial word translation solution with the central words of the aligned cliques.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":19,"sentence":"This coarse-to-fine approach not only leverages clique-level information, which is richer and more accurate, but also effectively reduces the bad effect of the noise in the pre-trained embeddings.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":19,"sentence":"Finally, we take the initial solution as the seed to learn cross-lingual embeddings, from which we induce bilingual lexicons.","offset":8,"pro":0.8,"labels":"MTD"},{"idx":19,"sentence":"Experiments show that our approach improves the performance of bilingual lexicon induction compared with previous methods.","offset":9,"pro":0.9,"labels":"RST"},{"idx":0,"sentence":"The recently proposed DEtection TRansformer (DETR) achieves promising performance for end-to-end object detection.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"However, it has relatively lower detection performance on small objects and suffers from slow convergence.","offset":1,"pro":0.08333333333333333,"labels":"GAP"},{"idx":0,"sentence":"Specifically, we propose a novel Coarse-to-Fine (CF) decoder layer constituted of a coarse layer and a carefully designed fine layer.","offset":2,"pro":0.16666666666666666,"labels":"MTD"},{"idx":0,"sentence":"Within each CF decoder layer, the extracted local information (region of interest feature) is introduced into the flow of global context information from the coarse layer to refine and enrich the object query features via the fine layer.","offset":3,"pro":0.25,"labels":"MTD"},{"idx":0,"sentence":"In the fine layer, the multi-scale information can be fully explored and exploited via the Adaptive Scale Fusion(ASF) module and Local Cross-Attention (LCA) module.","offset":4,"pro":0.3333333333333333,"labels":"MTD"},{"idx":0,"sentence":"The multi-scale information can also be enhanced by another proposed Transformer Enhanced FPN (TEF) module to further improve the performance.","offset":5,"pro":0.4166666666666667,"labels":"MTD"},{"idx":0,"sentence":"With our proposed framework (named CF-DETR), the localization accuracy of objects (especially for small objects) can be largely improved.","offset":6,"pro":0.5,"labels":"RST"},{"idx":0,"sentence":"The effectiveness of CF-DETR is validated via extensive experiments on the coco benchmark.","offset":7,"pro":0.5833333333333334,"labels":"MTD"},{"idx":0,"sentence":"CF-DETR achieves state-of-the-art performance among end-to-end detectors, e.g., achieving 47.8 AP using ResNet-50 with 36 epochs in the standard 3x training schedule.","offset":8,"pro":0.6666666666666666,"labels":"RST"},{"idx":0,"sentence":"This paper observed that DETR performs surprisingly well even on small objects when measuring Average Precision (AP) at decreased Intersection-over-Union (IoU) thresholds. ","offset":9,"pro":0.75,"labels":"BAC"},{"idx":0,"sentence":"Motivated by this observation, we propose a simple way to improve DETR by refining the coarse features and predicted locations. ","offset":10,"pro":0.8333333333333334,"labels":"PUR"},{"idx":0,"sentence":"As a byproduct, the slow convergence issue of DETR can also be addressed. ","offset":11,"pro":0.9166666666666666,"labels":"RST"},{"idx":1,"sentence":"This paper starts by revealing a surprising finding: without any learning, a randomly initialized CNN can localize objects surprisingly well.","offset":0,"pro":0,"labels":"PUR"},{"idx":1,"sentence":"That is, a CNN has an inductive bias to naturally focus on objects, named as Tobias (\"The object is at sight\") in this paper.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":1,"sentence":"This empirical inductive bias is further analyzed and successfully applied to self-supervised learning (SSL).","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":1,"sentence":"A CNN is encouraged to learn representations that focus on the foreground object, by transforming every image into various versions with different backgrounds, where the foreground and background separation is guided by Tobias.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":1,"sentence":"Experimental results show that the proposed Tobias significantly improves downstream tasks, especially for object detection.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":1,"sentence":"This paper also shows that Tobias has consistent improvements on training sets of different sizes, and is more resilient to changes in image augmentations.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":2,"sentence":"When estimating a texture map, a single image is insufficient as it reveals only one facet of a 3D object.","offset":0,"pro":0,"labels":"GAP"},{"idx":2,"sentence":"To provide sufficient information for estimating a complete texture map, the proposed model simultaneously generates multi-view hallucinations in the image domain and an estimated texture map in the texture domain.","offset":1,"pro":0.125,"labels":"MTD"},{"idx":2,"sentence":"During the generating process, each domain generator exchanges features to the other by a flow-based local attention mechanism.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":2,"sentence":"In this manner, the proposed model can estimate a texture map utilizing abundant multi-view image features from which multiview hallucinations are generated.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":2,"sentence":"As a result, the estimated texture map contains consistent colors and patterns over the entire region.","offset":4,"pro":0.5,"labels":"RST"},{"idx":2,"sentence":"Experiments show the superiority of our model for estimating a directly render-able texture map, which is applicable to 3D animation rendering.","offset":5,"pro":0.625,"labels":"CLN"},{"idx":2,"sentence":"Furthermore, our model also improves an overall generation quality in the image domain for pose and viewpoint transfer tasks.","offset":6,"pro":0.75,"labels":"CLN"},{"idx":2,"sentence":"We propose a dual-domain generative model to estimate a texture map from a single image for colorizing a 3D human model. ","offset":7,"pro":0.875,"labels":"PUR"},{"idx":3,"sentence":"Scene Graph Generation (SGG) aims to build a structured representation of a scene using objects and pairwise relationships, which benefits downstream tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"However, current SGG methods usually suffer from sub-optimal scene graph generation because of the long-tailed distribution of training data.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":3,"sentence":"To address this problem, we propose Resistance Training using Prior Bias (RTPB) for the scene graph generation.","offset":2,"pro":0.2222222222222222,"labels":"PUR"},{"idx":3,"sentence":"Specifically, RTPB uses a distributed-based prior bias to improve models' detecting ability on less frequent relationships during training, thus improving the model generalizability on tail categories.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":3,"sentence":"In addition, to further explore the contextual information of objects and relationships, we design a contextual encoding backbone network, termed as Dual Transformer (DTrans).","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":3,"sentence":"We perform extensive experiments on a very popular benchmark, VG150, to demonstrate the effectiveness of our method for the unbiased scene graph generation.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":3,"sentence":"In specific, our RTPB achieves an improvement of over 10% under the mean recall when applied to current SGG methods.","offset":6,"pro":0.6666666666666666,"labels":"RST"},{"idx":3,"sentence":"Furthermore, DTrans with RTPB outperforms nearly all state-of-the-art methods with a large margin.","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":3,"sentence":"Code is available at https://github.com/ChCh1999/RTPB","offset":8,"pro":0.8888888888888888,"labels":"CTN"},{"idx":4,"sentence":"Although point-based networks are demonstrated to be accurate for 3D point cloud modeling, they are still falling behind their voxel-based competitors in 3D detection.","offset":0,"pro":0,"labels":"GAP"},{"idx":4,"sentence":"We observe that the prevailing set abstraction design for down-sampling points may maintain too much unimportant background information that can affect feature learning for detecting objects.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":4,"sentence":"To tackle this issue, we propose a novel set abstraction method named Semantics-Augmented Set Abstraction (SASA).","offset":2,"pro":0.2222222222222222,"labels":"PUR"},{"idx":4,"sentence":"Technically, we first add a binary segmentation module as the side output to help identify foreground points.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":4,"sentence":"Based on the estimated point-wise foreground scores, we then propose a semantics-guided point sampling algorithm to help retain more important foreground points during down-sampling.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":4,"sentence":"Additionally, it is an easy-to-plug-in module and able to boost various point-based detectors, including single-stage and two-stage ones.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":4,"sentence":"Extensive experiments on the popular KITTI and nuScenes datasets validate the superiority of SASA, lifting point-based detection models to reach comparable performance to state-of-the-art voxel-based methods.","offset":6,"pro":0.6666666666666666,"labels":"CLN"},{"idx":4,"sentence":"Code is available at https://github.com/blakechen97/SASA.","offset":7,"pro":0.7777777777777778,"labels":"CTN"},{"idx":4,"sentence":"In practice, SASA shows to be effective in identifying valuable points related to foreground objects and improving feature learning for point-based 3D detection. ","offset":8,"pro":0.8888888888888888,"labels":"MTD"},{"idx":5,"sentence":"Video anomaly detection aims to automatically identify unusual objects or behaviours by learning from normal videos.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"Previous methods tend to use simplistic reconstruction or prediction constraints, which leads to the insufficiency of learned representations for normal data.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":5,"sentence":"First, predictive consistency is proposed to consider the symmetry property of motion and appearance in forwards and backwards time, which ensures the highly realistic appearance and motion predictions at the pixel-wise level.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":5,"sentence":"Second, association consistency considers the relevance between different modalities and uses one modality to regularize the prediction of another one.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":5,"sentence":"Finally, temporal consistency utilizes the relationship of the video sequence and ensures that the predictive network generates temporally consistent frames.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":5,"sentence":"During inference, the pattern of abnormal frames is unpredictable and will therefore cause higher prediction errors.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":5,"sentence":"Experiments show that our method outperforms advanced anomaly detectors and achieves state-of-the-art results on UCSD Ped2, CUHK Avenue, and ShanghaiTech datasets.","offset":6,"pro":0.75,"labels":"CLN"},{"idx":5,"sentence":"As such, we propose a novel bi-directional architecture with three consistency constraints to comprehensively regularize the prediction task from pixel-wise, cross-modal, and temporal-sequence levels. ","offset":7,"pro":0.875,"labels":"PUR"},{"idx":6,"sentence":"Video-based person re-identification~(re-ID) is an important technique in visual surveillance systems which aims to match video snippets of people captured by different cameras.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"In this paper, we propose to overcome the limitations of normal convolutions with a human-oriented graph method.","offset":1,"pro":0.125,"labels":"PUR"},{"idx":6,"sentence":"Specifically, features located at person joint keypoints are extracted and connected as a spatial-temporal graph.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":6,"sentence":"These keypoint features are then updated by message passing from their connected nodes with a graph convolutional network~(GCN).","offset":3,"pro":0.375,"labels":"MTD"},{"idx":6,"sentence":"During training, the GCN can be attached to any CNN-based person re-ID model to assist representation learning on feature maps, whilst it can be dropped after training for better inference speed.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":6,"sentence":"Our method brings significant improvements over the CNN-based baseline model on the MARS dataset with generated person keypoints and a newly annotated dataset: PoseTrackReID.","offset":5,"pro":0.625,"labels":"CLN"},{"idx":6,"sentence":"Existing methods are mostly based on convolutional neural networks~(CNNs), whose building blocks either process local neighbor pixels at a time, or, when 3D convolutions are used to model temporal information, suffer from the misalignment problem caused by person movement. ","offset":6,"pro":0.75,"labels":"GAP"},{"idx":6,"sentence":"It also defines a new state-of-the-art method in terms of top-1 accuracy and mean average precision in comparison to prior works. ","offset":7,"pro":0.875,"labels":"RST"},{"idx":7,"sentence":"Temporal action detection aims to locate the boundaries of action in the video.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"The current method based on boundary matching enumerates and calculates all possible boundary matchings to generate proposals.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":7,"sentence":"However, these methods neglect the long-range context aggregation in boundary prediction.","offset":2,"pro":0.2,"labels":"GAP"},{"idx":7,"sentence":"At the same time, due to the similar semantics of adjacent matchings, local semantic aggregation of densely-generated matchings cannot improve semantic richness and discrimination.","offset":3,"pro":0.3,"labels":"GAP"},{"idx":7,"sentence":"In this paper, we propose the end-to-end proposal generation method named Dual Context Aggregation Network (DCAN) to aggregate context on two levels, namely, boundary level and proposal level, for generating high-quality action proposals, thereby improving the performance of temporal action detection.","offset":4,"pro":0.4,"labels":"PUR"},{"idx":7,"sentence":"Specifically, we design the Multi-Path Temporal Context Aggregation (MTCA) to achieve smooth context aggregation on boundary level and precise evaluation of boundaries.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":7,"sentence":"For matching evaluation, Coarse-to-fine Matching (CFM) is designed to aggregate context on the proposal level and refine the matching map from coarse to fine.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":7,"sentence":"We conduct extensive experiments on ActivityNet v1.3 and THUMOS-14.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":7,"sentence":"DCAN obtains an average mAP of 35.39% on ActivityNet v1.3 and reaches mAP 54.14% at IoU@0.5 on THUMOS-14, which demonstrates DCAN can generate high-quality proposals and achieve state-of-the-art performance.","offset":8,"pro":0.8,"labels":"RST"},{"idx":7,"sentence":"We release the code at https://github.com/cg1177/DCAN.","offset":9,"pro":0.9,"labels":"CTN"},{"idx":8,"sentence":"As the 3D pose transfer essentially is a deformation procedure dependent on the given meshes, the intuition of this work is to perceive the geometric inconsistency between the given meshes with the powerful self-attention mechanism.","offset":0,"pro":0,"labels":"PUR"},{"idx":8,"sentence":"Specifically, we propose a novel geometry-contrastive Transformer that has an efficient 3D structured perceiving ability to the global geometric inconsistencies across the given meshes.","offset":1,"pro":0.1111111111111111,"labels":"MTD"},{"idx":8,"sentence":"Moreover, locally, a simple yet efficient central geodesic contrastive loss is further proposed to improve the regional geometric-inconsistency learning.","offset":2,"pro":0.2222222222222222,"labels":"MTD"},{"idx":8,"sentence":"At last, we present a latent isometric regularization module together with a novel semi-synthesized dataset for the cross-dataset 3D pose transfer task towards unknown spaces.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":8,"sentence":"The massive experimental results prove the efficacy of our approach by showing state-of-the-art quantitative performances on SMPL-NPT, FAUST and our new proposed dataset SMG-3D datasets, as well as promising qualitative results on MG-cloth and SMAL datasets.","offset":4,"pro":0.4444444444444444,"labels":"CLN"},{"idx":8,"sentence":"It's demonstrated that our method can achieve robust 3D pose transfer and be generalized to challenging meshes from unknown spaces on cross-dataset tasks.","offset":5,"pro":0.5555555555555556,"labels":"CLN"},{"idx":8,"sentence":"The code and dataset are made available.","offset":6,"pro":0.6666666666666666,"labels":"CTN"},{"idx":8,"sentence":"Code is available: https://github.com/mikecheninoulu/CGT.","offset":7,"pro":0.7777777777777778,"labels":"CTN"},{"idx":8,"sentence":"We present a customized 3D mesh Transformer model for the pose transfer task. ","offset":8,"pro":0.8888888888888888,"labels":"PUR"},{"idx":9,"sentence":"Weakly supervised temporal sentence grounding aims to temporally localize the target segment corresponding to a given natural language query, where it provides video-query pairs without temporal annotations during training.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"Most existing methods use the fused visual-linguistic feature to reconstruct the query, where the least reconstruction error determines the target segment.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":9,"sentence":"This work introduces a novel approach that explores the inter-contrast between videos in a composed video by selecting components from two different videos and fusing them into a single video.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":9,"sentence":"Such a straightforward yet effective composition strategy provides the temporal annotations at multiple composed positions, resulting in numerous videos with temporal ground-truths for training the temporal sentence grounding task.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":9,"sentence":"A transformer framework is introduced with multi-tasks training to learn a compact but efficient visual-linguistic space.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":9,"sentence":"The experimental results on the public Charades-STA and ActivityNet-Caption dataset demonstrate the effectiveness of the proposed method, where our approach achieves comparable performance over the state-of-the-art weakly-supervised baselines.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":9,"sentence":"The code is available at https://github.com/PPjmchen/Composition_WSTG.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":10,"sentence":"Scene graph in a video conveys a wealth of information about objects and their relationships in the scene, thus benefiting many downstream tasks such as video captioning and visual question answering.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"Existing methods of scene graph generation require large-scale training videos annotated with objects and relationships in each frame to learn a powerful model.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":10,"sentence":"However, such comprehensive annotation is time-consuming and labor-intensive.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":10,"sentence":"This task presents two challenges: 1) infer unseen dynamic relationships in videos from static relationships in images due to the absence of motion information in images; 2) adapt objects and static relationships from images to video frames due to the domain shift between them.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":10,"sentence":"We tackle the second challenge by hierarchical adversarial learning to reduce the data distribution discrepancy between images and video frames.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":10,"sentence":"Extensive experiment results on two benchmark video datasets demonstrate the effectiveness of our method.","offset":5,"pro":0.625,"labels":"CLN"},{"idx":10,"sentence":"On the other hand, it is much easier and less cost to annotate images with scene graphs, so we investigate leveraging annotated images to facilitate training a scene graph generation model for unannotated videos, namely image-to-video scene graph generation. ","offset":6,"pro":0.75,"labels":"PUR"},{"idx":10,"sentence":"To address the first challenge, we exploit external commonsense knowledge to infer the unseen dynamic relationship from the temporal evolution of static relationships. ","offset":7,"pro":0.875,"labels":"MTD"},{"idx":11,"sentence":"In the last decade, the blossom of deep learning has witnessed the rapid development of scene text recognition.","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"However, the recognition of low-resolution scene text images remains a challenge.","offset":1,"pro":0.1,"labels":"GAP"},{"idx":11,"sentence":"Even though some super-resolution methods have been proposed to tackle this problem, they usually treat text images as general images while ignoring the fact that the visual quality of strokes (the atomic unit of text) plays an essential role for text recognition.","offset":2,"pro":0.2,"labels":"GAP"},{"idx":11,"sentence":"According to Gestalt Psychology, humans are capable of composing parts of details into the most similar objects guided by prior knowledge.","offset":3,"pro":0.3,"labels":"BAC"},{"idx":11,"sentence":"Likewise, when humans observe a low-resolution text image, they will inherently use partial stroke-level details to recover the appearance of holistic characters.","offset":4,"pro":0.4,"labels":"BAC"},{"idx":11,"sentence":"Specifically, we attempt to design rules for decomposing English characters and digits at stroke-level, then pre-train a text recognizer to provide stroke-level attention maps as positional clues with the purpose of controlling the consistency between the generated super-resolution image and high-resolution ground truth.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":11,"sentence":"The extensive experimental results validate that the proposed method can indeed generate more distinguishable images on TextZoom and manually constructed Chinese character dataset Degraded-IC13.","offset":6,"pro":0.6,"labels":"CLN"},{"idx":11,"sentence":"Code is available at https://github.com/FudanVI/FudanOCR/tree/main/text-gestalt.","offset":7,"pro":0.7,"labels":"CTN"},{"idx":11,"sentence":"Inspired by Gestalt Psychology, we put forward a Stroke-Aware Scene Text Image Super-Resolution method containing a Stroke-Focused Module (SFM) to concentrate on stroke-level internal structures of characters in text images.","offset":8,"pro":0.8,"labels":"PUR"},{"idx":11,"sentence":"Furthermore, since the proposed SFM is only used to provide stroke-level guidance when training, it will not bring any time overhead during the test phase. ","offset":9,"pro":0.9,"labels":"CLN"},{"idx":12,"sentence":"Face self-occlusions are inevitable due to the 3D nature of the human face and the loss of information in the projection process from 3D to 2D images.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"While recovering face self-occlusions based on 3D face reconstruction, e.g., 3D Morphable Model (3DMM) and its variants provides an effective solution, most of the existing methods show apparent limitations in expressing high-fidelity, natural, and diverse facial details.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":12,"sentence":"To overcome these limitations, we propose in this paper a new generative adversarial network (MvInvert) for natural face self-occlusion recovery without using paired image-texture data.","offset":2,"pro":0.2222222222222222,"labels":"PUR"},{"idx":12,"sentence":"We design a coarse-to-fine generator for photorealistic texture generation.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":12,"sentence":"A coarse texture is computed by inpainting the invisible areas in the photorealistic but incomplete texture sampled directly from the 2D image using the unrealistic but complete statistical texture from 3DMM.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":12,"sentence":"Then, we design a multi-view Residual-based GAN Inversion, which re-renders and refines multi-view 2D images, which are used for extracting multiple high-fidelity textures.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":12,"sentence":"Finally, these high-fidelity textures are fused based on their visibility maps via Poisson blending.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":12,"sentence":"To perform adversarial learning to assure the quality of the recovered texture, we design a discriminator consisting of two heads, i.e., one for global and local discrimination between the recovered texture and a small set of real textures in UV space, and the other for discrimination between the input image and the re-rendered 2D face images via pixel-wise, identity, and adversarial losses.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":12,"sentence":"Extensive experiments demonstrate that our approach outperforms the state-of-the-art methods in face self-occlusion recovery under unconstrained scenarios.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":13,"sentence":"Dynamic Vision Sensor (DVS) can asynchronously output the events reflecting apparent motion of objects with microsecond resolution, and shows great application potential in monitoring and other fields.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"However, the output event stream of existing DVS inevitably contains background activity noise (BA noise) due to dark current and junction leakage current, which will affect the temporal correlation of objects, resulting in deteriorated motion estimation performance.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":13,"sentence":"Particularly, the existing filter-based denoising methods cannot be directly applied to suppress the noise in event stream, since there is no spatial correlation.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":13,"sentence":"To address this issue, this paper presents a novel progressive framework, in which a Motion Estimation (ME) module and an Event Denoising (ED) module are jointly optimized in a mutually reinforced manner.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":13,"sentence":"Specifically, based on the maximum sharpness criterion, ME module divides the input event into several segments by adaptive clustering in a motion compensating warp field, and captures the temporal correlation of event stream according to the clustered motion parameters.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":13,"sentence":"Taking temporal correlation as guidance, ED module calculates the confidence that each event belongs to real activity events, and transmits it to ME module to update energy function of motion segmentation for noise suppression.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":13,"sentence":"The two steps are iteratively updated until stable motion segmentation results are obtained.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":13,"sentence":"Extensive experimental results on both synthetic and real datasets demonstrate the superiority of our proposed approaches against the State-Of-The-Art (SOTA) methods.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":14,"sentence":"Recent research has demonstrated that Deep Neural Networks (DNNs) are vulnerable to adversarial patches which introduce perceptible but localized changes to the input.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"Nevertheless, existing approaches have focused on generating adversarial patches on images, their counterparts in videos have been less explored.","offset":1,"pro":0.1,"labels":"GAP"},{"idx":14,"sentence":"Compared with images, attacking videos is much more challenging as it needs to consider not only spatial cues but also temporal cues.","offset":2,"pro":0.2,"labels":"GAP"},{"idx":14,"sentence":"To close this gap, we introduce a novel adversarial attack in this paper, the bullet-screen comment (BSC) attack, which attacks video recognition models with BSCs.","offset":3,"pro":0.3,"labels":"PUR"},{"idx":14,"sentence":"Specifically, adversarial BSCs are generated with a Reinforcement Learning (RL) framework, where the environment is set as the target model and the agent plays the role of selecting the position and transparency of each BSC.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":14,"sentence":"By continuously querying the target models and receiving feedback, the agent gradually adjusts its selection strategies in order to achieve a high fooling rate with non-overlapping BSCs.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":14,"sentence":"As BSCs can be regarded as a kind of meaningful patch, adding it to a clean video will not affect people’s understanding of the video content, nor will arouse people’s suspicion.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":14,"sentence":"We conduct extensive experiments to verify the effectiveness of the proposed method.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":14,"sentence":"On both UCF-101 and HMDB-51 datasets, our BSC attack method can achieve about 90% fooling rate when attacking three mainstream video recognition models, while only occluding < 8% areas in the video.","offset":8,"pro":0.8,"labels":"RST"},{"idx":14,"sentence":"Our code is available at https://github.com/kay-ck/BSC-attack.","offset":9,"pro":0.9,"labels":"CTN"},{"idx":15,"sentence":"Invariance to diverse types of image corruption, such as noise, blurring, or colour shifts, is essential to establish robust models in computer vision.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"Data augmentation has been the major approach in improving the robustness against common corruptions.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":15,"sentence":"However, the samples produced by popular augmentation strategies deviate significantly from the underlying data manifold.","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":15,"sentence":"As a result, performance is skewed toward certain types of corruption.","offset":3,"pro":0.3333333333333333,"labels":"GAP"},{"idx":15,"sentence":"To address this issue, we propose a multi-source vicinal transfer augmentation (VITA) method for generating diverse on-manifold samples.","offset":4,"pro":0.4444444444444444,"labels":"PUR"},{"idx":15,"sentence":"The proposed VITA consists of two complementary parts: tangent transfer and integration of multi-source vicinal samples.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":15,"sentence":"The tangent transfer creates initial augmented samples for improving corruption robustness.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":15,"sentence":"The integration employs a generative model to characterize the underlying manifold built by vicinal samples, facilitating the generation of on-manifold samples.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":15,"sentence":"Our proposed VITA significantly outperforms the current state-of-the-art augmentation methods, demonstrated in extensive experiments on corruption benchmarks.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":16,"sentence":"Zero-shot learning (ZSL) aims to recognize novel classes by transferring semantic knowledge from seen classes to unseen ones.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"Semantic knowledge is learned from attribute descriptions shared between different classes, which are strong prior for localization of object attribute for representing discriminative region features enabling significant visual-semantic interaction.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":16,"sentence":"Although few attention-based models have attempted to learn such region features in a single image, the transferability and discriminative attribute localization of visual features are typically neglected.","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":16,"sentence":"In this paper, we propose an attribute-guided Transformer network to learn the attribute localization for discriminative visual-semantic embedding representations in ZSL, termed TransZero.","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":16,"sentence":"Specifically, TransZero takes a feature augmentation encoder to alleviate the cross-dataset bias between ImageNet and ZSL benchmarks and improve the transferability of visual features by reducing the entangled relative geometry relationships among region features.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":16,"sentence":"To learn locality-augmented visual features, TransZero employs a visual-semantic decoder to localize the most relevant image regions to each attributes from a given image under the guidance of attribute semantic information.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":16,"sentence":"Then, the locality-augmented visual features and semantic vectors are used for conducting effective visual-semantic interaction in a visual-semantic embedding network.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":16,"sentence":"Extensive experiments show that TransZero achieves a new state-of-the-art on three ZSL benchmarks.","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":16,"sentence":"The codes are available at: https://github.com/shiming-chen/TransZero.","offset":8,"pro":0.8888888888888888,"labels":"CTN"},{"idx":0,"sentence":"We present a novel generalized zero-shot algorithm to recognize perceived emotions from gestures.","offset":0,"pro":0,"labels":"MTD"},{"idx":0,"sentence":"Our task is to map gestures to novel emotion categories not encountered in training.","offset":1,"pro":0.125,"labels":"PUR"},{"idx":0,"sentence":"We introduce an adversarial autoencoder-based representation learning that correlates 3D motion-captured gesture sequences with the vectorized representation of the natural-language perceived emotion terms using word2vec embeddings.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":0,"sentence":"The language-semantic embedding provides a representation of the emotion label space, and we leverage this underlying distribution to map the gesture sequences to the appropriate categorical emotion labels.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":0,"sentence":"We train our method using a combination of gestures annotated with known emotion terms and gestures not annotated with any emotions.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":0,"sentence":"We evaluate our method on the MPI Emotional Body Expressions Database (EBEDB) and obtain an accuracy of 58.43%.","offset":5,"pro":0.625,"labels":"RST"},{"idx":0,"sentence":"We see an improvement in performance compared to current state-of-the-art algorithms for generalized zero-shot learning by an absolute 25-27%.","offset":6,"pro":0.75,"labels":"RST"},{"idx":0,"sentence":"We also demonstrate our approach on publicly available online videos and movie scenes, where the actors' pose has been extracted and map to their respective emotive states.","offset":7,"pro":0.875,"labels":"MTD"},{"idx":1,"sentence":"Spiking Neural Networks (SNNs) have been attached great importance due to the distinctive properties of low power consumption, biological plausibility, and adversarial robustness.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"The most effective way to train deep SNNs is through ANN-to-SNN conversion, which have yielded the best performance in deep network structure and large-scale datasets.","offset":1,"pro":0.09090909090909091,"labels":"BAC"},{"idx":1,"sentence":"However, there is a trade-off between accuracy and latency.","offset":2,"pro":0.18181818181818182,"labels":"GAP"},{"idx":1,"sentence":"In order to achieve high precision as original ANNs, a long simulation time is needed to match the firing rate of a spiking neuron with the activation value of an analog neuron, which impedes the practical application of SNN.","offset":3,"pro":0.2727272727272727,"labels":"GAP"},{"idx":1,"sentence":"In this paper, we aim to achieve high-performance converted SNNs with extremely low latency (fewer than 32 time-steps).","offset":4,"pro":0.36363636363636365,"labels":"PUR"},{"idx":1,"sentence":"We start by theoretically analyzing ANN-to-SNN conversion and show that scaling the thresholds does play a similar role as weight normalization.","offset":5,"pro":0.45454545454545453,"labels":"MTD"},{"idx":1,"sentence":"Instead of introducing constraints that facilitate ANN-to-SNN conversion at the cost of model capacity, we applied a more direct way by optimizing the initial membrane potential to reduce the conversion loss in each layer.","offset":6,"pro":0.5454545454545454,"labels":"MTD"},{"idx":1,"sentence":"Besides, we demonstrate that optimal initialization of membrane potentials can implement expected error-free ANN-to-SNN conversion.","offset":7,"pro":0.6363636363636364,"labels":"CLN"},{"idx":1,"sentence":"We evaluate our algorithm on the CIFAR-10 dataset and CIFAR-100 dataset and achieve state-of-the-art accuracy, using fewer time-steps.","offset":8,"pro":0.7272727272727273,"labels":"RST"},{"idx":1,"sentence":"For example, we reach top-1 accuracy of 93.38% on CIFAR-10 with 16 time-steps.","offset":9,"pro":0.8181818181818182,"labels":"RST"},{"idx":1,"sentence":"Moreover, our method can be applied to other ANN-SNN conversion methodologies and remarkably promote performance when the time-steps is small.","offset":10,"pro":0.9090909090909091,"labels":"CLN"},{"idx":2,"sentence":"We revisit the planning problem in the blocks world, and we implement a known heuristic for this task.","offset":0,"pro":0,"labels":"MTD"},{"idx":2,"sentence":"Importantly, our implementation is biologically plausible, in the sense that it is carried out exclusively through the spiking of neurons.","offset":1,"pro":0.125,"labels":"MTD"},{"idx":2,"sentence":"Even though much has been accomplished in the blocks world over the past five decades, we believe that this is the first algorithm of its kind.","offset":2,"pro":0.25,"labels":"CLN"},{"idx":2,"sentence":"The input is a sequence of symbols encoding an initial set of block stacks as well as a target set, and the output is a sequence of motion commands such as \"put the top block in stack 1 on the table\".","offset":3,"pro":0.375,"labels":"MTD"},{"idx":2,"sentence":"The program is written in the Assembly Calculus, a recently proposed computational framework meant to model computation in the brain by bridging the gap between neural activity and cognitive function.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":2,"sentence":"Its elementary objects are assemblies of neurons (stable sets of neurons whose simultaneous firing signifies that the subject is thinking of an object, concept, word, etc.), its commands include project and merge, and its execution model is based on widely accepted tenets of neuroscience.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":2,"sentence":"A program in this framework essentially sets up a dynamical system of neurons and synapses that eventually, with high probability, accomplishes the task.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":2,"sentence":"The purpose of this work is to establish empirically that reasonably large programs in the Assembly Calculus can execute correctly and reliably; and that rather realistic --- if idealized --- higher cognitive functions, such as planning in the blocks world, can be implemented successfully by such programs.","offset":7,"pro":0.875,"labels":"PUR"},{"idx":3,"sentence":"In humans, perceptual awareness facilitates the fast recognition and extraction of information from sensory input.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"This awareness largely depends on how the human agent interacts with the environment.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":3,"sentence":"In this work, we propose active neural generative coding, a computational framework for learning action-driven generative models without backpropagation of errors (backprop) in dynamic environments.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":3,"sentence":"Specifically, we develop an intelligent agent that operates even with sparse rewards, drawing inspiration from the cognitive theory of planning as inference.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":3,"sentence":"We demonstrate on several simple control problems that our framework performs competitively with deep Q-learning.","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":3,"sentence":"The robust performance of our agent offers promising evidence that a backprop-free approach for neural inference and learning can drive goal-directed behavior.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":4,"sentence":"The developmental approach, simulating a cognitive development of a human, arises as a way to nurture a human-level commonsense and overcome the limitations of data-driven approaches.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"However, neither a virtual environment nor an evaluation platform exists for the overall development of core cognitive skills.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":4,"sentence":"We present the VECA(Virtual Environment for Cognitive Assessment), which consists of two main components: (i) a first benchmark to assess the overall cognitive development of an AI agent, and (ii) a novel toolkit to generate diverse and distinct cognitive tasks.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":4,"sentence":"VECA benchmark virtually implements the cognitive scale of Bayley Scales of Infant and Toddler Development-IV(Bayley-4), the gold-standard developmental assessment for human infants and toddlers.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":4,"sentence":"Our VECA toolkit provides a human toddler-like embodied agent with various human-like perceptual features crucial to human cognitive development, e.g., binocular vision, 3D-spatial audio, and tactile receptors.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":4,"sentence":"We compare several modern RL algorithms on our VECA benchmark and seek their limitations in modeling human-like cognitive development.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":4,"sentence":"We further analyze the validity of the VECA benchmark, as well as the effect of human-like sensory characteristics on cognitive skills.","offset":6,"pro":0.8571428571428571,"labels":"MTD"},{"idx":5,"sentence":"Cognitive processing signals can be used to improve natural language processing (NLP) tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"However, it is not clear how these signals correlate with linguistic information.","offset":1,"pro":0.1,"labels":"GAP"},{"idx":5,"sentence":"Bridging between human language processing and linguistic features has been widely studied in neurolinguistics, usually via single-variable controlled experiments with highly-controlled stimuli.","offset":2,"pro":0.2,"labels":"BAC"},{"idx":5,"sentence":"Such methods not only compromises the authenticity of natural reading, but also are time-consuming and expensive.","offset":3,"pro":0.3,"labels":"GAP"},{"idx":5,"sentence":"In this paper, we propose a data-driven method to investigate the relationship between cognitive processing signals and linguistic features.","offset":4,"pro":0.4,"labels":"PUR"},{"idx":5,"sentence":"Specifically, we present a unified attentional framework that is composed of embedding, attention, encoding and predicting layers to selectively map cognitive processing signals to linguistic features.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":5,"sentence":"We define the mapping procedure as a bridging task and develop 12 bridging tasks for lexical, syntactic and semantic features.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":5,"sentence":"The proposed framework only requires cognitive processing signals recorded under natural reading as inputs, and can be used to detect a wide range of linguistic features with a single cognitive dataset.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":5,"sentence":"Observations from experiment results resonate with previous neuroscience findings.","offset":8,"pro":0.8,"labels":"CLN"},{"idx":5,"sentence":"In addition to this, our experiments also reveal a number of interesting findings, such as the correlation between contextual eye-tracking features and tense of sentence.","offset":9,"pro":0.9,"labels":"CLN"},{"idx":6,"sentence":"With the help of deep neural networks (DNNs), deep reinforcement learning (DRL) has achieved great success on many complex tasks, from games to robotic control.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"Compared to DNNs with partial brain-inspired structures and functions, spiking neural networks (SNNs) consider more biological features, including spiking neurons with complex dynamics and learning paradigms with biologically plausible plasticity principles.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":6,"sentence":"Inspired by the efficient computation of cell assembly in the biological brain, whereby memory-based coding is much more complex than readout, we propose a multiscale dynamic coding improved spiking actor network (MDC-SAN) for reinforcement learning to achieve effective decision-making.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":6,"sentence":"The population coding at the network scale is integrated with the dynamic neurons coding (containing 2nd-order neuronal dynamics) at the neuron scale towards a powerful spatial-temporal state representation.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":6,"sentence":"Extensive experimental results show that our MDC-SAN performs better than its counterpart deep actor network (based on DNNs) on four continuous control tasks from OpenAI gym.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":6,"sentence":"We think this is a significant attempt to improve SNNs from the perspective of efficient coding towards effective decision-making, just like that in biological networks.","offset":5,"pro":0.8333333333333334,"labels":"CTN"},{"idx":7,"sentence":"Despite the advances in multi-person pose estimation, state-of-the-art techniques only deliver the human pose structure.Yet, they do not leverage the keypoints of human pose to deliver whole-body shape information for human instance segmentation.","offset":0,"pro":0,"labels":"GAP"},{"idx":7,"sentence":"This paper presents PosePlusSeg, a joint model designed for both human pose estimation and instance segmentation.","offset":1,"pro":0.125,"labels":"PUR"},{"idx":7,"sentence":"For pose estimation, PosePlusSeg first takes a bottom-up approach to detect the soft and hard keypoints of individuals by producing a strong keypoint heat map, then improves the keypoint detection confidence score by producing a body heat map.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":7,"sentence":"For instance segmentation, PosePlusSeg generates a mask offset where keypoint is defined as a centroid for the pixels in the embedding space, enabling instance-level segmentation for the human class.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":7,"sentence":"Finally, we propose a new pose and instance segmentation algorithm that enables PosePlusSeg to determine the joint structure of the human pose and instance segmentation.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":7,"sentence":"Experiments using the COCO challenging dataset demonstrate that PosePlusSeg copes better with challenging scenarios, like occlusions, en-tangled limbs, and overlapped people.","offset":5,"pro":0.625,"labels":"CLN"},{"idx":7,"sentence":"PosePlusSeg outperforms state-of-the-art detection-based approaches achieving a 0.728 mAP for human pose estimation and a 0.445 mAP for instance segmentation.","offset":6,"pro":0.75,"labels":"RST"},{"idx":7,"sentence":"Code has been made available at:https://github.com/RaiseLab/PosePlusSeg.","offset":7,"pro":0.875,"labels":"CTN"},{"idx":8,"sentence":"With the increasing demands for understanding the internal behaviors of deep networks, Explainable AI (XAI) has been made remarkable progress in interpreting the model's decision.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"A family of attribution techniques has been proposed, highlighting whether the input pixels are responsible for the model's prediction.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":8,"sentence":"However, the existing attribution methods suffer from the lack of rule guidance and require further human interpretations.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":8,"sentence":"In this paper, we construct the 'if-then' logic rules that are sufficiently precise locally.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":8,"sentence":"Moreover, a novel rule-guided method, dynamic ablation (DA), is proposed to find a minimal bound sufficient in an input image to justify the network's prediction and aggregate iteratively to reach a complete attribution.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":8,"sentence":"Both qualitative and quantitative experiments are conducted to evaluate the proposed DA.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":8,"sentence":"We demonstrate the advantages of our method in providing clear and explicit explanations that are also easy for human experts to understand.","offset":6,"pro":0.75,"labels":"CLN"},{"idx":8,"sentence":"Besides, through the attribution on a series of trained networks with different architectures, we show that more complex networks require less information to make a specific prediction.","offset":7,"pro":0.875,"labels":"RST"},{"idx":9,"sentence":"We present Neural Marionette, an unsupervised approach that discovers the skeletal structure from a dynamic sequence and learns to generate diverse motions that are consistent with the observed motion dynamics.","offset":0,"pro":0,"labels":"PUR"},{"idx":9,"sentence":"Given a video stream of point cloud observation of an articulated body under arbitrary motion, our approach discovers the unknown low-dimensional skeletal relationship that can effectively represent the movement.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":9,"sentence":"Then the discovered structure is utilized to encode the motion priors of dynamic sequences in a latent structure, which can be decoded to the relative joint rotations to represent the full skeletal motion.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":9,"sentence":"Our approach works without any prior knowledge of the underlying motion or skeletal structure, and we demonstrate that the discovered structure is even comparable to the hand-labeled ground truth skeleton in representing a 4D sequence of motion.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":9,"sentence":"The skeletal structure embeds the general semantics of possible motion space that can generate motions for diverse scenarios.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":9,"sentence":"We verify that the learned motion prior is generalizable to the multi-modal sequence generation, interpolation of two poses, and motion retargeting to a different skeletal structure.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":10,"sentence":"In a convolutional object detector, the detection accuracy can be degraded often due to the low feature discriminability caused by geometric variation or transformation of an object.","offset":0,"pro":0,"labels":"GAP"},{"idx":10,"sentence":"In this paper, we propose a deformable part region learning in order to allow decomposed part regions to be deformable according to geometric transformation of an object.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":10,"sentence":"To this end, we introduce trainable geometric parameters for the location of each part model.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":10,"sentence":"Because the ground truth of the part models is not available, we design classification and mask losses for part models, and learn the geometric parameters by minimizing an integral loss including those part losses.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":10,"sentence":"As a result, we can train a deformable part region network without extra super-vision and make each part model deformable according to object scale variation.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":10,"sentence":"Furthermore, for improving cascade object detection and instance segmentation, we present a Cascade deformable part region architecture which can refine whole and part detections iteratively in the cascade manner.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":10,"sentence":"Without bells and whistles, our implementation of a Cascade deformable part region detector achieves better detection and segmentation mAPs on COCO and VOC datasets, compared to the recent cascade and other state-of-the-art detectors.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":11,"sentence":"We propose an end-to-end image compression and analysis model with Transformers, targeting to the cloud-based image classification application.","offset":0,"pro":0,"labels":"PUR"},{"idx":11,"sentence":"Instead of placing an existing Transformer-based image classification model directly after an image codec, we aim to redesign the Vision Transformer (ViT) model to perform image classification from the compressed features and facilitate image compression with the long-term information from the Transformer.","offset":1,"pro":0.125,"labels":"PUR"},{"idx":11,"sentence":"Specifically, we first replace the patchify stem (i.e., image splitting and embedding) of the ViT model with a lightweight image encoder modelled by a convolutional neural network.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":11,"sentence":"The compressed features generated by the image encoder are injected convolutional inductive bias and are fed to the Transformer for image classification bypassing image reconstruction.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":11,"sentence":"Meanwhile, we propose a feature aggregation module to fuse the compressed features with the selected intermediate features of the Transformer, and feed the aggregated features to a deconvolutional neural network for image reconstruction.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":11,"sentence":"The aggregated features can obtain the long-term information from the self-attention mechanism of the Transformer and improve the compression performance.","offset":5,"pro":0.625,"labels":"RST"},{"idx":11,"sentence":"The rate-distortion-accuracy optimization problem is finally solved by a two-step training strategy.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":11,"sentence":"Experimental results demonstrate the effectiveness of the proposed model in both the image compression and the classification tasks.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":12,"sentence":"Handwritten mathematical expression recognition aims to automatically generate LaTeX sequences from given images.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"Currently, attention-based encoder-decoder models are widely used in this task.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":12,"sentence":"They typically generate target sequences in a left-to-right (L2R) manner, leaving the right-to-left (R2L) contexts unexploited.","offset":2,"pro":0.2222222222222222,"labels":"BAC"},{"idx":12,"sentence":"In this paper, we propose an Attention aggregation based Bi-directional Mutual learning Network (ABM) which consists of one shared encoder and two parallel inverse decoders (L2R and R2L).","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":12,"sentence":"The two decoders are enhanced via mutual distillation, which involves one-to-one knowledge transfer at each training step, making full use of the complementary information from two inverse directions.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":12,"sentence":"Moreover, in order to deal with mathematical symbols in diverse scales, an Attention Aggregation Module (AAM) is proposed to effectively integrate multi-scale coverage attentions.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":12,"sentence":"Notably, in the inference phase, given that the model already learns knowledge from two inverse directions, we only use the L2R branch for inference, keeping the original parameter size and inference speed.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":12,"sentence":"Extensive experiments demonstrate that our proposed approach achieves the recognition accuracy of 56.85 % on CROHME 2014, 52.92 % on CROHME 2016, and 53.96 % on CROHME 2019 without data augmentation and model ensembling, substantially outperforming the state-of-the-art methods.","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":12,"sentence":"The source code is available in https://github.com/XH-B/ABM.","offset":8,"pro":0.8888888888888888,"labels":"CTN"},{"idx":13,"sentence":"Despite significant advancements of deep learning-based forgery detectors for distinguishing manipulated deepfake images, most detection approaches suffer from moderate to significant performance degradation with low-quality compressed deepfake images.","offset":0,"pro":0,"labels":"GAP"},{"idx":13,"sentence":"Because of the limited information in low-quality images, detecting low-quality deepfake remains an important challenge.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":13,"sentence":"In this work, we apply frequency domain learning and optimal transport theory in knowledge distillation (KD) to specifically improve the detection of low-quality compressed deepfake images.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":13,"sentence":"We explore transfer learning capability in KD to enable a student network to learn discriminative features from low-quality images effectively.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":13,"sentence":"In particular, we propose the Attention-based Deepfake detection Distiller (ADD), which consists of two novel distillations: 1) frequency attention distillation that effectively retrieves the removed high-frequency components in the student network, and 2) multi-view attention distillation that creates multiple attention vectors by slicing the teacher’s and student’s tensors under different views to transfer the teacher tensor’s distribution to the student more efficiently.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":13,"sentence":"Our extensive experimental results demonstrate that our approach outperforms state-of-the-art baselines in detecting low-quality compressed deepfake images.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":14,"sentence":"The predefined artificially-balanced training classes in object recognition have limited capability in modeling real-world scenarios where objects are imbalanced-distributed with unknown classes.","offset":0,"pro":0,"labels":"GAP"},{"idx":14,"sentence":"In this paper, we discuss a promising solution to the Open-set Long-Tailed Recognition (OLTR) task utilizing metric learning.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":14,"sentence":"Firstly, we propose a distribution-sensitive loss, which weighs more on the tail classes to decrease the intra-class distance in the feature space.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":14,"sentence":"Building upon these concentrated feature clusters, a local-density-based metric is introduced, called Localizing Unfamiliarity Near Acquaintance (LUNA), to measure the novelty of a testing sample.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":14,"sentence":"LUNA is flexible with different cluster sizes and is reliable on the cluster boundary by considering neighbors of different properties.","offset":4,"pro":0.5714285714285714,"labels":"BAC"},{"idx":14,"sentence":"Moreover, contrary to most of the existing works that alleviate the open-set detection as a simple binary decision, LUNA is a quantitative measurement with interpretable meanings.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":14,"sentence":"Our proposed method exceeds the state-of-the-art algorithm by 4-6% in the closed-set recognition accuracy and 4% in F-measure under the open-set on the public benchmark datasets, including our own newly introduced fine-grained OLTR dataset about marine species (MS-LT), which is the first naturally-distributed OLTR dataset revealing the genuine genetic relationships of the classes.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":15,"sentence":"We proposed a Prior Gradient Mask Guided Pruning-aware Fine-Tuning (PGMPF) framework to accelerate deep Convolutional Neural Networks (CNNs).","offset":0,"pro":0,"labels":"PUR"},{"idx":15,"sentence":"In detail, the proposed PGMPF selectively suppresses the gradient of those ”unimportant” parameters via a prior gradient mask generated by the pruning criterion during fine-tuning.","offset":1,"pro":0.1111111111111111,"labels":"RST"},{"idx":15,"sentence":"PGMPF has three charming characteristics over previous works: (1) Pruning-aware network fine-tuning.","offset":2,"pro":0.2222222222222222,"labels":"MTD"},{"idx":15,"sentence":"A typical pruning pipeline consists of training, pruning and fine-tuning, which are relatively independent, while PGMPF utilizes a variant of the pruning mask as a prior gradient mask to guide fine-tuning, without complicated pruning criteria.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":15,"sentence":"(2) An excellent tradeoff between large model capacity during fine-tuning and stable convergence speed to obtain the final compact model.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":15,"sentence":"Previous works preserve more training information of pruned parameters during fine-tuning to pursue better performance, which would incur catastrophic non-convergence of the pruned model for relatively large pruning rates, while our PGMPF greatly stabilizes the fine-tuning phase by gradually constraining the learning rate of those ”unimportant” parameters.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":15,"sentence":"(3) Channel-wise random dropout of the prior gradient mask to impose some gradient noise to fine-tuning to further improve the robustness of final compact model.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":15,"sentence":"Experimental results on three image classification benchmarks CIFAR10/ 100 and ILSVRC-2012 demonstrate the effectiveness of our method for various CNN architectures, datasets and pruning rates.","offset":7,"pro":0.7777777777777778,"labels":"CLN"},{"idx":15,"sentence":"Notably, on ILSVRC-2012, PGMPF reduces 53.5% FLOPs on ResNet-50 with only 0.90% top-1 accuracy drop and 0.52% top-5 accuracy drop, which has advanced the state-of-the-art with negligible extra computational cost.","offset":8,"pro":0.8888888888888888,"labels":"RST"},{"idx":16,"sentence":"Blackbox transfer attacks for image classifiers have been extensively studied in recent years.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"In contrast, little progress has been made on transfer attacks for object detectors.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":16,"sentence":"Object detectors take a holistic view of the image and the detection of one object (or lack thereof) often depends on other objects in the scene.","offset":2,"pro":0.2857142857142857,"labels":"BAC"},{"idx":16,"sentence":"This makes such detectors inherently context-aware and adversarial attacks in this space are more challenging than those targeting image classifiers.","offset":3,"pro":0.42857142857142855,"labels":"GAP"},{"idx":16,"sentence":"In this paper, we present a new approach to generate context-aware attacks for object detectors.","offset":4,"pro":0.5714285714285714,"labels":"PUR"},{"idx":16,"sentence":"We show that by using co-occurrence of objects and their relative locations and sizes as context information, we can successfully generate targeted mis-categorization attacks that achieve higher transfer success rates on blackbox object detectors than the state-of-the-art.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":16,"sentence":"We test our approach on a variety of object detectors with images from PASCAL VOC and MS COCO datasets and demonstrate up to 20 percentage points improvement in performance compared to the other state-of-the-art methods.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":17,"sentence":"Recently, deep learning has been proven to be a promising approach in standard dynamic range (SDR) image compression.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"However, due to the wide luminance distribution of high dynamic range (HDR) images and the lack of large standard datasets, developing a deep model for HDR image compression is much more challenging.","offset":1,"pro":0.08333333333333333,"labels":"GAP"},{"idx":17,"sentence":"To tackle this issue, we view HDR data as distributional shifts of SDR data and the HDR image compression can be modeled as an out-of-distribution generalization (OoD) problem.","offset":2,"pro":0.16666666666666666,"labels":"PUR"},{"idx":17,"sentence":"Herein, we propose a novel out-of-distribution (OoD) HDR image compression framework (OoDHDR-codec).","offset":3,"pro":0.25,"labels":"PUR"},{"idx":17,"sentence":"It learns the general representation across HDR and SDR environments, and allows the model to be trained effectively using a large set of SDR datases supplemented with much fewer HDR samples.","offset":4,"pro":0.3333333333333333,"labels":"MTD"},{"idx":17,"sentence":"Specifically, OoDHDR-codec consists of two branches to process the data from two environments.","offset":5,"pro":0.4166666666666667,"labels":"MTD"},{"idx":17,"sentence":"The SDR branch is a standard blackbox network.","offset":6,"pro":0.5,"labels":"BAC"},{"idx":17,"sentence":"For the HDR branch, we develop a hybrid system that models luminance masking and tone mapping with white-box modules and performs content compression with black-box neural networks.","offset":7,"pro":0.5833333333333334,"labels":"MTD"},{"idx":17,"sentence":"To improve the generalization from SDR training data on HDR data, we introduce an invariance regularization term to learn the common representation for both SDR and HDR compression.","offset":8,"pro":0.6666666666666666,"labels":"MTD"},{"idx":17,"sentence":"Extensive experimental results show that the OoDHDR codec achieves strong competitive in-distribution performance and state-of-the-art OoD performance.","offset":9,"pro":0.75,"labels":"RST"},{"idx":17,"sentence":"To the best of our knowledge, our proposed approach is the first work to model HDR compression as OoD generalization problems and our OoD generalization algorithmic framework can be applied to any deep compression model in addition to the network architectural choice demonstrated in the paper.","offset":10,"pro":0.8333333333333334,"labels":"CTN"},{"idx":17,"sentence":"Code available at https://github.com/caolinfeng/OoDHDR-codec.","offset":11,"pro":0.9166666666666666,"labels":"CTN"},{"idx":18,"sentence":"In this paper, we propose a novel method to mine the commonsense knowledge shared between the video and text modalities for video-text retrieval, namely visual consensus modeling.","offset":0,"pro":0,"labels":"PUR"},{"idx":18,"sentence":"Different from the existing works, which learn the video and text representations and their complicated relationships solely based on the pairwise video-text data, we make the first attempt to model the visual consensus by mining the visual concepts from videos and exploiting their co-occurrence patterns within the video and text modalities with no reliance on any additional concept annotations.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":18,"sentence":"Specifically, we build a shareable and learnable graph as the visual consensus, where the nodes denoting the mined visual concepts and the edges connecting the nodes representing the co-occurrence relationships between the visual concepts.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":18,"sentence":"Extensive experimental results on the public benchmark datasets demonstrate that our proposed method, with the ability to effectively model the visual consensus, achieves state-of-the-art performances on the bidirectional video-text retrieval task.","offset":3,"pro":0.6,"labels":"CLN"},{"idx":18,"sentence":"Our code is available at https://github.com/sqiangcao99/VCM.","offset":4,"pro":0.8,"labels":"CTN"},{"idx":19,"sentence":"Recently, deep learning techniques have been extensively studied for pansharpening, which aims to generate a high resolution multispectral (HRMS) image by fusing a low resolution multispectral (LRMS) image with a high resolution panchromatic (PAN) image.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"However, existing deep learning-based pansharpening methods directly learn the mapping from LRMS and PAN to HRMS.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":19,"sentence":"These network architectures always lack sufficient interpretability, which limits further performance improvements.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":19,"sentence":"To alleviate this issue, we propose a novel deep network for pansharpening by combining the model-based methodology with the deep learning method.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":19,"sentence":"Firstly, we build an observation model for pansharpening using the convolutional sparse coding (CSC) technique and design a proximal gradient algorithm to solve this model.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":19,"sentence":"Secondly, we unfold the iterative algorithm into a deep network, dubbed as Proximal PanNet, by learning the proximal operators using convolutional neural networks.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":19,"sentence":"Finally, all the learnable modules can be automatically learned in an end-to-end manner.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":19,"sentence":"Experimental results on some benchmark datasets show that our network performs better than other advanced methods both quantitatively and qualitatively.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":0,"sentence":"Multi-label image recognition is a fundamental yet practical task because real-world images inherently possess multiple semantic labels.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"However, it is difficult to collect large-scale multi-label annotations due to the complexity of both the input images and output label spaces.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":0,"sentence":"To reduce the annotation cost, we propose a structured semantic transfer (SST) framework that enables training multi-label recognition models with partial labels, i.e., merely some labels are known while other labels are missing (also called unknown labels) per image.","offset":2,"pro":0.2222222222222222,"labels":"PUR"},{"idx":0,"sentence":"The framework consists of two complementary transfer modules that explore within-image and cross-image semantic correlations to transfer knowledge of known labels to generate pseudo labels for unknown labels.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":0,"sentence":"Specifically, an intra-image semantic transfer module learns image-specific label co-occurrence matrix and maps the known labels to complement unknown labels based on this matrix.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":0,"sentence":"Meanwhile, a cross-image transfer module learns category-specific feature similarities and helps complement unknown labels with high similarities.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":0,"sentence":"Finally, both known and generated labels are used to train the multi-label recognition models.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":0,"sentence":"Extensive experiments on the Microsoft COCO, Visual Genome and Pascal VOC datasets show that the proposed SST framework obtains superior performance over current state-of-the-art algorithms.","offset":7,"pro":0.7777777777777778,"labels":"CLN"},{"idx":0,"sentence":"Codes are available at https://github.com/HCPLab-SYSU/HCP-MLR-PL.","offset":8,"pro":0.8888888888888888,"labels":"CTN"},{"idx":1,"sentence":"Vehicle re-identification (ReID) has attracted considerable attention in computer vision.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"Although several methods have been proposed to achieve state-of-the-art performance on this topic, re-identifying vehicle in foggy scenes remains a great challenge due to the degradation of visibility.","offset":1,"pro":0.09090909090909091,"labels":"GAP"},{"idx":1,"sentence":"To our knowledge, this problem is still not well-addressed so far.","offset":2,"pro":0.18181818181818182,"labels":"GAP"},{"idx":1,"sentence":"In this paper, to address this problem, we propose a novel training framework called Semi-supervised Joint Defogging Learning (SJDL) framework.","offset":3,"pro":0.2727272727272727,"labels":"PUR"},{"idx":1,"sentence":"First, the fog removal branch and the re-identification branch are integrated to perform simultaneous training.","offset":4,"pro":0.36363636363636365,"labels":"MTD"},{"idx":1,"sentence":"With the collaborative training scheme, defogged features generated by the defogging branch from input images can be shared to learn better representation for the re-identification branch.","offset":5,"pro":0.45454545454545453,"labels":"MTD"},{"idx":1,"sentence":"However, since the fog-free image of real-world data is intractable, this architecture can only be trained on the synthetic data, which may cause the domain gap problem between real-world and synthetic scenarios.","offset":6,"pro":0.5454545454545454,"labels":"GAP"},{"idx":1,"sentence":"To solve this problem, we design a semi-supervised defogging training scheme that can train two kinds of data alternatively in each iteration.","offset":7,"pro":0.6363636363636364,"labels":"PUR"},{"idx":1,"sentence":"Due to the lack of a dataset specialized for vehicle ReID in the foggy weather, we construct a dataset called FVRID which consists of real-world and synthetic foggy images to train and evaluate the performance.","offset":8,"pro":0.7272727272727273,"labels":"MTD"},{"idx":1,"sentence":"Experimental results show that the proposed method is effective and outperforms other existing vehicle ReID methods in the foggy weather.","offset":9,"pro":0.8181818181818182,"labels":"CLN"},{"idx":1,"sentence":"The code and dataset are available in https://github.com/Cihsaing/SJDL-Foggy-Vehicle-Re-Identification--AAAI2022.","offset":10,"pro":0.9090909090909091,"labels":"CTN"},{"idx":2,"sentence":"Real-world data often follows a long-tailed distribution, which makes the performance of existing classification algorithms degrade heavily.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"A key issue is that the samples in tail categories fail to depict their intra-class diversity.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":2,"sentence":"Humans can imagine a sample in new poses, scenes and view angles with their prior knowledge even if it is the first time to see this category.","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":2,"sentence":"Inspired by this, we propose a novel reasoning-based implicit semantic data augmentation method to borrow transformation directions from other classes.","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":2,"sentence":"Since the covariance matrix of each category represents the feature transformation directions, we can sample new directions from similar categories to generate definitely different instances.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":2,"sentence":"Specifically, the long-tailed distributed data is first adopted to train a backbone and a classifier.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":2,"sentence":"Then, a covariance matrix for each category is estimated, and a knowledge graph is constructed to store the relations of any two categories.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":2,"sentence":"Finally, tail samples are adaptively enhanced via propagating information from all the similar categories in the knowledge graph.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":2,"sentence":"Experimental results on CIFAR-LT-100, ImageNet-LT, and iNaturalist 2018 have demonstrated the effectiveness of our proposed method compared with the state-of-the-art methods.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":3,"sentence":"Local image feature matching under large appearance, viewpoint, and distance changes is challenging yet important.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"Conventional methods detect and match tentative local features across the whole images, with heuristic consistency checks to guarantee reliable matches.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":3,"sentence":"In this paper, we introduce a novel Overlap Estimation method conditioned on image pairs with TRansformer, named OETR, to constrain local feature matching in the commonly visible region.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":3,"sentence":"OETR performs overlap estimation in a two step process of feature correlation and then overlap regression.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":3,"sentence":"As a preprocessing module, OETR can be plugged into any existing local feature detection and matching pipeline, to mitigate potential view angle or scale variance.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":3,"sentence":"Intensive experiments show that OETR can boost state of the art local feature matching performance substantially, especially for image pairs with small shared regions.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":3,"sentence":"The code will be publicly available at https://github.com/AbyssGaze/OETR.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":4,"sentence":"Subject-invariant facial action unit (AU) recognition remains challenging for the reason that the data distribution varies among subjects.","offset":0,"pro":0,"labels":"GAP"},{"idx":4,"sentence":"In this paper, we propose a causal inference framework for subject-invariant facial action unit recognition.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":4,"sentence":"To illustrate the causal effect existing in AU recognition task, we formulate the causalities among facial images, subjects, latent AU semantic relations, and estimated AU occurrence probabilities via a structural causal model.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":4,"sentence":"By constructing such a causal diagram, we clarify the causal-effect among variables and propose a plug-in causal intervention module, CIS, to deconfound the confounder Subject in the causal diagram.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":4,"sentence":"Extensive experiments conducted on two commonly used AU benchmark datasets, BP4D and DISFA, show the effectiveness of our CIS, and the model with CIS inserted, CISNet, has achieved state-of-the-art performance.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":5,"sentence":"One-class classification (OCC) aims to learn an effective data description to enclose all normal training samples and detect anomalies based on the deviation from the data description.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"Current state-of-the-art OCC models learn a compact normality description by hyper-sphere minimisation, but they often suffer from overfitting the training data, especially when the training set is small or contaminated with anomalous samples.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":5,"sentence":"To address this issue, we introduce the interpolated Gaussian descriptor (IGD) method, a novel OCC model that learns a one-class Gaussian anomaly classifier trained with adversarially interpolated training samples.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":5,"sentence":"The Gaussian anomaly classifier differentiates the training samples based on their distance to the Gaussian centre and the standard deviation of these distances, offering the model a discriminability w.r.t. the given samples during training.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":5,"sentence":"The adversarial interpolation is enforced to consistently learn a smooth Gaussian descriptor, even when the training data is small or contaminated with anomalous samples.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":5,"sentence":"This enables our model to learn the data description based on the representative normal samples rather than fringe or anomalous samples, resulting in significantly improved normality description.","offset":5,"pro":0.625,"labels":"CLN"},{"idx":5,"sentence":"In extensive experiments on diverse popular benchmarks, including MNIST, Fashion MNIST, CIFAR10, MVTec AD and two medical datasets, IGD achieves better detection accuracy than current state-of-the-art models.","offset":6,"pro":0.75,"labels":"RST"},{"idx":5,"sentence":"IGD also shows better robustness in problems with small or contaminated training sets.","offset":7,"pro":0.875,"labels":"RST"},{"idx":6,"sentence":"We present an extremely simple Ultra-Resolution Style Transfer framework, termed URST, to flexibly process arbitrary high-resolution images (e.g., 10000x10000 pixels) style transfer for the first time.","offset":0,"pro":0,"labels":"PUR"},{"idx":6,"sentence":"Most of the existing state-of-the-art methods would fall short due to massive memory cost and small stroke size when processing ultra-high resolution images.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":6,"sentence":"URST completely avoids the memory problem caused by ultra-high resolution images by (1) dividing the image into small patches and (2) performing patch-wise style transfer with a novel Thumbnail Instance Normalization (TIN).","offset":2,"pro":0.2222222222222222,"labels":"MTD"},{"idx":6,"sentence":"Specifically, TIN can extract thumbnail features' normalization statistics and apply them to small patches, ensuring the style consistency among different patches.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":6,"sentence":"Overall, the URST framework has three merits compared to prior arts.","offset":4,"pro":0.4444444444444444,"labels":"CLN"},{"idx":6,"sentence":"(1) We divide input image into small patches and adopt TIN, successfully transferring image style with arbitrary high-resolution.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":6,"sentence":"(2) Experiments show that our URST surpasses existing SOTA methods on ultra-high resolution images benefiting from the effectiveness of the proposed stroke perceptual loss in enlarging the stroke size.","offset":6,"pro":0.6666666666666666,"labels":"RST"},{"idx":6,"sentence":"(3) Our URST can be easily plugged into most existing style transfer methods and directly improve their performance even without training.","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":6,"sentence":"Code is available at https://git.io/URST.","offset":8,"pro":0.8888888888888888,"labels":"CTN"},{"idx":7,"sentence":"Point cloud registration is a fundamental step for many tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"In this paper, we propose a neural network named DetarNet to decouple the translation t and rotation R, so as to overcome the performance degradation due to their mutual interference in point cloud registration.","offset":1,"pro":0.1111111111111111,"labels":"PUR"},{"idx":7,"sentence":"First, a Siamese Network based Progressive and Coherent Feature Drift (PCFD) module is proposed to align the source and target points in high-dimensional feature space, and accurately recover translation from the alignment process.","offset":2,"pro":0.2222222222222222,"labels":"MTD"},{"idx":7,"sentence":"Then we propose a Consensus Encoding Unit (CEU) to construct more distinguishable features for a set of putative correspondences.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":7,"sentence":"After that, a Spatial and Channel Attention (SCA) block is adopted to build a classification network for finding good correspondences.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":7,"sentence":"Finally, the rotation is obtained by Singular Value Decomposition (SVD).","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":7,"sentence":"In this way, the proposed network decouples the estimation of translation and rotation, resulting in better performance for both of them.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":7,"sentence":"Experimental results demonstrate that the proposed DetarNet improves registration performance on both indoor and outdoor scenes.","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":7,"sentence":"Our code will be available in https://github.com/ZhiChen902/DetarNet.","offset":8,"pro":0.8888888888888888,"labels":"CTN"},{"idx":8,"sentence":"Weakly supervised object localization (WSOL) aims to learn object localizer solely by using image-level labels.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"The convolution neural network (CNN) based techniques often result in highlighting the most discriminative part of objects while ignoring the entire object extent.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":8,"sentence":"Recently, the transformer architecture has been deployed to WSOL to capture the long-range feature dependencies with self-attention mechanism and multilayer perceptron structure.","offset":2,"pro":0.25,"labels":"BAC"},{"idx":8,"sentence":"Nevertheless, transformers lack the locality inductive bias inherent to CNNs and therefore may deteriorate local feature details in WSOL.","offset":3,"pro":0.375,"labels":"GAP"},{"idx":8,"sentence":"In this paper, we propose a novel framework built upon the transformer, termed LCTR (Local Continuity TRansformer), which targets at enhancing the local perception capability of global features among long-range feature dependencies.","offset":4,"pro":0.5,"labels":"PUR"},{"idx":8,"sentence":"To this end, we propose a relational patch-attention module (RPAM), which considers cross-patch information on a global basis.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":8,"sentence":"We further design a cue digging module (CDM), which utilizes local features to guide the learning trend of the model for highlighting the weak local responses.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":8,"sentence":"Finally, comprehensive experiments are carried out on two widely used datasets, ie, CUB-200-2011 and ILSVRC, to verify the effectiveness of our method.","offset":7,"pro":0.875,"labels":"MTD"},{"idx":9,"sentence":"3D hand pose estimation from single depth is a fundamental problem in computer vision, and has wide applications.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"However, the existing methods still can not achieve satisfactory hand pose estimation results due to view variation and occlusion of human hand.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":9,"sentence":"In this paper, we propose a new virtual view selection and fusion module for 3D hand pose estimation from single depth.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":9,"sentence":"We propose to automatically select multiple virtual viewpoints for pose estimation and fuse the results of all and find this empirically delivers accurate and robust pose estimation.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":9,"sentence":"In order to select most effective virtual views for pose fusion, we evaluate the virtual views based on the confidence of virtual views using a light-weight network via network distillation.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":9,"sentence":"Experiments on three main benchmark datasets including NYU, ICVL and Hands2019 demonstrate that our method outperforms the state-of-the-arts on NYU and ICVL, and achieves very competitive performance on Hands2019-Task1, and our proposed virtual view selection and fusion module is both effective for 3D hand pose estimation.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":10,"sentence":"We present a pose adaptive few-shot learning procedure and a two-stage data interpolation regularization, termed Pose Adaptive Dual Mixup (PADMix), for single-image 3D reconstruction.","offset":0,"pro":0,"labels":"PUR"},{"idx":10,"sentence":"While augmentations via interpolating feature-label pairs are effective in classification tasks, they fall short in shape predictions potentially due to inconsistencies between interpolated products of two images and volumes when rendering viewpoints are unknown.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":10,"sentence":"PADMix targets this issue with two sets of mixup procedures performed sequentially.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":10,"sentence":"We first perform an input mixup which, combined with a pose adaptive learning procedure, is helpful in learning 2D feature extraction and pose adaptive latent encoding.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":10,"sentence":"The stagewise training allows us to build upon the pose invariant representations to perform a follow-up latent mixup under one-to-one correspondences between features and ground-truth volumes.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":10,"sentence":"PADMix significantly outperforms previous literature on few-shot settings over the ShapeNet dataset and sets new benchmarks on the more challenging real-world Pix3D dataset.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":11,"sentence":"Gaze estimation methods learn eye gaze from facial features.","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"However, among rich information in the facial image, real gaze-relevant features only correspond to subtle changes in eye region, while other gaze-irrelevant features like illumination, personal appearance and even facial expression may affect the learning in an unexpected way.","offset":1,"pro":0.08333333333333333,"labels":"GAP"},{"idx":11,"sentence":"This is a major reason why existing methods show significant performance degradation in cross-domain/dataset evaluation.","offset":2,"pro":0.16666666666666666,"labels":"GAP"},{"idx":11,"sentence":"In this paper, we tackle the cross-domain problem in gaze estimation.","offset":3,"pro":0.25,"labels":"PUR"},{"idx":11,"sentence":"Different from common domain adaption methods, we propose a domain generalization method to improve the cross-domain performance without touching target samples.","offset":4,"pro":0.3333333333333333,"labels":"MTD"},{"idx":11,"sentence":"The domain generalization is realized by gaze feature purification.","offset":5,"pro":0.4166666666666667,"labels":"MTD"},{"idx":11,"sentence":"We eliminate gaze-irrelevant factors such as illumination and identity to improve the cross-domain performance.","offset":6,"pro":0.5,"labels":"MTD"},{"idx":11,"sentence":"We design a plug-and-play self-adversarial framework for the gaze feature purification.","offset":7,"pro":0.5833333333333334,"labels":"MTD"},{"idx":11,"sentence":"The framework enhances not only our baseline but also existing gaze estimation methods directly and significantly.","offset":8,"pro":0.6666666666666666,"labels":"CLN"},{"idx":11,"sentence":"To the best of our knowledge, we are the first to propose domain generalization methods in gaze estimation.","offset":9,"pro":0.75,"labels":"CTN"},{"idx":11,"sentence":"Our method achieves not only state-of-the-art performance among typical gaze estimation methods but also competitive results among domain adaption methods.","offset":10,"pro":0.8333333333333334,"labels":"RST"},{"idx":11,"sentence":"The code is released in https://github.com/yihuacheng/PureGaze.","offset":11,"pro":0.9166666666666666,"labels":"CTN"},{"idx":12,"sentence":"Spatio-temporal scene-graph approaches to video-based reasoning tasks, such as video question-answering (QA), typically construct such graphs for every video frame.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"These approaches often ignore the fact that videos are essentially sequences of 2D ``views'' of events happening in a 3D space, and that the semantics of the 3D scene can thus be carried over from frame to frame.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":12,"sentence":"Leveraging this insight, we propose a (2.5+1)D scene graph representation to better capture the spatio-temporal information flows inside the videos.","offset":2,"pro":0.2222222222222222,"labels":"PUR"},{"idx":12,"sentence":"Specifically, we first create a 2.5D (pseudo-3D) scene graph by transforming every 2D frame to have an inferred 3D structure using an off-the-shelf 2D-to-3D transformation module, following which we register the video frames into a shared (2.5+1)D spatio-temporal space and ground each 2D scene graph within it.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":12,"sentence":"Such a (2.5+1)D graph is then segregated into a static sub-graph and a dynamic sub-graph, corresponding to whether the objects within them usually move in the world.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":12,"sentence":"The nodes in the dynamic graph are enriched with motion features capturing their interactions with other graph nodes.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":12,"sentence":"Next, for the video QA task, we present a novel transformer-based reasoning pipeline that embeds the (2.5+1)D graph into a spatio-temporal hierarchical latent space, where the sub-graphs and their interactions are captured at varied granularity.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":12,"sentence":"To demonstrate the effectiveness of our approach, we present experiments on the NExT-QA and AVSD-QA datasets.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":12,"sentence":"Our results show that our proposed (2.5+1)D representation leads to faster training and inference, while our hierarchical model showcases superior performance on the video QA task versus the state of the art.","offset":8,"pro":0.8888888888888888,"labels":"RST"},{"idx":13,"sentence":"Event cameras asynchronously output the polarity values of pixel-level log intensity alterations.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"They are robust against motion blur and can be adopted in challenging light conditions.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":13,"sentence":"Owing to these advantages, event cameras have been employed in various vision tasks such as depth estimation, visual odometry, and object detection.","offset":2,"pro":0.2,"labels":"BAC"},{"idx":13,"sentence":"In particular, event cameras are effective in stereo depth estimation to ﬁnd correspondence points between two cameras under challenging illumination conditions and/or fast motion.","offset":3,"pro":0.3,"labels":"BAC"},{"idx":13,"sentence":"However, because event cameras provide spatially sparse event stream data, it is difﬁcult to obtain a dense disparity map.","offset":4,"pro":0.4,"labels":"GAP"},{"idx":13,"sentence":"Although it is possible to estimate disparity from event data at the edge of a structure where intensity changes are likely to occur, estimating the disparity in a region where event occurs rarely is challenging.","offset":5,"pro":0.5,"labels":"GAP"},{"idx":13,"sentence":"In this study, we propose a deep network that combines the features of an image with the features of an event to generate a dense disparity map.","offset":6,"pro":0.6,"labels":"PUR"},{"idx":13,"sentence":"The proposed network uses images to obtain spatially dense features that are lacking in events.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":13,"sentence":"In addition, we propose a spatial multi-scale correlation between two fused feature maps for an accurate disparity map.","offset":8,"pro":0.8,"labels":"PUR"},{"idx":13,"sentence":"To validate our method, we conducted experiments using synthetic and real-world datasets.","offset":9,"pro":0.9,"labels":"MTD"},{"idx":14,"sentence":"Recently, various image-to-image translation (I2I) methods have improved mode diversity and visual quality in terms of neural networks or regularization terms.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"However, conventional I2I methods relies on a static decision boundary and the encoded representations in those methods are entangled with each other, so they often face with ‘mode collapse’ phenomenon.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":14,"sentence":"To mitigate mode collapse, 1) we design a so-called style-guided discriminator that guides an input image to the target image style based on the strategy of flexible decision boundary.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":14,"sentence":"2) Also, we make the encoded representations include independent domain attributes.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":14,"sentence":"Based on two ideas, this paper proposes Style-Guided and Disentangled Representation for Robust Image-to-Image Translation (SRIT).","offset":4,"pro":0.5,"labels":"PUR"},{"idx":14,"sentence":"SRIT showed outstanding FID by 8%, 22.8%, and 10.1% for CelebA-HQ, AFHQ, and Yosemite datasets, respectively.","offset":5,"pro":0.625,"labels":"RST"},{"idx":14,"sentence":"The translated images of SRIT reflect the styles of target domain successfully.","offset":6,"pro":0.75,"labels":"RST"},{"idx":14,"sentence":"This indicates that SRIT shows better mode diversity than previous works.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":15,"sentence":"Source-Free Unsupervised Domain Adaptation(SFUDA) aims to adapt a pre-trained source model to an unlabeled target domain without access to the original labeled source domain samples.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"Many existing SFUDA approaches apply the self-training strategy, which involves iteratively selecting confidently predicted target samples as pseudo-labeled samples used to train the model to fit the target domain.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":15,"sentence":"However, the self-training strategy may also suffer from sample selection bias and be impacted by the label noise of the pseudo-labeled samples.","offset":2,"pro":0.2,"labels":"GAP"},{"idx":15,"sentence":"In this work, we provide a rigorous theoretical analysis on how these two issues affect the model generalization ability when applying the self-training strategy for the SFUDA problem.","offset":3,"pro":0.3,"labels":"PUR"},{"idx":15,"sentence":"Based on this theoretical analysis, we then propose a new Denoised Maximum Classifier Discrepancy (D-MCD) method for SFUDA to effectively address these two issues.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":15,"sentence":"In particular, we first minimize the distribution mismatch between the selected pseudo-labeled samples and the remaining target domain samples to alleviate the sample selection bias.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":15,"sentence":"Moreover, we design a strong-weak self-training paradigm to denoise the selected pseudo-labeled samples, where the strong network is used to select pseudo-labeled samples while the weak network helps the strong network to filter out hard samples to avoid incorrect labels.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":15,"sentence":"In this way, we are able to ensure both the quality of the pseudo-labels and the generalization ability of the trained model on the target domain.","offset":7,"pro":0.7,"labels":"RST"},{"idx":15,"sentence":"We achieve state-of-the-art results on three domain adaptation benchmark datasets, which clearly validates the effectiveness of our proposed approach.","offset":8,"pro":0.8,"labels":"CLN"},{"idx":15,"sentence":"Full code is available at https://github.com/kkkkkkon/D-MCD.","offset":9,"pro":0.9,"labels":"CTN"},{"idx":16,"sentence":"Digital cameras transform sensor RAW readings into RGB images by means of their Image Signal Processor (ISP).","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"Computational photography tasks such as image denoising and colour constancy are commonly performed in the RAW domain, in part due to the inherent hardware design, but also due to the appealing simplicity of noise statistics that result from the direct sensor readings.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":16,"sentence":"Despite this, the availability of RAW images is limited in comparison with the abundance and diversity of available RGB data.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":16,"sentence":"Recent approaches have attempted to bridge this gap by estimating the RGB to RAW mapping: handcrafted model-based methods that are interpretable and controllable usually require manual parameter fine-tuning, while end-to-end learnable neural networks require large amounts of training data, at times with complex training procedures, and generally lack interpretability and parametric control.","offset":3,"pro":0.375,"labels":"GAP"},{"idx":16,"sentence":"Towards addressing these existing limitations, we present a novel hybrid model-based and data-driven ISP that builds on canonical ISP operations and is both learnable and interpretable.","offset":4,"pro":0.5,"labels":"PUR"},{"idx":16,"sentence":"Our proposed invertible model, capable of bidirectional mapping between RAW and RGB domains, employs end-to-end learning of rich parameter representations, i.e. dictionaries, that are free from direct parametric supervision and additionally enable simple and plausible data augmentation.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":16,"sentence":"We evidence the value of our data generation process by extensive experiments under both RAW image reconstruction and RAW image denoising tasks, obtaining state-of-the-art performance in both.","offset":6,"pro":0.75,"labels":"CLN"},{"idx":16,"sentence":"Additionally, we show that our ISP can learn meaningful mappings from few data samples, and that denoising models trained with our dictionary-based data augmentation are competitive despite having only few or zero ground-truth labels.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":17,"sentence":"In order to fully perceive the surrounding environment, many intelligent robots and self-driving cars are equipped with a multi-camera system.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"Based on this system, the structure-from-motion (SfM) technology is used to realize scene reconstruction, but the fixed relative poses between cameras in the multi-camera system are usually not considered.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":17,"sentence":"This paper presents a tailor-made multi-camera based motion averaging system, where the fixed relative poses are utilized to improve the accuracy and robustness of SfM.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":17,"sentence":"Our approach starts by dividing the images into reference images and non-reference images, and edges in view-graph are divided into four categories accordingly.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":17,"sentence":"Then, a multi-camera based rotating averaging problem is formulated and solved in two stages, where an iterative re-weighted least squares scheme is used to deal with outliers.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":17,"sentence":"Finally, a multi-camera based translation averaging problem is formulated and a l1-norm based optimization scheme is proposed to compute the relative translations of multi-camera system and reference camera positions simultaneously.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":17,"sentence":"Experiments demonstrate that our algorithm achieves superior accuracy and robustness on various data sets compared to the state-of-the-art methods.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":18,"sentence":"Training effective Generative Adversarial Networks (GANs) requires large amounts of training data, without which the trained models are usually sub-optimal with discriminator over-fitting.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"Several prior studies address this issue by expanding the distribution of the limited training data via massive and hand-crafted data augmentation.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":18,"sentence":"We handle data-limited image generation from a very different perspective.","offset":2,"pro":0.2222222222222222,"labels":"MTD"},{"idx":18,"sentence":"Specifically, we design GenCo, a Generative Co-training network that mitigates the discriminator over-fitting issue by introducing multiple complementary discriminators that provide diverse supervision from multiple distinctive views in training.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":18,"sentence":"We instantiate the idea of GenCo in two ways.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":18,"sentence":"The first way is Weight-Discrepancy Co-training (WeCo) which co-trains multiple distinctive discriminators by diversifying their parameters.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":18,"sentence":"The second way is Data-Discrepancy Co-training (DaCo) which achieves co-training by feeding discriminators with different views of the input images.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":18,"sentence":"Extensive experiments over multiple benchmarks show that GenCo achieves superior generation with limited training data.","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":18,"sentence":"In addition, GenCo also complements the augmentation approach with consistent and clear performance gains when combined.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":19,"sentence":"As one of the fundamental components of object detection, intersection-over-union (IoU) calculations between two bounding boxes play an important role in samples selection, NMS operation and evaluation of object detection algorithms.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"This procedure is well-defined and solved for planar images, while it is challenging for spherical ones.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":19,"sentence":"Some existing methods utilize planar bounding boxes to represent spherical objects.","offset":2,"pro":0.2222222222222222,"labels":"BAC"},{"idx":19,"sentence":"However, they are biased due to the distortions of spherical objects.","offset":3,"pro":0.3333333333333333,"labels":"GAP"},{"idx":19,"sentence":"Others use spherical rectangles as unbiased representations, but they adopt excessive approximate algorithms when computing the IoU.","offset":4,"pro":0.4444444444444444,"labels":"GAP"},{"idx":19,"sentence":"In this paper, we propose an unbiased IoU as a novel evaluation criterion for spherical image object detection, which is based on the unbiased representations and utilize unbiased analytical method for IoU calculation.","offset":5,"pro":0.5555555555555556,"labels":"PUR"},{"idx":19,"sentence":"This is the first time that the absolutely accurate IoU calculation is applied to the evaluation criterion, thus object detection algorithms can be correctly evaluated for spherical images.","offset":6,"pro":0.6666666666666666,"labels":"CTN"},{"idx":19,"sentence":"With the unbiased representation and calculation, we also present Spherical CenterNet, an anchor free object detection algorithm for spherical images.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":19,"sentence":"The experiments show that our unbiased IoU gives accurate results and the proposed Spherical CenterNet achieves better performance on one real-world and two synthetic spherical object detection datasets than existing methods.","offset":8,"pro":0.8888888888888888,"labels":"RST"},{"idx":0,"sentence":"This work aims at improving instance retrieval with self-supervision.","offset":0,"pro":0,"labels":"PUR"},{"idx":0,"sentence":"We find that fine-tuning using the recently developed self-supervised learning (SSL) methods, such as SimCLR and MoCo, fails to improve the performance of instance retrieval.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":0,"sentence":"In this work, we identify that the learnt representations for instance retrieval should be invariant to large variations in viewpoint and background etc., whereas self-augmented positives applied by the current SSL methods can not provide strong enough signals for learning robust instance-level representations.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":0,"sentence":"To overcome this problem, we propose InsCLR, a new SSL method that builds on the instance-level contrast, to learn the intra-class invariance by dynamically mining meaningful pseudo positive samples from both mini-batches and a memory bank during training.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":0,"sentence":"Extensive experiments demonstrate that InsCLR achieves similar or even better performance than the state-of-the-art SSL methods on instance retrieval.","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":0,"sentence":"Code is available at https://github.com/zeludeng/insclr.","offset":5,"pro":0.8333333333333334,"labels":"CTN"},{"idx":1,"sentence":"Event camera has offered promising alternative for visual perception, especially in high speed and high dynamic range scenes.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"Recently, many deep learning methods have shown great success in providing model-free solutions to many event-based problems, such as optical flow estimation.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":1,"sentence":"However, existing deep learning methods did not address the importance of temporal information well from the perspective of architecture design and cannot effectively extract spatio-temporal features.","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":1,"sentence":"Another line of research that utilizes Spiking Neural Network suffers from training issues for deeper architecture.","offset":3,"pro":0.3333333333333333,"labels":"GAP"},{"idx":1,"sentence":"To address these points, a novel input representation is proposed that captures the events temporal distribution for signal enhancement.","offset":4,"pro":0.4444444444444444,"labels":"PUR"},{"idx":1,"sentence":"Moreover, we introduce a spatio-temporal recurrent encoding-decoding neural network architecture for event-based optical flow estimation, which utilizes Convolutional Gated Recurrent Units to extract feature maps from a series of event images.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":1,"sentence":"Besides, our architecture allows some traditional frame-based core modules, such as correlation layer and iterative residual refine scheme, to be incorporated.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":1,"sentence":"The network is end-to-end trained with self-supervised learning on the Multi-Vehicle Stereo Event Camera dataset.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":1,"sentence":"We have shown that it outperforms all the existing state-of-the-art methods by a large margin.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":2,"sentence":"Feature Pyramid Network (FPN) has been widely adopted to exploit multi-scale features for scale variation in object detection.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"However, intrinsic defects in most of the current methods with FPN make it difficult to adapt to the feature of different geometric objects.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":2,"sentence":"To address this issue, we introduce geometric prior into FPN to obtain more discriminative features.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":2,"sentence":"In this paper, we propose Geometry-aware Feature Pyramid Network (GaFPN), which mainly consists of the novel Geometry-aware Mapping Module and Geometry-aware Predictor Head.The Geometry-aware Mapping Module is proposed to make full use of all pyramid features to obtain better proposal features by the weight-generation subnetwork.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":2,"sentence":"The weights generation subnetwork generates fusion weight for each layer proposal features by using the geometric information of the proposal.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":2,"sentence":"The Geometry-aware Predictor Head introduces geometric prior into predictor head by the embedding generation network to strengthen feature representation for classification and regression.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":2,"sentence":"Our GaFPN can be easily extended to other two-stage object detectors with feature pyramid and applied to instance segmentation task.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":2,"sentence":"The proposed GaFPN significantly improves detection performance compared to baseline detectors with ResNet-50-FPN: +1.9, +2.0, +1.7, +1.3, +0.8 points Average Precision (AP) on Faster-RCNN, Cascade R-CNN, Dynamic R-CNN, SABL, and AugFPN respectively on MS COCO dataset.","offset":7,"pro":0.875,"labels":"RST"},{"idx":3,"sentence":"Pedestrian trajectory prediction is crucial in many practical applications due to the diversity of pedestrian movements, such as social interactions and individual motion behaviors.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"With similar observable trajectories and social environments, different pedestrians may make completely different future decisions.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":3,"sentence":"However, most existing methods only focus on the frequent modal of the trajectory and thus are difficult to generalize to the peculiar scenario, which leads to the decline of the multimodal fitting ability when facing similar scenarios.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":3,"sentence":"In this paper, we propose a complementary attention gated network (CAGN) for pedestrian trajectory prediction, in which a dual-path architecture including normal and inverse attention is proposed to capture both frequent and peculiar modals in spatial and temporal patterns, respectively.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":3,"sentence":"Specifically, a complementary block is proposed to guide normal and inverse attention, which are then be summed with learnable weights to get attention features by a gated network.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":3,"sentence":"Finally, multiple trajectory distributions are estimated based on the fused spatio-temporal attention features due to the multimodality of future trajectory.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":3,"sentence":"Experimental results on benchmark datasets, i.e., the ETH, and the UCY, demonstrate that our method outperforms state-of-the-art methods by 13.8% in Average Displacement Error (ADE) and 10.4% in Final Displacement Error (FDE).","offset":6,"pro":0.75,"labels":"RST"},{"idx":3,"sentence":"Code will be available at https://github.com/jinghaiD/CAGN","offset":7,"pro":0.875,"labels":"CTN"},{"idx":4,"sentence":"Simultaneous Localization and Mapping (SLAM) and Autonomous Driving are becoming increasingly more important in recent years.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"Point cloud-based large scale place recognition is the spine of them.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":4,"sentence":"While many models have been proposed and have achieved acceptable performance by learning short-range local features, they always skip long-range contextual properties.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":4,"sentence":"Moreover, the model size also becomes a serious shackle for their wide applications.","offset":3,"pro":0.375,"labels":"GAP"},{"idx":4,"sentence":"To overcome these challenges, we propose a super light-weight network model termed SVT-Net.","offset":4,"pro":0.5,"labels":"PUR"},{"idx":4,"sentence":"On top of the highly efficient 3D Sparse Convolution (SP-Conv), an Atom-based Sparse Voxel Transformer (ASVT) and a Cluster-based Sparse Voxel Transformer (CSVT) are proposed respectively to learn both short-range local features and long-range contextual features.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":4,"sentence":"Consisting of ASVT and CSVT, SVT-Net can achieve state-of-the-art performance in terms of both recognition accuracy and running speed with a super-light model size (0.9M parameters).","offset":6,"pro":0.75,"labels":"RST"},{"idx":4,"sentence":"Meanwhile, for the purpose of further boosting efficiency, we introduce two simplified versions, which also achieve state-of-the-art performance and further reduce the model size to 0.8M and 0.4M respectively.","offset":7,"pro":0.875,"labels":"RST"},{"idx":5,"sentence":"Interpretability is crucial to understand the inner workings of deep neural networks (DNNs).","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"Many interpretation methods help to understand the decision-making of DNNs by generating saliency maps that highlight parts of the input image that contribute the most to the prediction made by the DNN.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":5,"sentence":"In this paper we design a backdoor attack that alters the saliency map produced by the network for an input image with a specific trigger pattern while not losing the prediction performance significantly.","offset":2,"pro":0.2222222222222222,"labels":"PUR"},{"idx":5,"sentence":"The saliency maps are incorporated in the penalty term of the objective function that is used to train a deep model and its influence on model training is conditioned upon the presence of a trigger.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":5,"sentence":"We design two types of attacks: a targeted attack that enforces a specific modification of the saliency map and a non-targeted attack when the importance scores of the top pixels from the original saliency map are significantly reduced.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":5,"sentence":"We perform empirical evaluations of the proposed backdoor attacks on gradient-based interpretation methods, Grad-CAM and SimpleGrad, and a gradient-free scheme, VisualBackProp, for a variety of deep learning architectures.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":5,"sentence":"We show that our attacks constitute a serious security threat to the reliability of the interpretation methods when deploying models developed by untrusted sources.","offset":6,"pro":0.6666666666666666,"labels":"CLN"},{"idx":5,"sentence":"We furthermore show that existing backdoor defense mechanisms are ineffective in detecting our attacks.","offset":7,"pro":0.7777777777777778,"labels":"CLN"},{"idx":5,"sentence":"Finally, we demonstrate that the proposed methodology can be used in an inverted setting, where the correct saliency map can be obtained only in the presence of a trigger (key), effectively making the interpretation system available only to selected users.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":6,"sentence":"Transfer adversarial attack is a non-trivial black-box adversarial attack that aims to craft adversarial perturbations on the surrogate model and then apply such perturbations to the victim model.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"However, the transferability of perturbations from existing methods is still limited, since the adversarial perturbations are easily overﬁtting with a single surrogate model and speciﬁc data pattern.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":6,"sentence":"In this paper, we propose a Learning to Learn Transferable Attack (LLTA) method, which makes the adversarial perturbations more generalized via learning from both data and model augmentation.","offset":2,"pro":0.2222222222222222,"labels":"PUR"},{"idx":6,"sentence":"For data augmentation, we adopt simple random resizing and padding.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":6,"sentence":"For model augmentation, we randomly alter the back propagation instead of the forward propagation to eliminate the effect on the model prediction.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":6,"sentence":"By treating the attack of both speciﬁc data and a modiﬁed model as a task, we expect the adversarial perturbations to adopt enough tasks for generalization.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":6,"sentence":"To this end, the meta-learning algorithm is further introduced during the iteration of perturbation generation.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":6,"sentence":"Empirical results on the widely-used dataset demonstrate the effectiveness of our attack method with a 12.85% higher success rate of transfer attack compared with the state-of-the-art methods.","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":6,"sentence":"We also evaluate our method on the real-world online system, i.e., Google Cloud Vision API, to further show the practical potentials of our method.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":7,"sentence":"Omnidirectional images, also called 360◦images, have attracted extensive attention in recent years, due to the rapid development of virtual reality (VR) technologies.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"During omnidirectional image processing including capture, transmission, consumption, and so on, measuring the perceptual quality of omnidirectional images is highly desired, since it plays a great role in guaranteeing the immersive quality of experience (IQoE).","offset":1,"pro":0.1,"labels":"BAC"},{"idx":7,"sentence":"In this paper, we conduct a comprehensive study on the perceptual quality of omnidirectional images from both subjective and objective perspectives.","offset":2,"pro":0.2,"labels":"PUR"},{"idx":7,"sentence":"Specifically, we construct the largest so far subjective omnidirectional image quality database, where we consider several key influential elements, i.e., realistic non-uniform distortion, viewing condition, and viewing behavior, from the user view.","offset":3,"pro":0.3,"labels":"MTD"},{"idx":7,"sentence":"In addition to subjective quality scores, we also record head and eye movement data.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":7,"sentence":"Besides, we make the first attempt by using the proposed database to train a convolutional neural network (CNN) for blind omnidirectional image quality assessment.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":7,"sentence":"To be consistent with the human viewing behavior in the VR device, we extract viewports from each omnidirectional image and incorporate the user viewing conditions naturally in the proposed model.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":7,"sentence":"The proposed model is composed of two parts, including a multi-scale CNN-based feature extraction module and a perceptual quality prediction module.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":7,"sentence":"The feature extraction module is used to incorporate the multi-scale features, and the perceptual quality prediction module is designed to regress them to perceived quality scores.","offset":8,"pro":0.8,"labels":"MTD"},{"idx":7,"sentence":"The experimental results on our database verify that the proposed model achieves the competing performance compared with the state-of-the-art methods.","offset":9,"pro":0.9,"labels":"CLN"},{"idx":8,"sentence":"Large capacity deep learning models are often prone to a high generalization gap when trained with a limited amount of labeled training data.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"A recent class of methods to address this problem uses various ways to construct a new training sample by mixing a pair (or more) of training samples.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":8,"sentence":"We propose PatchUp, a hidden state block-level regularization technique for Convolutional Neural Networks (CNNs), that is applied on selected contiguous blocks of feature maps from a random pair of samples.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":8,"sentence":"Our approach improves the robustness of CNN models against the manifold intrusion problem that may occur in other state-of-the-art mixing approaches.","offset":3,"pro":0.42857142857142855,"labels":"CLN"},{"idx":8,"sentence":"Moreover, since we are mixing the contiguous block of features in the hidden space, which has more dimensions than the input space, we obtain more diverse samples for training towards different dimensions.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":8,"sentence":"Our experiments on CIFAR10/100, SVHN, Tiny-ImageNet, and ImageNet using ResNet architectures including PreActResnet18/34, WRN-28-10, ResNet101/152 models show that PatchUp improves upon, or equals, the performance of current state-of-the-art regularizers for CNNs.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":8,"sentence":"We also show that PatchUp can provide a better generalization to deformed samples and is more robust against adversarial attacks.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":9,"sentence":"Existing permutation-invariant methods can be divided into two categories according to the aggregation scope, i.e. global aggregation and local one.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"Although the global aggregation methods, e. g., PointNet and Deep Sets, get involved in simpler structures, their performance is poorer than the local aggregation ones like PointNet++ and Point Transformer.","offset":1,"pro":0.1,"labels":"GAP"},{"idx":9,"sentence":"It remains an open problem whether there exists a global aggregation method with a simple structure, competitive performance, and even much fewer parameters.","offset":2,"pro":0.2,"labels":"GAP"},{"idx":9,"sentence":"In this paper, we propose a novel global aggregation permutation-invariant network based on dual MLP dot-product, called DuMLP-Pin, which is capable of being employed to extract features for set inputs, including unordered or unstructured pixel, attribute, and point cloud data sets.","offset":3,"pro":0.3,"labels":"PUR"},{"idx":9,"sentence":"We strictly prove that any permutation-invariant function implemented by DuMLP-Pin can be decomposed into two or more permutation-equivariant ones in a dot-product way as the cardinality of the given input set is greater than a threshold.","offset":4,"pro":0.4,"labels":"RST"},{"idx":9,"sentence":"We also show that the DuMLP-Pin can be viewed as Deep Sets with strong constraints under certain conditions.","offset":5,"pro":0.5,"labels":"CLN"},{"idx":9,"sentence":"The performance of DuMLP-Pin is evaluated on several different tasks with diverse data sets.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":9,"sentence":"The experimental results demonstrate that our DuMLP-Pin achieves the best results on the two classification problems for pixel sets and attribute sets.","offset":7,"pro":0.7,"labels":"RST"},{"idx":9,"sentence":"On both the point cloud classification and the part segmentation, the accuracy of DuMLP-Pin is very close to the so-far best-performing local aggregation method with only a 1-2% difference, while the number of required parameters is significantly reduced by more than 85% in classification and 69% in segmentation, respectively.","offset":8,"pro":0.8,"labels":"RST"},{"idx":9,"sentence":"The code is publicly available on https://github.com/JaronTHU/DuMLP-Pin.","offset":9,"pro":0.9,"labels":"CTN"},{"idx":10,"sentence":"Recently, attention-based image captioning models, which are expected to ground correct image regions for proper word generations, have achieved remarkable performance.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"However, some researchers have argued “deviated focus” problem of existing attention mechanisms in determining the effective and influential image features.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":10,"sentence":"In this paper, we present A2 - an attention-aligned Transformer for image captioning, which guides attention learning in a perturbation-based self-supervised manner, without any annotation overhead.","offset":2,"pro":0.2222222222222222,"labels":"PUR"},{"idx":10,"sentence":"Specifically, we add mask operation on image regions through a learnable network to estimate the true function in ultimate description generation.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":10,"sentence":"We hypothesize that the necessary image region features, where small disturbance causes an obvious performance degradation, deserve more attention weight.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":10,"sentence":"Then, we propose four aligned strategies to use this information to refine attention weight distribution.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":10,"sentence":"Under such a pattern, image regions are attended correctly with the output words.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":10,"sentence":"Extensive experiments conducted on the MS COCO dataset demonstrate that the proposed A2 Transformer consistently outperforms baselines in both automatic metrics and human evaluation.","offset":7,"pro":0.7777777777777778,"labels":"CLN"},{"idx":10,"sentence":"Trained models and code for reproducing the experiments are publicly available.","offset":8,"pro":0.8888888888888888,"labels":"CTN"},{"idx":11,"sentence":"Recently, Convolutional Neural Network (CNN) has achieved excellent performance in the classification task.","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"It is widely known that CNN is deemed as a 'blackbox', which is hard for understanding the prediction mechanism and debugging the wrong prediction.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":11,"sentence":"Some model debugging and explanation works are developed for solving the above drawbacks.","offset":2,"pro":0.25,"labels":"BAC"},{"idx":11,"sentence":"However, those methods focus on explanation and diagnosing possible causes for model prediction, based on which the researchers handle the following optimization of models manually.","offset":3,"pro":0.375,"labels":"GAP"},{"idx":11,"sentence":"In this paper, we propose the first completely automatic model diagnosing and treating tool, termed as Model Doctor.","offset":4,"pro":0.5,"labels":"PUR"},{"idx":11,"sentence":"Based on two discoveries that 1) each category is only correlated with sparse and specific convolution kernels, and 2) adversarial samples are isolated while normal samples are successive in the feature space, a simple aggregate gradient constraint is devised for effectively diagnosing and optimizing CNN classifiers.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":11,"sentence":"The aggregate gradient strategy is a versatile module for mainstream CNN classifiers.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":11,"sentence":"Extensive experiments demonstrate that the proposed Model Doctor applies to all existing CNN classifiers, and improves the accuracy of 16 mainstream CNN classifiers by 1%~5%.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":12,"sentence":"In point cloud compression, sufficient contexts are significant for modeling the point cloud distribution.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"However, the contexts gathered by the previous voxel-based methods decrease when handling sparse point clouds.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":12,"sentence":"To address this problem, we propose a multiple-contexts deep learning framework called OctAttention employing the octree structure, a memory-efficient representation for point clouds.","offset":2,"pro":0.2222222222222222,"labels":"PUR"},{"idx":12,"sentence":"Our approach encodes octree symbol sequences in a lossless way by gathering the information of sibling and ancestor nodes.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":12,"sentence":"Expressly, we first represent point clouds with octree to reduce spatial redundancy, which is robust for point clouds with different resolutions.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":12,"sentence":"We then design a conditional entropy model with a large receptive field that models the sibling and ancestor contexts to exploit the strong dependency among the neighboring nodes and employ an attention mechanism to emphasize the correlated nodes in the context.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":12,"sentence":"Furthermore, we introduce a mask operation during training and testing to make a trade-off between encoding time and performance.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":12,"sentence":"Compared to the previous state-of-the-art works, our approach obtains a 10%-35% BD-Rate gain on the LiDAR benchmark (e.g. SemanticKITTI) and object point cloud dataset (e.g. MPEG 8i, MVUB), and saves 95% coding time compared to the voxel-based baseline.","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":12,"sentence":"The code is available at https://github.com/zb12138/OctAttention.","offset":8,"pro":0.8888888888888888,"labels":"CTN"},{"idx":13,"sentence":"Creating presentation materials requires complex multimodal reasoning skills to summarize key concepts and arrange them in a logical and visually pleasing manner.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"Can machines learn to emulate this laborious process?","offset":1,"pro":0.125,"labels":"BAC"},{"idx":13,"sentence":"We present a novel task and approach for document-to-slide generation.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":13,"sentence":"Solving this involves document summarization, image and text retrieval, slide structure and layout prediction to arrange key elements in a form suitable for presentation.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":13,"sentence":"We propose a hierarchical sequence-to-sequence approach to tackle our task in an end-to-end manner.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":13,"sentence":"Our approach exploits the inherent structures within documents and slides and incorporates paraphrasing and layout prediction modules to generate slides.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":13,"sentence":"To help accelerate research in this domain, we release a dataset about 6K paired documents and slide decks used in our experiments.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":13,"sentence":"We show that our approach outperforms strong baselines and produces slides with rich content and aligned imagery.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":14,"sentence":"Underwater images suffer from degradation due to light scattering and absorption.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"It remains challenging to restore such degraded images using deep neural networks since real-world paired data is scarcely available while synthetic paired data cannot approximate real-world data perfectly.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":14,"sentence":"In this paper, we propose an UnSupervised Underwater Image Restoration method (USUIR) by leveraging the homology property between a raw underwater image and a re-degraded image.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":14,"sentence":"Specifically, USUIR first estimates three latent components of the raw underwater image, i.e., the global background light, the transmission map, and the scene radiance (the clean image).","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":14,"sentence":"Then, a re-degraded image is generated by randomly mixing up the estimated scene radiance and the raw underwater image.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":14,"sentence":"We demonstrate that imposing a homology constraint between the raw underwater image and the re-degraded image is equivalent to minimizing the restoration error and hence can be used for the unsupervised restoration.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":14,"sentence":"Extensive experiments show that USUIR achieves promising performance in both inference time and restoration quality.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":15,"sentence":"Large-scale pre-training has recently revolutionized vision-and-language (VL) research.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"Models such as LXMERT and UNITER have significantly lifted the state of the art over a wide range of VL tasks.","offset":1,"pro":0.09090909090909091,"labels":"BAC"},{"idx":15,"sentence":"However, the large number of parameters in such models hinders their application in practice.","offset":2,"pro":0.18181818181818182,"labels":"GAP"},{"idx":15,"sentence":"In parallel, work on the lottery ticket hypothesis (LTH) has shown that deep neural networks contain small matching subnetworks that can achieve on par or even better performance than the dense networks when trained in isolation.","offset":3,"pro":0.2727272727272727,"labels":"BAC"},{"idx":15,"sentence":"In this work, we perform the first empirical study to assess whether such trainable subnetworks also exist in pre-trained VL models.","offset":4,"pro":0.36363636363636365,"labels":"PUR"},{"idx":15,"sentence":"We use UNITER as the main testbed (also test on LXMERT and ViLT), and consolidate 7 representative VL tasks for experiments, including visual question answering, visual commonsense reasoning, visual entailment, referring expression comprehension, image-text retrieval, GQA, and NLVR2.","offset":5,"pro":0.45454545454545453,"labels":"MTD"},{"idx":15,"sentence":"Through comprehensive analysis, we summarize our main findings as follows.","offset":6,"pro":0.5454545454545454,"labels":"CLN"},{"idx":15,"sentence":"(ii) Subnetworks found by task-specific pruning transfer reasonably well to the other tasks, while those found on the pre-training tasks at 60%/70% sparsity transfer universally, matching 98%/96% of the full accuracy on average over all the tasks.","offset":7,"pro":0.6363636363636364,"labels":"CLN"},{"idx":15,"sentence":"(iv) LTH also remains relevant when using other training methods (e.g., adversarial training). ","offset":8,"pro":0.7272727272727273,"labels":"CLN"},{"idx":15,"sentence":"(i) It is difficult to find subnetworks that strictly match the performance of the full model. \nHowever, we can find relaxed winning tickets at 50%-70% sparsity that maintain 99% of the full accuracy.","offset":9,"pro":0.8181818181818182,"labels":"CLN"},{"idx":15,"sentence":"(iii) Besides UNITER, other models such as LXMERT and ViLT can also play lottery tickets. \nHowever, the highest sparsity we can achieve for ViLT is far lower than LXMERT and UNITER (30% vs. 70%).","offset":10,"pro":0.9090909090909091,"labels":"CLN"},{"idx":16,"sentence":"Convolutional neural networks based single-image superresolution (SISR) has made great progress in recent years.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"However, it is difficult to apply these methods to real-world scenarios due to the computational and memory cost.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":16,"sentence":"Meanwhile, how to take full advantage of the intermediate features under the constraints of limited parameters and calculations is also a huge challenge.","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":16,"sentence":"To alleviate these issues, we propose a lightweight yet efficient Feature Distillation Interaction Weighted Network (FDIWN).","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":16,"sentence":"Specifically, FDIWN utilizes a series of specially designed Feature Shuffle Weighted Groups (FSWG) as the backbone, and several novel mutual Wide-residual Distillation Interaction Blocks (WDIB) form an FSWG.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":16,"sentence":"In addition, Wide Identical Residual Weighting (WIRW) units and Wide Convolutional Residual Weighting (WCRW) units are introduced into WDIB for better feature distillation.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":16,"sentence":"Moreover, a Wide-Residual Distillation Connection (WRDC) framework and a Self-Calibration Fusion (SCF) unit are proposed to interact features with different scales more flexibly and efficiently.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":16,"sentence":"Extensive experiments show that our FDIWN is superior to other models to strike a good balance between model performance and efficiency.","offset":7,"pro":0.7777777777777778,"labels":"CLN"},{"idx":16,"sentence":"The code is available at https://github.com/IVIPLab/FDIWN.","offset":8,"pro":0.8888888888888888,"labels":"CTN"},{"idx":17,"sentence":"Current state-of-the-art saliency detection models rely heavily on large datasets of accurate pixel-wise annotations, but manually labeling pixels is time-consuming and labor-intensive.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"There are some weakly supervised methods developed for alleviating the problem, such as image label, bounding box label, and scribble label, while point label still has not been explored in this field.","offset":1,"pro":0.09090909090909091,"labels":"GAP"},{"idx":17,"sentence":"In this paper, we propose a novel weakly-supervised salient object detection method using point supervision.","offset":2,"pro":0.18181818181818182,"labels":"PUR"},{"idx":17,"sentence":"To infer the saliency map, we first design an adaptive masked flood filling algorithm to generate pseudo labels.","offset":3,"pro":0.2727272727272727,"labels":"MTD"},{"idx":17,"sentence":"Then we develop a transformer-based point-supervised saliency detection model to produce the first round of saliency maps.","offset":4,"pro":0.36363636363636365,"labels":"MTD"},{"idx":17,"sentence":"However, due to the sparseness of the label, the weakly supervised model tends to degenerate into a general foreground detection model.","offset":5,"pro":0.45454545454545453,"labels":"GAP"},{"idx":17,"sentence":"To address this issue, we propose a Non-Salient Suppression (NSS) method to optimize the erroneous saliency maps generated in the first round and leverage them for the second round of training.","offset":6,"pro":0.5454545454545454,"labels":"PUR"},{"idx":17,"sentence":"Moreover, we build a new point-supervised dataset (P-DUTS) by relabeling the DUTS dataset.","offset":7,"pro":0.6363636363636364,"labels":"MTD"},{"idx":17,"sentence":"In P-DUTS, there is only one labeled point for each salient object.","offset":8,"pro":0.7272727272727273,"labels":"MTD"},{"idx":17,"sentence":"Comprehensive experiments on five largest benchmark datasets demonstrate our method outperforms the previous state-of-the-art methods trained with the stronger supervision and even surpass several fully supervised state-of-the-art models.","offset":9,"pro":0.8181818181818182,"labels":"CLN"},{"idx":17,"sentence":"The code is available at: https://github.com/shuyonggao/PSOD.","offset":10,"pro":0.9090909090909091,"labels":"CTN"},{"idx":18,"sentence":"The success of deep neural nets heavily relies on their ability to encode complex relations between their input and their output.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"While this property serves to fit the training data well, it also obscures the mechanism that drives prediction.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":18,"sentence":"This study aims to reveal hidden concepts by employing an intervention mechanism that shifts the predicted class based on discrete variational autoencoders.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":18,"sentence":"An explanatory model then visualizes the encoded information from any hidden layer and its corresponding intervened representation.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":18,"sentence":"By the assessment of differences between the original representation and the intervened representation, one can determine the concepts that can alter the class, hence providing interpretability.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":18,"sentence":"We demonstrate the effectiveness of our approach on CelebA, where we show various visualizations for bias in the data and suggest different interventions to reveal and change bias.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":19,"sentence":"In real world, training data for person re-identification (Re-ID) is collected discretely with spatial and temporal variations, which requires a model to incrementally learn new knowledge without forgetting old knowledge.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"This problem is called lifelong person re-identification (LReID).","offset":1,"pro":0.1,"labels":"BAC"},{"idx":19,"sentence":"Variations of illumination and background for images of each task exhibit task-specific image style and lead to task-wise domain gap.","offset":2,"pro":0.2,"labels":"GAP"},{"idx":19,"sentence":"In addition to missing data from the old tasks, task-wise domain gap is a key factor for catastrophic forgetting in LReID, which is ignored in existing approaches for LReID.","offset":3,"pro":0.3,"labels":"GAP"},{"idx":19,"sentence":"The model tends to learn task-specific knowledge with task-wise domain gap, which results in stability and plasticity dilemma.","offset":4,"pro":0.4,"labels":"GAP"},{"idx":19,"sentence":"To overcome this problem, we cast LReID as a domain adaptation problem and propose a pseudo task knowledge preservation framework to alleviate the domain gap.","offset":5,"pro":0.5,"labels":"PUR"},{"idx":19,"sentence":"Our framework is based on a pseudo task transformation module which maps the features of the new task into the feature space of the old tasks to complement the limited saved exemplars of the old tasks.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":19,"sentence":"With extra transformed features in the task-specific feature space, we propose a task-specific domain consistency loss to implicitly alleviate the task-wise domain gap for learning task-shared knowledge instead of task-specific one.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":19,"sentence":"Furthermore, to guide knowledge preservation with the feature distributions of the old tasks, we propose to preserve knowledge on extra pseudo tasks which jointly distills knowledge and discriminates identity, in order to achieve a better trade-off between stability and plasticity for lifelong learning with task-wise domain gap.","offset":8,"pro":0.8,"labels":"MTD"},{"idx":19,"sentence":"Extensive experiments demonstrate the superiority of our method as compared with the state-of-the-art lifelong learning and LReID methods.","offset":9,"pro":0.9,"labels":"CLN"},{"idx":0,"sentence":"Unsupervised domain adaptive person re-identification aims at learning on an unlabeled target domain with only labeled data in source domain.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"Currently, the state-of-the-arts usually solve this problem by pseudo-label-based clustering and fine-tuning in target domain.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":0,"sentence":"However, the reason behind the noises of pseudo labels is not sufficiently explored, especially for the popular multi-branch models.","offset":2,"pro":0.2,"labels":"GAP"},{"idx":0,"sentence":"We argue that the consistency between different feature spaces is the key to the pseudo labels’ quality.","offset":3,"pro":0.3,"labels":"PUR"},{"idx":0,"sentence":"Then a SElf-Consistent pseudo label RefinEmenT method, termed as SECRET, is proposed to improve consistency by mutually refining the pseudo labels generated from different feature spaces.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":0,"sentence":"The proposed SECRET gradually encourages the improvement of pseudo labels’ quality during training process, which further leads to better cross-domain Re-ID performance.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":0,"sentence":"Extensive experiments on benchmark datasets show the superiority of our method.","offset":6,"pro":0.6,"labels":"CLN"},{"idx":0,"sentence":"Specifically, our method outperforms the state-of-the-arts by 6.3% in terms of mAP on the challenging dataset MSMT17.","offset":7,"pro":0.7,"labels":"RST"},{"idx":0,"sentence":"In the purely unsupervised setting, our method also surpasses existing works by a large margin.","offset":8,"pro":0.8,"labels":"RST"},{"idx":0,"sentence":"Code is available at https://github.com/LunarShen/SECRET.","offset":9,"pro":0.9,"labels":"CTN"},{"idx":1,"sentence":"Existing Scene Text Recognition (STR) methods typically use a language model to optimize the joint probability of the 1D character sequence predicted by a visual recognition (VR) model, which ignore the 2D spatial context of visual semantics within and between character instances, making them not generalize well to arbitrary shape scene text.","offset":0,"pro":0,"labels":"GAP"},{"idx":1,"sentence":"To address this issue, we make the first attempt to perform textual reasoning based on visual semantics in this paper.","offset":1,"pro":0.1111111111111111,"labels":"PUR"},{"idx":1,"sentence":"Technically, given the character segmentation maps predicted by a VR model, we construct a subgraph for each instance, where nodes represent the pixels in it and edges are added between nodes based on their spatial similarity.","offset":2,"pro":0.2222222222222222,"labels":"MTD"},{"idx":1,"sentence":"Then, these subgraphs are sequentially connected by their root nodes and merged into a complete graph.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":1,"sentence":"Based on this graph, we devise a graph convolutional network for textual reasoning (GTR) by supervising it with a cross-entropy loss.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":1,"sentence":"GTR can be easily plugged in representative STR models to improve their performance owing to better textual reasoning.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":1,"sentence":"Specifically, we construct our model, namely S-GTR, by paralleling GTR to the language model in a segmentation-based STR baseline, which can effectively exploit the visual-linguistic complementarity via mutual learning.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":1,"sentence":"S-GTR sets new state-of-the-art on six challenging STR benchmarks and generalizes well to multi-linguistic datasets.","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":1,"sentence":"Code is available at https://github.com/adeline-cs/GTR.","offset":8,"pro":0.8888888888888888,"labels":"CTN"},{"idx":2,"sentence":"This paper introduces Ranking Info Noise Contrastive Estimation (RINCE), a new member in the family of InfoNCE losses that preserves a ranked ordering of positive samples.","offset":0,"pro":0,"labels":"PUR"},{"idx":2,"sentence":"In contrast to the standard InfoNCE loss, which requires a strict binary separation of the training pairs into similar and dissimilar samples, RINCE can exploit information about a similarity ranking for learning a corresponding embedding space.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":2,"sentence":"We show that the proposed loss function learns favorable embeddings compared to the standard InfoNCE whenever at least noisy ranking information can be obtained or when the definition of positives and negatives is blurry.","offset":2,"pro":0.3333333333333333,"labels":"CLN"},{"idx":2,"sentence":"We demonstrate this for a supervised classification task with additional superclass labels and noisy similarity scores.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":2,"sentence":"Furthermore, we show that RINCE can also be applied to unsupervised training with experiments on unsupervised representation learning from videos.","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":2,"sentence":"In particular, the embedding yields higher classification accuracy, retrieval rates and performs better on out-of-distribution detection than the standard InfoNCE loss. ","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":3,"sentence":"Deep learning has made remarkable achievements for single image haze removal.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"However, existing deep dehazing models only give deterministic results without discussing the uncertainty of them.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":3,"sentence":"We first introduce an Uncertainty Estimation Block (UEB) to predict the aleatoric and epistemic uncertainty together.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":3,"sentence":"Then, we propose an Uncertainty-aware Feature Modulation (UFM) block to adaptively enhance the learned features.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":3,"sentence":"There exist two types of uncertainty in the dehazing models: aleatoric uncertainty that comes from noise inherent in the observations and epistemic uncertainty that accounts for uncertainty in the model.","offset":4,"pro":0.5714285714285714,"labels":"GAP"},{"idx":3,"sentence":"UFM predicts a convolution kernel and channel-wise modulation cofficients conitioned on the uncertainty weighted representation.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":3,"sentence":"Extensive experimental results on synthetic datasets and real-world images show that UDN achieves significant quantitative and qualitative improvements, outperforming the state-of-the-arts. ","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":4,"sentence":"Image composition targets at inserting a foreground object into a background image.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"Most previous image composition methods focus on adjusting the foreground to make it compatible with background while ignoring the shadow effect of foreground on the background.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":4,"sentence":"In this work, we focus on generating plausible shadow for the foreground object in the composite image.","offset":2,"pro":0.2222222222222222,"labels":"PUR"},{"idx":4,"sentence":"First, we contribute a real-world shadow generation dataset DESOBA by generating synthetic composite images based on paired real images and deshadowed images.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":4,"sentence":"Then, we propose a novel shadow generation network SGRNet, which consists of a shadow mask prediction stage and a shadow filling stage.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":4,"sentence":"In the shadow mask prediction stage, foreground and background information are thoroughly interacted to generate foreground shadow mask.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":4,"sentence":"In the shadow filling stage, shadow parameters are predicted to fill the shadow area.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":4,"sentence":"Our dataset and code are available at https://github.com/bcmi/Object-Shadow-Generation- Dataset-DESOBA.","offset":7,"pro":0.7777777777777778,"labels":"CTN"},{"idx":4,"sentence":"Extensive experiments on our DESOBA dataset and real composite images demonstrate the effectiveness of our proposed method.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":5,"sentence":"A considerable obstacle is the wide variation in the shape (e.g., aspect ratio) of objects.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"Sample selection in general object detection has been widely studied as it plays a crucial role in the performance of the detection method and has achieved great progress.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":5,"sentence":"However, existing sample selection strategies still overlook some issues: (1) most of them ignore the object shape information; (2) they do not make a potential distinction between selected positive samples; and (3) some of them can only be applied to either anchor-free or anchor-based methods and cannot be used for both of them simultaneously.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":5,"sentence":"In this paper, we propose novel flexible shape-adaptive selection (SA-S) and shape-adaptive measurement (SA-M) strategies for oriented object detection, which comprise an SA-S strategy for sample selection and SA-M strategy for the quality estimation of positive samples.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":5,"sentence":"Specifically, the SA-S strategy dynamically selects samples according to the shape information and characteristics distribution of objects.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":5,"sentence":"The SA-M strategy measures the localization potential and adds quality information on the selected positive samples.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":5,"sentence":"The experimental results on both anchor-free and anchor-based baselines and four publicly available oriented datasets (DOTA, HRSC2016, UCAS-AOD, and ICDAR2015) demonstrate the effectiveness of the proposed method.","offset":6,"pro":0.75,"labels":"CLN"},{"idx":5,"sentence":"The development of detection methods for oriented object detection remains a challenging task.","offset":7,"pro":0.875,"labels":"BAC"},{"idx":6,"sentence":"Current representation learning methods for whole slide image (WSI) with pyramidal resolutions are inherently homogeneous and flat, which cannot fully exploit the multiscale and heterogeneous diagnostic information of different structures for comprehensive analysis.","offset":0,"pro":0,"labels":"GAP"},{"idx":6,"sentence":"This paper presents a novel graph neural network-based multiple instance learning framework (i.e., H^2-MIL) to learn hierarchical representation from a heterogeneous graph with different resolutions for WSI analysis.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":6,"sentence":"A heterogeneous graph with the “resolution” attribute is constructed to explicitly model the feature and spatial-scaling relationship of multi-resolution patches.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":6,"sentence":"We then design a novel resolution-aware attention convolution (RAConv) block to learn compact yet discriminative representation from the graph, which tackles the heterogeneity of node neighbors with different resolutions and yields more reliable message passing.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":6,"sentence":"More importantly, to explore the task-related structured information of WSI pyramid, we elaborately design a novel iterative hierarchical pooling (IHPool) module to progressively aggregate the heterogeneous graph based on scaling relationships of different nodes.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":6,"sentence":"We evaluated our method on two public WSI datasets from the TCGA project, i.e., esophageal cancer and kidney cancer.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":6,"sentence":"Experimental results show that our method clearly outperforms the state-of-the-art methods on both tumor typing and staging tasks.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":7,"sentence":"Recent work has shown that Binarized Neural Networks (BNNs) are able to greatly reduce computational costs and memory footprints, facilitating model deployment on resource-constrained devices.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"However, in comparison to their full-precision counterparts, BNNs suffer from severe accuracy degradation.","offset":1,"pro":0.1,"labels":"GAP"},{"idx":7,"sentence":"Research aiming to reduce this accuracy gap has thus far largely focused on specific network architectures with few or no 1 × 1 convolutional layers, for which standard binarization methods do not work well.","offset":2,"pro":0.2,"labels":"GAP"},{"idx":7,"sentence":"Because 1 × 1 convolutions are common in the design of modern architectures (e.g. GoogleNet, ResNet, DenseNet), it is crucial to develop a method to binarize them effectively for BNNs to be more widely adopted.","offset":3,"pro":0.3,"labels":"BAC"},{"idx":7,"sentence":"In this work, we propose an “Elastic-Link” (EL) module to enrich information flow within a BNN by adaptively adding real-valued input features to the subsequent convolutional output features.","offset":4,"pro":0.4,"labels":"PUR"},{"idx":7,"sentence":"The proposed EL module is easily implemented and can be used in conjunction with other methods for BNNs.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":7,"sentence":"We demonstrate that adding EL to BNNs produces a significant improvement on the challenging large-scale ImageNet dataset.","offset":6,"pro":0.6,"labels":"CLN"},{"idx":7,"sentence":"For example, we raise the top-1 accuracy of binarized ResNet26 from 57.9% to 64.0%.","offset":7,"pro":0.7,"labels":"RST"},{"idx":7,"sentence":"EL also aids con-vergence in the training of binarized MobileNet, for which a top-1 accuracy of 56.4% is achieved.","offset":8,"pro":0.8,"labels":"RST"},{"idx":7,"sentence":"Finally, with the integration of ReActNet, it yields a new state-of-the-art result of 71.9% top-1 accuracy.","offset":9,"pro":0.9,"labels":"RST"},{"idx":8,"sentence":"Deepfake has ignited hot research interests in both academia and industry due to its potential security threats.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"Many countermeasures have been proposed to mitigate such risks.","offset":1,"pro":0.09090909090909091,"labels":"BAC"},{"idx":8,"sentence":"Current Deepfake detection methods achieve superior performances in dealing with low-visual-quality Deepfake media which can be distinguished by the obvious visual artifacts.","offset":2,"pro":0.18181818181818182,"labels":"BAC"},{"idx":8,"sentence":"However, with the development of deep generative models, the realism of Deepfake media has been significantly improved and becomes tough challenging to current detection models.","offset":3,"pro":0.2727272727272727,"labels":"GAP"},{"idx":8,"sentence":"In this paper, we propose a frame inference-based detection framework (FInfer) to solve the problem of high-visual-quality Deepfake detection.","offset":4,"pro":0.36363636363636365,"labels":"PUR"},{"idx":8,"sentence":"Specifically, we first learn the referenced representations of the current and future frames’ faces.","offset":5,"pro":0.45454545454545453,"labels":"MTD"},{"idx":8,"sentence":"Then, the current frames’ facial representations are utilized to predict the future frames’ facial representations by using an autoregressive model.","offset":6,"pro":0.5454545454545454,"labels":"MTD"},{"idx":8,"sentence":"Finally, a representation-prediction loss is devised to maximize the discriminability of real videos and fake videos.","offset":7,"pro":0.6363636363636364,"labels":"MTD"},{"idx":8,"sentence":"We demonstrate the effectiveness of our FInfer framework through information theory analyses.","offset":8,"pro":0.7272727272727273,"labels":"MTD"},{"idx":8,"sentence":"Extensive experiments demonstrate the performance of our method is promising in terms of in-dataset detection performance, detection efficiency, and cross-dataset detection performance in high-visual-quality Deepfake videos.","offset":9,"pro":0.8181818181818182,"labels":"CLN"},{"idx":8,"sentence":"The entropy and mutual information analyses indicate the correlation between the predicted representations and referenced representations in real videos is higher than that of high-visual-quality Deepfake videos. ","offset":10,"pro":0.9090909090909091,"labels":"RST"},{"idx":9,"sentence":"Dynamic convolution has achieved significant gain in performance and computational complexity, thanks to its powerful representation capability given limited filter number/layers.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"On the one hand, Bi-volution is designed as a dual-branch structure to fully leverage complementary properties of static/dynamic convolution, which endows Bi-volution more robust properties and higher performance.On the other hand, the Spatial Augmented Kernel Generation module is proposed to improve the dynamic convolution, realizing the learning of spatial context information with negligible additional computational complexity.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":9,"sentence":"Extensive experiments illustrate that the ResNet-50 equipped with Bi-volution achieves a highly competitive boost in performance (+2.8% top-1 accuracy on ImageNet classification, +2.4% box AP and +2.2% mask AP on COCO detection and instance segmentation) while maintaining extremely low FLOPs (i.e., ResNet50@2.7 GFLOPs).","offset":2,"pro":0.3333333333333333,"labels":"RST"},{"idx":9,"sentence":"Furthermore, our Bi-volution shows better robustness than dynamic convolution against various noise and input corruptions.","offset":3,"pro":0.5,"labels":"CLN"},{"idx":9,"sentence":"Our code is available at https://github.com/neuralchen/Bivolution.","offset":4,"pro":0.6666666666666666,"labels":"CTN"},{"idx":9,"sentence":"However, SOTA dynamic convolution operators are sensitive to input noises (e.g., Gaussian noise, shot noise, e.t.c.) and lack sufficient spatial contextual information in filter generation.","offset":5,"pro":0.8333333333333334,"labels":"GAP"},{"idx":10,"sentence":"There have been two streams in the 3D detection from point clouds: single-stage methods and two-stage methods.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"In this scenario, the second stage mainly rescores the boxes such that the boxes with better localization get selected.","offset":1,"pro":0.06666666666666667,"labels":"MTD"},{"idx":10,"sentence":"From this observation, we have devised a single-stage anchor-free network that can fulfill these requirements.","offset":2,"pro":0.13333333333333333,"labels":"MTD"},{"idx":10,"sentence":"This network, named AFDetV2, extends the previous work by incorporating a self-calibrated convolution block in the backbone, a keypoint auxiliary supervision, and an IoU prediction branch in the multi-task head.","offset":3,"pro":0.2,"labels":"MTD"},{"idx":10,"sentence":"We take a simple product of the predicted IoU score with the classification heatmap to form the final classification confidence.","offset":4,"pro":0.26666666666666666,"labels":"MTD"},{"idx":10,"sentence":"The enhanced backbone strengthens the box localization capability, and the rescoring approach effectively joins the object presence confidence and the box regression accuracy.","offset":5,"pro":0.3333333333333333,"labels":"RST"},{"idx":10,"sentence":"As a result, the detection accuracy is drastically boosted in the single-stage.","offset":6,"pro":0.4,"labels":"RST"},{"idx":10,"sentence":"To evaluate our approach, we have conducted extensive experiments on the Waymo Open Dataset and the nuScenes Dataset.","offset":7,"pro":0.4666666666666667,"labels":"MTD"},{"idx":10,"sentence":"We have observed that our AFDetV2 achieves the state-of-the-art results on these two datasets, superior to all the prior arts, including both the single-stage and the two-stage 3D detectors.","offset":8,"pro":0.5333333333333333,"labels":"RST"},{"idx":10,"sentence":"AFDetV2 won the 1st place in the Real-Time 3D Detection of the Waymo Open Dataset Challenge 2021.","offset":9,"pro":0.6,"labels":"RST"},{"idx":10,"sentence":"In addition, a variant of our model AFDetV2-Base was entitled the \"Most Efficient Model\" by the Challenge Sponsor, showing a superior computational efficiency.","offset":10,"pro":0.6666666666666666,"labels":"MTD"},{"idx":10,"sentence":"To demonstrate the generality of this single-stage method, we have also applied it to the first stage of the two-stage networks.","offset":11,"pro":0.7333333333333333,"labels":"MTD"},{"idx":10,"sentence":"While the former is more computationally efficient, the latter usually provides better detection accuracy.","offset":12,"pro":0.8,"labels":"BAC"},{"idx":10,"sentence":"By carefully examining the two-stage approaches, we have found that if appropriately designed, the first stage can produce accurate box regression.","offset":13,"pro":0.8666666666666667,"labels":"RST"},{"idx":10,"sentence":"Without exception, the results show that with the strengthened backbone and the rescoring approach, the second stage refinement is no longer needed.","offset":14,"pro":0.9333333333333333,"labels":"RST"},{"idx":11,"sentence":"Clustering is important for domain adaptive person re-identification(re-ID).","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"A majority of unsupervised domain adaptation (UDA) methods conduct clustering on the target domain and then use the generated pseudo labels for adaptive training.","offset":1,"pro":0.07142857142857142,"labels":"BAC"},{"idx":11,"sentence":"Albeit important, the clustering pipeline adopted by current literature is quite standard and lacks consideration for two characteristics of re-ID, i.e., 1) a single person has various feature distribution in multiple cameras.","offset":2,"pro":0.14285714285714285,"labels":"GAP"},{"idx":11,"sentence":"In contrast, the temporal continuity prior is beneficial, because it offers clue for distinguishing some look-alike person (who are temporally far away from each other).","offset":3,"pro":0.21428571428571427,"labels":"MTD"},{"idx":11,"sentence":"Specifically, DARC divides the unlabeled data into multiple camera-specific groups and conducts local clustering within each camera.","offset":4,"pro":0.2857142857142857,"labels":"MTD"},{"idx":11,"sentence":"Afterwards, it regroups those local clusters potentially belonging to the same person into a unity.","offset":5,"pro":0.35714285714285715,"labels":"MTD"},{"idx":11,"sentence":"Through this divide-and-regroup pipeline, DARC avoids directly clustering across multiple cameras and focuses on the feature distribution within each individual camera.","offset":6,"pro":0.42857142857142855,"labels":"MTD"},{"idx":11,"sentence":"Moreover, during the local clustering, DARC uses the temporal continuity prior to distinguish some look-alike person and thus reduces false positive pseudo labels.","offset":7,"pro":0.5,"labels":"MTD"},{"idx":11,"sentence":"Consequentially, DARC effectively reduces clustering errors and improves UDA.","offset":8,"pro":0.5714285714285714,"labels":"CLN"},{"idx":11,"sentence":"Importantly, we show that DARC is compatible to many pseudo label-based UDA methods and brings general improvement.","offset":9,"pro":0.6428571428571429,"labels":"CLN"},{"idx":11,"sentence":"Based on a recent UDA method, DARC advances the state of the art (e.g, 85.1% mAP on MSMT-to-Market and 83.1% mAP on PersonX-to-Market).","offset":10,"pro":0.7142857142857143,"labels":"RST"},{"idx":11,"sentence":"2) a person’s occurrence in the same camera are usually temporally continuous.","offset":11,"pro":0.7857142857142857,"labels":"GAP"},{"idx":11,"sentence":"We argue that the multi-camera distribution hinders clustering because it enlarges the intra-class distances. ","offset":12,"pro":0.8571428571428571,"labels":"PUR"},{"idx":11,"sentence":"These two insight motivate us to propose a novel Divide-And-Regroup Clustering (DARC) pipeline for re-ID UDA.","offset":13,"pro":0.9285714285714286,"labels":"MTD"},{"idx":12,"sentence":"Malicious applications of deepfakes (i.e., technologies generating target facial attributes or entire faces from facial images) have posed a huge threat to individuals' reputation and security.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"To mitigate these threats, recent studies have proposed adversarial watermarks to combat deepfake models, leading them to generate distorted outputs.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":12,"sentence":"Despite achieving impressive results, these adversarial watermarks have low image-level and model-level transferability, meaning that they can protect only one facial image from one specific deepfake model.","offset":2,"pro":0.2,"labels":"GAP"},{"idx":12,"sentence":"To address these issues, we propose a novel solution that can generate a Cross-Model Universal Adversarial Watermark (CMUA-Watermark), protecting a large number of facial images from multiple deepfake models.","offset":3,"pro":0.3,"labels":"PUR"},{"idx":12,"sentence":"Specifically, we begin by proposing a cross-model universal attack pipeline that attacks multiple deepfake models iteratively.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":12,"sentence":"Then, we design a two-level perturbation fusion strategy to alleviate the conflict between the adversarial watermarks generated by different facial images and models.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":12,"sentence":"Moreover, we address the key problem in cross-model optimization with a heuristic approach to automatically find the suitable attack step sizes for different models, further weakening the model-level conflict.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":12,"sentence":"Finally, we introduce a more reasonable and comprehensive evaluation method to fully test the proposed method and compare it with existing ones.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":12,"sentence":"Extensive experimental results demonstrate that the proposed CMUA-Watermark can effectively distort the fake facial images generated by multiple deepfake models while achieving a better performance than existing methods.","offset":8,"pro":0.8,"labels":"CLN"},{"idx":12,"sentence":"Our code is available at https://github.com/VDIGPKU/CMUA-Watermark.","offset":9,"pro":0.9,"labels":"CTN"},{"idx":13,"sentence":"First, we frame the visual grounding pipeline into a causal graph, which shows the causalities among image, query, target location and underlying confounder.","offset":0,"pro":0,"labels":"MTD"},{"idx":13,"sentence":"Through the causal graph, we know how to break the grounding bottleneck: deconfounded visual grounding.","offset":1,"pro":0.125,"labels":"MTD"},{"idx":13,"sentence":"Second, to tackle the challenge that the confounder is unobserved in general, we propose a confounder-agnostic approach called: Referring Expression Deconfounder (RED), to remove the confounding bias.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":13,"sentence":"Third, we implement RED as a simple language attention, which can be applied in any grounding method.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":13,"sentence":"On popular benchmarks, RED improves various state-of-the-art grounding methods by a significant margin.","offset":4,"pro":0.5,"labels":"RST"},{"idx":13,"sentence":"Code is available at: https://github.com/JianqiangH/Deconfounded_VG.","offset":5,"pro":0.625,"labels":"CTN"},{"idx":13,"sentence":"We focus on the confounding bias between language and location in the visual grounding pipeline, where we find that the bias is the major visual reasoning bottleneck.","offset":6,"pro":0.75,"labels":"PUR"},{"idx":13,"sentence":"For example, the grounding process is usually a trivial languagelocation association without visual reasoning, e.g., grounding any language query containing sheep to the nearly central regions, due to that most queries about sheep have ground-truth locations at the image center.","offset":7,"pro":0.875,"labels":"BAC"},{"idx":14,"sentence":"Homogeneous instance segmentation aims to identify each instance in an image where all interested instances belong to the same category, such as plant leaves and microscopic cells.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"Recently, proposal-free methods, which straightforwardly generate instance-aware information to group pixels into different instances, have received increasing attention due to their efficient pipeline.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":14,"sentence":"However, they often fail to distinguish adjacent instances due to similar appearances, dense distribution and ambiguous boundaries of instances in homogeneous images.","offset":2,"pro":0.2,"labels":"GAP"},{"idx":14,"sentence":"In this paper, we propose a pixel-embedded affinity modeling method for homogeneous instance segmentation, which is able to preserve the semantic information of instances and improve the distinguishability of adjacent instances.","offset":3,"pro":0.3,"labels":"PUR"},{"idx":14,"sentence":"Instead of predicting affinity directly, we propose a self-correlation module to explicitly model the pairwise relationships between pixels, by estimating the similarity between embeddings generated from the input image through CNNs.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":14,"sentence":"Based on the self-correlation module, we further design a cross-correlation module to maintain the semantic consistency between instances.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":14,"sentence":"Specifically, we map the transformed input images with different views and appearances into the same embedding space, and then mutually estimate the pairwise relationships of embeddings generated from the original input and its transformed variants.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":14,"sentence":"In addition, to integrate the global instance information, we introduce an embedding pyramid module to model affinity on different scales.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":14,"sentence":"Extensive experiments demonstrate the versatile and superior performance of our method on three representative datasets.","offset":8,"pro":0.8,"labels":"CLN"},{"idx":14,"sentence":"Code and models are available at https://github.com/weih527/Pixel-Embedded-Affinity.","offset":9,"pro":0.9,"labels":"CTN"},{"idx":15,"sentence":"Spatial and channel attentions, modelling the semantic interdependencies in spatial and channel dimensions respectively, have recently been widely used for semantic segmentation.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"However, computing spatial and channel attentions separately sometimes causes errors, especially for those difficult cases.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":15,"sentence":"In this paper, we propose Channelized Axial Attention (CAA) to seamlessly integrate channel attention and spatial attention into a single operation with negligible computation overhead.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":15,"sentence":"Specifically, we break down the dot-product operation of the spatial attention into two parts and insert channel relation in between, allowing for independently optimized channel attention on each spatial location.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":15,"sentence":"We further develop grouped vectorization, which allows our model to run with very little memory consumption without slowing down the running speed.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":15,"sentence":"Comparative experiments conducted on multiple benchmark datasets, including Cityscapes, PASCAL Context, and COCO-Stuff, demonstrate that our CAA outperforms many state-of-the-art segmentation models (including dual attention) on all tested datasets.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":16,"sentence":"This paper proposes a novel approach to object detection on drone imagery, namely Multi-Proxy Detection Network with Unified Foreground Packing (UFPMP-Det).","offset":0,"pro":0,"labels":"PUR"},{"idx":16,"sentence":"By such means, UFPMP-Det largely promotes both the detection accuracy and efficiency.","offset":1,"pro":0.16666666666666666,"labels":"RST"},{"idx":16,"sentence":"Extensive experiments are carried out on the widely used VisDrone and UAVDT datasets, and UFPMP-Det reports new state-of-the-art scores at a much higher speed, highlighting its advantages.","offset":2,"pro":0.3333333333333333,"labels":"RST"},{"idx":16,"sentence":"The code is available at https://github.com/PuAnysh/UFPMP-Det.","offset":3,"pro":0.5,"labels":"CTN"},{"idx":16,"sentence":"To deal with the numerous instances of very small scales, different from the common solution that divides the high-resolution input image into quite a number of chips with low foreground ratios to perform detection on them each, the Unified Foreground Packing (UFP) module is designed, where the sub-regions given by a coarse detector are initially merged through clustering to suppress background and the resulting ones are subsequently packed into a mosaic for a single inference, thus significantly reducing overall time cost.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":16,"sentence":"Furthermore, to address the more serious confusion between inter-class similarities and intra-class variations of instances, which deteriorates detection performance but is rarely discussed, the Multi-Proxy Detection Network (MP-Det) is presented to model object distributions in a fine-grained manner by employing multiple proxy learning, and the proxies are enforced to be diverse by minimizing a Bag-of-Instance-Words (BoIW) guided optimal transport loss.","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":17,"sentence":"In this work, we propose a novel modality-adaptive mixup and invariant decomposition (MID) approach for RGB-infrared person re-identification towards learning modality-invariant and discriminative representations.","offset":0,"pro":0,"labels":"PUR"},{"idx":17,"sentence":"MID designs a modality-adaptive mixup scheme to generate suitable mixed modality images between RGB and infrared images for mitigating the inherent modality discrepancy at the pixel-level.","offset":1,"pro":0.14285714285714285,"labels":"MTD"},{"idx":17,"sentence":"It formulates modality mixup procedure as Markov decision process, where an actor-critic agent learns dynamical and local linear interpolation policy between different regions of cross-modality images under a deep reinforcement learning framework.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":17,"sentence":"Such policy guarantees modality-invariance in a more continuous latent space and avoids manifold intrusion by the corrupted mixed modality samples.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":17,"sentence":"Moreover, to further counter modality discrepancy and enforce invariant visual semantics at the feature-level, MID employs modality-adaptive convolution decomposition to disassemble a regular convolution layer into modality-specific basis layers and a modality-shared coefficient layer.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":17,"sentence":"Extensive experimental results on two challenging benchmarks demonstrate superior performance of MID over state-of-the-art methods.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":17,"sentence":"RGB-infrared person re-identification is an emerging cross-modality re-identification task, which is very challenging due to significant modality discrepancy between RGB and infrared images.","offset":6,"pro":0.8571428571428571,"labels":"BAC"},{"idx":18,"sentence":"Multimodal sensors (visual, non-visual, and wearable) can provide complementary information to develop robust perception systems for recognizing activities accurately.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"However, it is challenging to extract robust multimodal representations due to the heterogeneous characteristics of data from multimodal sensors and disparate human activities, especially in the presence of noisy and misaligned sensor data.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":18,"sentence":"In this work, we propose a cooperative multitask learning-based guided multimodal fusion approach, MuMu, to extract robust multimodal representations for human activity recognition (HAR).","offset":2,"pro":0.2222222222222222,"labels":"PUR"},{"idx":18,"sentence":"MuMu employs an auxiliary task learning approach to extract features specific to each set of activities with shared characteristics (activity-group).","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":18,"sentence":"MuMu then utilizes activity-group-specific features to direct our proposed Guided Multimodal Fusion Approach (GM-Fusion) for extracting complementary multimodal representations, designed as the target task.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":18,"sentence":"We evaluated MuMu by comparing its performance to state-of-the-art multimodal HAR approaches on three activity datasets.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":18,"sentence":"Our extensive experimental results suggest that MuMu outperforms all the evaluated approaches across all three datasets.","offset":6,"pro":0.6666666666666666,"labels":"RST"},{"idx":18,"sentence":"Additionally, the ablation study suggests that MuMu significantly outperforms the baseline models (p<0.05), which do not use our guided multimodal fusion.","offset":7,"pro":0.7777777777777778,"labels":"CLN"},{"idx":18,"sentence":"Finally, the robust performance of MuMu on noisy and misaligned sensor data posits that our approach is suitable for HAR in real-world settings.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":19,"sentence":"While recent studies try to detect featuremap units that cause artifacts and evaluate individual samples, these approaches require additional resources such as external networks or a number of training data to approximate the real data manifold.","offset":0,"pro":0,"labels":"GAP"},{"idx":19,"sentence":"Finally, we discuss a geometrical analysis to partially reveal the relation between the proposed concept and low visual fidelity.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":19,"sentence":"Despite significant improvements on the image generation performance of Generative Adversarial Networks (GANs), generations with low visual fidelity still have been observed.","offset":2,"pro":0.4,"labels":"BAC"},{"idx":19,"sentence":"As widely used metrics for GANs focus more on the overall performance of the model, evaluation on the quality of individual generations or detection of defective generations is challenging.","offset":3,"pro":0.6,"labels":"BAC"},{"idx":19,"sentence":"In this work, we propose the concept of local activation, and devise a metric on the local activation to detect artifact generations without additional supervision.","offset":4,"pro":0.8,"labels":"PUR"},{"idx":0,"sentence":"Vulnerability to adversarial attacks is a well-known weakness of Deep Neural networks.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"While most of the studies focus on single-task neural networks with computer vision datasets, very little research has considered complex multi-task models that are common in real applications.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":0,"sentence":"In this paper, we evaluate the design choices that impact the robustness of multi-task deep learning networks.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":0,"sentence":"We provide evidence that blindly adding auxiliary tasks, or weighing the tasks provides a false sense of robustness.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":0,"sentence":"Thereby, we tone down the claim made by previous research and study the different factors which may affect robustness.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":0,"sentence":"In particular, we show that the choice of the task to incorporate in the loss function are important factors that can be leveraged to yield more robust models.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":0,"sentence":"We provide the appendix, all our algorithms, models, and open source-code at https://github.com/yamizi/taskaugment","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":1,"sentence":"We present a novel non-iterative learnable method for partial-to-partial 3D shape registration.The partial alignment task is extremely complex, as it jointly tries to match between points, and identify which points do not appear in the corresponding shape, causing the solution to be non-unique and ill-posed in most cases.","offset":0,"pro":0,"labels":"PUR"},{"idx":1,"sentence":"Until now, two main methodologies have been suggested to solve this problem: sample a subset of points that are likely to have correspondences, or perform soft alignment between the point clouds and try to avoid a match to an occluded part.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":1,"sentence":"These heuristics work when the partiality is mild or when the transformation is small but fails for severe occlusions, or when outliers are present.","offset":2,"pro":0.3333333333333333,"labels":"BAC"},{"idx":1,"sentence":"We present a unique approach named Confidence Guided Distance Network (CGD-net), where we fuse learnable similarity between point embeddings and spatial distance between point clouds, inducing an optimized solution for the overlapping points while ignoring parts that only appear in one of the shapes","offset":3,"pro":0.5,"labels":"MTD"},{"idx":1,"sentence":"The point feature generation is done by a self-supervised architecture that repels far points to have different embeddings, therefore succeeds to align partial views of shapes, even with excessive internal symmetries, or acute rotations","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":1,"sentence":"We compare our network to recently presented learning-based and axiomatic methods and report a fundamental boost in performance.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":2,"sentence":"In recent years, we have seen significant steps taken in the development of self-driving cars.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"Multiple companies are starting to roll out impressive systems that work in a variety of settings.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":2,"sentence":"These systems can sometimes give the impression that full self-driving is just around the corner and that we would soon build cars without even a steering wheel.","offset":2,"pro":0.2222222222222222,"labels":"BAC"},{"idx":2,"sentence":"The increase in the level of autonomy and control given to an AI provides an opportunity for new modes of human-vehicle interaction.","offset":3,"pro":0.3333333333333333,"labels":"BAC"},{"idx":2,"sentence":"However, surveys have shown that giving more control to an AI in self-driving cars is accompanied by a degree of uneasiness by passengers.","offset":4,"pro":0.4444444444444444,"labels":"GAP"},{"idx":2,"sentence":"In an attempt to alleviate this issue, recent works have taken a natural language-oriented approach by allowing the passenger to give commands that refer to specific objects in the visual scene.","offset":5,"pro":0.5555555555555556,"labels":"GAP"},{"idx":2,"sentence":"Nevertheless, this is only half the task as the car should also understand the physical destination of the command, which is what we focus on in this paper.","offset":6,"pro":0.6666666666666666,"labels":"PUR"},{"idx":2,"sentence":"We propose an extension in which we annotate the 3D destination that the car needs to reach after executing the given command and evaluate multiple different baselines on predicting this destination location.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":2,"sentence":"Additionally, we introduce a model that outperforms the prior works adapted for this particular setting.","offset":8,"pro":0.8888888888888888,"labels":"MTD"},{"idx":3,"sentence":"Previous deep learning-based line segment detection (LSD) suffers from the immense model size and high computational cost for line prediction.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"This constrains them from real-time inference on computationally restricted environments.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":3,"sentence":"In this paper, we propose a real-time and light-weight line segment detector for resource-constrained environments named Mobile LSD (M-LSD).","offset":2,"pro":0.2,"labels":"PUR"},{"idx":3,"sentence":"We design an extremely efficient LSD architecture by minimizing the backbone network and removing the typical multi-module process for line prediction found in previous methods.","offset":3,"pro":0.3,"labels":"MTD"},{"idx":3,"sentence":"To maintain competitive performance with a light-weight network, we present novel training schemes: Segments of Line segment (SoL) augmentation, matching and geometric loss.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":3,"sentence":"SoL augmentation splits a line segment into multiple subparts, which are used to provide auxiliary line data during the training process.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":3,"sentence":"Moreover, the matching and geometric loss allow a model to capture additional geometric cues.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":3,"sentence":"Compared with TP-LSD-Lite, previously the best real-time LSD method, our model (M-LSD-tiny) achieves competitive performance with 2.5% of model size and an increase of 130.5% in inference speed on GPU.","offset":7,"pro":0.7,"labels":"RST"},{"idx":3,"sentence":"Furthermore, our model runs at 56.8 FPS and 48.6 FPS on the latest Android and iPhone mobile devices, respectively.","offset":8,"pro":0.8,"labels":"RST"},{"idx":3,"sentence":"To the best of our knowledge, this is the first real-time deep LSD available on mobile devices.","offset":9,"pro":0.9,"labels":"IMP"},{"idx":4,"sentence":"With the rapid development of facial forgery techniques, forgery detection has attracted more and more attention due to security concerns.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"Existing approaches attempt to use frequency information to mine subtle artifacts under high-quality forged faces.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":4,"sentence":"However, the exploitation of frequency information is coarse-grained, and more importantly, their vanilla learning process struggles to extract fine-grained forgery traces.","offset":2,"pro":0.2,"labels":"GAP"},{"idx":4,"sentence":"To address this issue, we propose a progressive enhancement learning framework to exploit both the RGB and fine-grained frequency clues.","offset":3,"pro":0.3,"labels":"PUR"},{"idx":4,"sentence":"Specifically, we perform a fine-grained decomposition of RGB images to completely decouple the real and fake traces in the frequency space.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":4,"sentence":"Subsequently, we propose a progressive enhancement learning framework based on a two-branch network, combined with self-enhancement and mutual-enhancement modules.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":4,"sentence":"The self-enhancement module captures the traces in different input spaces based on spatial noise enhancement and channel attention.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":4,"sentence":"The Mutual-enhancement module concurrently enhances RGB and frequency features by communicating in the shared spatial dimension.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":4,"sentence":"The progressive enhancement process facilitates the learning of discriminative features with fine-grained face forgery clues.","offset":8,"pro":0.8,"labels":"MTD"},{"idx":4,"sentence":"Extensive experiments on several datasets show that our method outperforms the state-of-the-art face forgery detection methods.","offset":9,"pro":0.9,"labels":"RST"},{"idx":5,"sentence":"The rapid development of facial manipulation techniques has aroused public concerns in recent years.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"Existing deepfake video detection approaches attempt to capture the discrim- inative features between real and fake faces based on tem- poral modelling.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":5,"sentence":"However, these works impose supervisions on sparsely sampled video frames but overlook the local mo- tions among adjacent frames, which instead encode rich in- consistency information that can serve as an efficient indica- tor for DeepFake video detection.","offset":2,"pro":0.2,"labels":"GAP"},{"idx":5,"sentence":"To mitigate this issue, we delves into the local motion and propose a novel sampling unit named snippet which contains a few successive videos frames for local temporal inconsistency learning.","offset":3,"pro":0.3,"labels":"PUR"},{"idx":5,"sentence":"Moreover, we elaborately design an Intra-Snippet Inconsistency Module (Intra-SIM) and an Inter-Snippet Interaction Module (Inter- SIM) to establish a dynamic inconsistency modelling frame- work.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":5,"sentence":"Specifically, the Intra-SIM applies bi-directional tem- poral difference operations and a learnable convolution ker- nel to mine the short-term motions within each snippet.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":5,"sentence":"The Inter-SIM is then devised to promote the cross-snippet infor- mation interaction to form global representations.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":5,"sentence":"The Intra- SIM and Inter-SIM work in an alternate manner and can be plugged into existing 2D CNNs.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":5,"sentence":"Our method outperforms the state of the art competitors on four popular benchmark dataset, i.e., FaceForensics++, Celeb-DF, DFDC and Wild- Deepfake.","offset":8,"pro":0.8,"labels":"RST"},{"idx":5,"sentence":"Besides, extensive experiments and visualizations are also presented to further illustrate its effectiveness.","offset":9,"pro":0.9,"labels":"RST"},{"idx":6,"sentence":"Assessing the performance of Generative Adversarial Networks (GANs) has been an important topic due to its practical significance.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"Although several evaluation metrics have been proposed, they generally assess the quality of the whole generated image distribution.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":6,"sentence":"For Reference-guided Image Synthesis (RIS) tasks, i.e., rendering a source image in the style of another reference image, where assessing the quality of a single generated image is crucial, these metrics are not applicable.","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":6,"sentence":"In this paper, we propose a general learning-based framework, Reference-guided Image Synthesis Assessment (RISA) to quantitatively evaluate the quality of a single generated image.","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":6,"sentence":"Notably, the training of RISA does not require human annotations.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":6,"sentence":"In specific, the training data for RISA are acquired by the intermediate models from the training procedure in RIS, and weakly annotated by the number of models' iterations, based on the positive correlation between image quality and iterations.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":6,"sentence":"As this annotation is too coarse as a supervision signal, we introduce two techniques: 1) a pixel-wise interpolation scheme to refine the coarse labels, and 2) multiple binary classifiers to replace a naïve regressor.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":6,"sentence":"In addition, an unsupervised contrastive loss is introduced to effectively capture the style similarity between a generated image and its reference image.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":6,"sentence":"Empirical results on various datasets demonstrate that RISA is highly consistent with human preference and transfers well across models.","offset":8,"pro":0.8888888888888888,"labels":"RST"},{"idx":7,"sentence":"In recent years, self-supervised representation learning for skeleton-based action recognition has been developed with the advance of contrastive learning methods.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"The existing contrastive learning methods use normal augmentations to construct similar positive samples, which limits the ability to explore novel movement patterns.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":7,"sentence":"In this paper, to make better use of the movement patterns introduced by extreme augmentations, a Contrastive Learning framework utilizing Abundant Information Mining for self-supervised action Representation (AimCLR) is proposed.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":7,"sentence":"First, the extreme augmentations and the Energy-based Attention-guided Drop Module (EADM) are proposed to obtain diverse positive samples, which bring novel movement patterns to improve the universality of the learned representations.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":7,"sentence":"Second, since directly using extreme augmentations may not be able to boost the performance due to the drastic changes in original identity, the Dual Distributional Divergence Minimization Loss (D3M Loss) is proposed to minimize the distribution divergence in a more gentle way.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":7,"sentence":"Third, the Nearest Neighbors Mining (NNM) is proposed to further expand positive samples to make the abundant information mining process more reasonable.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":7,"sentence":"Exhaustive experiments on NTU RGB+D 60, PKU-MMD, NTU RGB+D 120 datasets have verified that our AimCLR can significantly perform favorably against state-of-the-art methods under a variety of evaluation protocols with observed higher quality action representations.","offset":6,"pro":0.75,"labels":"RST"},{"idx":7,"sentence":"Our code is available at https://github.com/Levigty/AimCLR.","offset":7,"pro":0.875,"labels":"CTN"},{"idx":8,"sentence":"Modern Convolutional Neural Network (CNN) architectures, despite their superiority in solving various problems, are generally too large to be deployed on resource constrained edge devices.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"In this paper, we reduce memory usage and floating-point operations required by convolutional layers in CNNs.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":8,"sentence":"We compress these layers by generalizing the Kronecker Product Decomposition to apply to multidimensional tensors, leading to the Generalized Kronecker Product Decomposition (GKPD).","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":8,"sentence":"Our approach yields a plug-and-play module that can be used as a drop-in replacement for any convolutional layer.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":8,"sentence":"Experimental results for image classification on CIFAR-10 and ImageNet datasets using ResNet, MobileNetv2 and SeNet architectures substantiate the effectiveness of our proposed approach.","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":8,"sentence":"We find that GKPD outperforms state-of-the-art decomposition methods including Tensor-Train and Tensor-Ring as well as other relevant compression methods such as pruning and knowledge distillation.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":9,"sentence":"Few-shot object detection (FSOD) aims to detect objects using only a few examples.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"How to adapt state-of-the-art object detectors to the few-shot domain remains challenging.","offset":1,"pro":0.1,"labels":"GAP"},{"idx":9,"sentence":"Object proposal is a key ingredient in modern object detectors.","offset":2,"pro":0.2,"labels":"BAC"},{"idx":9,"sentence":"However, the quality of proposals generated for few-shot classes using existing methods is far worse than that of many-shot classes, e.g., missing boxes for few-shot classes due to misclassification or inaccurate spatial locations with respect to true objects.","offset":3,"pro":0.3,"labels":"GAP"},{"idx":9,"sentence":"To address the noisy proposal problem, we propose a novel meta-learning based FSOD model by jointly optimizing the few-shot proposal generation and fine-grained few-shot proposal classification.","offset":4,"pro":0.4,"labels":"PUR"},{"idx":9,"sentence":"To improve proposal generation for few-shot classes, we propose to learn a lightweight metric-learning based prototype matching network, instead of the conventional simple linear object/nonobject classifier, e.g., used in RPN.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":9,"sentence":"Our non-linear classifier with the feature fusion network could improve the discriminative prototype matching and the proposal recall for few-shot classes.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":9,"sentence":"To improve the fine-grained few-shot proposal classification, we propose a novel attentive feature alignment method to address the spatial misalignment between the noisy proposals and few-shot classes, thus improving the performance of few-shot object detection.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":9,"sentence":"Meanwhile we learn a separate Faster R-CNN detection head for many-shot base classes and show strong performance of maintaining base-classes knowledge.","offset":8,"pro":0.8,"labels":"RST"},{"idx":9,"sentence":"Our model achieves state-of-the-art performance on multiple FSOD benchmarks over most of the shots and metrics.","offset":9,"pro":0.9,"labels":"RST"},{"idx":10,"sentence":"Clustering-based unsupervised domain adaptive (UDA) person re-identification (ReID) reduces exhaustive annotations.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"However, owing to unsatisfactory feature embedding and imperfect clustering, pseudo labels for target domain data inherently contain an unknown proportion of wrong ones, which would mislead feature learning.","offset":1,"pro":0.1,"labels":"GAP"},{"idx":10,"sentence":"In this paper, we propose an approach named probabilistic uncertainty guided progressive label refinery (P2LR) for domain adaptive person re-identification.","offset":2,"pro":0.2,"labels":"PUR"},{"idx":10,"sentence":"First, we propose to model the labeling uncertainty with the probabilistic distance along with ideal single-peak distributions.","offset":3,"pro":0.3,"labels":"MTD"},{"idx":10,"sentence":"A quantitative criterion is established to measure the uncertainty of pseudo labels and facilitate the network training.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":10,"sentence":"Second, we explore a progressive strategy for refining pseudo labels.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":10,"sentence":"With the uncertainty-guided alternative optimization, we balance between the exploration of target domain data and the negative effects of noisy labeling.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":10,"sentence":"On top of a strong baseline, we obtain significant improvements and achieve the state-of-the-art performance on four UDA ReID benchmarks.","offset":7,"pro":0.7,"labels":"RST"},{"idx":10,"sentence":"Specifically, our method outperforms the baseline by 6.5% mAP on the Duke2Market task, while surpassing the state-of-the-art method by 2.5% mAP on the Market2MSMT task.","offset":8,"pro":0.8,"labels":"RST"},{"idx":10,"sentence":"Code is available at: https://github.com/JeyesHan/P2LR.","offset":9,"pro":0.9,"labels":"CTN"},{"idx":11,"sentence":"We present Laneformer, a conceptually simple yet powerful transformer-based architecture tailored for lane detection that is a long-standing research topic for visual perception in autonomous driving.","offset":0,"pro":0,"labels":"PUR"},{"idx":11,"sentence":"The dominant paradigms rely on purely CNN-based architectures which often fail in incorporating relations of long-range lane points and global contexts induced by surrounding objects (e.g., pedestrians, vehicles).","offset":1,"pro":0.125,"labels":"GAP"},{"idx":11,"sentence":"Inspired by recent advances of the transformer encoder-decoder architecture in various vision tasks, we move forwards to design a new end-to-end Laneformer architecture that revolutionizes the conventional transformers into better capturing the shape and semantic characteristics of lanes, with minimal overhead in latency.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":11,"sentence":"First, coupling with deformable pixel-wise self-attention in the encoder, Laneformer presents two new row and column self-attention operations to efficiently mine point context along with the lane shapes.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":11,"sentence":"Second, motivated by the appearing objects would affect the decision of predicting lane segments, Laneformer further includes the detected object instances as extra inputs of multi-head attention blocks in the encoder and decoder to facilitate the lane point detection by sensing semantic contexts.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":11,"sentence":"Specifically, the bounding box locations of objects are added into Key module to provide interaction with each pixel and query while the ROI-aligned features are inserted into Value module.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":11,"sentence":"Extensive experiments demonstrate our Laneformer achieves state-of-the-art performances on CULane benchmark, in terms of 77.1% F1 score.","offset":6,"pro":0.75,"labels":"RST"},{"idx":11,"sentence":"We hope our simple and effective Laneformer will serve as a strong baseline for future research in self-attention models for lane detection.","offset":7,"pro":0.875,"labels":"IMP"},{"idx":12,"sentence":"Although considerable progress has been achieved regarding the transformers in recent years, the large number of parameters, quadratic computational complexity, and memory cost conditioned on long sequences make the transformers hard to train and implement, especially in edge computing configurations.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"In this case, a dizzying number of works have sought to make improvements around computational and memory efficiency upon the original transformer architecture.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":12,"sentence":"Nevertheless, many of them restrict the context in the attention to seek a trade-off between cost and performance with prior knowledge of orderly stored data.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":12,"sentence":"It is imperative to dig deep into an efficient feature extractor for point clouds due to their irregularity and a large number of points.","offset":3,"pro":0.375,"labels":"GAP"},{"idx":12,"sentence":"In this paper, we propose a novel skeleton decomposition-based self-attention (SD-SA) which has no sequence length limit and exhibits favorable scalability in long-sequence models.","offset":4,"pro":0.5,"labels":"PUR"},{"idx":12,"sentence":"Due to the numerical low-rank nature of self-attention, we approximate it by the skeleton decomposition method while maintaining its effectiveness.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":12,"sentence":"At this point, we have shown that the proposed method works for the proposed approach on point cloud classification, segmentation, and detection tasks on the ModelNet40, ShapeNet, and KITTI datasets, respectively.","offset":6,"pro":0.75,"labels":"RST"},{"idx":12,"sentence":"Our approach significantly improves the efficiency of the point cloud transformer and exceeds other efficient transformers on point cloud tasks in terms of the speed at comparable performance.","offset":7,"pro":0.875,"labels":"RST"},{"idx":13,"sentence":"In this paper, we investigate the generalization problem of person re-identification (re-id), whose major challenge is the distribution shift on an unseen domain.","offset":0,"pro":0,"labels":"PUR"},{"idx":13,"sentence":"As an important tool of regularizing the distribution, batch normalization (BN) has been widely used in existing methods.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":13,"sentence":"However, they neglect that BN is severely biased to the training domain and inevitably suffers the performance drop if directly generalized without being updated.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":13,"sentence":"To tackle this issue, we propose Batch Norm Test-time Adaption (BNTA), a novel re-id framework that applies the self-supervised strategy to update BN parameters adaptively.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":13,"sentence":"Specifically, BNTA quickly explores the domain-aware information within unlabeled target data before inference, and accordingly modulates the feature distribution normalized by BN to adapt to the target domain.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":13,"sentence":"This is accomplished by two designed self-supervised auxiliary tasks, namely part positioning and part nearest neighbor matching, which help the model mine the domain-aware information with respect to the structure and identity of body parts, respectively.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":13,"sentence":"To demonstrate the effectiveness of our method, we conduct extensive experiments on three re-id datasets and confirm the superior performance to the state-of-the-art methods.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":14,"sentence":"Convolutional Neural Networks (CNNs) perform very well in image classification and object detection in recent years, but even the most advanced models have limited rotation invariance.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"Known solutions include the enhancement of training data and the increase of rotation invariance by globally merging the rotation equivariant features.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":14,"sentence":"These methods either increase the workload of training or increase the number of model parameters.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":14,"sentence":"To address this problem, this paper proposes a module that can be inserted into the existing networks, and directly incorporates the rotation invariance into the feature extraction layers of the CNNs.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":14,"sentence":"This module does not have learnable parameters and will not increase the complexity of the model.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":14,"sentence":"At the same time, only by training the upright data, it can perform well on the rotated testing set.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":14,"sentence":"These ad-vantages will be suitable for fields such as biomedicine and astronomy where it is difficult to obtain upright samples or the target has no directionality.","offset":6,"pro":0.75,"labels":"IMP"},{"idx":14,"sentence":"Evaluate our module with LeNet-5, ResNet-18 and tiny-yolov3, we get impressive results.","offset":7,"pro":0.875,"labels":"RST"},{"idx":15,"sentence":"Video object detection has been an important yet challenging topic in computer vision.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"Traditional methods mainly focus on designing the image-level or box-level feature propagation strategies to exploit temporal information.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":15,"sentence":"This paper argues that with a more effective and efficient feature propagation framework, video object detectors can gain improvement in terms of both accuracy and speed.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":15,"sentence":"For this purpose, this paper studies object-level feature propagation, and proposes an object query propagation (QueryProp) framework for high-performance video object detection.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":15,"sentence":"The proposed QueryProp contains two propagation strategies: 1) query propagation is performed from sparse key frames to dense non-key frames to reduce the redundant computation on non-key frames; 2) query propagation is performed from previous key frames to the current key frame to improve feature representation by temporal context modeling.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":15,"sentence":"To further facilitate query propagation, an adaptive propagation gate is designed to achieve flexible key frame selection.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":15,"sentence":"We conduct extensive experiments on the ImageNet VID dataset.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":15,"sentence":"QueryProp achieves comparable accuracy with state-of-the-art methods and strikes a decent accuracy/speed trade-off.","offset":7,"pro":0.875,"labels":"RST"},{"idx":16,"sentence":"Unconstrained lip-to-speech aims to generate corresponding speeches based on silent facial videos with no restriction to head pose or vocabulary.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"It is desirable to generate intelligible and natural speech with a fast speed in unconstrained settings.Currently, to handle the more complicated scenarios, most existing methods adopt the autoregressive architecture, which is optimized with the MSE loss.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":16,"sentence":"Although these methods have achieved promising performance, they are prone to bring issues including high inference latency and mel-spectrogram over-smoothness.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":16,"sentence":"To tackle these problems, we propose a novel flow-based non-autoregressive lip-to-speech model (GlowLTS) to break autoregressive constraints and achieve faster inference.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":16,"sentence":"Concretely, we adopt a flow-based decoder which is optimized by maximizing the likelihood of the training data and is capable of more natural and fast speech generation.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":16,"sentence":"Moreover, we devise a condition module to improve the intelligibility of generated speech.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":16,"sentence":"We demonstrate the superiority of our proposed method through objective and subjective evaluation on Lip2Wav-Chemistry-Lectures and Lip2Wav-Chess-Analysis datasets.","offset":6,"pro":0.75,"labels":"CLN"},{"idx":16,"sentence":"Our demo video can be found at https://glowlts.github.io/.","offset":7,"pro":0.875,"labels":"CTN"},{"idx":17,"sentence":"Fine-grained visual classification (FGVC) which aims at recognizing objects from subcategories is a very challenging task due to the inherently subtle inter-class differences.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"Most existing works mainly tackle this problem by reusing the backbone network to extract features of detected discriminative regions.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":17,"sentence":"However, this strategy inevitably complicates the pipeline and pushes the proposed regions to contain most parts of the objects thus fails to locate the really important parts.","offset":2,"pro":0.2,"labels":"GAP"},{"idx":17,"sentence":"Recently, vision transformer (ViT) shows its strong performance in the traditional classification task.","offset":3,"pro":0.3,"labels":"BAC"},{"idx":17,"sentence":"The self-attention mechanism of the transformer links every patch token to the classification token.","offset":4,"pro":0.4,"labels":"BAC"},{"idx":17,"sentence":"In this work, we first evaluate the effectiveness of the ViT framework in the fine-grained recognition setting.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":17,"sentence":"Then motivated by the strength of the attention link can be intuitively considered as an indicator of the importance of tokens, we further propose a novel Part Selection Module that can be applied to most of the transformer architectures where we integrate all raw attention weights of the transformer into an attention map for guiding the network to effectively and accurately select discriminative image patches and compute their relations.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":17,"sentence":"A contrastive loss is applied to enlarge the distance between feature representations of confusing classes.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":17,"sentence":"We name the augmented transformer-based model TransFG and demonstrate the value of it by conducting experiments on five popular fine-grained benchmarks where we achieve state-of-the-art performance.","offset":8,"pro":0.8,"labels":"CLN"},{"idx":17,"sentence":"Qualitative results are presented for better understanding of our model.","offset":9,"pro":0.9,"labels":"CTN"},{"idx":18,"sentence":"In this paper, we present a new self-supervised scene flow estimation approach for a pair of consecutive point clouds.","offset":0,"pro":0,"labels":"PUR"},{"idx":18,"sentence":"The key idea of our approach is to represent discrete point clouds as continuous probability density functions using Gaussian mixture models.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":18,"sentence":"Scene flow estimation is therefore converted into the problem of recovering motion from the alignment of probability density functions, which we achieve using a closed-form expression of the classic Cauchy-Schwarz divergence.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":18,"sentence":"Unlike existing nearest-neighbor-based approaches that use hard pairwise correspondences, our proposed approach establishes soft and implicit point correspondences between point clouds and generates more robust and accurate scene flow in the presence of missing correspondences and outliers.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":18,"sentence":"Comprehensive experiments show that our method makes noticeable gains over the Chamfer Distance and the Earth Mover’s Distance in real-world environments and achieves state-of-the-art performance among self-supervised learning methods on FlyingThings3D and KITTI, even outperforming some supervised methods with ground truth annotations.","offset":4,"pro":0.8,"labels":"RST"},{"idx":19,"sentence":"Accurate 3D object detection from point clouds has become a crucial component in autonomous driving.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"However, the volumetric representations and the projection methods in previous works fail to establish the relationships between the local point sets.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":19,"sentence":"In this paper, we propose Sparse Voxel-Graph Attention Network (SVGA-Net), a novel end-to-end trainable network which mainly contains voxel-graph module and sparse-to-dense regression module to achieve comparable 3D detection tasks from raw LIDAR data.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":19,"sentence":"Specifically, SVGA-Net constructs the local complete graph within each divided 3D spherical voxel and global KNN graph through all voxels.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":19,"sentence":"The local and global graphs serve as the attention mechanism to enhance the extracted features.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":19,"sentence":"In addition, the novel sparse-to-dense regression module enhances the 3D box estimation accuracy through feature maps aggregation at different levels.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":19,"sentence":"Experiments on KITTI detection benchmark and Waymo Open dataset demonstrate the efficiency of extending the graph representation to 3D object detection and the proposed SVGA-Net can achieve decent detection accuracy.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":0,"sentence":"RGB-NIR fusion is a promising method for low-light imaging.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"However, high-intensity noise in low-light images amplifies the effect of structure inconsistency between RGB-NIR images, which fails existing algorithms.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":0,"sentence":"To handle this, we propose a new RGB-NIR fusion algorithm called Dark Vision Net (DVN) with two technical novelties: Deep Structure and Deep Inconsistency Prior (DIP).","offset":2,"pro":0.25,"labels":"PUR"},{"idx":0,"sentence":"The Deep Structure extracts clear structure details in deep multiscale feature space rather than raw input space, which is more robust to noisy inputs.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":0,"sentence":"Based on the deep structures from both RGB and NIR domains, we introduce the DIP to leverage the structure inconsistency to guide the fusion of RGB-NIR.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":0,"sentence":"Benefits from this, the proposed DVN obtains high-quality low-light images without the visual artifacts.","offset":5,"pro":0.625,"labels":"RST"},{"idx":0,"sentence":"We also propose a new dataset called Dark Vision Dataset (DVD), consisting of aligned RGB-NIR image pairs, as the first public RGB-NIR fusion benchmark.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":0,"sentence":"Quantitative and qualitative results on the proposed benchmark show that DVN significantly outperforms other comparison algorithms in PSNR and SSIM, especially in extremely low light conditions.","offset":7,"pro":0.875,"labels":"RST"},{"idx":1,"sentence":"Pansharpening is a critical yet challenging low-level vision task that aims to obtain a higher-resolution image by fusing a multispectral (MS) image and a panchromatic (PAN) image.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"While most pansharpening methods are based on convolutional neural network (CNN) architectures with standard convolution operations, few attempts have been made with context-adaptive/dynamic convolution, which delivers impressive results on high-level vision tasks.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":1,"sentence":"In this paper, we propose a novel strategy to generate local-context adaptive (LCA) convolution kernels and introduce a new global harmonic (GH) bias mechanism, exploiting image local specificity as well as integrating global information, dubbed LAGConv.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":1,"sentence":"The proposed LAGConv can replace the standard convolution that is context-agnostic to fully perceive the particularity of each pixel for the task of remote sensing pansharpening.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":1,"sentence":"Furthermore, by applying the LAGConv, we provide an image fusion network architecture, which is more effective than conventional CNN-based pansharpening approaches.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":1,"sentence":"The superiority of the proposed method is demonstrated by extensive experiments implemented on a wide range of datasets compared with state-of-the-art pansharpening methods.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":1,"sentence":"Besides, more discussions testify that the proposed LAGConv outperforms recent adaptive convolution techniques for pansharpening.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":2,"sentence":"Reasoning is a dynamic process.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"In cognitive theories, the dynamics of reasoning refers to reasoning states over time after successive state transitions.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":2,"sentence":"Modeling the cognitive dynamics is of utmost importance to simulate human reasoning capability.","offset":2,"pro":0.2857142857142857,"labels":"BAC"},{"idx":2,"sentence":"In this paper, we propose to learn the reasoning dynamics of visual relational reasoning by casting it as a path routing task.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":2,"sentence":"We present a reinforced path routing method that represents an input image via a structured visual graph and introduces a reinforcement learning based model to explore paths (sequences of nodes) over the graph based on an input sentence to infer reasoning results.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":2,"sentence":"By exploring such paths, the proposed method represents reasoning states clearly and characterizes state transitions explicitly to fully model the reasoning dynamics for accurate and transparent visual relational reasoning.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":2,"sentence":"Extensive experiments on referring expression comprehension and visual question answering demonstrate the effectiveness of our method.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":3,"sentence":"Graph Convolutional Networks (GCNs) have been widely used to model the high-order dynamic dependencies for skeleton-based action recognition.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"Most existing approaches do not explicitly embed the high-order spatio-temporal importance to joints’ spatial connection topology and intensity, and they do not have direct objectives on their attention module to jointly learn when and where to focus on in the action sequence.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":3,"sentence":"To address these problems, we propose the To-a-T Spatio-Temporal Focus (STF), a skeleton-based action recognition framework that utilizes the spatio-temporal gradient to focus on relevant spatio-temporal features.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":3,"sentence":"We first propose the STF modules with learnable gradient-enforced and instance-dependent adjacency matrices to model the high-order spatio-temporal dynamics.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":3,"sentence":"Second, we propose three loss terms defined on the gradient-based spatio-temporal focus to explicitly guide the classifier when and where to look at, distinguish confusing classes, and optimize the stacked STF modules.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":3,"sentence":"STF outperforms the state-of-the-art methods on the NTU RGB+D 60, NTU RGB+D 120, and Kinetics Skeleton 400 datasets in all 15 settings over different views, subjects, setups, and input modalities, and STF also shows better accuracy on scarce data and dataset shifting settings.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":4,"sentence":"Existing portrait matting methods either require auxiliary inputs that are costly to obtain or involve multiple stages that are computationally expensive, making them less suitable for real-time applications.","offset":0,"pro":0,"labels":"GAP"},{"idx":4,"sentence":"In this work, we present a light-weight matting objective decomposition network (MODNet) for portrait matting in real-time with a single input image.","offset":1,"pro":0.1,"labels":"PUR"},{"idx":4,"sentence":"The key idea behind our efficient design is by optimizing a series of sub-objectives simultaneously via explicit constraints.","offset":2,"pro":0.2,"labels":"MTD"},{"idx":4,"sentence":"In addition, MODNet includes two novel techniques for improving model efficiency and robustness.","offset":3,"pro":0.3,"labels":"MTD"},{"idx":4,"sentence":"First, an Efficient Atrous Spatial Pyramid Pooling (e-ASPP) module is introduced to fuse multi-scale features for semantic estimation.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":4,"sentence":"Second, a self-supervised sub-objectives consistency (SOC) strategy is proposed to adapt MODNet to real-world data to address the domain shift problem common to trimap-free methods.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":4,"sentence":"MODNet is easy to be trained in an end-to-end manner.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":4,"sentence":"It is much faster than contemporaneous methods and runs at 67 frames per second on a 1080Ti GPU.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":4,"sentence":"Experiments show that MODNet outperforms prior trimap-free methods by a large margin on both Adobe Matting Dataset and a carefully designed photographic portrait matting (PPM-100) benchmark proposed by us.","offset":8,"pro":0.8,"labels":"RST"},{"idx":4,"sentence":"Further, MODNet achieves remarkable results on daily photos and videos.","offset":9,"pro":0.9,"labels":"RST"},{"idx":5,"sentence":"Since human drivers only consider the driving-related factors that affect vehicle control depending on the situation, they can drive safely even in diverse driving environments.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"To mimic this behavior, we propose an autonomous driving framework based on the two-stage representation learning that initially splits the latent features as domain-specific features and domain-general features.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":5,"sentence":"Subsequently, the dynamic-object features, which contain information of dynamic objects, are disentangled from latent features using mutual information estimator.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":5,"sentence":"The proposed mixture of domain-specific experts (MoDE) model predicts the final control values through the cooperation of experts using a gating function.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":5,"sentence":"The domain-specific features are used to calculate the importance weight of the domain-specific experts, and the disentangled domain-general and dynamic-object features are applied in estimating the control values.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":5,"sentence":"In this study, the problem in behavior cloning is divided into several domain-specific subspaces, with experts becoming specialized on each domain-specific policy. ","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":5,"sentence":"To validate the proposed MoDE model, we conducted several experiments and achieved a higher success rate on the CARLA benchmarks under several conditions and tasks than state-of-the-art approaches. ","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":6,"sentence":"Recently, automated surveillance cameras can change a visible sensor and a thermal sensor for all-day operation.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"However, existing single-modal pedestrian detectors mainly focus on detecting pedestrians in only one specific modality (i.e., visible or thermal), so they cannot cope with other modal inputs.","offset":1,"pro":0.09090909090909091,"labels":"GAP"},{"idx":6,"sentence":"In addition, recent multispectral pedestrian detectors have shown remarkable performance by adopting multispectral modalities, but they also have limitations in practical applications (e.g., different Field-of-View (FoV) and frame rate).","offset":2,"pro":0.18181818181818182,"labels":"GAP"},{"idx":6,"sentence":"In this paper, we introduce a versatile pedestrian detector that shows robust detection performance in any single modality.","offset":3,"pro":0.2727272727272727,"labels":"PUR"},{"idx":6,"sentence":"We propose a multisensory-matching contrastive loss to reduce the difference between the visual representation of pedestrians in the visible and thermal modalities.","offset":4,"pro":0.36363636363636365,"labels":"MTD"},{"idx":6,"sentence":"Moreover, for the robust detection on a single modality, we design a Multispectral Recalling (MSR) Memory.","offset":5,"pro":0.45454545454545453,"labels":"MTD"},{"idx":6,"sentence":"The MSR Memory enhances the visual representation of the single modal features by recalling that of the multispectral modalities.","offset":6,"pro":0.5454545454545454,"labels":"MTD"},{"idx":6,"sentence":"To guide the MSR Memory to store the multispectral modal contexts, we introduce a multispectral recalling loss.","offset":7,"pro":0.6363636363636364,"labels":"MTD"},{"idx":6,"sentence":"It enables the pedestrian detector to encode more discriminative features with a single input modality.","offset":8,"pro":0.7272727272727273,"labels":"MTD"},{"idx":6,"sentence":"We believe our method is a step forward detector that can be applied to a variety of real-world applications.","offset":9,"pro":0.8181818181818182,"labels":"CLN"},{"idx":6,"sentence":"The comprehensive experimental results verify the effectiveness of the proposed method.","offset":10,"pro":0.9090909090909091,"labels":"CLN"},{"idx":7,"sentence":"Generalized zero-shot learning (GZSL) is a technique to train a deep learning model to identify unseen classes using the attribute.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"In this paper, we put forth a new GZSL technique that improves the GZSL classification performance greatly.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":7,"sentence":"Key idea of the proposed approach, henceforth referred to as semantic feature extraction-based GZSL (SE-GZSL), is to use the semantic feature containing only attribute-related information in learning the relationship between the image and the attribute.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":7,"sentence":"In doing so, we can remove the interference, if any, caused by the attribute-irrelevant information contained in the image feature.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":7,"sentence":"To train a network extracting the semantic feature, we present two novel loss functions, 1) mutual information-based loss to capture all the attribute-related information in the image feature and 2) similarity-based loss to remove unwanted attribute-irrelevant information.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":7,"sentence":"From extensive experiments using various datasets, we show that the proposed SE-GZSL technique outperforms conventional GZSL approaches by a large margin.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":8,"sentence":"Recognizing speech from silent lip movement, which is called lip reading, is a challenging task due to 1) the inherent information insufficiency of lip movement to fully represent the speech, and 2) the existence of homophenes that have similar lip movement with different pronunciations.","offset":0,"pro":0,"labels":"GAP"},{"idx":8,"sentence":"In this paper, we try to alleviate the aforementioned two challenges in lip reading by proposing a Multi-head Visual-audio Memory (MVM).","offset":1,"pro":0.1,"labels":"PUR"},{"idx":8,"sentence":"Firstly, MVM is trained with audio-visual datasets and remembers audio representations by modelling the inter-relationships of paired audio-visual representations.","offset":2,"pro":0.2,"labels":"MTD"},{"idx":8,"sentence":"At the inference stage, visual input alone can extract the saved audio representation from the memory by examining the learned inter-relationships.","offset":3,"pro":0.3,"labels":"MTD"},{"idx":8,"sentence":"Secondly, MVM is composed of multi-head key memories for saving visual features and one value memory for saving audio knowledge, which is designed to distinguish the homophenes.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":8,"sentence":"With the multi-head key memories, MVM extracts possible candidate audio features from the memory, which allows the lip reading model to consider the possibility of which pronunciations can be represented from the input lip movement.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":8,"sentence":"This also can be viewed as an explicit implementation of the one-to-many mapping of viseme-to-phoneme.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":8,"sentence":"Moreover, MVM is employed in multi-temporal levels to consider the context when retrieving the memory and distinguish the homophenes.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":8,"sentence":"Extensive experimental results verify the effectiveness of the proposed method in lip reading and in distinguishing the homophenes.","offset":8,"pro":0.8,"labels":"CLN"},{"idx":8,"sentence":"Therefore, the lip reading model can complement the insufficient visual information with the extracted audio representations. ","offset":9,"pro":0.9,"labels":"MTD"},{"idx":9,"sentence":"Recent techniques to solve photorealistic style transfer within deep convolutional neural networks (CNNs) generally require intensive training from large-scale datasets, thus having limited applicability and poor generalization ability to unseen images or styles.","offset":0,"pro":0,"labels":"GAP"},{"idx":9,"sentence":"To overcome this, we propose a novel framework, dubbed Deep Translation Prior (DTP), to accomplish photorealistic style transfer through test-time training on given input image pair with untrained networks, which learns an image pair-specific translation prior and thus yields better performance and generalization.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":9,"sentence":"Tailored for such test-time training for style transfer, we present novel network architectures, with two sub-modules of correspondence and generation modules, and loss functions consisting of contrastive content, style, and cycle consistency losses.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":9,"sentence":"Our framework does not require offline training phase for style transfer, which has been one of the main challenges in existing methods, but the networks are to be solely learned during test time.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":9,"sentence":"Experimental results prove that our framework has a better generalization ability to unseen image pairs and even outperforms the state-of-the-art methods.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":10,"sentence":"In this paper, we propose PrivateSNN, which aims to build low-power Spiking Neural Networks (SNNs) from a pre-trained ANN model without leaking sensitive information contained in a dataset.","offset":0,"pro":0,"labels":"PUR"},{"idx":10,"sentence":"Here, we tackle two types of leakage problems: 1) Data leakage is caused when the networks access real training data during an ANN-SNN conversion process.","offset":1,"pro":0.1,"labels":"PUR"},{"idx":10,"sentence":"Therefore, we encrypt SNN weights by training SNNs with a temporal spike-based learning rule.","offset":2,"pro":0.2,"labels":"MTD"},{"idx":10,"sentence":"Updating weight parameters with temporal data makes SNNs difficult to be interpreted in the spatial domain.","offset":3,"pro":0.3,"labels":"MTD"},{"idx":10,"sentence":"We observe that the encrypted PrivateSNN eliminates data and class leakage issues with a slight performance drop (less than ~2%) and significant energy-efficiency gain (about 55x) compared to the standard ANN.","offset":4,"pro":0.4,"labels":"RST"},{"idx":10,"sentence":"We conduct extensive experiments on various datasets including CIFAR10, CIFAR100, and TinyImageNet, highlighting the importance of privacy-preserving SNN training.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":10,"sentence":"How can we bring both privacy and energy-efficiency to a neural system? ","offset":6,"pro":0.6,"labels":"PUR"},{"idx":10,"sentence":"2) Class leakage is caused when class-related features can be reconstructed from network parameters. ","offset":7,"pro":0.7,"labels":"PUR"},{"idx":10,"sentence":"In order to address the data leakage issue, we generate synthetic images from the pre-trained ANNs and convert ANNs to SNNs using the generated images. ","offset":8,"pro":0.8,"labels":"MTD"},{"idx":10,"sentence":"However, converted SNNs remain vulnerable to class leakage since the weight parameters have the same (or scaled) value with respect to ANN parameters. ","offset":9,"pro":0.9,"labels":"MTD"},{"idx":11,"sentence":"We introduce NaturalInversion, a novel model inversion-based method to synthesize images that agrees well with the original data distribution without using real data.","offset":0,"pro":0,"labels":"PUR"},{"idx":11,"sentence":"In NaturalInversion, we propose: (1) a Feature Transfer Pyramid which uses enhanced image prior of the original data by combining the multi-scale feature maps extracted from the pre-trained classifier, (2) a one-to-one approach generative model where only one batch of images are synthesized by one generator to bring the non-linearity to optimization and to ease the overall optimizing process, (3) learnable Adaptive Channel Scaling parameters which are end-to-end trained to scale the output image channel to utilize the original image prior further.","offset":1,"pro":0.25,"labels":"MTD"},{"idx":11,"sentence":"Furthermore, our synthesized images outperform prior works on various applications such as knowledge distillation and pruning, demonstrating the effectiveness of our proposed method.","offset":2,"pro":0.5,"labels":"CLN"},{"idx":11,"sentence":"With our NaturalInversion, we synthesize images from classifiers trained on CIFAR-10/100 ","offset":3,"pro":0.75,"labels":"MTD"},{"idx":12,"sentence":"In this paper, we propose a new joint object detection and tracking (JoDT) framework for 3D object detection and tracking based on camera and LiDAR sensors.","offset":0,"pro":0,"labels":"PUR"},{"idx":12,"sentence":"The proposed method, referred to as 3D DetecTrack, enables the detector and tracker to cooperate to generate a spatio-temporal representation of the camera and LiDAR data, with which 3D object detection and tracking are then performed.","offset":1,"pro":0.14285714285714285,"labels":"MTD"},{"idx":12,"sentence":"The detector constructs the spatio-temporal features via the weighted temporal aggregation of the spatial features obtained by the camera and LiDAR fusion.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":12,"sentence":"Then, the detector reconfigures the initial detection results using information from the tracklets maintained up to the previous time step.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":12,"sentence":"Based on the spatio-temporal features generated by the detector, the tracker associates the detected objects with previously tracked objects using a graph neural network (GNN).","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":12,"sentence":"We devise a fully-connected GNN facilitated by a combination of rule-based edge pruning and attention-based edge gating, which exploits both spatial and temporal object contexts to improve tracking performance.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":12,"sentence":"The experiments conducted on both KITTI and nuScenes benchmarks demonstrate that the proposed 3D DetecTrack achieves significant improvements in both detection and tracking performances over baseline methods and achieves state-of-the-art performance among existing methods through collaboration between the detector and tracker.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":13,"sentence":"Open compound domain adaptation (OCDA) has emerged as a practical adaptation setting which considers a single labeled source domain against a compound of multi-modal unlabeled target data in order to generalize better on novel unseen domains.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"We hypothesize that an improved disentanglement of domain-related and task-related factors of dense intermediate layer features can greatly aid OCDA.","offset":1,"pro":0.1,"labels":"PUR"},{"idx":13,"sentence":"Prior-arts attempt this indirectly by employing adversarial domain discriminators on the spatial CNN output.","offset":2,"pro":0.2,"labels":"MTD"},{"idx":13,"sentence":"Motivated by this, we propose a novel feature space Amplitude Spectrum Transformation (AST).","offset":3,"pro":0.3,"labels":"MTD"},{"idx":13,"sentence":"During adaptation, we employ the AST auto-encoder for two purposes.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":13,"sentence":"First, carefully mined source-target instance pairs undergo a simulation of cross-domain feature stylization (AST-Sim) at a particular layer by altering the AST-latent.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":13,"sentence":"Second, AST operating at a later layer is tasked to normalize (AST-Norm) the domain content by fixing its latent to a mean prototype.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":13,"sentence":"Our simplified adaptation technique is not only clustering-free but also free from complex adversarial alignment.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":13,"sentence":"We achieve leading performance against the prior arts on the OCDA scene segmentation benchmarks.","offset":8,"pro":0.8,"labels":"RST"},{"idx":13,"sentence":"However, we find that latent features derived from the Fourier-based amplitude spectrum of deep CNN features hold a more tractable mapping with domain discrimination. ","offset":9,"pro":0.9,"labels":"MTD"},{"idx":14,"sentence":"Semi-supervised video object segmentation (VOS) refers to segmenting the target object in remaining frames given its annotation in the first frame, which has been actively studied in recent years.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"In this paper, we propose a novel Siamese network with a specifically designed interactive transformer, called SITVOS, to enable effective context propagation from historical to current frames.","offset":1,"pro":0.125,"labels":"PUR"},{"idx":14,"sentence":"Technically, we use the transformer encoder and decoder to handle the past frames and current frame separately, i.e., the encoder encodes robust spatio-temporal context of target object from the past frames, while the decoder takes the feature embedding of current frame as the query to retrieve the target from the encoder output.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":14,"sentence":"To further enhance the target representation, a feature interaction module (FIM) is devised to promote the information flow between the encoder and decoder.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":14,"sentence":"Moreover, we employ the Siamese architecture to extract backbone features of both past and current frames, which enables feature reuse and is more efficient than existing methods.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":14,"sentence":"Experimental results on three challenging benchmarks validate the superiority of SITVOS over state-of-the-art methods.","offset":5,"pro":0.625,"labels":"CLN"},{"idx":14,"sentence":"Code is available at https://github.com/LANMNG/SITVOS.","offset":6,"pro":0.75,"labels":"CTN"},{"idx":14,"sentence":"The key challenge lies in finding effective ways to exploit the spatio-temporal context of past frames to help learn discriminative target representation of current frame. ","offset":7,"pro":0.875,"labels":"GAP"},{"idx":15,"sentence":"Deep neural networks (DNNs) are vulnerable to adversarial examples that are carefully designed to cause the deep learning model to make mistakes.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"Adversarial examples of 2D images and 3D point clouds have been extensively studied, but studies on event-based data are limited.","offset":1,"pro":0.08333333333333333,"labels":"BAC"},{"idx":15,"sentence":"Event-based data can be an alternative to a 2D image under high-speed movements, such as autonomous driving.","offset":2,"pro":0.16666666666666666,"labels":"BAC"},{"idx":15,"sentence":"However, the given adversarial events make the current deep learning model vulnerable to safety issues.","offset":3,"pro":0.25,"labels":"GAP"},{"idx":15,"sentence":"In this work, we generate adversarial examples and then train the robust models for event-based data, for the first time.","offset":4,"pro":0.3333333333333333,"labels":"PUR"},{"idx":15,"sentence":"Our algorithm shifts the time of the original events and generates additional adversarial events.","offset":5,"pro":0.4166666666666667,"labels":"MTD"},{"idx":15,"sentence":"Additional adversarial events are generated in two stages.","offset":6,"pro":0.5,"labels":"MTD"},{"idx":15,"sentence":"First, null events are added to the event-based data to generate additional adversarial events.","offset":7,"pro":0.5833333333333334,"labels":"MTD"},{"idx":15,"sentence":"The perturbation size can be controlled with the number of null events.","offset":8,"pro":0.6666666666666666,"labels":"MTD"},{"idx":15,"sentence":"Second, the location and time of additional adversarial events are set to mislead DNNs in a gradient-based attack.","offset":9,"pro":0.75,"labels":"MTD"},{"idx":15,"sentence":"Our algorithm achieves an attack success rate of 97.95% on the N-Caltech101 dataset.","offset":10,"pro":0.8333333333333334,"labels":"RST"},{"idx":15,"sentence":"Furthermore, the adversarial training model improves robustness on the adversarial event data compared to the original model.","offset":11,"pro":0.9166666666666666,"labels":"CLN"},{"idx":16,"sentence":"Unsupervised video object segmentation (UVOS) is a per-pixel binary labeling problem which aims at separating the foreground object from the background in the video without using the ground truth (GT) mask of the foreground object.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"Most of the previous UVOS models use the first frame or the entire video as a reference frame to specify the mask of the foreground object.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":16,"sentence":"Our question is why the first frame should be selected as a reference frame or why the entire video should be used to specify the mask.","offset":2,"pro":0.2,"labels":"PUR"},{"idx":16,"sentence":"In our paper, we propose Easy Frame Selector (EFS).","offset":3,"pro":0.3,"labels":"PUR"},{"idx":16,"sentence":"The EFS enables us to select an \"easy\" reference frame that makes the subsequent VOS become easy, thereby improving the VOS performance.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":16,"sentence":"Furthermore, we propose a new framework named as Iterative Mask Prediction (IMP).","offset":5,"pro":0.5,"labels":"MTD"},{"idx":16,"sentence":"In the framework, we repeat applying EFS to the given video and selecting an \"easier\" reference frame from the video than the previous iteration, increasing the VOS performance incrementally.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":16,"sentence":"The IMP consists of EFS, Bi-directional Mask Prediction (BMP), and Temporal Information Updating (TIU).","offset":7,"pro":0.7,"labels":"MTD"},{"idx":16,"sentence":"From the proposed framework, we achieve state-of-the-art performance in three UVOS benchmark sets: DAVIS16, FBMS, and SegTrack-V2.","offset":8,"pro":0.8,"labels":"RST"},{"idx":16,"sentence":"We believe that we can select a better reference frame to achieve the better UVOS performance than using only the first frame or the entire video as a reference frame. ","offset":9,"pro":0.9,"labels":"PUR"},{"idx":17,"sentence":"We propose a novel scene flow estimation approach to capture and infer 3D motions from point clouds.","offset":0,"pro":0,"labels":"PUR"},{"idx":17,"sentence":"Estimating 3D motions for point clouds is challenging, since a point cloud is unordered and its density is significantly non-uniform.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":17,"sentence":"Such unstructured data poses difficulties in matching corresponding points between point clouds, leading to inaccurate flow estimation.","offset":2,"pro":0.2,"labels":"GAP"},{"idx":17,"sentence":"We propose a novel architecture named Sparse Convolution-Transformer Network (SCTN) that equips the sparse convolution with the transformer.","offset":3,"pro":0.3,"labels":"PUR"},{"idx":17,"sentence":"Specifically, by leveraging the sparse convolution, SCTN transfers irregular point cloud into locally consistent flow features for estimating spatially consistent motions within an object/local object part.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":17,"sentence":"We further propose to explicitly learn point relations using a point transformer module, different from exiting methods.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":17,"sentence":"We show that the learned relation-based contextual information is rich and helpful for matching corresponding points, benefiting scene flow estimation.","offset":6,"pro":0.6,"labels":"CLN"},{"idx":17,"sentence":"In addition, a novel loss function is proposed to adaptively encourage flow consistency according to feature similarity.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":17,"sentence":"Extensive experiments demonstrate that our proposed approach achieves a new state of the art in scene flow estimation.","offset":8,"pro":0.8,"labels":"CLN"},{"idx":17,"sentence":"Our approach achieves an error of 0.038 and 0.037 (EPE3D) on FlyingThings3D and KITTI Scene Flow respectively, which significantly outperforms previous methods by large margins.","offset":9,"pro":0.9,"labels":"RST"},{"idx":18,"sentence":"Spatiotemporal modeling in an unified architecture is key for video action recognition.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"This paper proposes a Shrinking Temporal Attention Transformer (STAT), which efficiently builts spatiotemporal attention maps considering the attenuation of spatial attention in short and long temporal sequences.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":18,"sentence":"Specifically, for short-term temporal tokens, query token interacts with them in a fine-grained manner in dealing with short-range motion.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":18,"sentence":"It then shrinks to a coarse attention in neighborhood for long-term tokens, to provide larger receptive field for long-range spatial aggregation.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":18,"sentence":"Both of them are composed in a short-long temporal integrated block to build visual appearances and temporal structure concurrently with lower costly in computation.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":18,"sentence":"We conduct thorough ablation studies, and achieve state-of-the-art results on multiple action recognition benchmarks including Kinetics400 and Something-Something v2, outperforming prior methods with 50% less FLOPs and without any pretrained model.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":19,"sentence":"Generating 3D dances from music is an emerged research task that benefits a lot of applications in vision and graphics.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"Previous works treat this task as sequence generation, however, it is challenging to render a music-aligned long-term sequence with high kinematic complexity and coherent movements.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":19,"sentence":"In this paper, we reformulate it by a two-stage process, i.e., a key pose generation and then an in-between parametric motion curve prediction, where the key poses are easier to be synchronized with the music beats and the parametric curves can be efficiently regressed to render fluent rhythm-aligned movements.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":19,"sentence":"We named the proposed method as DanceFormer, which includes two cascading kinematics-enhanced transformer-guided networks (called DanTrans) that tackle each stage, respectively.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":19,"sentence":"Furthermore, we propose a large-scale music conditioned 3D dance dataset, called PhantomDance, that is accurately labeled by experienced animators rather than reconstruction or motion capture.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":19,"sentence":"This dataset also encodes dances as key poses and parametric motion curves apart from pose sequences, thus benefiting the training of our DanceFormer.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":19,"sentence":"Extensive experiments demonstrate that the proposed method, even trained by existing datasets, can generate fluent, performative, and music-matched 3D dances that surpass previous works quantitatively and qualitatively.","offset":6,"pro":0.75,"labels":"CLN"},{"idx":19,"sentence":"Moreover, the proposed DanceFormer, together with the PhantomDance dataset, are seamlessly compatible with industrial animation software, thus facilitating the adaptation for various downstream applications.","offset":7,"pro":0.875,"labels":"CTN"},{"idx":0,"sentence":"In many visual systems, visual tracking often bases on RGB image sequences, in which some targets are invalid in low-light conditions, and tracking performance is thus affected significantly.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"Introducing other modalities such as depth and infrared data is an effective way to handle imaging limitations of individual sources, but multi-modal imaging platforms usually require elaborate designs and cannot be applied in many real-world applications at present.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":0,"sentence":"Near-infrared (NIR) imaging becomes an essential part of many surveillance cameras, whose imaging is switchable between RGB and NIR based on the light intensity.","offset":2,"pro":0.2,"labels":"BAC"},{"idx":0,"sentence":"These two modalities are heterogeneous with very different visual properties and thus bring big challenges for visual tracking.","offset":3,"pro":0.3,"labels":"BAC"},{"idx":0,"sentence":"However, existing works have not studied this challenging problem.","offset":4,"pro":0.4,"labels":"GAP"},{"idx":0,"sentence":"In this work, we address the cross-modal object tracking problem and contribute a new video dataset, including 654 cross-modal image sequences with over 481K frames in total, and the average video length is more than 735 frames.","offset":5,"pro":0.5,"labels":"PUR"},{"idx":0,"sentence":"To promote the research and development of cross-modal object tracking, we propose a new algorithm, which learns the modality-aware target representation to mitigate the appearance gap between RGB and NIR modalities in the tracking process.","offset":6,"pro":0.6,"labels":"PUR"},{"idx":0,"sentence":"It is plug-and-play and could thus be flexibly embedded into different tracking frameworks.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":0,"sentence":"Extensive experiments on the dataset are conducted, and we demonstrate the effectiveness of the proposed algorithm in two representative tracking frameworks against 19 state-of-the-art tracking methods.","offset":8,"pro":0.8,"labels":"CLN"},{"idx":0,"sentence":"Dataset, code, model and results are available at https://github.com/mmic-lcl/source-code.","offset":9,"pro":0.9,"labels":"CTN"},{"idx":1,"sentence":"We present YOFO (You Only inFer Once), a new paradigm for referring video object segmentation (RVOS) that operates in an one-stage manner.","offset":0,"pro":0,"labels":"PUR"},{"idx":1,"sentence":"Our key insight is that the language descriptor should serve as target-specific guidance to identify the target object, while a direct feature fusion of image and language can increase feature complexity and thus may be sub-optimal for RVOS.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":1,"sentence":"To this end, we propose a meta-transfer module, which is trained in a learning-to-learn fashion and aims to transfer the target-specific information from the language domain to the image domain, while discarding the uncorrelated complex variations of language description.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":1,"sentence":"To bridge the gap between the image and language domains, we develop a multi-scale cross-modal feature mining block that aggregates all the essential features required by RVOS from both domains and generates regression labels for the meta-transfer module.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":1,"sentence":"The whole system can be trained in an end-to-end manner and shows competitive performance against state-of-the-art two-stage approaches.","offset":4,"pro":0.8,"labels":"RST"},{"idx":2,"sentence":"Knowledge Distillation (KD) is a widely-used technology to inherit information from cumbersome teacher models to compact student models, consequently realizing model compression and acceleration.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"Compared with image classification, object detection is a more complex task, and designing specific KD methods for object detection is non-trivial.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":2,"sentence":"In this work, we elaborately study the behaviour difference between the teacher and student detection models, and obtain two intriguing observations: First, the teacher and student rank their detected candidate boxes quite differently, which results in their precision discrepancy.","offset":2,"pro":0.2222222222222222,"labels":"PUR"},{"idx":2,"sentence":"Second, there is a considerable gap between the feature response differences and prediction differences between teacher and student, indicating that equally imitating all the feature maps of the teacher is the sub-optimal choice for improving the student's accuracy.","offset":3,"pro":0.3333333333333333,"labels":"CLN"},{"idx":2,"sentence":"Based on the two observations, we propose Rank Mimicking (RM) and Prediction-guided Feature Imitation (PFI) for distilling one-stage detectors, respectively.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":2,"sentence":"RM takes the rank of candidate boxes from teachers as a new form of knowledge to distill, which consistently outperforms the traditional soft label distillation.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":2,"sentence":"PFI attempts to correlate feature differences with prediction differences, making feature imitation directly help to improve the student's accuracy.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":2,"sentence":"On MS COCO and PASCAL VOC benchmarks, extensive experiments are conducted on various detectors with different backbones to validate the effectiveness of our method.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":2,"sentence":"Specifically, RetinaNet with ResNet50 achieves 40.4% mAP on MS COCO, which is 3.5% higher than its baseline, and also outperforms previous KD methods.","offset":8,"pro":0.8888888888888888,"labels":"RST"},{"idx":3,"sentence":"Recent advances in semi-supervised object detection (SSOD) are largely driven by consistency-based pseudo-labeling methods for image classification tasks, producing pseudo labels as supervisory signals.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"However, when using pseudo labels, there is a lack of consideration in localization precision and amplified class imbalance, both of which are critical for detection tasks.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":3,"sentence":"In this paper, we introduce certainty-aware pseudo labels tailored for object detection, which can effectively estimate the classification and localization quality of derived pseudo labels.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":3,"sentence":"This is achieved by converting conventional localization as a classification task followed by refinement.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":3,"sentence":"Conditioned on classification and localization quality scores, we dynamically adjust the thresholds used to generate pseudo labels and reweight loss functions for each category to alleviate the class imbalance problem.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":3,"sentence":"Extensive experiments demonstrate that our method improves state-of-the-art SSOD performance by 1-2% AP on COCO and PASCAL VOC while being orthogonal and complementary to most existing methods.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":3,"sentence":"In the limited-annotation regime, our approach improves supervised baselines by up to 10% AP using only 1-10% labeled data from COCO.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":4,"sentence":"Image-text retrieval plays a central role in bridging vision and language, which aims to reduce the semantic discrepancy between images and texts.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"Most of existing works rely on refined words and objects representation through the data-oriented method to capture the word-object cooccurrence.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":4,"sentence":"Such approaches are prone to ignore the asymmetric action relation between images and texts, that is, the text has explicit action representation (i.e., verb phrase) while the image only contains implicit action information.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":4,"sentence":"In this paper, we propose Action-aware Memory-Enhanced embedding (AME) method for image-text retrieval, which aims to emphasize the action information when mapping the images and texts into a shared embedding space.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":4,"sentence":"Specifically, we integrate action prediction along with an action-aware memory bank to enrich the image and text features with action-similar text features.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":4,"sentence":"The effectiveness of our proposed AME method is verified by comprehensive experimental results on two benchmark datasets.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":5,"sentence":"Due to high-speed motion blur and challenging illumination, conventional frame-based cameras have encountered an important challenge in object detection tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"Neuromorphic cameras that output asynchronous visual streams instead of intensity frames, by taking the advantage of high temporal resolution and high dynamic range, have brought a new perspective to address the challenge.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":5,"sentence":"In this paper, we propose a novel problem setting, retinomorphic object detection, which is the first trial that integrates foveal-like and peripheral-like visual streams.","offset":2,"pro":0.2222222222222222,"labels":"PUR"},{"idx":5,"sentence":"Technically, we first build a large-scale multimodal neuromorphic object detection dataset (i.e., PKU-Vidar-DVS) over 215.5k spatio-temporal synchronized labels.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":5,"sentence":"Then, we design temporal aggregation representations to preserve the spatio-temporal information from asynchronous visual streams.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":5,"sentence":"Finally, we present a novel bio-inspired unifying framework to fuse two sensing modalities via a dynamic interaction mechanism.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":5,"sentence":"Our experimental evaluation shows that our approach has significant improvements over the state-of-the-art methods with the single-modality, especially in high-speed motion and low-light scenarios.","offset":6,"pro":0.6666666666666666,"labels":"RST"},{"idx":5,"sentence":"We hope that our work will attract further research into this newly identified, yet crucial research direction.","offset":7,"pro":0.7777777777777778,"labels":"IMP"},{"idx":5,"sentence":"Our dataset can be available at https://www.pkuml.org/resources/pku-vidar-dvs.html.","offset":8,"pro":0.8888888888888888,"labels":"CTN"},{"idx":6,"sentence":"Learning visual knowledge from massive weakly-labeled web videos has attracted growing research interests thanks to the large corpus of easily accessible video data on the Internet.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"However, for video action recognition, the action of interest might only exist in arbitrary clips of untrimmed web videos, resulting in high label noises in the temporal space.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":6,"sentence":"To address this challenge, we introduce a new method for pre-training video action recognition models using queried web videos.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":6,"sentence":"Instead of trying to filter out potential noises, we propose to provide fine-grained supervision signals by defining the concept of Sub-Pseudo Label (SPL).","offset":3,"pro":0.375,"labels":"MTD"},{"idx":6,"sentence":"Specifically, SPL spans out a new set of meaningful \"middle ground\" label space constructed by extrapolating the original weak labels during video querying and the prior knowledge distilled from a teacher model.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":6,"sentence":"Consequently, SPL provides enriched supervision for video models to learn better representations and improves data utilization efficiency of untrimmed videos.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":6,"sentence":"We validate the effectiveness of our method on four video action recognition datasets and a weakly-labeled image dataset.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":6,"sentence":"Experiments show that SPL outperforms several existing pre-training strategies and the learned representations lead to competitive results on several benchmarks.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":7,"sentence":"Deep learning models have shown to be susceptible to universal adversarial perturbation (UAP), which has aroused wide concerns in the community.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"Compared with the conventional adversarial attacks that generate adversarial samples at the instance level, UAP can fool the target model for different instances with only a single perturbation, enabling us to evaluate the robustness of the model from a more effective and accurate perspective.","offset":1,"pro":0.1111111111111111,"labels":"CLN"},{"idx":7,"sentence":"The existing universal attack methods fail to exploit the differences and connections between the instance and universal levels to produce dominant perturbations.","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":7,"sentence":"To address this challenge, we propose a new universal attack method that unifies instance-specific and universal attacks from a feature perspective to generate a more dominant UAP.","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":7,"sentence":"Specifically, we reformulate the UAP generation task as a minimax optimization problem and then utilize the instance-specific attack method to solve the minimization problem thereby obtaining better training data for generating UAP.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":7,"sentence":"At the same time, we also introduce a consistency regularizer to explore the relationship between training data, thus further improving the dominance of the generated UAP.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":7,"sentence":"Furthermore, our method is generic with no additional assumptions about the training data and hence can be applied to both data-dependent (supervised) and data-independent (unsupervised) manners.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":7,"sentence":"Extensive experiments demonstrate that the proposed method improves the performance by a significant margin over the existing methods in both data-dependent and data-independent settings.","offset":7,"pro":0.7777777777777778,"labels":"CLN"},{"idx":7,"sentence":"Code is available at https://github.com/lisenxd/AT-UAP.","offset":8,"pro":0.8888888888888888,"labels":"CTN"},{"idx":8,"sentence":"Features, logits, and labels are the three primary data when a sample passes through a deep neural network.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"Feature perturbation and label perturbation receive increasing attention in recent years.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":8,"sentence":"They have been proven to be useful in various deep learning approaches.","offset":2,"pro":0.2,"labels":"BAC"},{"idx":8,"sentence":"For example, (adversarial) feature perturbation can improve the robustness or even generalization capability of learned models.","offset":3,"pro":0.3,"labels":"BAC"},{"idx":8,"sentence":"However, limited studies have explicitly explored for the perturbation of logit vectors.","offset":4,"pro":0.4,"labels":"GAP"},{"idx":8,"sentence":"This work discusses several existing methods related to logit perturbation.","offset":5,"pro":0.5,"labels":"PUR"},{"idx":8,"sentence":"Based on a unified viewpoint between positive/negative data augmentation and loss variations incurred by logit perturbation, a new method is proposed to explicitly learn to perturb logits.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":8,"sentence":"A comparative analysis is conducted for the perturbations used in our and existing methods.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":8,"sentence":"Extensive experiments on benchmark image classification data sets and their long-tail versions indicated the competitive performance of our learning method.","offset":8,"pro":0.8,"labels":"CLN"},{"idx":8,"sentence":"In addition, existing methods can be further improved by utilizing our method.","offset":9,"pro":0.9,"labels":"CTN"},{"idx":9,"sentence":"Most metric learning techniques typically focus on sample embedding learning, while implicitly assume a homogeneous local neighborhood around each sample, based on the metrics used in training ( e.g., hypersphere for Euclidean distance or unit hyperspherical crown for cosine distance).","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"As real-world data often lies on a low-dimensional manifold curved in a high-dimensional space, it is unlikely that everywhere of the manifold shares the same local structures in the input space.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":9,"sentence":"Besides, considering the non-linearity of neural networks, the local structure in the output embedding space may not be homogeneous as assumed.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":9,"sentence":"Therefore, representing each sample simply with its embedding while ignoring its individual neighborhood structure would have limitations in Embedding-Based Retrieval (EBR).","offset":3,"pro":0.42857142857142855,"labels":"GAP"},{"idx":9,"sentence":"By exploiting the heterogeneity of local structures in the embedding space, we propose a Neighborhood-Adaptive Structure Augmented metric learning framework (NASA), where the neighborhood structure is realized as a structure embedding, and learned along with the sample embedding in a self-supervised manner.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":9,"sentence":"In this way, without any modifications, most indexing techniques can be used to support large-scale EBR with NASA embeddings.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":9,"sentence":"Experiments on six standard benchmarks with two kinds of embeddings, i.e., binary embeddings and real-valued embeddings, show that our method significantly improves and outperforms the state-of-the-art methods.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":10,"sentence":"We propose a new object-centric framework for learning-based stereo 3D object detection.","offset":0,"pro":0,"labels":"PUR"},{"idx":10,"sentence":"Previous studies build scene-centric representations that do not consider the significant variation among outdoor instances and thus lack the flexibility and functionalities that an instance-level model can offer.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":10,"sentence":"We build such an instance-level model by formulating and tackling a local update problem, i.e., how to predict a refined update given an initial 3D cuboid guess.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":10,"sentence":"We demonstrate how solving this problem can complement scene-centric approaches in (i) building a coarse-to-fine multi-resolution system, (ii) performing model-agnostic object location refinement, and (iii) conducting stereo 3D tracking-by-detection.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":10,"sentence":"Extensive experiments demonstrate the effectiveness of our approach, which achieves state-of-the-art performance on the KITTI benchmark.","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":10,"sentence":"Code and pre-trained models are available at https://github.com/Nicholasli1995/SNVC.","offset":5,"pro":0.8333333333333334,"labels":"CTN"},{"idx":11,"sentence":"This paper tackles the problem of parts-aware point cloud generation.","offset":0,"pro":0,"labels":"PUR"},{"idx":11,"sentence":"Unlike existing works which require the point cloud to be segmented into parts a priori, our parts-aware editing and generation are performed in an unsupervised manner.","offset":1,"pro":0.14285714285714285,"labels":"MTD"},{"idx":11,"sentence":"We achieve this with a simple modification of the Variational Auto-Encoder which yields a joint model of the point cloud itself along with a schematic representation of it as a combination of shape primitives.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":11,"sentence":"In particular, we introduce a latent representation of the point cloud which can be decomposed into a disentangled representation for each part of the shape.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":11,"sentence":"These parts are in turn disentangled into both a shape primitive and a point cloud representation, along with a standardising transformation to a canonical coordinate system.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":11,"sentence":"The dependencies between our standardising transformations preserve the spatial dependencies between the parts in a manner that allows meaningful parts-aware point cloud generation and shape editing.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":11,"sentence":"In addition to the flexibility afforded by our disentangled representation, the inductive bias introduced by our joint modeling approach yields state-of-the-art experimental results on the ShapeNet dataset.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":12,"sentence":"Weakly supervised Video Anomaly Detection (VAD) using Multi-Instance Learning (MIL) is usually based on the fact that the anomaly score of an abnormal snippet is higher than that of a normal snippet.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"In the beginning of training, due to the limited accuracy of the model, it is easy to select the wrong abnormal snippet.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":12,"sentence":"In order to reduce the probability of selection errors, we first propose a Multi-Sequence Learning (MSL) method and a hinge-based MSL ranking loss that uses a sequence composed of multiple snippets as an optimization unit.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":12,"sentence":"We then design a Transformer-based MSL network to learn both video-level anomaly probability and snippet-level anomaly scores.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":12,"sentence":"In the inference stage, we propose to use the video-level anomaly probability to suppress the fluctuation of snippet-level anomaly scores.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":12,"sentence":"Finally, since VAD needs to predict the snippet-level anomaly scores, by gradually reducing the length of selected sequence, we propose a self-training strategy to gradually refine the anomaly scores.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":12,"sentence":"Experimental results show that our method achieves significant improvements on ShanghaiTech, UCF-Crime, and XD-Violence.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":13,"sentence":"Few-shot action recognition aims to recognize novel action classes (query) using just a few samples (support).","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"The majority of current approaches follow the metric learning paradigm, which learns to compare the similarity between videos.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":13,"sentence":"Recently, it has been observed that directly measuring this similarity is not ideal since different action instances may show distinctive temporal distribution, resulting in severe misalignment issues across query and support videos.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":13,"sentence":"In this paper, we arrest this problem from two distinct aspects -- action duration misalignment and action evolution misalignment.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":13,"sentence":"We address them sequentially through a Two-stage Action Alignment Network (TA2N).","offset":4,"pro":0.5,"labels":"MTD"},{"idx":13,"sentence":"The first stage locates the action by learning a temporal affine transform, which warps each video feature to its action duration while dismissing the action-irrelevant feature (e.g. background).","offset":5,"pro":0.625,"labels":"MTD"},{"idx":13,"sentence":"Next, the second stage coordinates query feature to match the spatial-temporal action evolution of support by performing temporally rearrange and spatially offset prediction.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":13,"sentence":"Extensive experiments on benchmark datasets show the potential of the proposed method in achieving state-of-the-art performance for few-shot action recognition.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":14,"sentence":"We consider the single image super-resolution (SISR) problem, where a high-resolution (HR) image is generated based on a low-resolution (LR) input.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"Recently, generative adversarial networks (GANs) become popular to hallucinate details.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":14,"sentence":"Most methods along this line rely on a predefined single-LR-single-HR mapping, which is not flexible enough for the ill-posed SISR task.","offset":2,"pro":0.2222222222222222,"labels":"BAC"},{"idx":14,"sentence":"Also, GAN-generated fake details may often undermine the realism of the whole image.","offset":3,"pro":0.3333333333333333,"labels":"GAP"},{"idx":14,"sentence":"We address these issues by proposing best-buddy GANs (Beby-GAN) for rich-detail SISR.","offset":4,"pro":0.4444444444444444,"labels":"PUR"},{"idx":14,"sentence":"Relaxing the rigid one-to-one constraint, we allow the estimated patches to dynamically seek trustworthy surrogates of supervision during training, which is beneficial to producing more reasonable details.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":14,"sentence":"Besides, we propose a region-aware adversarial learning strategy that directs our model to focus on generating details for textured areas adaptively.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":14,"sentence":"Extensive experiments justify the effectiveness of our method.","offset":7,"pro":0.7777777777777778,"labels":"CLN"},{"idx":14,"sentence":"An ultra-high-resolution 4K dataset is also constructed to facilitate future super-resolution research.","offset":8,"pro":0.8888888888888888,"labels":"MTD"},{"idx":15,"sentence":"The domain gap severely limits the transferability and scalability of object detectors trained in a specific domain when applied to a novel one.","offset":0,"pro":0,"labels":"GAP"},{"idx":15,"sentence":"Most existing works bridge the domain gap by minimizing the domain discrepancy in the category space and aligning category-agnostic global features.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":15,"sentence":"Though great success, these methods model domain discrepancy with prototypes within a batch, yielding a biased estimation of domain-level distribution.","offset":2,"pro":0.2,"labels":"GAP"},{"idx":15,"sentence":"Besides, the category-agnostic alignment leads to the disagreement of class-specific distributions in the two domains, further causing inevitable classification errors.","offset":3,"pro":0.3,"labels":"GAP"},{"idx":15,"sentence":"To overcome these two challenges, we propose a novel Semantic Conditioned AdaptatioN (SCAN) framework such that well-modeled unbiased semantics can support semantic conditioned adaptation for precise domain adaptive object detection.","offset":4,"pro":0.4,"labels":"PUR"},{"idx":15,"sentence":"Specifically, class-specific semantics crossing different images in the source domain are graphically aggregated as the input to learn an unbiased semantic paradigm incrementally.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":15,"sentence":"The paradigm is then sent to a lightweight manifestation module to obtain conditional kernels to serve as the role of extracting semantics from the target domain for better adaptation.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":15,"sentence":"Subsequently, conditional kernels are integrated into global alignment to support the class-specific adaptation in a well-designed Conditional Kernel guided Alignment (CKA) module.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":15,"sentence":"Meanwhile, rich knowledge of the unbiased paradigm is transferred to the target domain with a novel Graph-based Semantic Transfer (GST) mechanism, yielding the adaptation in the category-based feature space.","offset":8,"pro":0.8,"labels":"MTD"},{"idx":15,"sentence":"Comprehensive experiments conducted on three adaptation benchmarks demonstrate that SCAN outperforms existing works by a large margin.","offset":9,"pro":0.9,"labels":"RST"},{"idx":16,"sentence":"Recently, transformer-based image segmentation methods have achieved notable success against previous solutions.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"While for video domains, how to effectively model temporal context with the attention of object instances across frames remains an open problem.","offset":1,"pro":0.09090909090909091,"labels":"GAP"},{"idx":16,"sentence":"In this paper, we propose an online video instance segmentation framework with a novel instance-aware temporal fusion method.","offset":2,"pro":0.18181818181818182,"labels":"PUR"},{"idx":16,"sentence":"We first leverage the representation, \\ie, a latent code in the global context (instance code) and CNN feature maps to represent instance- and pixel-level features.","offset":3,"pro":0.2727272727272727,"labels":"MTD"},{"idx":16,"sentence":"Based on this representation, we introduce a cropping-free temporal fusion approach to model the temporal consistency between video frames.","offset":4,"pro":0.36363636363636365,"labels":"MTD"},{"idx":16,"sentence":"Specifically, we encode global instance-specific information in the instance code and build up inter-frame contextual fusion with hybrid attentions between the instance codes and CNN feature maps.","offset":5,"pro":0.45454545454545453,"labels":"MTD"},{"idx":16,"sentence":"Inter-frame consistency between the instance codes is further enforced with order constraints.","offset":6,"pro":0.5454545454545454,"labels":"MTD"},{"idx":16,"sentence":"By leveraging the learned hybrid temporal consistency, we are able to directly retrieve and maintain instance identities across frames, eliminating the complicated frame-wise instance matching in prior methods.","offset":7,"pro":0.6363636363636364,"labels":"RST"},{"idx":16,"sentence":"Extensive experiments have been conducted on popular VIS datasets, i.e. Youtube-VIS-19/21.","offset":8,"pro":0.7272727272727273,"labels":"MTD"},{"idx":16,"sentence":"Our model achieves the best performance among all online VIS methods.","offset":9,"pro":0.8181818181818182,"labels":"RST"},{"idx":16,"sentence":"Notably, our model also eclipses all offline methods when using the ResNet-50 backbone.","offset":10,"pro":0.9090909090909091,"labels":"CLN"},{"idx":17,"sentence":"In this work, we focus on a very practical problem: image segmentation under rain conditions.","offset":0,"pro":0,"labels":"PUR"},{"idx":17,"sentence":"Image deraining is a classic low-level restoration task, while image segmentation is a typical high-level understanding task.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":17,"sentence":"Most of the existing methods intuitively employ the bottom-up paradigm by taking deraining as a preprocessing step for subsequent segmentation.","offset":2,"pro":0.2222222222222222,"labels":"BAC"},{"idx":17,"sentence":"However, our statistical analysis indicates that not only deraining would benefit segmentation (bottom-up), but also segmentation would further improve deraining performance (top-down) in turn.","offset":3,"pro":0.3333333333333333,"labels":"CLN"},{"idx":17,"sentence":"This motivates us to solve the rainy image segmentation task within a novel top-down and bottom-up unified paradigm, in which two sub-tasks are alternatively performed and collaborated with each other.","offset":4,"pro":0.4444444444444444,"labels":"PUR"},{"idx":17,"sentence":"Specifically, the bottom-up procedure yields both clearer images and rain-robust features from both image and feature domains, so as to ease the segmentation ambiguity caused by rain streaks.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":17,"sentence":"The top-down procedure adopts semantics to adaptively guide the restoration for different contents via a novel multi-path semantic attentive module (SAM).","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":17,"sentence":"Thus the deraining and segmentation could boost the performance of each other cooperatively and progressively.","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":17,"sentence":"Extensive experiments and ablations demonstrate that the proposed method outperforms the state-of-the-art on rainy image segmentation.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":18,"sentence":"Weakly-Supervised Semantic Segmentation (WSSS) segments objects without heavy burden of dense annotation.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"While as a price, generated pseudo-masks exist obvious noisy pixels, which result in sub-optimal segmentation models trained over these pseudo-masks.","offset":1,"pro":0.1,"labels":"GAP"},{"idx":18,"sentence":"But rare studies notice or work on this problem, even these noisy pixels are inevitable after their improvements on pseudo-mask.","offset":2,"pro":0.2,"labels":"GAP"},{"idx":18,"sentence":"So we try to improve WSSS in the aspect of noise mitigation.","offset":3,"pro":0.3,"labels":"PUR"},{"idx":18,"sentence":"And we observe that many noisy pixels are of high confidences, especially when the response range is too wide or narrow, presenting an uncertain status.","offset":4,"pro":0.4,"labels":"RST"},{"idx":18,"sentence":"Thus, in this paper, we simulate noisy variations of response by scaling the prediction map in multiple times for uncertainty estimation.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":18,"sentence":"The uncertainty is then used to weight the segmentation loss to mitigate noisy supervision signals.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":18,"sentence":"We call this method URN, abbreviated from Uncertainty estimation via Response scaling for Noise mitigation.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":18,"sentence":"Experiments validate the benefits of URN, and our method achieves state-of-the-art results at 71.2% and 41.5% on PASCAL VOC 2012 and MS COCO 2014 respectively, without extra models like saliency detection.","offset":8,"pro":0.8,"labels":"RST"},{"idx":18,"sentence":"Code is available at https://github.com/XMed-Lab/URN.","offset":9,"pro":0.9,"labels":"CTN"},{"idx":19,"sentence":"Multi-modal fusion is proven to be an effective method to improve the accuracy and robustness of speaker tracking, especially in complex scenarios.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"However, how to combine the heterogeneous information and exploit the complementarity of multi-modal signals remains a challenging issue.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":19,"sentence":"In this paper, we propose a novel Multi-modal Perception Tracker (MPT) for speaker tracking using both audio and visual modalities.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":19,"sentence":"Specifically, a novel acoustic map based on spatial-temporal Global Coherence Field (stGCF) is first constructed for heterogeneous signal fusion, which employs a camera model to map audio cues to the localization space consistent with the visual cues.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":19,"sentence":"Then a multi-modal perception attention network is introduced to derive the perception weights that measure the reliability and effectiveness of intermittent audio and video streams disturbed by noise.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":19,"sentence":"Moreover, a unique cross-modal self-supervised learning method is presented to model the confidence of audio and visual observations by leveraging the complementarity and consistency between different modalities.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":19,"sentence":"Experimental results show that the proposed MPT achieves 98.6% and 78.3% tracking accuracy on the standard and occluded datasets, respectively, which demonstrates its robustness under adverse conditions and outperforms the current state-of-the-art methods.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":0,"sentence":"Image-guided depth completion aims to generate dense depth maps with sparse depth measurements and corresponding RGB images.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"Currently, spatial propagation networks (SPNs) are the most popular affinity-based methods in depth completion, but they still suffer from the representation limitation of the fixed affinity and the over smoothing during iterations.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":0,"sentence":"Our solution is to estimate independent affinity matrices in each SPN iteration, but it is over-parameterized and heavy calculation.This paper introduces an efficient model that learns the affinity among neighboring pixels with an attention-based, dynamic approach.","offset":2,"pro":0.2222222222222222,"labels":"PUR"},{"idx":0,"sentence":"Specifically, the Dynamic Spatial Propagation Network (DySPN) we proposed makes use of a non-linear propagation model (NLPM).","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":0,"sentence":"It decouples the neighborhood into parts regarding to different distances and recursively generates independent attention maps to refine these parts into adaptive affinity matrices.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":0,"sentence":"Furthermore, we adopt a diffusion suppression (DS) operation so that the model converges at an early stage to prevent over-smoothing of dense depth.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":0,"sentence":"Finally, in order to decrease the computational cost required, we also introduce three variations that reduce the amount of neighbors and attentions needed while still retaining similar accuracy.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":0,"sentence":"In practice, our method requires less iteration to match the performance of other SPNs and yields better results overall.","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":0,"sentence":"DySPN outperforms other state-of-the-art (SoTA) methods on KITTI Depth Completion (DC) evaluation by the time of submission and is able to yield SoTA performance in NYU Depth v2 dataset as well.","offset":8,"pro":0.8888888888888888,"labels":"RST"},{"idx":1,"sentence":"Although convolutional neural network based stereo matching architectures have made impressive achievements, there are still some limitations: 1) Convolutional Feature (CF) tends to capture appearance information, which is inadequate for accurate matching.","offset":0,"pro":0,"labels":"GAP"},{"idx":1,"sentence":"2) Due to the static filters, current convolution based disparity refinement modules often produce over-smooth results.","offset":1,"pro":0.1,"labels":"GAP"},{"idx":1,"sentence":"In this paper, we present two schemes to address these issues, where some traditional wisdoms are integrated.","offset":2,"pro":0.2,"labels":"PUR"},{"idx":1,"sentence":"Firstly, we introduce a pairwise feature for deep stereo matching networks, named LSP (Local Similarity Pattern).","offset":3,"pro":0.3,"labels":"MTD"},{"idx":1,"sentence":"Through explicitly revealing the neighbor relationships, LSP contains rich structural information, which can be leveraged to aid CF for more discriminative feature description.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":1,"sentence":"Secondly, we design a dynamic self-reassembling refinement strategy and apply it to the cost distribution and the disparity map respectively.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":1,"sentence":"The former could be equipped with the unimodal distribution constraint to alleviate the over-smoothing problem, and the latter is more practical.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":1,"sentence":"The effectiveness of the proposed methods is demonstrated via incorporating them into two well-known basic architectures, GwcNet and GANet-deep.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":1,"sentence":"Experimental results on the SceneFlow and KITTI benchmarks show that our modules significantly improve the performance of the model.","offset":8,"pro":0.8,"labels":"CLN"},{"idx":1,"sentence":"Code is available at https://github.com/SpadeLiu/Lac-GwcNet.","offset":9,"pro":0.9,"labels":"CTN"},{"idx":2,"sentence":"Current state-of-the-art deep learning based face recognition (FR) models require a large number of face identities for central training.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"However, due to the growing privacy awareness, it is prohibited to access the face images on user devices to continually improve face recognition models.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":2,"sentence":"Federated Learning (FL) is a technique to address the privacy issue, which can collaboratively optimize the model without sharing the data between clients.","offset":2,"pro":0.2222222222222222,"labels":"BAC"},{"idx":2,"sentence":"In this work, we propose a FL based framework called FedFR to improve the generic face representation in a privacy-aware manner.","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":2,"sentence":"Besides, the framework jointly optimizes personalized models for the corresponding clients via the proposed Decoupled Feature Customization module.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":2,"sentence":"The client-specific personalized model can serve the need of optimized face recognition experience for registered identities at the local device.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":2,"sentence":"To the best of our knowledge, we are the first to explore the personalized face recognition in FL setup.","offset":6,"pro":0.6666666666666666,"labels":"CTN"},{"idx":2,"sentence":"The proposed framework is validated to be superior to previous approaches on several generic and personalized face recognition benchmarks with diverse FL scenarios.","offset":7,"pro":0.7777777777777778,"labels":"CLN"},{"idx":2,"sentence":"The source codes and our proposed personalized FR benchmark under FL setup are available at https://github.com/jackie840129/FedFR.","offset":8,"pro":0.8888888888888888,"labels":"CTN"},{"idx":3,"sentence":"Temporal sentence grounding (TSG) is crucial and fundamental for video understanding.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"Although existing methods train well-designed deep networks with large amount of data, we find that they can easily forget the rarely appeared cases during training due to the off-balance data distribution, which influences the model generalization and leads to unsatisfactory performance.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":3,"sentence":"To tackle this issue, we propose a memory-augmented network, called Memory-Guided Semantic Learning Network (MGSL-Net), that learns and memorizes the rarely appeared content in TSG task.","offset":2,"pro":0.2222222222222222,"labels":"PUR"},{"idx":3,"sentence":"Specifically, our proposed model consists of three main parts: cross-modal interaction module, memory augmentation module, and heterogeneous attention module.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":3,"sentence":"We first align the given video-query pair by a cross-modal graph convolutional network, and then utilize memory module to record the cross-modal shared semantic features in the domain-specific persistent memory.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":3,"sentence":"During training, the memory slots are dynamically associated with both common and rare cases, alleviating the forgetting issue.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":3,"sentence":"In testing, the rare cases can thus be enhanced by retrieving the stored memories, leading to better generalization.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":3,"sentence":"At last, the heterogeneous attention module is utilized to integrate the enhanced multi-modal features in both video and query domains.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":3,"sentence":"Experimental results on three benchmarks show the superiority of our method on both effectiveness and efficiency, which substantially improves the accuracy not only on the entire dataset but also on the rare cases.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":4,"sentence":"This paper addresses temporal sentence grounding.","offset":0,"pro":0,"labels":"PUR"},{"idx":4,"sentence":"Previous works typically solve this task by learning frame-level video features and align them with the textual information.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":4,"sentence":"A major limitation of these works is that they fail to distinguish ambiguous video frames with subtle appearance differences due to frame-level feature extraction.","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":4,"sentence":"Recently, a few methods adopt Faster R-CNN to extract detailed object features in each frame to differentiate the fine-grained appearance similarities.","offset":3,"pro":0.3333333333333333,"labels":"BAC"},{"idx":4,"sentence":"However, the object-level features extracted by Faster R-CNN suffer from missing motion analysis since the object detection model lacks temporal modeling.","offset":4,"pro":0.4444444444444444,"labels":"GAP"},{"idx":4,"sentence":"To solve this issue, we propose a novel Motion-Appearance Reasoning Network (MARN), which incorporates both motion-aware and appearance-aware object features to better reason object relations for modeling the activity among successive frames.Specifically, we first introduce two individual video encoders to embed the video into corresponding motion-oriented and appearance-aspect object representations.","offset":5,"pro":0.5555555555555556,"labels":"PUR"},{"idx":4,"sentence":"Then, we develop separate motion and appearance branches to learn motion-guided and appearance-guided object relations, respectively.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":4,"sentence":"At last, both motion and appearance information from two branches are associated to generate more representative features for final grounding.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":4,"sentence":"Extensive experiments on two challenging datasets (Charades-STA and TACoS) show that our proposed MARN significantly outperforms previous state-of-the-art methods by a large margin.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":5,"sentence":"Temporal video grounding (TVG) aims to localize a target segment in a video according to a given sentence query.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"Though respectable works have made decent achievements in this task, they severely rely on abundant video-query paired data, which is expensive to collect in real-world scenarios.","offset":1,"pro":0.1,"labels":"GAP"},{"idx":5,"sentence":"In this paper, we explore whether a video grounding model can be learned without any paired annotations.","offset":2,"pro":0.2,"labels":"PUR"},{"idx":5,"sentence":"To the best of our knowledge, this paper is the first work trying to address TVG in an unsupervised setting.","offset":3,"pro":0.3,"labels":"CTN"},{"idx":5,"sentence":"Considering there is no paired supervision, we propose a novel Deep Semantic Clustering Network (DSCNet) to leverage all semantic information from the whole query set to compose the possible activity in each video for grounding.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":5,"sentence":"Specifically, we first develop a language semantic mining module, which extracts implicit semantic features from the whole query set.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":5,"sentence":"Then, these language semantic features serve as the guidance to compose the activity in video via a video-based semantic aggregation module.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":5,"sentence":"Finally, we utilize a foreground attention branch to filter out the redundant background activities and refine the grounding results.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":5,"sentence":"To validate the effectiveness of our DSCNet, we conduct experiments on both ActivityNet Captions and Charades-STA datasets.","offset":8,"pro":0.8,"labels":"MTD"},{"idx":5,"sentence":"The results demonstrate that our DSCNet achieves competitive performance, and even outperforms most weakly-supervised approaches.","offset":9,"pro":0.9,"labels":"RST"},{"idx":6,"sentence":"Spiking Neural Networks (SNNs) have recently attracted enormous research interest since their event-driven and brain-inspired structure enables low-power computation.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"In image recognition tasks, the best results are achieved by SNN so far utilizing ANN-SNN conversion methods that replace activation functions in artificial neural networks~(ANNs) with integrate-and-fire neurons.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":6,"sentence":"Compared to source ANNs, converted SNNs usually suffer from accuracy loss and require a considerable number of time steps to achieve competitive accuracy.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":6,"sentence":"We find that the performance degradation of converted SNN stems from the fact that the information capacity of spike trains in transferred networks is smaller than that of activation values in source ANN, resulting in less information being passed during SNN inference.","offset":3,"pro":0.375,"labels":"CLN"},{"idx":6,"sentence":"To better correlate ANN and SNN for better performance, we propose a conversion framework to mitigate the gap between the activation value of source ANN and the generated spike train of target SNN.","offset":4,"pro":0.5,"labels":"PUR"},{"idx":6,"sentence":"The conversion framework originates from exploring an identical relation in the conversion and exploits temporal separation scheme and novel neuron model for the relation to hold.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":6,"sentence":"We demonstrate almost lossless ANN-SNN conversion using SpikeConverter for VGG-16, ResNet-20/34, and MobileNet-v2 SNNs on challenging datasets including CIFAR-10, CIFAR-100, and ImageNet.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":6,"sentence":"Our results also show that SpikeConverter achieves the abovementioned accuracy across different network architectures and datasets using 32X - 512X fewer inference time-steps than state-of-the-art ANN-SNN conversion methods.","offset":7,"pro":0.875,"labels":"RST"},{"idx":7,"sentence":"We introduce Perceiving Stroke-Semantic Context (PerSec), a new approach to self-supervised representation learning tailored for Scene Text Recognition (STR) task.","offset":0,"pro":0,"labels":"PUR"},{"idx":7,"sentence":"Considering scene text images carry both visual and semantic properties, we equip our PerSec with dual context perceivers which can contrast and learn latent representations from low-level stroke and high-level semantic contextual spaces simultaneously via hierarchical contrastive learning on unlabeled text image data.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":7,"sentence":"Experiments in un- and semi-supervised learning settings on STR benchmarks demonstrate our proposed framework can yield a more robust representation for both CTC-based and attention-based decoders than other contrastive learning methods.","offset":2,"pro":0.3333333333333333,"labels":"CLN"},{"idx":7,"sentence":"To fully investigate the potential of our method, we also collect a dataset of 100 million unlabeled text images, named UTI-100M, covering 5 scenes and 4 languages.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":7,"sentence":"By leveraging hundred-million-level unlabeled data, our PerSec shows significant performance improvement when fine-tuning the learned representation on the labeled data.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":7,"sentence":"Furthermore, we observe that the representation learned by PerSec presents great generalization, especially under few labeled data scenes.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":8,"sentence":"Within the field of face recognition (FR), it is widely accepted that the key objective is to optimize the entire feature space in the training process and acquire robust feature representations.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"However, most real-world FR systems tend to operate at a pre-defined False Accept Rate (FAR), and the corresponding True Accept Rate (TAR) represents the performance of the FR systems, which indicates that the optimization on the pre-defined FAR is more meaningful and important in the practical evaluation process.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":8,"sentence":"In this paper, we call the predefined FAR as Anchor FAR, and we argue that the existing FR loss functions cannot guarantee the optimal TAR under the Anchor FAR, which impedes further improvements of FR systems.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":8,"sentence":"To this end, we propose AnchorFace to bridge the aforementioned gap between the training and practical evaluation process for FR.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":8,"sentence":"Given the Anchor FAR, AnchorFace can boost the performance of FR systems by directly optimizing the non-differentiable FR evaluation metrics.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":8,"sentence":"Specifically, in AnchorFace, we first calculate the similarities of the positive and negative pairs based on both the features of the current batch and the stored features in the maintained online-updating set.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":8,"sentence":"Then, we generate the differentiable TAR loss and FAR loss using a soften strategy.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":8,"sentence":"Our AnchorFace can be readily integrated into most existing FR loss functions, and extensive experimental results on multiple benchmark datasets demonstrate the effectiveness of AnchorFace.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":9,"sentence":"This paper considers deep visual recognition on long-tailed data.","offset":0,"pro":0,"labels":"PUR"},{"idx":9,"sentence":"To make our method general, we tackle two applied scenarios, i.e. , deep classification and deep metric learning.","offset":1,"pro":0.08333333333333333,"labels":"PUR"},{"idx":9,"sentence":"Under the long-tailed data distribution, the most classes (i.e., tail classes) only occupy relatively few samples and are prone to lack of within-class diversity.","offset":2,"pro":0.16666666666666666,"labels":"GAP"},{"idx":9,"sentence":"A radical solution is to augment the tail classes with higher diversity.","offset":3,"pro":0.25,"labels":"BAC"},{"idx":9,"sentence":"To this end, we introduce a simple and reliable method named Memory-based Jitter (MBJ).","offset":4,"pro":0.3333333333333333,"labels":"MTD"},{"idx":9,"sentence":"We observe that during training, the deep model constantly changes its parameters after every iteration, yielding the phenomenon of weight jitters.","offset":5,"pro":0.4166666666666667,"labels":"MTD"},{"idx":9,"sentence":"Consequentially, given a same image as the input, two historical editions of the model generate two different features in the deeply-embedded space, resulting in feature jitters.","offset":6,"pro":0.5,"labels":"BAC"},{"idx":9,"sentence":"Using a memory bank, we collect these (model or feature) jitters across multiple training iterations and get the so-called Memory-based Jitter.","offset":7,"pro":0.5833333333333334,"labels":"MTD"},{"idx":9,"sentence":"The accumulated jitters enhance the within-class diversity for the tail classes and consequentially improves long-tailed visual recognition.","offset":8,"pro":0.6666666666666666,"labels":"CLN"},{"idx":9,"sentence":"With slight modifications, MBJ is applicable for two fundamental visual recognition tasks, i.e., deep image classification and deep metric learning (on long-tailed data).","offset":9,"pro":0.75,"labels":"MTD"},{"idx":9,"sentence":"Extensive experiments on five long-tailed classification benchmarks and two deep metric learning benchmarks demonstrate significant improvement.","offset":10,"pro":0.8333333333333334,"labels":"CLN"},{"idx":9,"sentence":"Moreover, the achieved performance are on par with the state of the art on both tasks.","offset":11,"pro":0.9166666666666666,"labels":"RST"},{"idx":10,"sentence":"Generalizable person re-identification aims to learn a model with only several labeled source domains that can perform well on unseen domains.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"Without access to the unseen domain, the feature statistics of the batch normalization (BN) layer learned from a limited number of source domains is doubtlessly biased for unseen domain.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":10,"sentence":"This would mislead the feature representation learning for unseen domain and deteriorate the generalizaiton ability of the model.","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":10,"sentence":"In this paper, we propose a novel Debiased Batch Normalization via Gaussian Process approach (GDNorm) for generalizable person re-identification, which models the feature statistic estimation from BN layers as a dynamically self-refining Gaussian process to alleviate the bias to unseen domain for improving the generalization.","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":10,"sentence":"Specifically, we establish a lightweight model with multiple set of domain-specific BN layers to capture the discriminability of individual source domain, and learn the corresponding parameters of the domain-specific BN layers.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":10,"sentence":"These parameters of different source domains are employed to deduce a Gaussian process.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":10,"sentence":"We randomly sample several paths from this Gaussian process served as the BN estimations of potential new domains outside of existing source domains, which can further optimize these learned parameters from source domains, and estimate more accurate Gaussian process by them in return, tending to real data distribution.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":10,"sentence":"Even without a large number of source domains, GDNorm can still provide debiased BN estimation by using the mean path of the Gaussian process, while maintaining low computational cost during testing.","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":10,"sentence":"Extensive experiments demonstrate that our GDNorm effectively improves the generalization ability of the model on unseen domain.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":11,"sentence":"As a key component of talking face generation, lip movements generation determines the naturalness and coherence of the generated talking face video.","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"Prior literature mainly focuses on speech-to-lip generation while there is a paucity in text-to-lip (T2L) generation.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":11,"sentence":"T2L is a challenging task and existing end-to-end works depend on the attention mechanism and autoregressive (AR) decoding manner.","offset":2,"pro":0.2222222222222222,"labels":"BAC"},{"idx":11,"sentence":"However, the AR decoding manner generates current lip frame conditioned on frames generated previously, which inherently hinders the inference speed, and also has a detrimental effect on the quality of generated lip frames due to error propagation.","offset":3,"pro":0.3333333333333333,"labels":"GAP"},{"idx":11,"sentence":"This encourages the research of parallel T2L generation.","offset":4,"pro":0.4444444444444444,"labels":"BAC"},{"idx":11,"sentence":"In this work, we propose a parallel decoding model for fast and high-fidelity text-to-lip generation (ParaLip).","offset":5,"pro":0.5555555555555556,"labels":"PUR"},{"idx":11,"sentence":"Specifically, we predict the duration of the encoded linguistic features and model the target lip frames conditioned on the encoded linguistic features with their duration in a non-autoregressive manner.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":11,"sentence":"Furthermore, we incorporate the structural similarity index loss and adversarial learning to improve perceptual quality of generated lip frames and alleviate the blurry prediction problem.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":11,"sentence":"Extensive experiments conducted on GRID and TCD-TIMIT datasets demonstrate the superiority of proposed methods.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":12,"sentence":"We propose a novel zero-shot multi-frame image restoration method for removing unwanted obstruction elements (such as rains, snow, and moire patterns) that vary in successive frames.","offset":0,"pro":0,"labels":"PUR"},{"idx":12,"sentence":"It has three stages: transformer pre-training, zero-shot restoration, and hard patch refinement.","offset":1,"pro":0.14285714285714285,"labels":"MTD"},{"idx":12,"sentence":"Using the pre-trained transformers, our model is able to tell the motion difference between the true image information and the obstructing elements.","offset":2,"pro":0.2857142857142857,"labels":"RST"},{"idx":12,"sentence":"For zero-shot image restoration, we design a novel model, termed SiamTrans, which is constructed by Siamese transformers, encoders, and decoders.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":12,"sentence":"Each transformer has a temporal attention layer and several self-attention layers, to capture both temporal and spatial information of multiple frames.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":12,"sentence":"Only self-supervisedly pre-trained on the denoising task, SiamTrans is tested on three different low-level vision tasks (deraining, demoireing, and desnowing).","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":12,"sentence":"Compared with related methods, SiamTrans achieves the best performances, even outperforming those with supervised learning.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":13,"sentence":"Domain generalization typically requires data from multiple source domains for model learning.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"However, such strong assumption may not always hold in practice, especially in medical field where the data sharing is highly concerned and sometimes prohibitive due to privacy issue.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":13,"sentence":"This paper studies the important yet challenging single domain generalization problem, in which a model is learned under the worst-case scenario with only one source domain to directly generalize to different unseen target domains.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":13,"sentence":"We present a novel approach to address this problem in medical image segmentation, which extracts and integrates the semantic shape prior information of segmentation that are invariant across domains and can be well-captured even from single domain data to facilitate segmentation under distribution shifts.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":13,"sentence":"Besides, a test-time adaptation strategy with dual-consistency regularization is further devised to promote dynamic incorporation of these shape priors under each unseen domain to improve model generalizability.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":13,"sentence":"Extensive experiments on two medical image segmentation tasks demonstrate the consistent improvements of our method across various unseen domains, as well as its superiority over state-of-the-art approaches in addressing domain generalization under the worst-case scenario.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":14,"sentence":"Detecting 3D lanes from the camera is a rising problem for autonomous vehicles.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"In this task, the correct camera pose is the key to generating accurate lanes, which can transform an image from perspective-view to the top-view.","offset":1,"pro":0.09090909090909091,"labels":"MTD"},{"idx":14,"sentence":"With this transformation, we can get rid of the perspective effects so that 3D lanes would look similar and can accurately be fitted by low-order polynomials.","offset":2,"pro":0.18181818181818182,"labels":"MTD"},{"idx":14,"sentence":"However, mainstream 3D lane detectors rely on perfect camera poses provided by other sensors, which is expensive and encounters multi-sensor calibration issues.","offset":3,"pro":0.2727272727272727,"labels":"GAP"},{"idx":14,"sentence":"To overcome this problem, we propose to predict 3D lanes by estimating camera pose from a single image with a two-stage framework.","offset":4,"pro":0.36363636363636365,"labels":"PUR"},{"idx":14,"sentence":"The first stage aims at the camera pose task from perspective-view images.","offset":5,"pro":0.45454545454545453,"labels":"MTD"},{"idx":14,"sentence":"To improve pose estimation, we introduce an auxiliary 3D lane task and geometry constraints to benefit from multi-task learning, which enhances consistencies between 3D and 2D, as well as compatibility in the above two tasks.","offset":6,"pro":0.5454545454545454,"labels":"PUR"},{"idx":14,"sentence":"The second stage targets the 3D lane task.","offset":7,"pro":0.6363636363636364,"labels":"MTD"},{"idx":14,"sentence":"It uses previously estimated pose to generate top-view images containing distance-invariant lane appearances for predicting accurate 3D lanes.","offset":8,"pro":0.7272727272727273,"labels":"MTD"},{"idx":14,"sentence":"Experiments demonstrate that, without ground truth camera pose, our method outperforms the state-of-the-art perfect-camera-pose-based methods and has the fewest parameters and computations.","offset":9,"pro":0.8181818181818182,"labels":"CLN"},{"idx":14,"sentence":"Codes are available at https://github.com/liuruijin17/CLGo.","offset":10,"pro":0.9090909090909091,"labels":"CTN"},{"idx":15,"sentence":"We introduce the task of open-vocabulary visual instance search (OVIS).","offset":0,"pro":0,"labels":"PUR"},{"idx":15,"sentence":"Given an arbitrary textual search query, Open-vocabulary Visual Instance Search (OVIS) aims to return a ranked list of visual instances, i.e., image patches, that satisfies the search intent from an image database.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":15,"sentence":"The term ``open vocabulary'' means that there are neither restrictions to the visual instance to be searched nor restrictions to the word that can be used to compose the textual search query.","offset":2,"pro":0.25,"labels":"BAC"},{"idx":15,"sentence":"We propose to address such a search challenge via visual-semantic aligned representation learning (ViSA).","offset":3,"pro":0.375,"labels":"PUR"},{"idx":15,"sentence":"ViSA leverages massive image-caption pairs as weak image-level (not instance-level) supervision to learn a rich cross-modal semantic space where the representations of visual instances (not images) and those of textual queries are aligned, thus allowing us to measure the similarities between any visual instance and an arbitrary textual query.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":15,"sentence":"To evaluate the performance of ViSA, we build two datasets named OVIS40 and OVIS1600 and also introduce a pipeline for error analysis.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":15,"sentence":"Through extensive experiments on the two datasets, we demonstrate ViSA's ability to search for visual instances in images not available during training given a wide range of textual queries including those composed of uncommon words.","offset":6,"pro":0.75,"labels":"CLN"},{"idx":15,"sentence":"Experimental results show that ViSA achieves an mAP@50 of 27.8% on OVIS40 and achieves a recall@30 of 21.3% on OVIS1400 dataset under the most challenging settings.","offset":7,"pro":0.875,"labels":"RST"},{"idx":16,"sentence":"Although existing face anti-spoofing (FAS) methods achieve high accuracy in intra-domain experiments, their effects drop severely in cross-domain scenarios because of poor generalization.","offset":0,"pro":0,"labels":"GAP"},{"idx":16,"sentence":"Recently, multifarious techniques have been explored, such as domain generalization and representation disentanglement.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":16,"sentence":"However, the improvement is still limited by two issues: 1) It is difficult to perfectly map all faces to a shared feature space.","offset":2,"pro":0.2,"labels":"GAP"},{"idx":16,"sentence":"If faces from unknown domains are not mapped to the known region in the shared feature space, accidentally inaccurate predictions will be obtained.","offset":3,"pro":0.3,"labels":"GAP"},{"idx":16,"sentence":"2) It is hard to completely consider various spoof traces for disentanglement.","offset":4,"pro":0.4,"labels":"GAP"},{"idx":16,"sentence":"In this paper, we propose a Feature Generation and Hypothesis Verification framework to alleviate the two issues.","offset":5,"pro":0.5,"labels":"PUR"},{"idx":16,"sentence":"Above all, feature generation networks which generate hypotheses of real faces and known attacks are introduced for the first time in the FAS task.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":16,"sentence":"Subsequently, two hypothesis verification modules are applied to judge whether the input face comes from the real-face space and the real-face distribution respectively.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":16,"sentence":"Furthermore, some analyses of the relationship between our framework and Bayesian uncertainty estimation are given, which provides theoretical support for reliable defense in unknown domains.","offset":8,"pro":0.8,"labels":"MTD"},{"idx":16,"sentence":"Experimental results show our framework achieves promising results and outperforms the state-of-the-art approaches on extensive public datasets.","offset":9,"pro":0.9,"labels":"CLN"},{"idx":17,"sentence":"Though deep learning-based object detection methods have achieved promising results on the conventional datasets, it is still challenging to locate objects from the low-quality images captured in adverse weather conditions.","offset":0,"pro":0,"labels":"GAP"},{"idx":17,"sentence":"The existing methods either have difficulties in balancing the tasks of image enhancement and object detection, or often ignore the latent information beneficial for detection.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":17,"sentence":"To alleviate this problem, we propose a novel Image-Adaptive YOLO (IA-YOLO) framework, where each image can be adaptively enhanced for better detection performance.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":17,"sentence":"Specifically, a differentiable image processing (DIP) module is presented to take into account the adverse weather conditions for YOLO detector, whose parameters are predicted by a small convolutional neural network (CNN-PP).","offset":3,"pro":0.375,"labels":"MTD"},{"idx":17,"sentence":"We learn CNN-PP and YOLOv3 jointly in an end-to-end fashion, which ensures that CNN-PP can learn an appropriate DIP to enhance the image for detection in a weakly supervised manner.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":17,"sentence":"Our proposed IA-YOLO approach can adaptively process images in both normal and adverse weather conditions.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":17,"sentence":"The experimental results are very encouraging, demonstrating the effectiveness of our proposed IA-YOLO method in both foggy and low-light scenarios.","offset":6,"pro":0.75,"labels":"CLN"},{"idx":17,"sentence":"The source code can be found at https://github.com/wenyyu/Image-Adaptive-YOLO.","offset":7,"pro":0.875,"labels":"CTN"},{"idx":18,"sentence":"The task of audiovisual sound source localization has been well studied under constrained scenes, where the audio recordings are clean.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"However, in real world scenarios, audios are usually contaminated by off screen sound and background noise.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":18,"sentence":"They will interfere with the procedure of identifying desired sources and building visual sound connections, making previous studies nonapplicable.","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":18,"sentence":"In this work, we propose the Interference Eraser (IEr) framework, which tackles the problem of audiovisual sound source localization in the wild.","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":18,"sentence":"The key idea is to eliminate the interference by redefining and carving discriminative audio representations.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":18,"sentence":"Specifically, we observe that the previous practice of learning only a single audio representation is insufficient due to the additive nature of audio signals.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":18,"sentence":"We thus extend the audio representation with our Audio Instance Identifier module, which clearly distinguishes sounding instances when audio signals of different volumes are unevenly mixed.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":18,"sentence":"Then we erase the influence of the audible but off screen sounds and the silent but visible objects by a Cross modal Referrer module with cross modality distillation.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":18,"sentence":"Quantitative and qualitative evaluations demonstrate that our framework achieves superior results on sound localization tasks, especially under real world scenarios.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":19,"sentence":"Monocular 3D object detection aims to localize 3D bounding boxes in an input single 2D image.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"It is a highly challenging problem and remains open, especially when no extra information (e.g., depth, lidar and/or multi-frames) can be leveraged in training and/or inference.","offset":1,"pro":0.08333333333333333,"labels":"GAP"},{"idx":19,"sentence":"This paper proposes a simple yet effective formulation for monocular 3D object detection without exploiting any extra information.","offset":2,"pro":0.16666666666666666,"labels":"PUR"},{"idx":19,"sentence":"It presents the MonoCon method which learns Monocular Contexts, as auxiliary tasks in training, to help monocular 3D object detection.","offset":3,"pro":0.25,"labels":"MTD"},{"idx":19,"sentence":"The key idea is that with the annotated 3D bounding boxes of objects in an image, there is a rich set of well-posed projected 2D supervision signals available in training, such as the projected corner keypoints and their associated offset vectors with respect to the center of 2D bounding box, which should be exploited as auxiliary tasks in training.","offset":4,"pro":0.3333333333333333,"labels":"MTD"},{"idx":19,"sentence":"The proposed MonoCon is motivated by the Cramer–Wold theorem in measure theory at a high level.","offset":5,"pro":0.4166666666666667,"labels":"MTD"},{"idx":19,"sentence":"In implementation, it utilizes a very simple end-to-end design to justify the effectiveness of learning auxiliary monocular contexts, which consists of three components: a Deep Neural Network (DNN) based feature backbone, a number of regression head branches for learning the essential parameters used in the 3D bounding box prediction, and a number of regression head branches for learning auxiliary contexts.","offset":6,"pro":0.5,"labels":"MTD"},{"idx":19,"sentence":"After training, the auxiliary context regression branches are discarded for better inference efficiency.","offset":7,"pro":0.5833333333333334,"labels":"MTD"},{"idx":19,"sentence":"In experiments, the proposed MonoCon is tested in the KITTI benchmark (car, pedestrian and cyclist).","offset":8,"pro":0.6666666666666666,"labels":"MTD"},{"idx":19,"sentence":"It outperforms all prior arts in the leaderboard on the car category and obtains comparable performance on pedestrian and cyclist in terms of accuracy.","offset":9,"pro":0.75,"labels":"RST"},{"idx":19,"sentence":"Thanks to the simple design, the proposed MonoCon method obtains the fastest inference speed with 38.7 fps in comparisons.","offset":10,"pro":0.8333333333333334,"labels":"RST"},{"idx":19,"sentence":"Our code is released at https://git.io/MonoCon.","offset":11,"pro":0.9166666666666666,"labels":"CTN"},{"idx":0,"sentence":"Obtaining a well-trained model involves expensive data collection and training procedures, therefore the model is a valuable intellectual property.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"Recent studies revealed that adversaries can `steal' deployed models even when they have no training samples and can not get access to the model parameters or structures.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":0,"sentence":"Currently, there were some defense methods to alleviate this threat, mostly by increasing the cost of model stealing.","offset":2,"pro":0.2,"labels":"BAC"},{"idx":0,"sentence":"In this paper, we explore the defense from another angle by verifying whether a suspicious model contains the knowledge of defender-specified external features.","offset":3,"pro":0.3,"labels":"PUR"},{"idx":0,"sentence":"Specifically, we embed the external features by tempering a few training samples with style transfer.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":0,"sentence":"We then train a meta-classifier to determine whether a model is stolen from the victim.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":0,"sentence":"This approach is inspired by the understanding that the stolen models should contain the knowledge of features learned by the victim model.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":0,"sentence":"We examine our method on both CIFAR-10 and ImageNet datasets.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":0,"sentence":"Experimental results demonstrate that our method is effective in detecting different types of model stealing simultaneously, even if the stolen model is obtained via a multi-stage stealing process.","offset":8,"pro":0.8,"labels":"CLN"},{"idx":0,"sentence":"The codes for reproducing main results are available at Github (https://github.com/zlh-thu/StealingVerification).","offset":9,"pro":0.9,"labels":"CTN"},{"idx":1,"sentence":"Orthogonality regularization has proven effective in improving the precision, convergence speed and the training stability of CNNs.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"Here, we propose a novel Orthogonal Dictionary Convolution Strategy (ODCS) on CNNs to improve orthogonality effect by optimizing the network architecture and changing the regularized object.","offset":1,"pro":0.1111111111111111,"labels":"PUR"},{"idx":1,"sentence":"Specifically, we remove the nonlinear layer in typical convolution block “Conv(BN) + Nonlinear + Pointwise Conv(BN)”, and only impose orthogonal regularization on the front Conv.","offset":2,"pro":0.2222222222222222,"labels":"MTD"},{"idx":1,"sentence":"The structure, “Conv(BN) + Pointwise Conv(BN)”, is then equivalent to a pair of dictionary and encoding, defined in sparse dictionary learning.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":1,"sentence":"Thanks to the exact and efficient representation of signal with dictionaries in low-dimensional projections, our strategy could reduce the superfluous information in dictionary Conv kernels.","offset":4,"pro":0.4444444444444444,"labels":"RST"},{"idx":1,"sentence":"Meanwhile, the proposed strategy relieves the too strict orthogonality regularization in training, which makes hyper-parameters tuning of model to be more flexible.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":1,"sentence":"In addition, our ODCS can modify the state-of-the-art models easily without any extra consumption in inference phase.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":1,"sentence":"We evaluate it on a variety of CNNs in small-scale (CIFAR), large-scale (ImageNet) and fine-grained (CUB-200-2011) image classification tasks, respectively.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":1,"sentence":"The experimental results show that our method achieve a stable and superior improvement.","offset":8,"pro":0.8888888888888888,"labels":"RST"},{"idx":2,"sentence":"This paper describes an energy-based learning method that predicts the activities of multiple agents simultaneously.","offset":0,"pro":0,"labels":"PUR"},{"idx":2,"sentence":"It aims to forecast both upcoming actions and paths of all agents in a scene based on their past activities, which can be jointly formulated by a probabilistic model over time.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":2,"sentence":"Learning this model is challenging because: 1) it has a large number of time-dependent variables that must scale with the forecast horizon and the number of agents; 2) distribution functions have to contain multiple modes in order to capture the spatio-temporal complexities of each agent's activities.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":2,"sentence":"To address these challenges, we put forth a novel Energy-based Learning approach for Multi-Agent activity forecasting (ELMA) to estimate this complex model via maximum log-likelihood estimation.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":2,"sentence":"Specifically, by sampling from a sequence of factorized marginalized multi-model distributions, ELMA generates most possible future actions efficiently.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":2,"sentence":"Moreover, by graph-based representations, ELMA also explicitly resolves the spatio-temporal dependencies of all agents' activities in a single pass.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":2,"sentence":"Our experiments on two large-scale datasets prove that ELMA outperforms recent leading studies by an obvious margin.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":3,"sentence":"Binary networks are extremely efficient as they use only two symbols to define the network: {+1, −1}.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"One can make the prior distribution of these symbols a design choice.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":3,"sentence":"The recent IR-Net of Qin et al. argues that imposing a Bernoulli distribution with equal priors (equal bit ratios) over the binary weights leads to maximum entropy and thus minimizes information loss.","offset":2,"pro":0.25,"labels":"BAC"},{"idx":3,"sentence":"However, prior work cannot precisely control the binary weight distribution during training, and therefore cannot guarantee maximum entropy.","offset":3,"pro":0.375,"labels":"GAP"},{"idx":3,"sentence":"Here, we show that quantizing using optimal transport can guarantee any bit ratio, including equal ratios.","offset":4,"pro":0.5,"labels":"PUR"},{"idx":3,"sentence":"We investigate experimentally that equal bit ratios are indeed preferable and show that our method leads to optimization benefits.","offset":5,"pro":0.625,"labels":"RST"},{"idx":3,"sentence":"We show that our quantization method is effective when compared to state-of-the-art binarization methods, even when using binary weight pruning.","offset":6,"pro":0.75,"labels":"CLN"},{"idx":3,"sentence":"Our code is available at https://github.com/liyunqianggyn/Equal-Bits-BNN.","offset":7,"pro":0.875,"labels":"CTN"},{"idx":4,"sentence":"Pre-training has become a standard paradigm in many computer vision tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"However, most of the methods are generally designed on the RGB image domain.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":4,"sentence":"Due to the discrepancy between the two-dimensional image plane and the three-dimensional space, such pre-trained models fail to perceive spatial information and serve as sub-optimal solutions for 3D-related tasks.","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":4,"sentence":"To bridge this gap, we aim to learn a spatial-aware visual representation that can describe the three-dimensional space and is more suitable and effective for these tasks.","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":4,"sentence":"To leverage point clouds, which are much more superior in providing spatial information compared to images, we propose a simple yet effective 2D Image and 3D Point cloud Unsupervised pre-training strategy, called SimIPU.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":4,"sentence":"Specifically, we develop a multi-modal contrastive learning framework that consists of an intra-modal spatial perception module to learn a spatial-aware representation from point clouds and an inter-modal feature interaction module to transfer the capability of perceiving spatial information from the point cloud encoder to the image encoder, respectively.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":4,"sentence":"Positive pairs for contrastive losses are established by the matching algorithm and the projection matrix.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":4,"sentence":"The whole framework is trained in an unsupervised end-to-end fashion.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":4,"sentence":"To the best of our knowledge, this is the first study to explore contrastive learning pre-training strategies for outdoor multi-modal datasets, containing paired camera images and LIDAR point clouds.","offset":8,"pro":0.8888888888888888,"labels":"CTN"},{"idx":5,"sentence":"Human-Object Interaction (HOI) detection is a fundamental task in high-level human-centric scene understanding.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"We propose PhraseHOI, containing a HOI branch and a novel phrase branch, to leverage language prior and improve relation expression.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":5,"sentence":"Specifically, the phrase branch is supervised by semantic embeddings, whose ground truths are automatically converted from the original HOI annotations without extra human efforts.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":5,"sentence":"Meanwhile, a novel label composition method is proposed to deal with the long-tailed problem in HOI, which composites novel phrase labels by semantic neighbors.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":5,"sentence":"Further, to optimize the phrase branch, a loss composed of a distilling loss and a balanced triplet loss is proposed.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":5,"sentence":"Extensive experiments are conducted to prove the effectiveness of the proposed PhraseHOI, which achieves significant improvement over the baseline and surpasses previous state-of-the-art methods on Full and NonRare on the challenging HICO-DET benchmark.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":6,"sentence":"Optimising the approximation of Average Precision (AP) has been widely studied for image retrieval.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"Limited by the definition of AP, such methods consider both negative and positive instances ranking before each positive instance.","offset":1,"pro":0.07142857142857142,"labels":"BAC"},{"idx":6,"sentence":"However, we claim that only penalizing negative instances before positive ones is enough, because the loss only comes from these negative instances.","offset":2,"pro":0.14285714285714285,"labels":"GAP"},{"idx":6,"sentence":"To this end, we propose a novel loss, namely Penalizing Negative instances before Positive ones (PNP), which can directly minimize the number of negative instances before each positive one.","offset":3,"pro":0.21428571428571427,"labels":"PUR"},{"idx":6,"sentence":"In addition, AP-based methods adopt a fixed and sub-optimal gradient assignment strategy.","offset":4,"pro":0.2857142857142857,"labels":"MTD"},{"idx":6,"sentence":"Therefore, we systematically investigate different gradient assignment solutions via constructing derivative functions of the loss, resulting in PNP-I with increasing derivative functions and PNP-D with decreasing ones.","offset":5,"pro":0.35714285714285715,"labels":"MTD"},{"idx":6,"sentence":"PNP-I focuses more on the hard positive instances by assigning larger gradients to them and tries to make all relevant instances closer.","offset":6,"pro":0.42857142857142855,"labels":"MTD"},{"idx":6,"sentence":"In contrast, PNP-D pays less attention to such instances and slowly corrects them.","offset":7,"pro":0.5,"labels":"MTD"},{"idx":6,"sentence":"For most real-world data, one class usually contains several local clusters.","offset":8,"pro":0.5714285714285714,"labels":"BAC"},{"idx":6,"sentence":"PNP-I blindly gathers these clusters while PNP-D keeps them as they were.","offset":9,"pro":0.6428571428571429,"labels":"MTD"},{"idx":6,"sentence":"Therefore, PNP-D is more superior.","offset":10,"pro":0.7142857142857143,"labels":"RST"},{"idx":6,"sentence":"Experiments on three standard retrieval datasets show consistent results with the above analysis.","offset":11,"pro":0.7857142857142857,"labels":"CLN"},{"idx":6,"sentence":"Extensive evaluations demonstrate that PNP-D achieves the state-of-the-art performance.","offset":12,"pro":0.8571428571428571,"labels":"CLN"},{"idx":6,"sentence":"Code is available at https://github.com/interestingzhuo/PNPloss","offset":13,"pro":0.9285714285714286,"labels":"CTN"},{"idx":7,"sentence":"Person re-identifcation (Re-ID) based on unsupervised domain adaptation (UDA) aims to transfer the pre-trained model from one labeled source domain to an unlabeled target domain.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"Existing methods tackle this problem by using clustering methods to generate pseudo labels.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":7,"sentence":"However, pseudo labels produced by these techniques may be unstable and noisy, substantially deteriorating models’ performance.","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":7,"sentence":"In this paper, we propose a Reliability Exploration with Self-ensemble Learning (RESL) framework for domain adaptive person ReID.","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":7,"sentence":"First, to increase the feature diversity, multiple branches are presented to extract features from different data augmentations.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":7,"sentence":"Taking the temporally average model as a mean teacher model, online label refning is conducted by using its dynamic ensemble predictions from different branches as soft labels.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":7,"sentence":"Second, to combat the adverse effects of unreliable samples in clusters, sample reliability is estimated by evaluating the consistency of different clusters’ results, followed by selecting reliable instances for training and re-weighting sample contribution within Re-ID losses.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":7,"sentence":"A contrastive loss is also utilized with cluster-level memory features which are updated by the mean feature.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":7,"sentence":"The experiments demonstrate that our method can signifcantly surpass the state-of-the-art performance on the unsupervised domain adaptive person ReID.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":8,"sentence":"Discovering the underneath causal relations is the fundamental ability for reasoning about the surrounding environment and predicting the future states in the physical world.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"Counterfactual prediction from visual input, which requires simulating future states based on unrealized situations in the past, is a vital component in causal relation tasks.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":8,"sentence":"In this paper, we work on the confounders that have effect on the physical dynamics, including masses, friction coefficients, etc., to bridge relations between the intervened variable and the affected variable whose future state may be altered.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":8,"sentence":"We propose a neural network framework combining Global Causal Relation Attention (GCRA) and Confounder Transmission Structure (CTS).","offset":3,"pro":0.375,"labels":"MTD"},{"idx":8,"sentence":"The GCRA looks for the latent causal relations between different variables and estimates the confounders by capturing both spatial and temporal information.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":8,"sentence":"The CTS integrates and transmits the learnt confounders in a residual way, so that the estimated confounders can be encoded into the network as a constraint for object positions when performing counterfactual prediction.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":8,"sentence":"Without any access to ground truth information about confounders, our model outperforms the state-of-the-art method on various benchmarks by fully utilizing the constraints of confounders.","offset":6,"pro":0.75,"labels":"RST"},{"idx":8,"sentence":"Extensive experiments demonstrate that our model can generalize to unseen environments and maintain good performance.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":9,"sentence":"The one-shot multi-object tracking, which integrates object detection and ID embedding extraction into a unified network, has achieved groundbreaking results in recent years.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"However, current one-shot trackers solely rely on single-frame detections to predict candidate bounding boxes, which may be unreliable when facing disastrous visual degradation, e.g., motion blur, occlusions.","offset":1,"pro":0.1,"labels":"GAP"},{"idx":9,"sentence":"Once a target bounding box is mistakenly classified as background by the detector, the temporal consistency of its corresponding tracklet will be no longer maintained.","offset":2,"pro":0.2,"labels":"GAP"},{"idx":9,"sentence":"In this paper, we set out to restore the bounding boxes misclassified as ``fake background'' by proposing a re-check network.","offset":3,"pro":0.3,"labels":"PUR"},{"idx":9,"sentence":"The re-check network innovatively expands the role of ID embedding from data association to motion forecasting by effectively propagating previous tracklets to the current frame with a small overhead.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":9,"sentence":"Note that the propagation results are yielded by an independent and efficient embedding search, preventing the model from over-relying on detection results.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":9,"sentence":"Eventually, it helps to reload the ``fake background'' and repair the broken tracklets.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":9,"sentence":"Building on a strong baseline CSTrack, we construct a new one-shot tracker and achieve favorable gains by 70.7 ➡ 76.4, 70.6 ➡ 76.3 MOTA on MOT16 and MOT17, respectively.","offset":7,"pro":0.7,"labels":"RST"},{"idx":9,"sentence":"It also reaches a new state-of-the-art MOTA and IDF1 performance.","offset":8,"pro":0.8,"labels":"RST"},{"idx":9,"sentence":"Code is released at https://github.com/JudasDie/SOTS.","offset":9,"pro":0.9,"labels":"CTN"},{"idx":10,"sentence":"Low-light image enhancement (LLE) remains challenging due to the unfavorable prevailing low-contrast and weak-visibility problems of single RGB images.","offset":0,"pro":0,"labels":"GAP"},{"idx":10,"sentence":"In this paper, we respond to the intriguing learning-related question -- if leveraging both accessible unpaired over/underexposed images and high-level semantic guidance, can improve the performance of cutting-edge LLE models?","offset":1,"pro":0.125,"labels":"PUR"},{"idx":10,"sentence":"Here, we propose an effective semantically contrastive learning paradigm for LLE (namely SCL-LLE).","offset":2,"pro":0.25,"labels":"MTD"},{"idx":10,"sentence":"Beyond the existing LLE wisdom, it casts the image enhancement task as multi-task joint learning, where LLE is converted into three constraints of contrastive learning, semantic brightness consistency, and feature preservation for simultaneously ensuring the exposure, texture, and color consistency.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":10,"sentence":"SCL-LLE allows the LLE model to learn from unpaired positives (normal-light)/negatives (over/underexposed), and enables it to interact with the scene semantics to regularize the image enhancement network, yet the interaction of high-level semantic knowledge and the low-level signal prior is seldom investigated in previous methods.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":10,"sentence":"Training on readily available open data, extensive experiments demonstrate that our method surpasses the state-of-the-arts LLE models over six independent cross-scenes datasets.","offset":5,"pro":0.625,"labels":"CLN"},{"idx":10,"sentence":"Moreover, SCL-LLE's potential to benefit the downstream semantic segmentation under extremely dark conditions is discussed.","offset":6,"pro":0.75,"labels":"CLN"},{"idx":10,"sentence":"Source Code: https://github.com/LingLIx/SCL-LLE.","offset":7,"pro":0.875,"labels":"CTN"},{"idx":11,"sentence":"This self-supervision approach, termed as Continuity Perception Network (CPNet), solves the three tasks altogether and encourages the backbone network to learn local and long-ranged motion and context representations.","offset":0,"pro":0,"labels":"MTD"},{"idx":11,"sentence":"It outperforms prior arts on multiple downstream tasks, such as action recognition, video retrieval, and action localization.Additionally, the video continuity can be complementary to other coarse-grained video properties for representation learning, and integrating the proposed pretext task to prior arts can yield much performance gains.","offset":1,"pro":0.25,"labels":"RST"},{"idx":11,"sentence":"Recent self-supervised video representation learning methods have found significant success by exploring essential properties of videos, e.g. speed, temporal order, etc.","offset":2,"pro":0.5,"labels":"BAC"},{"idx":11,"sentence":"Specifically, we formulate three novel continuity-related pretext tasks, i.e. continuity justification, discontinuity localization, and missing section approximation, that jointly supervise a shared backbone for video representation learning. ","offset":3,"pro":0.75,"labels":"MTD"},{"idx":12,"sentence":"Inharmonious region localization aims to localize the region in a synthetic image which is incompatible with surrounding background.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"The inharmony issue is mainly attributed to the color and illumination inconsistency produced by image editing techniques.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":12,"sentence":"In this work, we tend to transform the input image to another color space to magnify the domain discrepancy between inharmonious region and background, so that the model can identify the inharmonious region more easily.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":12,"sentence":"To this end, we present a novel framework consisting of a color mapping module and an inharmonious region localization network, in which the former is equipped with a novel domain discrepancy magnification loss and the latter could be an arbitrary localization network.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":12,"sentence":"Extensive experiments on image harmonization dataset show the superiority of our designed framework.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":13,"sentence":"Occlusion is common in the actual 3D scenes, causing the boundary ambiguity of the targeted object.","offset":0,"pro":0,"labels":"GAP"},{"idx":13,"sentence":"This uncertainty brings difficulty for labeling and learning.","offset":1,"pro":0.09090909090909091,"labels":"GAP"},{"idx":13,"sentence":"Current 3D detectors predict the bounding box directly, regarding it as Dirac delta distribution.","offset":2,"pro":0.18181818181818182,"labels":"BAC"},{"idx":13,"sentence":"However, it does not fully consider such ambiguity.","offset":3,"pro":0.2727272727272727,"labels":"GAP"},{"idx":13,"sentence":"To deal with it, distribution learning is used to efficiently represent the boundary ambiguity.","offset":4,"pro":0.36363636363636365,"labels":"BAC"},{"idx":13,"sentence":"In this paper, we revise the common regression method by predicting the distribution of the 3D box and then present a distribution-aware regression (DAR) module for box refinement and localization quality estimation.","offset":5,"pro":0.45454545454545453,"labels":"PUR"},{"idx":13,"sentence":"It contains scale adaptive (SA) encoder and joint localization quality estimator (JLQE).","offset":6,"pro":0.5454545454545454,"labels":"MTD"},{"idx":13,"sentence":"With the adaptive receptive field, SA encoder refines discriminative features for precise distribution learning.","offset":7,"pro":0.6363636363636364,"labels":"MTD"},{"idx":13,"sentence":"JLQE provides a reliable location score by further leveraging the distribution statistics, correlating with the localization quality of the targeted object.","offset":8,"pro":0.7272727272727273,"labels":"MTD"},{"idx":13,"sentence":"Combining DAR module and the baseline VoteNet, we propose a novel 3D detector called DAVNet.","offset":9,"pro":0.8181818181818182,"labels":"MTD"},{"idx":13,"sentence":"Extensive experiments on both ScanNet V2 and SUN RGB-D datasets demonstrate that the proposed DAVNet achieves significant improvement and outperforms state-of-the-art 3D detectors.","offset":10,"pro":0.9090909090909091,"labels":"CLN"},{"idx":14,"sentence":"The vision-language navigation (VLN) task requires an agent to reach a target with the guidance of natural language instruction.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"Previous works learn to navigate step-by-step following an instruction.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":14,"sentence":"However, these works may fail to discriminate the similarities and discrepancies across instruction-trajectory pairs and ignore the temporal continuity of sub-instructions.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":14,"sentence":"These problems hinder agents from learning distinctive vision-and-language representations, harming the robustness and generalizability of the navigation policy.","offset":3,"pro":0.375,"labels":"GAP"},{"idx":14,"sentence":"In this paper, we propose a Contrastive Instruction-Trajectory Learning (CITL) framework that explores invariance across similar data samples and variance across different ones to learn distinctive representations for robust navigation.","offset":4,"pro":0.5,"labels":"PUR"},{"idx":14,"sentence":"Specifically, we propose: (1) a coarse-grained contrastive learning objective to enhance vision-and-language representations by contrasting semantics of full trajectory observations and instructions, respectively; (2) a fine-grained contrastive learning objective to perceive instructions by leveraging the temporal information of the sub-instructions; (3) a pairwise sample-reweighting mechanism for contrastive learning to mine hard samples and hence mitigate the influence of data sampling bias in contrastive learning.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":14,"sentence":"Our CITL can be easily integrated with VLN backbones to form a new learning paradigm and achieve better generalizability in unseen environments.","offset":6,"pro":0.75,"labels":"CTN"},{"idx":14,"sentence":"Extensive experiments show that the model with CITL surpasses the previous state-of-the-art methods on R2R, R4R, and RxR.","offset":7,"pro":0.875,"labels":"RST"},{"idx":15,"sentence":"When applying multi-instance learning (MIL) to make predictions for bags of instances, the prediction accuracy of an instance often depends on not only the instance itself but also its context in the corresponding bag.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"From the viewpoint of causal inference, such bag contextual prior works as a confounder and may result in model robustness and interpretability issues.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":15,"sentence":"Focusing on this problem, we propose a novel interventional multi-instance learning (IMIL) framework to achieve deconfounded instance-level prediction.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":15,"sentence":"Unlike traditional likelihood-based strategies, we design an Expectation-Maximization (EM) algorithm based on causal intervention, providing a robust instance selection in the training phase and suppressing the bias caused by the bag contextual prior.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":15,"sentence":"Experiments on pathological image analysis demonstrate that our IMIL method substantially reduces false positives and outperforms state-of-the-art MIL methods.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":16,"sentence":"Unsupervised Salient Object Detection (USOD) is a promising yet challenging task that aims to learn a salient object detection model without any ground-truth labels.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"Self-supervised learning based methods have achieved remarkable success recently and have become the dominant approach in USOD.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":16,"sentence":"However, we observed that two distribution biases of salient objects limit further performance improvement of the USOD methods, namely, contrast distribution bias and spatial distribution bias.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":16,"sentence":"Concretely, contrast distribution bias is essentially a confounder that makes images with similar high-level semantic contrast and/or low-level visual appearance contrast spuriously dependent, thus forming data-rich contrast clusters and leading the training process biased towards the data-rich contrast clusters in the data.","offset":3,"pro":0.375,"labels":"BAC"},{"idx":16,"sentence":"Spatial distribution bias means that the position distribution of all salient objects in a dataset is concentrated on the center of the image plane, which could be harmful to off-center objects prediction.","offset":4,"pro":0.5,"labels":"BAC"},{"idx":16,"sentence":"This paper proposes a causal based debiasing framework to disentangle the model from the impact of such biases.","offset":5,"pro":0.625,"labels":"PUR"},{"idx":16,"sentence":"Specifically, we use causal intervention to perform de-confounded model training to minimize the contrast distribution bias and propose an image-level weighting strategy that softly weights each image's importance according to the spatial distribution bias map.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":16,"sentence":"Extensive experiments on 6 benchmark datasets show that our method significantly outperforms previous unsupervised state-of-the-art methods and even surpasses some of the supervised methods, demonstrating our debiasing framework's effectiveness.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":17,"sentence":"Unsupervised video anomaly detection, a task that requires no labeled normal/abnormal training data in any form, is challenging yet of great importance to both industrial applications and academic research.","offset":0,"pro":0,"labels":"GAP"},{"idx":17,"sentence":"Existing methods typically follow an iterative pseudo label generation process.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":17,"sentence":"However, they lack a principled analysis of the impact of such pseudo label generation on training.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":17,"sentence":"Furthermore, the long-range temporal dependencies also has been overlooked, which is unreasonable since the definition of an abnormal event depends on the long-range temporal context.","offset":3,"pro":0.375,"labels":"GAP"},{"idx":17,"sentence":"To this end, first, we propose a causal graph to analyze the confounding effect of the pseudo label generation process.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":17,"sentence":"Then, we introduce a simple yet effective causal inference based framework to disentangle the noisy pseudo label's impact.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":17,"sentence":"Finally, we perform counterfactual based model ensemble that blends long-range temporal context with local image context in inference to make final anomaly detection.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":17,"sentence":"Extensive experiments on six standard benchmark datasets show that our proposed method significantly outperforms previous state-of-the-art methods, demonstrating our framework's effectiveness.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":18,"sentence":"As an essential step in the pathological diagnosis, histochemical staining can show specific tissue structure information and, consequently, assist pathologists in making accurate diagnoses.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"Clinical kidney histopathological analyses usually employ more than one type of staining: H&E, MAS, PAS, PASM, etc. However, due to the interference of colors among multiple stains, it is not easy to perform multiple staining simultaneously on one biological tissue.","offset":1,"pro":0.08333333333333333,"labels":"BAC"},{"idx":18,"sentence":"To address this problem, we propose a network based on unpaired training data to virtually generate multiple types of staining from one staining.","offset":2,"pro":0.16666666666666666,"labels":"PUR"},{"idx":18,"sentence":"Our method can preserve the content of input images while transferring them to multiple target styles accurately.","offset":3,"pro":0.25,"labels":"RST"},{"idx":18,"sentence":"To efficiently control the direction of stain transfer, we propose a style guided normalization (SGN).","offset":4,"pro":0.3333333333333333,"labels":"PUR"},{"idx":18,"sentence":"Furthermore, a multiple style encoding (MSE) is devised to represent the relationship among different staining styles dynamically.","offset":5,"pro":0.4166666666666667,"labels":"MTD"},{"idx":18,"sentence":"An improved one-hot label is also proposed to enhance the generalization ability and extendibility of our method.","offset":6,"pro":0.5,"labels":"MTD"},{"idx":18,"sentence":"Vast experiments have demonstrated that our model can achieve superior performance on a tiny dataset.","offset":7,"pro":0.5833333333333334,"labels":"RST"},{"idx":18,"sentence":"The results exhibit not only good performance but also great visualization and interpretability.","offset":8,"pro":0.6666666666666666,"labels":"RST"},{"idx":18,"sentence":"Especially, our method also achieves satisfactory results over cross-tissue, cross-staining as well as cross-task.","offset":9,"pro":0.75,"labels":"RST"},{"idx":18,"sentence":"We believe that our method will significantly influence clinical stain transfer and reduce the workload greatly for pathologists.","offset":10,"pro":0.8333333333333334,"labels":"IMP"},{"idx":18,"sentence":"Our code and Supplementary materials are available at https://github.com/linyiyang98/UMDST.","offset":11,"pro":0.9166666666666666,"labels":"CTN"},{"idx":0,"sentence":"Human-Object Interaction (HOI) detection plays a core role in activity understanding.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"As a compositional learning problem (human-verb-object), studying its generalization matters.","offset":1,"pro":0.09090909090909091,"labels":"BAC"},{"idx":0,"sentence":"However, widely-used metric mean average precision (mAP) fails to model the compositional generalization well.","offset":2,"pro":0.18181818181818182,"labels":"GAP"},{"idx":0,"sentence":"Thus, we propose a novel metric, mPD (mean Performance Degradation), as a complementary of mAP to evaluate the performance gap among compositions of different objects and the same verb.","offset":3,"pro":0.2727272727272727,"labels":"PUR"},{"idx":0,"sentence":"The idea is to prevent model from learning spurious object-verb correlations as a short-cut to over-fit the train set.","offset":4,"pro":0.36363636363636365,"labels":"MTD"},{"idx":0,"sentence":"In both conventional and zero-shot experiments, our method achieves decent improvements.","offset":5,"pro":0.45454545454545453,"labels":"RST"},{"idx":0,"sentence":"The code is available at https://github.com/Foruck/OC-Immunity.","offset":6,"pro":0.5454545454545454,"labels":"CTN"},{"idx":0,"sentence":"Surprisingly, mPD reveals that previous methods usually generalize poorly.","offset":7,"pro":0.6363636363636364,"labels":"MTD"},{"idx":0,"sentence":"With mPD as a cue, we propose Object Category (OC) Immunity to boost HOI generalization.","offset":8,"pro":0.7272727272727273,"labels":"MTD"},{"idx":0,"sentence":"To achieve OC-immunity, we propose an OC-immune network that decouples the inputs from OC, extracts OC-immune representations, and leverages uncertainty quantification to generalize to unseen objects.","offset":9,"pro":0.8181818181818182,"labels":"MTD"},{"idx":0,"sentence":"To fully evaluate the generalization, we design a new and more difficult benchmark, on which we present significant advantage.","offset":10,"pro":0.9090909090909091,"labels":"RST"},{"idx":1,"sentence":"Few-shot learning (FSL) aims to classify images under low-data regimes, where the conventional pooled global feature is likely to lose useful local characteristics.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"Recent work has achieved promising performances by using deep descriptors.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":1,"sentence":"They generally take all deep descriptors from neural networks into consideration while ignoring that some of them are useless in classification due to their limited receptive field, e.g., task-irrelevant descriptors could be misleading and multiple aggregative descriptors from background clutter could even overwhelm the object's presence.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":1,"sentence":"In this paper, we argue that a Mutual Nearest Neighbor (MNN) relation should be established to explicitly select the query descriptors that are most relevant to each task and discard less relevant ones from aggregative clutters in FSL.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":1,"sentence":"Specifically, we propose Discriminative Mutual Nearest Neighbor Neural Network (DMN4) for FSL.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":1,"sentence":"Extensive experiments demonstrate that our method outperforms the existing state-of-the-arts on both fine-grained and generalized datasets.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":2,"sentence":"As a popular deep neural networks (DNN) compression technique, knowledge distillation (KD) has attracted increasing attentions recently.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"Existing KD methods usually utilize one kind of knowledge in an intermediate layer of DNN for classification tasks to transfer useful information from cumbersome teacher networks to compact student networks.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":2,"sentence":"However, this paradigm is not very suitable for semantic segmentation, a comprehensive vision task based on both pixel-level and contextual information, since it cannot provide rich information for distillation.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":2,"sentence":"In this paper, we propose a novel multi-knowledge aggregation and transfer (MKAT) framework to comprehensively distill knowledge within an intermediate layer for semantic segmentation.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":2,"sentence":"Specifically, the proposed framework consists of three parts: Independent Transformers and Encoders module (ITE), Auxiliary Prediction Branch (APB), and Mutual Label Calibration (MLC) mechanism, which can take advantage of abundant knowledge from intermediate features.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":2,"sentence":"To demonstrate the effectiveness of our proposed approach, we conduct extensive experiments on three segmentation datasets: Pascal VOC, Cityscapes, and CamVid, showing that MKAT outperforms the other KD methods.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":3,"sentence":"In recent years, creative content generations like style transfer and neural photo editing have attracted more and more attention.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"Among these, cartoonization of real-world scenes has promising applications in entertainment and industry.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":3,"sentence":"Different from image translations focusing on improving the style effect of generated images, video cartoonization has additional requirements on the temporal consistency.","offset":2,"pro":0.2222222222222222,"labels":"BAC"},{"idx":3,"sentence":"In this paper, we propose a spatially-adaptive semantic alignment framework with perceptual motion consistency for coherent video cartoonization in an unsupervised manner.","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":3,"sentence":"The semantic alignment module is designed to restore deformation of semantic structure caused by spatial information lost in the encoder-decoder architecture.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":3,"sentence":"Furthermore, we introduce the spatio-temporal correlative map as a style-independent, global-aware regularization on perceptual motion consistency.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":3,"sentence":"Deriving from similarity measurement of high-level features in photo and cartoon frames, it captures global semantic information beyond raw pixel-value of optical flow.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":3,"sentence":"Besides, the similarity measurement disentangles temporal relationship from domain-specific style properties, which helps regularize the temporal consistency without hurting style effects of cartoon images.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":3,"sentence":"Qualitative and quantitative experiments demonstrate our method is able to generate highly stylistic and temporal consistent cartoon videos.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":4,"sentence":"Self-supervised learning (SSL), especially contrastive methods, has raised attraction recently as it learns effective transferable representations without semantic annotations.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"A common practice for self-supervised pre-training is to use as much data as possible.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":4,"sentence":"For a specific downstream task, however, involving irrelevant data in pre-training may degenerate the downstream performance, observed from our extensive experiments.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":4,"sentence":"On the other hand, for existing SSL methods, it is burdensome and infeasible to use different downstream-task-customized datasets in pre-training for different tasks.","offset":3,"pro":0.375,"labels":"GAP"},{"idx":4,"sentence":"To address this issue, we propose a novel SSL paradigm called Scalable Dynamic Routing (SDR), which can be trained once and deployed efficiently to different downstream tasks with task-customized pre-trained models.","offset":4,"pro":0.5,"labels":"PUR"},{"idx":4,"sentence":"Specifically, we construct the SDRnet with various sub-nets and train each sub-net with only one subset of the data by data-aware progressive training.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":4,"sentence":"When a downstream task arrives, we route among all the pre-trained sub-nets to get the best along with its corresponding weights.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":4,"sentence":"Experiment results show that our SDR can train 256 sub-nets on ImageNet simultaneously, which provides better transfer performance than a unified model trained on the full ImageNet, achieving state-of-the-art (SOTA) averaged accuracy over 11 downstream classification tasks and AP on PASCAL VOC detection task.","offset":7,"pro":0.875,"labels":"RST"},{"idx":5,"sentence":"Generating new images with desired properties (e.g. new view/poses) from source images has been enthusiastically pursued recently, due to its wide range of potential applications.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"One way to ensure high-quality generation is to use multiple sources with complementary information such as different views of the same object.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":5,"sentence":"However, as source images are often misaligned due to the large disparities among the camera settings, strong assumptions have been made in the past with respect to the camera(s) or/and the object in interest, limiting the application of such techniques.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":5,"sentence":"Therefore, we propose a new general approach which models multiple types of variations among sources, such as view angles, poses, facial expressions, in a unified framework, so that it can be employed on datasets of vastly different nature.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":5,"sentence":"We verify our approach on a variety of data including humans bodies, faces, city scenes and 3D objects.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":5,"sentence":"Both the qualitative and quantitative results demonstrate the better performance of our method than the state of the art. ","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":6,"sentence":"Open Set Recognition (OSR) has been an emerging topic.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"Besides recognizing predefined classes, the system needs to reject the unknowns.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":6,"sentence":"Prototype learning is a potential manner to handle the problem, as its ability to improve intra-class compactness of representations is much needed in discrimination between the known and the unknowns.","offset":2,"pro":0.2222222222222222,"labels":"BAC"},{"idx":6,"sentence":"In this work, we propose a novel Prototype Mining And Learning (PMAL) framework.","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":6,"sentence":"It has a prototype mining mechanism before the phase of optimizing embedding space, explicitly considering two crucial properties, namely high-quality and diversity of the prototype set.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":6,"sentence":"Concretely, a set of high-quality candidates are firstly extracted from training samples based on data uncertainty learning, avoiding the interference from unexpected noise.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":6,"sentence":"Considering the multifarious appearance of objects even in a single category, a diversity-based strategy for prototype set filtering is proposed.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":6,"sentence":"Accordingly, the embedding space can be better optimized to discriminate therein the predefined classes and between known and unknowns.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":6,"sentence":"Extensive experiments verify the two good characteristics (i.e., high-quality and diversity) embraced in prototype mining, and show the remarkable performance of the proposed framework compared to state-of-the-arts.","offset":8,"pro":0.8888888888888888,"labels":"RST"},{"idx":7,"sentence":"This paper tackles the problem of semi-supervised learning when the set of labeled samples is limited to a small number of images per class, typically less than 10, problem that we refer to as barely-supervised learning.","offset":0,"pro":0,"labels":"PUR"},{"idx":7,"sentence":"We analyze in depth the behavior of a state-of-the-art semi-supervised method, FixMatch, which relies on a weakly-augmented version of an image to obtain supervision signal for a more strongly-augmented version.","offset":1,"pro":0.14285714285714285,"labels":"MTD"},{"idx":7,"sentence":"We propose a method to leverage self-supervised methods that provides training signal in the absence of confident pseudo-labels.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":7,"sentence":"We then propose two methods to refine the pseudo-label selection process which lead to further improvements.The first one relies on a per-sample history of the model predictions, akin to a voting scheme.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":7,"sentence":"The second iteratively up-dates class-dependent confidence thresholds to better explore classes that are under-represented in the pseudo-labels.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":7,"sentence":"Our experiments show that our approach performs significantly better on STL-10 in the barely-supervised regime,e.g. with 4 or 8 labeled images per class.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":7,"sentence":"We show that it frequently fails in barely-supervised scenarios, due to a lack of training signal when no pseudo-label can be predicted with high confidence.","offset":6,"pro":0.8571428571428571,"labels":"PUR"},{"idx":8,"sentence":"Estimating per-pixel motion between video frames, known as optical flow, is a long-standing problem in video understanding and analysis.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"Most contemporary optical flow techniques largely focus on addressing the cross-image matching with feature similarity, with few methods considering how to explicitly reason over the given scene for achieving a holistic motion understanding.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":8,"sentence":"In this work, taking a fresh perspective, we introduce a novel graph-based approach, called adaptive graph reasoning for optical flow (AGFlow), to emphasize the value of scene/context information in optical flow.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":8,"sentence":"Our key idea is to decouple the context reasoning from the matching procedure, and exploit scene information to effectively assist motion estimation by learning to reason over the adaptive graph.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":8,"sentence":"The proposed AGFlow can effectively exploit the context information and incorporate it within the matching procedure, producing more robust and accurate results.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":8,"sentence":"On both Sintel clean and final passes, our AGFlow achieves the best accuracy with EPE of 1.43 and 2.47 pixels, outperforming state-of-the-art approaches by 11.2% and 13.6%, respectively.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":8,"sentence":"Code is publicly available at https://github.com/megvii-research/AGFlow.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":9,"sentence":"InstaHide is a state-of-the-art mechanism for protecting private training images, by mixing multiple private images and modifying them such that their visual features are indistinguishable to the naked eye.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"In recent work, however, Carlini et al. show that it is possible to reconstruct private images from the encrypted dataset generated by InstaHide.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":9,"sentence":"This leads to a natural question: is InstaHide with data augmentation secure?","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":9,"sentence":"In this paper, we provide a negative answer to this question, by devising an attack for recovering private images from the outputs of InstaHide even when data augmentation is present.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":9,"sentence":"The basic idea is to use a comparative network to identify encrypted images that are likely to correspond to the same private image, and then employ a fusion-denoising network for restoring the private image from the encrypted ones, taking into account the effects of data augmentation.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":9,"sentence":"Extensive experiments demonstrate the effectiveness of the proposed attack in comparison to Carlini et al.’s attack.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":9,"sentence":"Nevertheless, we demonstrate that Carlini et al.’s attack can be easily defeated by incorporating data augmentation into InstaHide.","offset":6,"pro":0.8571428571428571,"labels":"BAC"},{"idx":10,"sentence":"How deep neural networks (DNNs) learn from noisy labels has been studied extensively in image classification but much less in image segmentation.","offset":0,"pro":0,"labels":"GAP"},{"idx":10,"sentence":"So far, our understanding of the learning behavior of DNNs trained by noisy segmentation labels remains limited.","offset":1,"pro":0.1,"labels":"GAP"},{"idx":10,"sentence":"In this study, we address this deficiency in both binary segmentation of biological microscopy images and multi-class segmentation of natural images.","offset":2,"pro":0.2,"labels":"PUR"},{"idx":10,"sentence":"We generate extremely noisy labels by randomly sampling a small fraction (e.g., 10%) or flipping a large fraction (e.g., 90%) of the ground truth labels.","offset":3,"pro":0.3,"labels":"MTD"},{"idx":10,"sentence":"When trained with these noisy labels, DNNs provide largely the same segmentation performance as trained by the original ground truth.","offset":4,"pro":0.4,"labels":"RST"},{"idx":10,"sentence":"This indicates that DNNs learn structures hidden in labels rather than pixel-level labels per se in their supervised training for semantic segmentation.","offset":5,"pro":0.5,"labels":"CLN"},{"idx":10,"sentence":"We refer to these hidden structures in labels as meta-structures.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":10,"sentence":"When DNNs are trained by labels with different perturbations to the meta-structure, we find consistent degradation in their segmentation performance.","offset":7,"pro":0.7,"labels":"RST"},{"idx":10,"sentence":"In contrast, incorporation of meta-structure information substantially improves performance of an unsupervised segmentation model developed for binary semantic segmentation.","offset":8,"pro":0.8,"labels":"CLN"},{"idx":10,"sentence":"We define meta-structures mathematically as spatial density distributions and show both theoretically and experimentally how this formulation explains key observed learning behavior of DNNs.","offset":9,"pro":0.9,"labels":"MTD"},{"idx":11,"sentence":"Large deformations of organs, caused by diverse shapes and nonlinear shape changes, pose a significant challenge for medical image registration.","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"Traditional registration methods need to iteratively optimize an objective function via a specific deformation model along with meticulous parameter tuning, but which have limited capabilities in registering images with large deformations.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":11,"sentence":"While deep learning-based methods can learn the complex mapping from input images to their respective deformation field, it is regression-based and is prone to be stuck at local minima, particularly when large deformations are involved.","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":11,"sentence":"To this end, we present Stochastic Planner-Actor-Critic (spac), a novel reinforcement learning-based framework that performs step-wise registration.","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":11,"sentence":"The key notion is warping a moving image successively by each time step to finally align to a fixed image.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":11,"sentence":"The entire framework is based on unsupervised training and operates in an end-to-end manner.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":11,"sentence":"We evaluate our method on several 2D and 3D medical image datasets, some of which contain large deformations.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":11,"sentence":"Our empirical results highlight that our work achieves consistent, significant gains and outperforms state-of-the-art methods.","offset":7,"pro":0.7777777777777778,"labels":"CLN"},{"idx":11,"sentence":"Considering that it is challenging to handle high dimensional continuous action and state spaces in the conventional reinforcement learning (RL) framework, we introduce a new concept `Plan' to the standard Actor-Critic model, which is of low dimension and can facilitate the actor to generate a tractable high dimensional action.","offset":8,"pro":0.8888888888888888,"labels":"MTD"},{"idx":12,"sentence":"Learning and generalizing from limited examples, i.e., few-shot learning, is of core importance to many real-world vision applications.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"A principal way of achieving few-shot learning is to realize an embedding where samples from different classes are distinctive.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":12,"sentence":"Recent studies suggest that embedding via hyperbolic geometry enjoys low distortion for hierarchical and structured data, making it suitable for few-shot learning.","offset":2,"pro":0.25,"labels":"BAC"},{"idx":12,"sentence":"In this paper, we propose to learn a context-aware hyperbolic metric to characterize the distance between a point and a set associated with a learned set to set distance.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":12,"sentence":"To this end, we formulate the metric as a weighted sum on the tangent bundle of the hyperbolic space and develop a mechanism to obtain the weights adaptively, based on the constellation of the points.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":12,"sentence":"This not only makes the metric local but also dependent on the task in hand, meaning that the metric will adapt depending on the samples that it compares.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":12,"sentence":"We empirically show that such metric yields robustness in the presence of outliers and achieves a tangible improvement over baseline models.","offset":6,"pro":0.75,"labels":"CLN"},{"idx":12,"sentence":"This includes the state-of-the-art results on five popular few-shot classification benchmarks, namely mini-ImageNet, tiered-ImageNet, Caltech-UCSD Birds-200-2011(CUB), CIFAR-FS, and FC100.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":13,"sentence":"Recently, deep learning techniques are soaring and have shown dramatic improvements in real-world noisy image denoising.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"However, the statistics of real noise generally vary with different camera sensors and in-camera signal processing pipelines.","offset":1,"pro":0.1,"labels":"GAP"},{"idx":13,"sentence":"This will induce problems of most deep denoisers for the overfitting or degrading performance due to the noise discrepancy between the training and test sets.","offset":2,"pro":0.2,"labels":"GAP"},{"idx":13,"sentence":"To remedy this issue, we propose a novel flexible and adaptive denoising network, coined as FADNet.","offset":3,"pro":0.3,"labels":"PUR"},{"idx":13,"sentence":"Our FADNet is equipped with a plane dynamic filter module, which generates weight filters with flexibility that can adapt to the specific input and thereby impedes the FADNet from overfitting to the training data.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":13,"sentence":"Specifically, we exploit the advantage of the spatial and channel attention, and utilize this to devise a decoupling filter generation scheme.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":13,"sentence":"The generated filters are conditioned on the input and collaboratively applied to the decoded features for representation capability enhancement.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":13,"sentence":"We additionally introduce the Fourier transform and its inverse to guide the predicted weight filters to adapt to the noisy input with respect to the image contents.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":13,"sentence":"Experimental results demonstrate the superior denoising performances of the proposed FADNet versus the state-of-the-art.","offset":8,"pro":0.8,"labels":"CLN"},{"idx":13,"sentence":"In contrast to the existing deep denoisers, our FADNet is not only flexible and efficient, but also exhibits a compelling generalization capability, enjoying tremendous potential for practical usage.","offset":9,"pro":0.9,"labels":"CLN"},{"idx":14,"sentence":"Existing approaches for 2D pose estimation in videos often require a large number of dense annotations, which are costly and labor intensive to acquire.","offset":0,"pro":0,"labels":"GAP"},{"idx":14,"sentence":"In this paper, we propose a semi-supervised REinforced MOtion Transformation nEtwork (REMOTE) to leverage a few labeled frames and temporal pose variations in videos, which enables effective learning of 2D pose estimation in sparsely annotated videos.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":14,"sentence":"Specifically, we introduce a Motion Transformer (MT) module to perform cross frame reconstruction, aiming to learn motion dynamic knowledge in videos.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":14,"sentence":"Besides, a novel reinforcement learning-based Frame Selection Agent (FSA) is designed within our framework, which is able to harness informative frame pairs on the fly to enhance the pose estimator under our cross reconstruction mechanism.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":14,"sentence":"We conduct extensive experiments that show the efficacy of our proposed REMOTE framework.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":15,"sentence":"Due to the scarcity of annotated samples, the diversity between support set and query set becomes the main obstacle for few shot semantic segmentation.","offset":0,"pro":0,"labels":"GAP"},{"idx":15,"sentence":"Along with the prototype extracted from the support set, we propose to build the pseudo-prototype based on foreground features in the query image.","offset":1,"pro":0.14285714285714285,"labels":"MTD"},{"idx":15,"sentence":"To achieve this goal, the cycle comparison module is developed to select reliable foreground features and generate the pseudo-prototype with them.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":15,"sentence":"Then, a prototype interaction module is utilized to integrate the information of the prototype and the pseudo-prototype based on their underlying correlation.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":15,"sentence":"Finally, a multi-scale fusion module is introduced to capture contextual information during the dense comparison between prototype (pseudo-prototype) and query feature.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":15,"sentence":"Extensive experiments conducted on two benchmarks demonstrate that our method exceeds previous state-of-the-arts with a sizable margin, verifying the effectiveness of the proposed method.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":15,"sentence":"Most existing prototype-based approaches only exploit the prototype from the support feature and ignore the information from the query sample, failing to remove this obstacle.","offset":6,"pro":0.8571428571428571,"labels":"GAP"},{"idx":16,"sentence":"Recent advances in generative adversarial networks (GANs) have led to remarkable achievements in face image synthesis.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"While methods that use style-based GANs can generate strikingly photorealistic face images, it is often difficult to control the characteristics of the generated faces in a meaningful and disentangled way.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":16,"sentence":"Prior approaches aim to achieve such semantic control and disentanglement within the latent space of a previously trained GAN.","offset":2,"pro":0.3333333333333333,"labels":"BAC"},{"idx":16,"sentence":"Our method, MOST-GAN, integrates the expressive power and photorealism of style-based GANs with the physical disentanglement and flexibility of nonlinear 3D morphable models, which we couple with a state-of-the-art 2D hair manipulation network.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":16,"sentence":"MOST-GAN achieves photorealistic manipulation of portrait images with fully disentangled 3D control over their physical attributes, enabling extreme manipulation of lighting, facial expression, and pose variations up to full profile view.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":16,"sentence":"In contrast, we propose a framework that a priori models physical attributes of the face such as 3D shape, albedo, pose, and lighting explicitly, thus providing disentanglement by design.","offset":5,"pro":0.8333333333333334,"labels":"PUR"},{"idx":17,"sentence":"In this paper, we give a new definition for sample complexity, and further develop a theoretical analysis to bridge the gap between sample complexity and model capacity.","offset":0,"pro":0,"labels":"PUR"},{"idx":17,"sentence":"In contrast to previous works which study on some toy samples, we conduct our analysis on more general data space, and build a qualitative relationship from sample complexity to model capacity required to achieve comparable performance.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":17,"sentence":"Besides, we introduce a simple indicator to evaluate the sample complexity based on continuous mapping.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":17,"sentence":"Moreover, we further analysis the relationship between sample complexity and data distribution, which paves the way to understand the present representation learning.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":17,"sentence":"Extensive experiments on several datasets well demonstrate the effectiveness of our evaluation method.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":18,"sentence":"Existing methods often formulate this problem as a 3D face reconstruction problem, which estimates the face attributes such as face identity and expression from face images.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"However, due to the lack of ground-truth labels for both identity and expression, most 3D-face reconstruction-based methods fail to capture the facial identity and expression accurately.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":18,"sentence":"As a result, these methods may not achieve promising performance.","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":18,"sentence":"To address this, we propose an identity-consistent constraint to learn accurate identities by encouraging consistent identity prediction across multiple frames.","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":18,"sentence":"Moreover, we further propose an expression-exclusive constraint to improve performance by avoiding the co-occurrence of contradictory expression units (e.g., ``brow lower'' vs. ``brow raise'').","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":18,"sentence":"Extensive experiments on facial motion retargeting and 3D face reconstruction tasks demonstrate the superiority of the proposed method over existing methods.","offset":5,"pro":0.5555555555555556,"labels":"CLN"},{"idx":18,"sentence":"Our code and supplementary materials are available at https://github.com/deepmo24/CPEM.","offset":6,"pro":0.6666666666666666,"labels":"CTN"},{"idx":18,"sentence":"We address the problem of facial motion retargeting that aims to transfer facial motion from a 2D face image to 3D characters.","offset":7,"pro":0.7777777777777778,"labels":"BAC"},{"idx":18,"sentence":"Based on a more accurate identity, we are able to obtain a more accurate facial expression.","offset":8,"pro":0.8888888888888888,"labels":"MTD"},{"idx":19,"sentence":"This question has become increasingly relevant in recent months because while current ViT pre-training tends to rely heavily on a large number of natural images and human-annotated labels, the recent use of natural images has resulted in problems related to privacy violation, inadequate fairness protection, and the need for labor-intensive annotations.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"In this paper, we experimentally verify that the results of formula-driven supervised learning (FDSL) framework are comparable with, and can even partially outperform, sophisticated self-supervised learning (SSL) methods like SimCLRv2 and MoCov2 without using any natural images in the pre-training phase.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":19,"sentence":"We also consider ways to reorganize FractalDB generation based on our tentative conclusion that there is room for configuration improvements in the iterated function system (IFS) parameter settings of such databases.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":19,"sentence":"Moreover, we show that while ViTs pre-trained without natural images produce visualizations that are somewhat different from ImageNet pre-trained ViTs, they can still interpret natural image datasets to a large extent.","offset":3,"pro":0.5,"labels":"RST"},{"idx":19,"sentence":"Finally, in experiments using the CIFAR-10 dataset, we show that our model achieved a performance rate of 97.8, which is comparable to the rate of 97.4 achieved with SimCLRv2 and 98.0 achieved with ImageNet.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":19,"sentence":"Is it possible to complete Vision Transformer (ViT) pre-training without natural images and human-annotated labels?","offset":5,"pro":0.8333333333333334,"labels":"BAC"},{"idx":0,"sentence":"With increasing appealing to privacy issues in face recognition, federated learning has emerged as one of the most prevalent approaches to study the unconstrained face recognition problem with private decentralized data.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"However, conventional decentralized federated algorithm sharing whole parameters of networks among clients suffers from privacy leakage in face recognition scene.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":0,"sentence":"In this work, we introduce a framework, FedGC, to tackle federated learning for face recognition and guarantees higher privacy.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":0,"sentence":"We explore a novel idea of correcting gradients from the perspective of backward propagation and propose a softmax-based regularizer to correct gradients of class embeddings by precisely injecting a cross-client gradient term.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":0,"sentence":"Theoretically, we show that FedGC constitutes a valid loss function similar to standard softmax.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":0,"sentence":"Extensive experiments have been conducted to validate the superiority of FedGC which can match the performance of conventional centralized methods utilizing full training dataset on several popular benchmark datasets.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":1,"sentence":"Image operators have been extensively applied to create visually attractive photos for users to share processed images on social media.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"However, most image operators often smooth out details or generate textures after the processing, which removes the original content and raises challenges for restoring the original image.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":1,"sentence":"To resolve this issue, we propose a quasi-invertible model that learns common image processing operators in a restorable fashion: the learned image operators can generate visually pleasing results with the original content embedded.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":1,"sentence":"Our model is trained on input-output pairs that represent an image processing operator's behavior and uses a network that consists of an invertible branch and a non-invertible branch to increase our model's approximation capability.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":1,"sentence":"We evaluate the proposed model on ten image operators, including detail enhancement, photographic style, and non-photorealistic style.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":1,"sentence":"Extensive experiments show that our approach outperforms relevant baselines in the restoration quality, and the learned restorable operator is fast in inference and robust to compression.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":1,"sentence":"Furthermore, we demonstrate that the invertible operator can be easily applied to practical applications such as restorable human face retouching and highlight preserved exposure adjustment.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":2,"sentence":"Robots operating in human spaces must be able to engage in natural language interaction, both understanding and executing instructions, and using conversation to resolve ambiguity and correct mistakes.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"To study this, we introduce TEACh, a dataset of over 3,000 human-human, interactive dialogues to complete household tasks in simulation.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":2,"sentence":"A Commander with access to oracle information about a task communicates in natural language with a Follower.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":2,"sentence":"The Follower navigates through and interacts with the environment to complete tasks varying in complexity from \"Make Coffee\" to \"Prepare Breakfast\", asking questions and getting additional information from the Commander.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":2,"sentence":"We propose three benchmarks using TEACh to study embodied intelligence challenges, and we evaluate initial models' abilities in dialogue understanding, language grounding, and task execution.","offset":4,"pro":0.8,"labels":"MTD"},{"idx":3,"sentence":"Due to the lack of expertise for medical image annotation, the investigation of label-efficient methodology for medical image segmentation becomes a heated topic.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"Recent progresses focus on the efficient utilization of weak annotations together with few strongly-annotated labels so as to achieve comparable segmentation performance in many unprofessional scenarios.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":3,"sentence":"However, these approaches only concentrate on the supervision inconsistency between strongly- and weakly-annotated instances but ignore the instance inconsistency inside the weakly-annotated instances, which inevitably leads to performance degradation.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":3,"sentence":"To address this problem, we propose a novel label-efficient hybrid-supervised framework, which considers each weakly-annotated instance individually and learns its weight guided by the gradient direction of the strongly-annotated instances, so that the high-quality prior in the strongly-annotated instances is better exploited and the weakly-annotated instances are depicted more precisely.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":3,"sentence":"Specially, our designed dynamic instance indicator (DII) realizes the above objectives, and is adapted to our dynamic co-regularization (DCR) framework further to alleviate the erroneous accumulation from distortions of weak annotations.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":3,"sentence":"Extensive experiments on two hybrid-supervised medical segmentation datasets demonstrate that with only 10% strong labels, the proposed framework can leverage the weak labels efficiently and achieve competitive performance against the 100% strong-label supervised scenario.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":4,"sentence":"Transformers have become one of the dominant architectures in deep learning, particularly as a powerful alternative to convolutional neural networks (CNNs) in computer vision.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"However, Transformer training and inference in previous works can be prohibitively expensive due to the quadratic complexity of self-attention over a long sequence of representations, especially for high-resolution dense prediction tasks.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":4,"sentence":"To this end, we present a novel Less attention vIsion Transformer (LIT), building upon the fact that the early self-attention layers in Transformers still focus on local patterns and bring minor benefits in recent hierarchical vision Transformers.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":4,"sentence":"Specifically, we propose a hierarchical Transformer where we use pure multi-layer perceptrons (MLPs) to encode rich local patterns in the early stages while applying self-attention modules to capture longer dependencies in deeper layers.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":4,"sentence":"Moreover, we further propose a learned deformable token merging module to adaptively fuse informative patches in a non-uniform manner.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":4,"sentence":"The proposed LIT achieves promising performance on image recognition tasks, including image classification, object detection and instance segmentation, serving as a strong backbone for many vision tasks.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":4,"sentence":"Code is available at https://github.com/zip-group/LIT.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":5,"sentence":"We study the unsupervised representation learning for the semantic segmentation task.","offset":0,"pro":0,"labels":"PUR"},{"idx":5,"sentence":"Different from previous works that aim at providing unsupervised pre-trained backbones for segmentation models which need further supervised fine-tune, here, we focus on providing representation that is only trained by unsupervised methods.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":5,"sentence":"This means models need to directly generate pixel-level, linearly separable semantic results.","offset":2,"pro":0.2857142857142857,"labels":"BAC"},{"idx":5,"sentence":"We first explore and present two factors that have significant effects on segmentation under the contrastive learning framework: 1) the difficulty and diversity of the positive contrastive pairs, 2) the balance of global and local features.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":5,"sentence":"With the intention of optimizing these factors, we propose the cycle-attention contrastive learning (CACL).","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":5,"sentence":"CACL makes use of semantic continuity of video frames, adopting unsupervised cycle-consistent attention mechanism to implicitly conduct contrastive learning with difficult, global-local-balanced positive pixel pairs.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":5,"sentence":"Compared with baseline model MoCo-v2 and other unsupervised methods, CACL demonstrates consistently superior performance on PASCAL VOC (+4.5 mIoU) and Cityscapes (+4.5 mIoU) datasets.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":6,"sentence":"In this paper, a new deep learning network named as graph-based point tracker (GPT) is proposed for 3D object tracking in point clouds.","offset":0,"pro":0,"labels":"PUR"},{"idx":6,"sentence":"GPT is not based on Siamese network applied to template and search area, but it is based on the transfer of target clue from the template to the search area.","offset":1,"pro":0.09090909090909091,"labels":"MTD"},{"idx":6,"sentence":"GPT is end-to-end trainable.","offset":2,"pro":0.18181818181818182,"labels":"MTD"},{"idx":6,"sentence":"GPT has two new modules: graph feature augmentation (GFA) and improved target clue (ITC) module.","offset":3,"pro":0.2727272727272727,"labels":"MTD"},{"idx":6,"sentence":"The key idea of GFA is to exploit one-to-many relationship between template and search area points using a bipartite graph.","offset":4,"pro":0.36363636363636365,"labels":"MTD"},{"idx":6,"sentence":"In GFA, edge features of the bipartite graph are generated by transferring the target clues of template points to search area points through edge convolution.","offset":5,"pro":0.45454545454545453,"labels":"MTD"},{"idx":6,"sentence":"It captures the relationship between template and search area points effectively from the perspective of geometry and shape of two point clouds.","offset":6,"pro":0.5454545454545454,"labels":"MTD"},{"idx":6,"sentence":"The second module is ITC.","offset":7,"pro":0.6363636363636364,"labels":"MTD"},{"idx":6,"sentence":"The key idea of ITC is to embed the information of the center of the target into the edges of the bipartite graph via Hough voting, strengthening the discriminative power of GFA.","offset":8,"pro":0.7272727272727273,"labels":"MTD"},{"idx":6,"sentence":"Both modules significantly contribute to the improvement of GPT by transferring geometric and shape information including target center from target template to search area effectively.","offset":9,"pro":0.8181818181818182,"labels":"MTD"},{"idx":6,"sentence":"Experiments on the KITTI tracking dataset show that GPT achieves state-of-the-art performance and can run in real-time.","offset":10,"pro":0.9090909090909091,"labels":"CLN"},{"idx":7,"sentence":"The challenge of talking face generation from speech lies in aligning two different modal information, audio and video, such that the mouth region corresponds to input audio.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"Previous methods either exploit audio-visual representation learning or leverage intermediate structural information such as landmarks and 3D models.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":7,"sentence":"However, they struggle to synthesize fine details of the lips varying at the phoneme level as they do not sufficiently provide visual information of the lips at the video synthesis step.","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":7,"sentence":"To overcome this limitation, our work proposes Audio-Lip Memory that brings in visual information of the mouth region corresponding to input audio and enforces fine-grained audio-visual coherence.","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":7,"sentence":"It stores lip motion features from sequential ground truth images in the value memory and aligns them with corresponding audio features so that they can be retrieved using audio input at inference time.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":7,"sentence":"Therefore, using the retrieved lip motion features as visual hints, it can easily correlate audio with visual dynamics in the synthesis step.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":7,"sentence":"By analyzing the memory, we demonstrate that unique lip features are stored in each memory slot at the phoneme level, capturing subtle lip motion based on memory addressing.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":7,"sentence":"In addition, we introduce visual-visual synchronization loss which can enhance lip-syncing performance when used along with audio-visual synchronization loss in our model.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":7,"sentence":"Extensive experiments are performed to verify that our method generates high-quality video with mouth shapes that best align with the input audio, outperforming previous state-of-the-art methods.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":8,"sentence":"Transformers, composed of multiple self-attention layers, hold strong promises toward a generic learning primitive applicable to different data modalities, including the recent breakthroughs in computer vision achieving state-of-the-art (SOTA) standard accuracy.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"What remains largely unexplored is their robustness evaluation and attribution.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":8,"sentence":"In this work, we study the robustness of the Vision Transformer (ViT) (Dosovitskiy et al. 2021) against common corruptions and perturbations, distribution shifts, and natural adversarial examples.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":8,"sentence":"We use six different diverse ImageNet datasets concerning robust classification to conduct a comprehensive performance comparison of ViT(Dosovitskiy et al. 2021) models and SOTA convolutional neural networks (CNNs), Big-Transfer (Kolesnikov et al. 2020).","offset":3,"pro":0.375,"labels":"MTD"},{"idx":8,"sentence":"Through a series of six systematically designed experiments, we then present analyses that provide both quantitative andqualitative indications to explain why ViTs are indeed more robust learners.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":8,"sentence":"For example, with fewer parameters and similar dataset and pre-training combinations, ViT gives a top-1accuracy of 28.10% on ImageNet-A which is 4.3x higher than a comparable variant of BiT.","offset":5,"pro":0.625,"labels":"RST"},{"idx":8,"sentence":"Our analyses on image masking, Fourier spectrum sensitivity, and spread on discrete cosine energy spectrum reveal intriguing properties of ViT attributing to improved robustness.","offset":6,"pro":0.75,"labels":"CLN"},{"idx":8,"sentence":"Code for reproducing our experiments is available at https://git.io/J3VO0.","offset":7,"pro":0.875,"labels":"CTN"},{"idx":9,"sentence":"Category-level 6D pose estimation can be better generalized to unseen objects in a category compared with instance-level 6D pose estimation.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"However, existing category-level 6D pose estimation methods usually require supervised training with a sufficient number of 6D pose annotations of objects which makes them difficult to be applied in real scenarios.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":9,"sentence":"To address this problem, we propose a self-supervised framework for category-level 6D pose estimation in this paper.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":9,"sentence":"We leverage DeepSDF as a 3D object representation and design several novel loss functions based on DeepSDF to help the self-supervised model predict unseen object poses without any 6D object pose labels and explicit 3D models in real scenarios.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":9,"sentence":"Experiments demonstrate that our method achieves comparable performance with the state-of-the-art fully supervised methods on the category-level NOCS benchmark.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":10,"sentence":"Training the multi-label image recognition models with partial labels, in which merely some labels are known while others are unknown for each image, is a considerably challenging and practical task.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"To address this task, current algorithms mainly depend on pre-training classification or similarity models to generate pseudo labels for the unknown labels.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":10,"sentence":"However, these algorithms depend on sufficient multi-label annotations to train the models, leading to poor performance especially with low known label proportion.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":10,"sentence":"In this work, we propose to blend category-specific representation across different images to transfer information of known labels to complement unknown labels, which can get rid of pre-training models and thus does not depend on sufficient annotations.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":10,"sentence":"To this end, we design a unified semantic-aware representation blending (SARB) framework that exploits instance-level and prototype-level semantic representation to complement unknown labels by two complementary modules: 1) an instance-level representation blending (ILRB) module blends the representations of the known labels in an image to the representations of the unknown labels in another image to complement these unknown labels.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":10,"sentence":"2) a prototype-level representation blending (PLRB) module learns more stable representation prototypes for each category and blends the representation of unknown labels with the prototypes of corresponding labels to complement these labels.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":10,"sentence":"Extensive experiments on the MS-COCO, Visual Genome, Pascal VOC 2007 datasets show that the proposed SARB framework obtains superior performance over current leading competitors on all known label proportion settings, i.e., with the mAP improvement of 4.6%, 4.6%, 2.2% on these three datasets when the known label proportion is 10%.","offset":6,"pro":0.75,"labels":"RST"},{"idx":10,"sentence":"Codes are available at https://github.com/HCPLab-SYSU/HCP-MLR-PL.","offset":7,"pro":0.875,"labels":"CTN"},{"idx":11,"sentence":"Exiting simple samples in adaptive multi-exit networks through early modules is an effective way to achieve high computational efficiency.","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"One can observe that deployments of multi-exit architectures on resource-constrained devices are easily limited by high memory footprint of early modules.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":11,"sentence":"In this paper, we propose a novel approach named recurrent aggregation operator (ReX), which uses recurrent neural networks (RNNs) to effectively aggregate intra-patch features within a large receptive field to get delicate local representations, while bypassing large early activations.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":11,"sentence":"The resulting model, named ReXNet, can be easily extended to dynamic inference by introducing a novel consistency-based early exit criteria, which is based on the consistency of classification decisions over several modules, rather than the entropy of the prediction distribution.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":11,"sentence":"Extensive experiments on two benchmark datasets, i.e., Visual Wake Words, ImageNet-1k, demonstrate that our method consistently reduces the peak RAM and average latency of a wide variety of adaptive models on low-power devices.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":12,"sentence":"Acquiring the most representative examples via active learning (AL) can benefit many data-dependent computer vision tasks by minimizing efforts of image-level or pixel-wise annotations.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"In this paper, we propose a novel Collaborative Panoptic-Regional Active Learning framework (CPRAL) to address the semantic segmentation task.","offset":1,"pro":0.125,"labels":"PUR"},{"idx":12,"sentence":"For a small batch of images initially sampled with pixel-wise annotations, we employ panoptic information to initially select unlabeled samples.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":12,"sentence":"Considering the class imbalance in the segmentation dataset, we import a Regional Gaussian Attention module (RGA) to achieve semantics-biased selection.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":12,"sentence":"The subset is highlighted by vote entropy and then attended by Gaussian kernels to maximize the biased regions.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":12,"sentence":"We also propose a Contextual Labels Extension (CLE) to boost regional annotations with contextual attention guidance.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":12,"sentence":"With the collaboration of semantics-agnostic panoptic matching and region-biased selection and extension, our CPRAL can strike a balance between labeling efforts and performance and compromise the semantics distribution.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":12,"sentence":"We perform extensive experiments on Cityscapes and BDD10K datasets and show that CPRAL outperforms the cutting-edge methods with impressive results and less labeling proportion.","offset":7,"pro":0.875,"labels":"RST"},{"idx":13,"sentence":"Image-level weakly supervised semantic segmentation (WSSS) is a fundamental yet challenging computer vision task facilitating scene understanding and automatic driving.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"Most existing methods resort to classification-based Class Activation Maps (CAMs) to play as the initial pseudo labels, which tend to focus on the discriminative image regions and lack customized characteristics for the segmentation task.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":13,"sentence":"To alleviate this issue, we propose a novel activation modulation and recalibration (AMR) scheme, which leverages a spotlight branch and a compensation branch to obtain weighted CAMs that can provide recalibration supervision and task-specific concepts.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":13,"sentence":"Specifically, an attention modulation module (AMM) is employed to rearrange the distribution of feature importance from the channel-spatial sequential perspective, which helps to explicitly model channel-wise interdependencies and spatial encodings to adaptively modulate segmentation-oriented activation responses.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":13,"sentence":"Furthermore, we introduce a cross pseudo supervision for dual branches, which can be regarded as a semantic similar regularization to mutually refine two branches.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":13,"sentence":"Extensive experiments show that AMR establishes a new state-of-the-art performance on the PASCAL VOC 2012 dataset, surpassing not only current methods trained with the image-level of supervision but also some methods relying on stronger supervision, such as saliency label.","offset":5,"pro":0.625,"labels":"RST"},{"idx":13,"sentence":"Experiments also reveal that our scheme is plug-and-play and can be incorporated with other approaches to boost their performance.","offset":6,"pro":0.75,"labels":"CLN"},{"idx":13,"sentence":"Our code is available at: https://github.com/jieqin-ai/AMR.","offset":7,"pro":0.875,"labels":"CTN"},{"idx":14,"sentence":"In this paper, we propose TransMEF, a transformer-based multi-exposure image fusion framework that uses self-supervised multi-task learning.","offset":0,"pro":0,"labels":"PUR"},{"idx":14,"sentence":"The framework is based on an encoder-decoder network, which can be trained on large natural image datasets and does not require ground truth fusion images.","offset":1,"pro":0.14285714285714285,"labels":"MTD"},{"idx":14,"sentence":"We design three self-supervised reconstruction tasks according to the characteristics of multi-exposure images and conduct these tasks simultaneously using multi-task learning; through this process, the network can learn the characteristics of multi-exposure images and extract more generalized features.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":14,"sentence":"In addition, to compensate for the defect in establishing long-range dependencies in CNN-based architectures, we design an encoder that combines a CNN module with a transformer module.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":14,"sentence":"This combination enables the network to focus on both local and global information.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":14,"sentence":"We evaluated our method and compared it to 11 competitive traditional and deep learning-based methods on the latest released multi-exposure image fusion benchmark dataset, and our method achieved the best performance in both subjective and objective evaluations.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":14,"sentence":"Code will be available at https://github.com/miccaiif/TransMEF.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":15,"sentence":"3D delineation of anatomical structures is a cardinal goal in medical imaging analysis.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"Prior to deep learning, statistical shape models (SSMs) that imposed anatomical constraints and produced high quality surfaces were a core technology.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":15,"sentence":"Today’s fully-convolutional networks (FCNs), while dominant, do not offer these capabilities.","offset":2,"pro":0.2,"labels":"GAP"},{"idx":15,"sentence":"We present deep implicit statistical shape models (DISSMs), a new approach that marries the representation power of deep networks with the benefits of SSMs.","offset":3,"pro":0.3,"labels":"PUR"},{"idx":15,"sentence":"DISSMs use an implicit representation to produce compact and descriptive deep surface embeddings that permit statistical models of anatomical variance.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":15,"sentence":"To reliably fit anatomically plausible shapes to an image, we introduce a novel rigid and non-rigid pose estimation pipeline that is modelled as a Markov decision process (MDP).","offset":5,"pro":0.5,"labels":"MTD"},{"idx":15,"sentence":"Intra-dataset experiments on the task of pathological liver segmentation demonstrate that DISSMs can perform more robustly than four leading FCN models, including nnU-Net + an adversarial prior: reducing the mean Hausdorff distance (HD) by 7.5-14.3 mm and improving the worst case Dice-Sørensen coefficient (DSC) by 1.2-2.3%.","offset":6,"pro":0.6,"labels":"RST"},{"idx":15,"sentence":"More critically, cross-dataset experiments on an external and highly challenging clinical dataset demonstrate that DISSMs improve the mean DSC and HD by 2.1-5.9% and 9.9-24.5 mm, respectively, and the worst-case DSC by 5.4-7.3%.","offset":7,"pro":0.7,"labels":"RST"},{"idx":15,"sentence":"Supplemental validation on a highly challenging and low-contrast larynx dataset further demonstrate DISSM’s improvements.","offset":8,"pro":0.8,"labels":"CLN"},{"idx":15,"sentence":"These improvements are over and above any benefits from representing delineations with high-quality surfaces.","offset":9,"pro":0.9,"labels":"IMP"},{"idx":16,"sentence":"In this paper, we propose a framework centering around a novel architecture called the Event Decomposition Recomposition Network (EDRNet) to tackle the Audio-Visual Event (AVE) localization problem in the supervised and weakly supervised settings.","offset":0,"pro":0,"labels":"PUR"},{"idx":16,"sentence":"AVEs in the real world exhibit common unraveling patterns (termed as Event Progress Checkpoints(EPC)), which humans can perceive through the cooperation of their auditory and visual senses.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":16,"sentence":"Unlike earlier methods which attempt to recognize entire event sequences, the EDRNet models EPCs and inter-EPC relationships using stacked temporal convolutions.","offset":2,"pro":0.2857142857142857,"labels":"BAC"},{"idx":16,"sentence":"Based on the postulation that EPC representations are theoretically consistent for an event category, we introduce the State Machine Based Video Fusion, a novel augmentation technique that blends source videos using different EPC template sequences.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":16,"sentence":"Additionally, we design a new loss function called the Land-Shore-Sea loss to compactify continuous foreground and background representations.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":16,"sentence":"Lastly, to alleviate the issue of confusing events during weak supervision, we propose a prediction stabilization method called Bag to Instance Label Correction.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":16,"sentence":"Experiments on the AVE dataset show that our collective framework outperforms the state-of-the-art by a sizable margin.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":17,"sentence":"The use of priors to avoid manual labeling for training machine learning methods has received much attention in the last few years.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"One of the critical subthemes in this regard is Learning from Label Proportions (LLP), where only the information about class proportions is available for training the models.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":17,"sentence":"While various LLP training settings verse in the literature, most approaches focus on bag-level label proportions errors, often leading to suboptimal solutions.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":17,"sentence":"This paper proposes a new model that jointly uses prototypical contrastive learning and bag-level cluster proportions to implement efficient LLP classification.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":17,"sentence":"Our proposal explicitly relaxes the equipartition constraint commonly used in prototypical contrastive learning methods and incorporates the exact cluster proportions into the optimal transport algorithm used for cluster assignments.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":17,"sentence":"At inference time, we compute the clusters' assignment, delivering instance-level classification.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":17,"sentence":"We experimented with our method on two widely used image classification benchmarks and report a new state-of-art LLP performance, achieving results close to fully supervised methods.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":18,"sentence":"We attempt to train deep neural networks for classification without using any labeled data.","offset":0,"pro":0,"labels":"PUR"},{"idx":18,"sentence":"Existing unsupervised methods, though mine useful clusters or features, require some annotated samples to facilitate the final task-specific predictions.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":18,"sentence":"This defeats the true purpose of unsupervised learning and hence we envisage a paradigm of `true' self-supervision, where absolutely no annotated instances are used for training a classifier.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":18,"sentence":"The proposed method first pretrains a deep network through self-supervision and performs clustering on the learned features.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":18,"sentence":"A classifier layer is then appended to the self-supervised network and is trained by matching the distribution of the predictions to that of a predefined prior.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":18,"sentence":"This approach leverages the distribution of labels for supervisory signals and consequently, no image-label pair is needed.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":18,"sentence":"Experiments reveal that the method works on major nominal as well as ordinal classification datasets and delivers significant performance.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":19,"sentence":"Learning a disentangled representation is still a challenge in the field of the interpretability of generative adversarial networks (GANs).","offset":0,"pro":0,"labels":"GAP"},{"idx":19,"sentence":"This paper proposes a generic method to modify a traditional GAN into an interpretable GAN, which ensures that filters in an intermediate layer of the generator encode disentangled localized visual concepts.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":19,"sentence":"Each filter in the layer is supposed to consistently generate image regions corresponding to the same visual concept when generating different images.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":19,"sentence":"The interpretable GAN learns to automatically discover meaningful visual concepts without any annotations of visual concepts.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":19,"sentence":"The interpretable GAN enables people to modify a specific visual concept on generated images by manipulating feature maps of the corresponding filters in the layer.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":19,"sentence":"Our method can be broadly applied to different types of GANs.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":19,"sentence":"Experiments have demonstrated the effectiveness of our method.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":0,"sentence":"Answering natural language questions over tables is usually seen as a semantic parsing task.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"To alleviate the collection cost of full logical forms, one popular approach focuses on weak supervision consisting of denotations instead of logical forms.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":0,"sentence":"However, training semantic parsers from weak supervision poses difficulties, and in addition, the generated logical forms are only used as an intermediate step prior to retrieving the denotation.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":0,"sentence":"In this paper, we present TaPas, an approach to question answering over tables without generating logical forms.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":0,"sentence":"TaPas trains from weak supervision, and predicts the denotation by selecting table cells and optionally applying a corresponding aggregation operator to such selection.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":0,"sentence":"TaPas extends BERT's architecture to encode tables as input, initializes from an effective joint pre-training of text segments and tables crawled from Wikipedia, and is trained end-to-end.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":0,"sentence":"We additionally find that transfer learning, which is trivial in our setting, from WikiSQL to WikiTQ, yields 48.7 accuracy, 4.2 points above the state-of-the-art.","offset":6,"pro":0.75,"labels":"RST"},{"idx":0,"sentence":"We experiment with three different semantic parsing datasets, ","offset":7,"pro":0.875,"labels":"MTD"},{"idx":1,"sentence":"In argumentation, people state premises to reason towards a conclusion.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"The conclusion conveys a stance towards some target, such as a concept or statement.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":1,"sentence":"Often, the conclusion remains implicit, though, since it is self-evident in a discussion or left out for rhetorical reasons.","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":1,"sentence":"However, the conclusion is key to understanding an argument and, hence, to any application that processes argumentation.","offset":3,"pro":0.3333333333333333,"labels":"GAP"},{"idx":1,"sentence":"We thus study the question to what extent an argument's conclusion can be reconstructed from its premises.","offset":4,"pro":0.4444444444444444,"labels":"PUR"},{"idx":1,"sentence":"We develop two complementary target inference approaches: one ranks premise targets and selects the top-ranked target as the conclusion target, the other finds a new conclusion target in a learned embedding space using a triplet neural network.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":1,"sentence":"Our evaluation on corpora from two domains indicates that a hybrid of both approaches is best, outperforming several strong baselines.","offset":6,"pro":0.6666666666666666,"labels":"CLN"},{"idx":1,"sentence":"According to human annotators, we infer a reasonably adequate conclusion target in 89% of the cases.","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":1,"sentence":"In particular, we argue here that a decisive step is to infer a conclusion's target, and we hypothesize that this target is related to the premises' targets. ","offset":8,"pro":0.8888888888888888,"labels":"PUR"},{"idx":2,"sentence":"Multimodal Machine Translation (MMT) aims to introduce information from other modality, generally static images, to improve the translation quality.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"Equally treating all modalities may encode too much useless information from less important modalities.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":2,"sentence":"In this paper, we introduce the multimodal self-attention in Transformer to solve the issues above in MMT.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":2,"sentence":"The proposed method learns the representation of images based on the text, which avoids encoding irrelevant information in images.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":2,"sentence":"Experiments and visualization analysis demonstrate that our model benefits from visual information and substantially outperforms previous works and competitive baselines in terms of various metrics.","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":2,"sentence":"Previous works propose various incorporation methods, but most of them do not consider the relative importance of multiple modalities.","offset":5,"pro":0.8333333333333334,"labels":"GAP"},{"idx":3,"sentence":"In this paper, we hypothesize that sarcasm is closely related to sentiment and emotion, and thereby propose a multi-task deep learning framework to solve all these three problems simultaneously in a multi-modal conversational scenario.","offset":0,"pro":0,"labels":"PUR"},{"idx":3,"sentence":"We, at first, manually annotate the recently released multi-modal MUStARD sarcasm dataset with sentiment and emotion classes, both implicit and explicit.","offset":1,"pro":0.1111111111111111,"labels":"MTD"},{"idx":3,"sentence":"For multi-tasking, we propose two attention mechanisms, viz.","offset":2,"pro":0.2222222222222222,"labels":"MTD"},{"idx":3,"sentence":"Inter-segment Inter-modal Attention (Ie-Attention) and Intra-segment Inter-modal Attention (Ia-Attention).","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":3,"sentence":"In contrast, Ia-Attention focuses within the same segment of the sentence across the modalities.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":3,"sentence":"Finally, representations from both the attentions are concatenated and shared across the five classes (i.e., sarcasm, implicit sentiment, explicit sentiment, implicit emotion, explicit emotion) for multi-tasking.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":3,"sentence":"Experimental results on the extended version of the MUStARD dataset show the efficacy of our proposed approach for sarcasm detection over the existing state-of-the-art systems.","offset":6,"pro":0.6666666666666666,"labels":"CLN"},{"idx":3,"sentence":"The main motivation of Ie-Attention is to learn the relationship between the different segments of the sentence across the modalities. ","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":3,"sentence":"The evaluation also shows that the proposed multi-task framework yields better performance for the primary task, i.e., sarcasm detection, with the help of two secondary tasks, emotion and sentiment analysis.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":4,"sentence":"The task of Dialogue Act Classification (DAC) that purports to capture communicative intent has been studied extensively.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"But these studies limit themselves to text.","offset":1,"pro":0.1,"labels":"GAP"},{"idx":4,"sentence":"Non-verbal features (change of tone, facial expressions etc.) can provide cues to identify DAs, thus stressing the benefit of incorporating multi-modal inputs in the task.","offset":2,"pro":0.2,"labels":"BAC"},{"idx":4,"sentence":"Also, the emotional state of the speaker has a substantial effect on the choice of the dialogue act, since conversations are often influenced by emotions.","offset":3,"pro":0.3,"labels":"BAC"},{"idx":4,"sentence":"Hence, the effect of emotion too on automatic identification of DAs needs to be studied.","offset":4,"pro":0.4,"labels":"GAP"},{"idx":4,"sentence":"In this work, we address the role of both multi-modality and emotion recognition (ER) in DAC.","offset":5,"pro":0.5,"labels":"PUR"},{"idx":4,"sentence":"DAC and ER help each other by way of multi-task learning.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":4,"sentence":"To demonstrate the utility of EMOTyDA, we build an attention based (self, inter-modal, inter-task) multi-modal, multi-task Deep Neural Network (DNN) for joint learning of DAs and emotions.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":4,"sentence":"We show empirically that multi-modality and multi-tasking achieve better performance of DAC compared to uni-modal and single task DAC variants.","offset":8,"pro":0.8,"labels":"RST"},{"idx":4,"sentence":"One of the major contributions of this work is a new dataset- multimodal Emotion aware Dialogue Act dataset called EMOTyDA, collected from open-sourced dialogue datasets. ","offset":9,"pro":0.9,"labels":"CTN"},{"idx":5,"sentence":"Parody is a figurative device used to imitate an entity for comedic or critical purposes and represents a widespread phenomenon in social media through many popular parody accounts.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"In this paper, we present the first computational study of parody.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":5,"sentence":"We introduce a new publicly available data set of tweets from real politicians and their corresponding parody accounts.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":5,"sentence":"We run a battery of supervised machine learning models for automatically detecting parody tweets with an emphasis on robustness by testing on tweets from accounts unseen in training, across different genders and across countries.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":5,"sentence":"Our results show that political parody tweets can be predicted with an accuracy up to 90%.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":5,"sentence":"Finally, we identify the markers of parody through a linguistic analysis.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":5,"sentence":"Beyond research in linguistics and political communication, accurately and automatically detecting parody is important to improving fact checking for journalists and analytics such as sentiment analysis through filtering out parodical utterances. ","offset":6,"pro":0.8571428571428571,"labels":"IMP"},{"idx":6,"sentence":"A central concern in Computational Social Sciences (CSS) is fairness: where the role of NLP is to scale up text analysis to large corpora, the quality of automatic analyses should be as independent as possible of textual properties.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"We analyze the performance of a state-of-the-art neural model on the task of political claims detection (i.e., the identification of forward-looking statements made by political actors) and identify a strong frequency bias: claims made by frequent actors are recognized better.","offset":1,"pro":0.25,"labels":"MTD"},{"idx":6,"sentence":"We propose two simple debiasing methods which mask proper names and pronouns during training of the model, thus removing personal information bias.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":6,"sentence":"We find that (a) these methods significantly decrease frequency bias while keeping the overall performance stable; and (b) the resulting models improve when evaluated in an out-of-domain setting.","offset":3,"pro":0.75,"labels":"RST"},{"idx":7,"sentence":"Here, we investigate the extent to which publicly-available word embeddings accurately reflect beliefs about certain kinds of people as measured via traditional survey methods.","offset":0,"pro":0,"labels":"PUR"},{"idx":7,"sentence":"We find that biases found in word embeddings do, on average, closely mirror survey data across seventeen dimensions of social meaning.","offset":1,"pro":0.2,"labels":"RST"},{"idx":7,"sentence":"However, we also find that biases in embeddings are much more reflective of survey data for some dimensions of meaning (e.g. gender) than others (e.g. race), and that we can be highly confident that embedding-based measures reflect survey data only for the most salient biases.","offset":2,"pro":0.4,"labels":"RST"},{"idx":7,"sentence":"Social biases are encoded in word embeddings. ","offset":3,"pro":0.6,"labels":"BAC"},{"idx":7,"sentence":"This presents a unique opportunity to study society historically and at scale, and a unique danger when embeddings are used in downstream applications. ","offset":4,"pro":0.8,"labels":"BAC"},{"idx":8,"sentence":"In an era where generating content and publishing it is so easy, we are bombarded with information and are exposed to all kinds of claims, some of which do not always rank high on the truth scale.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"This paper suggests that the key to a longer-term, holistic, and systematic approach to navigating this information pollution is capturing the provenance of claims.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":8,"sentence":"To do that, we develop a formal definition of provenance graph for a given natural language claim, aiming to understand where the claim may come from and how it has evolved.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":8,"sentence":"To construct the graph, we model provenance inference, formulated mainly as an information extraction task and addressed via a textual entailment model.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":8,"sentence":"We evaluate our approach using two benchmark datasets, showing initial success in capturing the notion of provenance and its effectiveness on the application of claim verification.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":9,"sentence":"Natural language allows us to refer to novel composite concepts by combining expressions denoting their parts according to systematic rules, a property known as compositionality.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"In this paper, we study whether the language emerging in deep multi-agent simulations possesses a similar ability to refer to novel primitive combinations, and whether it accomplishes this feat by strategies akin to human-language compositionality.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":9,"sentence":"We conclude that compositionality does not arise from simple generalization pressure, but if an emergent language does chance upon it, it will be more likely to survive and thrive.","offset":2,"pro":0.3333333333333333,"labels":"CLN"},{"idx":9,"sentence":"Equipped with new ways to measure compositionality in emergent languages inspired by disentanglement in representation learning, we establish three main results: First, given sufficiently large input spaces, the emergent language will naturally develop the ability to refer to novel composite concepts. ","offset":3,"pro":0.5,"labels":"RST"},{"idx":9,"sentence":"Second, there is no correlation between the degree of compositionality of an emergent language and its ability to generalize. ","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":9,"sentence":"Third, while compositionality is not necessary for generalization, it provides an advantage in terms of language transmission: The more compositional a language is, the more easily it will be picked up by new learners, even when the latter differ in architecture from the original agents. ","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":10,"sentence":"State-of-the-art models in NLP are now predominantly based on deep neural networks that are opaque in terms of how they come to make predictions.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"This limitation has increased interest in designing more interpretable deep models for NLP that reveal the reasoning' behind model outputs.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":10,"sentence":"But work in this direction has been conducted on different datasets and tasks with correspondingly unique aims and metrics; this makes it difficult to track progress.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":10,"sentence":"We propose the Evaluating Rationales And Simple English Reasoning (ERASER a benchmark to advance research on interpretable models in NLP.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":10,"sentence":"This benchmark comprises multiple datasets and tasks for which human annotations of rationales (supporting evidence) have been collected.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":10,"sentence":"We propose several metrics that aim to capture how well the rationales provided by models align with human rationales, and also how faithful these rationales are (i.e., the degree to which provided rationales influenced the corresponding predictions).","offset":5,"pro":0.625,"labels":"MTD"},{"idx":10,"sentence":"Our hope is that releasing this benchmark facilitates progress on designing more interpretable NLP systems.","offset":6,"pro":0.75,"labels":"IMP"},{"idx":10,"sentence":"The benchmark, code, and documentation are available at url{https://www.eraserbenchmark.com/","offset":7,"pro":0.875,"labels":"CTN"},{"idx":11,"sentence":"In many settings it is important for one to be able to understand why a model made a particular prediction.","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"In NLP this often entails extracting snippets of an input text responsible for' corresponding model output; when such a snippet comprises tokens that indeed informed the model's prediction, it is a faithful explanation.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":11,"sentence":"In some settings, faithfulness may be critical to ensure transparency.","offset":2,"pro":0.2,"labels":"BAC"},{"idx":11,"sentence":"Lei et al. (2016) proposed a model to produce faithful rationales for neural text classification by defining independent snippet extraction and prediction modules.","offset":3,"pro":0.3,"labels":"BAC"},{"idx":11,"sentence":"However, the discrete selection over input tokens performed by this method complicates training, leading to high variance and requiring careful hyperparameter tuning.","offset":4,"pro":0.4,"labels":"GAP"},{"idx":11,"sentence":"We propose a simpler variant of this approach that provides faithful explanations by construction.","offset":5,"pro":0.5,"labels":"PUR"},{"idx":11,"sentence":"In our scheme, named FRESH, arbitrary feature importance scores (e.g., gradients from a trained model) are used to induce binary labels over token inputs, which an extractor can be trained to predict.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":11,"sentence":"An independent classifier module is then trained exclusively on snippets provided by the extractor; these snippets thus constitute faithful explanations, even if the classifier is arbitrarily complex.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":11,"sentence":"In both automatic and manual evaluations we find that variants of this simple framework yield predictive performance superior to end-to-end' approaches, while being more general and easier to train.","offset":8,"pro":0.8,"labels":"RST"},{"idx":11,"sentence":"Code is available at https://github.com/successar/FRESH.","offset":9,"pro":0.9,"labels":"CTN"},{"idx":12,"sentence":"Machine reading comprehension has made great progress in recent years owing to large-scale annotated datasets.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"In the clinical domain, however, creating such datasets is quite difficult due to the domain expertise required for annotation.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":12,"sentence":"Recently, Pampari et al. (EMNLP'18) tackled this issue by using expert-annotated question templates and existing i2b2 annotations to create emrQA, the first large-scale dataset for question answering (QA) based on clinical notes.","offset":2,"pro":0.25,"labels":"BAC"},{"idx":12,"sentence":"In this paper, we provide an in-depth analysis of this dataset and the clinical reading comprehension (CliniRC) task.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":12,"sentence":"From our qualitative analysis, we find that (i) emrQA answers are often incomplete, and (ii) emrQA questions are often answerable without using domain knowledge.","offset":4,"pro":0.5,"labels":"RST"},{"idx":12,"sentence":"From our quantitative experiments, surprising results include that (iii) using a small sampled subset (5%-20%), we can obtain roughly equal performance compared to the model trained on the entire dataset, (iv) this performance is close to human expert's performance, and (v) BERT models do not beat the best performing base model.","offset":5,"pro":0.625,"labels":"RST"},{"idx":12,"sentence":"Following our analysis of the emrQA, we further explore two desired aspects of CliniRC systems: the ability to utilize clinical domain knowledge and to generalize to unseen questions and contexts.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":12,"sentence":"We argue that both should be considered when creating future datasets.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":13,"sentence":"It turns out that we can get by without input-wide self-attention at all layers, especially in the lower layers.","offset":0,"pro":0,"labels":"RST"},{"idx":13,"sentence":"We introduce DeFormer, a decomposed transformer, which substitutes the full self-attention with question-wide and passage-wide self-attentions in the lower layers.","offset":1,"pro":0.14285714285714285,"labels":"MTD"},{"idx":13,"sentence":"This allows for question-independent processing of the input text representations, which in turn enables pre-computing passage representations reducing runtime compute drastically.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":13,"sentence":"Furthermore, because DeFormer is largely similar to the original model, we can initialize DeFormer with the pre-training weights of a standard transformer, and directly fine-tune on the target QA dataset.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":13,"sentence":"We show DeFormer versions of BERT and XLNet can be used to speed up QA by over 4.3x and with simple distillation-based losses they incur only a 1% drop in accuracy.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":13,"sentence":"We open source the code at https://github.com/StonyBrookNLP/deformer.","offset":5,"pro":0.7142857142857143,"labels":"CTN"},{"idx":13,"sentence":"Transformer-based QA models use input-wide self-attention - i.e. across both the question and the input passage - at all layers, causing them to be slow and memory-intensive. ","offset":6,"pro":0.8571428571428571,"labels":"GAP"},{"idx":14,"sentence":"Knowledge Graphs (KG) are multi-relational graphs consisting of entities as nodes and relations among them as typed edges.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"Goal of the Question Answering over KG (KGQA) task is to answer natural language queries posed over the KG.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":14,"sentence":"KGs are often incomplete with many missing links, posing additional challenges for KGQA, especially for multi-hop KGQA.","offset":2,"pro":0.2,"labels":"GAP"},{"idx":14,"sentence":"Recent research on multi-hop KGQA has attempted to handle KG sparsity using relevant external text, which isn't always readily available.","offset":3,"pro":0.3,"labels":"BAC"},{"idx":14,"sentence":"In a separate line of research, KG embedding methods have been proposed to reduce KG sparsity by performing missing link prediction.","offset":4,"pro":0.4,"labels":"BAC"},{"idx":14,"sentence":"Such KG embedding methods, even though highly relevant, have not been explored for multi-hop KGQA so far.","offset":5,"pro":0.5,"labels":"GAP"},{"idx":14,"sentence":"We fill this gap in this paper and propose EmbedKGQA.","offset":6,"pro":0.6,"labels":"PUR"},{"idx":14,"sentence":"EmbedKGQA is particularly effective in performing multi-hop KGQA over sparse KGs.","offset":7,"pro":0.7,"labels":"RST"},{"idx":14,"sentence":"Multi-hop KGQA requires reasoning over multiple edges of the KG to arrive at the right answer. ","offset":8,"pro":0.8,"labels":"BAC"},{"idx":14,"sentence":"EmbedKGQA also relaxes the requirement of answer selection from a pre-specified neighborhood, a sub-optimal constraint enforced by previous multi-hop KGQA methods. ","offset":9,"pro":0.9,"labels":"RST"},{"idx":15,"sentence":"Question Answering (QA) is in increasing demand as the amount of information available online and the desire for quick access to this content grows.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"A common approach to QA has been to fine-tune a pretrained language model on a task-specific labeled dataset.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":15,"sentence":"This paradigm, however, relies on scarce, and costly to obtain, large-scale human-labeled data.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":15,"sentence":"We propose an unsupervised approach to training QA models with generated pseudo-training data.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":15,"sentence":"We show that generating questions for QA training by applying a simple template on a related, retrieved sentence rather than the original context sentence improves downstream QA performance by allowing the model to learn more complex context-question relationships.","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":15,"sentence":"Training a QA model on this data gives a relative improvement over a previous unsupervised model in F1 score on the SQuAD dataset by about 14%, and 20% when the answer is a named entity, achieving state-of-the-art performance on SQuAD for unsupervised QA.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":16,"sentence":"Evidence retrieval is a critical stage of question answering (QA), necessary not only to improve performance, but also to explain the decisions of the QA method.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"We introduce a simple, fast, and unsupervised iterative evidence retrieval method, which relies on three ideas: (a) an unsupervised alignment approach to soft-align questions and answers with justification sentences using only GloVe embeddings, (b) an iterative process that reformulates queries focusing on terms that are not covered by existing justifications, which (c) stops when the terms in the given question and candidate answers are covered by the retrieved justifications.","offset":1,"pro":0.25,"labels":"MTD"},{"idx":16,"sentence":"Despite its simplicity, our approach outperforms all the previous methods (including supervised methods) on the evidence selection task on two datasets: MultiRC and QASC.","offset":2,"pro":0.5,"labels":"RST"},{"idx":16,"sentence":"When these evidence sentences are fed into a RoBERTa answer classification component, we achieve state-of-the-art QA performance on these two datasets.","offset":3,"pro":0.75,"labels":"RST"},{"idx":17,"sentence":"A major hurdle in data-driven research on typology is having sufficient data in many languages to draw meaningful conclusions.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"We present VoxClamantis v1.0, the first large-scale corpus for phonetic typology, with aligned segments and estimated phoneme-level labels in 690 readings spanning 635 languages, along with acoustic-phonetic measures of vowels and sibilants.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":17,"sentence":"Access to such data can greatly facilitate investigation of phonetic typology at a large scale and across many languages.","offset":2,"pro":0.3333333333333333,"labels":"CTN"},{"idx":17,"sentence":"However, it is non-trivial and computationally intensive to obtain such alignments for hundreds of languages, many of which have few to no resources presently available.","offset":3,"pro":0.5,"labels":"GAP"},{"idx":17,"sentence":"We describe the methodology to create our corpus, discuss caveats with current methods and their impact on the utility of this data, and illustrate possible research directions through a series of case studies on the 48 highest-quality readings.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":17,"sentence":"Our corpus and scripts are publicly available for non-commercial use at https://voxclamantisproject.github.io.","offset":5,"pro":0.8333333333333334,"labels":"CTN"},{"idx":18,"sentence":"Discourse representation structures (DRSs) are scoped semantic representations for texts of arbitrary length.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"Evaluating the accuracy of predicted DRSs plays a key role in developing semantic parsers and improving their performance.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":18,"sentence":"DRSs are typically visualized as boxes which are not straightforward to process automatically.","offset":2,"pro":0.2857142857142857,"labels":"BAC"},{"idx":18,"sentence":"Counter transforms DRSs to clauses and measures clause overlap by searching for variable mappings between two DRSs.","offset":3,"pro":0.42857142857142855,"labels":"BAC"},{"idx":18,"sentence":"However, this metric is computationally costly (with respect to memory and CPU time) and does not scale with longer texts.","offset":4,"pro":0.5714285714285714,"labels":"GAP"},{"idx":18,"sentence":"We introduce Dscorer, an efficient new metric which converts box-style DRSs to graphs and then measures the overlap of n-grams.","offset":5,"pro":0.7142857142857143,"labels":"PUR"},{"idx":18,"sentence":"Experiments show that Dscorer computes accuracy scores that are correlated with Counter at a fraction of the time.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":19,"sentence":"We report on methods to create the largest publicly available parallel corpora by crawling the web, using open source software.","offset":0,"pro":0,"labels":"MTD"},{"idx":19,"sentence":"We empirically compare alternative methods and publish benchmark data sets for sentence alignment and sentence pair filtering.","offset":1,"pro":0.3333333333333333,"labels":"MTD"},{"idx":19,"sentence":"We also describe the parallel corpora released and evaluate their quality and their usefulness to create machine translation systems.","offset":2,"pro":0.6666666666666666,"labels":"MTD"},{"idx":0,"sentence":"Correctly resolving textual mentions of people fundamentally entails making inferences about those people.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"Such inferences raise the risk of systemic biases in coreference resolution systems, including biases that can harm binary and non-binary trans and cis stakeholders.","offset":1,"pro":0.25,"labels":"GAP"},{"idx":0,"sentence":"To better understand such biases, we foreground nuanced conceptualizations of gender from sociology and sociolinguistics, and develop two new datasets for interrogating bias in crowd annotations and in existing coreference resolution systems.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":0,"sentence":"Through these studies, conducted on English text, we confirm that without acknowledging and building systems that recognize the complexity of gender, we build systems that lead to many potential harms.","offset":3,"pro":0.75,"labels":"RST"},{"idx":1,"sentence":"Motivated by human attention, computational attention mechanisms have been designed to help neural networks adjust their focus on specific parts of the input data.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"While attention mechanisms are claimed to achieve interpretability, little is known about the actual relationships between machine and human attention.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":1,"sentence":"In this work, we conduct the first quantitative assessment of human versus computational attention mechanisms for the text classification task.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":1,"sentence":"To achieve this, we design and conduct a large-scale crowd-sourcing study to collect human attention maps that encode the parts of a text that humans focus on when conducting text classification.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":1,"sentence":"Based on this new resource of human attention dataset for text classification, YELP-HAT, collected on the publicly available YELP dataset, we perform a quantitative comparative analysis of machine attention maps created by deep learning models and human attention maps.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":1,"sentence":"Our analysis offers insights into the relationships between human versus machine attention maps along three dimensions: overlap in word selections, distribution over lexical categories, and context-dependency of sentiment polarity.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":1,"sentence":"Our findings open promising future research opportunities ranging from supervised attention to the design of human-centric attention-based explanations.","offset":6,"pro":0.8571428571428571,"labels":"IMP"},{"idx":2,"sentence":"The success of neural networks on a diverse set of NLP tasks has led researchers to question how much these networks actually know about natural language.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"Probes are a natural way of assessing this.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":2,"sentence":"When probing, a researcher chooses a linguistic task and trains a supervised model to predict annotations in that linguistic task from the network's learned representations.","offset":2,"pro":0.2222222222222222,"labels":"BAC"},{"idx":2,"sentence":"If the probe does well, the researcher may conclude that the representations encode knowledge related to the task.","offset":3,"pro":0.3333333333333333,"labels":"BAC"},{"idx":2,"sentence":"A commonly held belief is that using simpler models as probes is better; the logic is that simpler models will identify linguistic structure, but not learn the task itself.","offset":4,"pro":0.4444444444444444,"labels":"BAC"},{"idx":2,"sentence":"We propose an information-theoretic operationalization of probing as estimating mutual information that contradicts this received wisdom: one should always select the highest performing probe one can, even if it is more complex, since it will result in a tighter estimate, and thus reveal more of the linguistic information inherent in the representation.","offset":5,"pro":0.5555555555555556,"labels":"PUR"},{"idx":2,"sentence":"The experimental portion of our paper focuses on empirically estimating the mutual information between a linguistic property and BERT, comparing these estimates to several baselines.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":2,"sentence":"We evaluate on a set of ten typologically diverse languages often underrepresented in NLP research-plus English-totalling eleven languages.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":2,"sentence":"Our implementation is available in https://github.com/rycolab/info-theoretic-probing.","offset":8,"pro":0.8888888888888888,"labels":"CTN"},{"idx":3,"sentence":"State-of-the-art unsupervised multilingual models (e.g., multilingual BERT) have been shown to generalize in a zero-shot cross-lingual setting.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"This generalization ability has been attributed to the use of a shared subword vocabulary and joint training across multiple languages giving rise to deep multilingual abstractions.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":3,"sentence":"We evaluate this hypothesis by designing an alternative approach that transfers a monolingual model to new languages at the lexical level.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":3,"sentence":"More concretely, we first train a transformer-based masked language model on one language, and transfer it to a new language by learning a new embedding matrix with the same masked language modeling objective, freezing parameters of all other layers.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":3,"sentence":"This approach does not rely on a shared vocabulary or joint training.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":3,"sentence":"However, we show that it is competitive with multilingual BERT on standard cross-lingual classification benchmarks and on a new Cross-lingual Question Answering Dataset (XQuAD).","offset":5,"pro":0.625,"labels":"RST"},{"idx":3,"sentence":"Our results contradict common beliefs of the basis of the generalization ability of multilingual models and suggest that deep monolingual models learn some abstractions that generalize across languages.","offset":6,"pro":0.75,"labels":"CLN"},{"idx":3,"sentence":"We also release XQuAD as a more comprehensive cross-lingual benchmark, which comprises 240 paragraphs and 1190 question-answer pairs from SQuAD v1.1 translated into ten languages by professional translators.","offset":7,"pro":0.875,"labels":"CTN"},{"idx":4,"sentence":"This paper investigates contextual word representation models from the lens of similarity analysis.","offset":0,"pro":0,"labels":"PUR"},{"idx":4,"sentence":"Given a collection of trained models, we measure the similarity of their internal representations and attention.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":4,"sentence":"Critically, these models come from vastly different architectures.","offset":2,"pro":0.2857142857142857,"labels":"BAC"},{"idx":4,"sentence":"We use existing and novel similarity measures that aim to gauge the level of localization of information in the deep models, and facilitate the investigation of which design factors affect model similarity, without requiring any external linguistic annotation.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":4,"sentence":"The analysis reveals that models within the same family are more similar to one another, as may be expected.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":4,"sentence":"Surprisingly, different architectures have rather similar representations, but different individual neurons.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":4,"sentence":"We also observed differences in information localization in lower and higher layers and found that higher layers are more affected by fine-tuning on downstream tasks.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":5,"sentence":"The ability to learn from large unlabeled corpora has allowed neural language models to advance the frontier in natural language understanding.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"However, existing self-supervision techniques operate at the word form level, which serves as a surrogate for the underlying semantic content.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":5,"sentence":"This paper proposes a method to employ weak-supervision directly at the word sense level.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":5,"sentence":"Our model, named SenseBERT, is pre-trained to predict not only the masked words but also their WordNet supersenses.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":5,"sentence":"Accordingly, we attain a lexical-semantic level language model, without the use of human annotation.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":5,"sentence":"SenseBERT achieves significantly improved lexical understanding, as we demonstrate by experimenting on SemEval Word Sense Disambiguation, and by attaining a state of the art result on the Word in Context' task.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":6,"sentence":"In order to simplify a sentence, human editors perform multiple rewriting transformations: they split it into several shorter sentences, paraphrase words (i.e. replacing complex words or phrases by simpler synonyms), reorder components, and/or delete information deemed unnecessary.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"Despite these varied range of possible text alterations, current models for automatic sentence simplification are evaluated using datasets that are focused on a single transformation, such as lexical paraphrasing or splitting.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":6,"sentence":"This makes it impossible to understand the ability of simplification models in more realistic settings.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":6,"sentence":"To alleviate this limitation, this paper introduces ASSET, a new dataset for assessing sentence simplification in English.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":6,"sentence":"ASSET is a crowdsourced multi-reference corpus where each simplification was produced by executing several rewriting transformations.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":6,"sentence":"Through quantitative and qualitative experiments, we show that simplifications in ASSET are better at capturing characteristics of simplicity when compared to other standard evaluation datasets for the task.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":6,"sentence":"Furthermore, we motivate the need for developing better methods for automatic evaluation using ASSET, since we show that current popular metrics may not be suitable when multiple simplification transformations are performed.","offset":6,"pro":0.8571428571428571,"labels":"IMP"},{"idx":7,"sentence":"Thanks to the wealth of high-quality annotated images available in popular repositories such as ImageNet, multimodal language-vision research is in full bloom.","offset":0,"pro":0,"labels":"MTD"},{"idx":7,"sentence":"However, events, feelings and many other kinds of concepts which can be visually grounded are not well represented in current datasets.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":7,"sentence":"Nevertheless, we would expect a wide-coverage language understanding system to be able to classify images depicting recess and remorse, not just cats, dogs and bridges.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":7,"sentence":"We fill this gap by presenting BabelPic, a hand-labeled dataset built by cleaning the image-synset association found within the BabelNet Lexical Knowledge Base (LKB).","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":7,"sentence":"BabelPic explicitly targets non-concrete concepts, thus providing refreshing new data for the community.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":7,"sentence":"We also show that pre-trained language-vision systems can be used to further expand the resource by exploiting natural language knowledge available in the LKB.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":7,"sentence":"BabelPic is available for download at http://babelpic.org.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":8,"sentence":"Predicting how events induce emotions in the characters of a story is typically seen as a standard multi-label classification task, which usually treats labels as anonymous classes to predict.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"They ignore information that may be conveyed by the emotion labels themselves.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":8,"sentence":"We propose that the semantics of emotion labels can guide a model's attention when representing the input story.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":8,"sentence":"Further, we observe that the emotions evoked by an event are often related: an event that evokes joy is unlikely to also evoke sadness.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":8,"sentence":"In this work, we explicitly model label classes via label embeddings, and add mechanisms that track label-label correlations both during training and inference.","offset":4,"pro":0.5714285714285714,"labels":"PUR"},{"idx":8,"sentence":"We also introduce a new semi-supervision strategy that regularizes for the correlations on unlabeled data.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":8,"sentence":"Our empirical evaluations show that modeling label semantics yields consistent benefits, and we advance the state-of-the-art on an emotion inference task.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":9,"sentence":"We propose a semantic parsing dataset focused on instruction-driven communication with an agent in the game Minecraft.","offset":0,"pro":0,"labels":"PUR"},{"idx":9,"sentence":"The dataset consists of 7K human utterances and their corresponding parses.","offset":1,"pro":0.25,"labels":"MTD"},{"idx":9,"sentence":"Given proper world state, the parses can be interpreted and executed in game.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":9,"sentence":"We report the performance of baseline models, and analyze their successes and failures.","offset":3,"pro":0.75,"labels":"MTD"},{"idx":10,"sentence":"Generative dialogue models currently suffer from a number of problems which standard maximum likelihood training does not address.","offset":0,"pro":0,"labels":"GAP"},{"idx":10,"sentence":"They tend to produce generations that (i) rely too much on copying from the context, (ii) contain repetitions within utterances, (iii) overuse frequent words, and (iv) at a deeper level, contain logical flaws.In this work we show how all of these problems can be addressed by extending the recently introduced unlikelihood loss (Welleck et al., 2019) to these cases.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":10,"sentence":"We show that appropriate loss functions which regularize generated outputs to match human distributions are effective for the first three issues.","offset":2,"pro":0.4,"labels":"CLN"},{"idx":10,"sentence":"For the last important general issue, we show applying unlikelihood to collected data of what a model should not do is effective for improving logical consistency, potentially paving the way to generative models with greater reasoning ability.","offset":3,"pro":0.6,"labels":"CLN"},{"idx":10,"sentence":"We demonstrate the efficacy of our approach across several dialogue tasks.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":11,"sentence":"Large pretrained language models like BERT, after fine-tuning to a downstream task, have achieved high performance on a variety of NLP problems.","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"Yet explaining their decisions is difficult despite recent work probing their internal representations.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":11,"sentence":"We propose a procedure and analysis methods that take a hypothesis of how a transformer-based model might encode a linguistic phenomenon, and test the validity of that hypothesis based on a comparison between knowledge-related downstream tasks with downstream control tasks, and measurement of cross-dataset consistency.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":11,"sentence":"We apply this methodology to test BERT and RoBERTa on a hypothesis that some attention heads will consistently attend from a word in negation scope to the negation cue.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":11,"sentence":"We find that after fine-tuning BERT and RoBERTa on a negation scope task, the average attention head improves its sensitivity to negation and its attention consistency across negation datasets compared to the pre-trained models.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":11,"sentence":"However, only the base models (not the large models) improve compared to a control task, indicating there is evidence for a shallow encoding of negation only in the base models.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":12,"sentence":"LSTM-based recurrent neural networks are the state-of-the-art for many natural language processing (NLP) tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"Despite their performance, it is unclear whether, or how, LSTMs learn structural features of natural languages such as subject-verb number agreement in English.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":12,"sentence":"Lacking this understanding, the generality of LSTM performance on this task and their suitability for related tasks remains uncertain.","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":12,"sentence":"Further, errors cannot be properly attributed to a lack of structural capability, training data omissions, or other exceptional faults.","offset":3,"pro":0.3333333333333333,"labels":"GAP"},{"idx":12,"sentence":"We introduce *influence paths*, a causal account of structural properties as carried by paths across gates and neurons of a recurrent neural network.","offset":4,"pro":0.4444444444444444,"labels":"PUR"},{"idx":12,"sentence":"The approach refines the notion of influence (the subject's grammatical number has influence on the grammatical number of the subsequent verb) into a set of gate or neuron-level paths.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":12,"sentence":"The set localizes and segments the concept (e.g., subject-verb agreement), its constituent elements (e.g., the subject), and related or interfering elements (e.g., attractors).","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":12,"sentence":"We exemplify the methodology on a widely-studied multi-layer LSTM language model, demonstrating its accounting for subject-verb number agreement.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":12,"sentence":"The results offer both a finer and a more complete view of an LSTM's handling of this structural aspect of the English language than prior results based on diagnostic classifiers and ablation.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":13,"sentence":"Contextualized representations (e.g. ELMo, BERT) have become the default pretrained representations for downstream NLP applications.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"In some settings, this transition has rendered their static embedding predecessors (e.g. Word2Vec, GloVe) obsolete.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":13,"sentence":"As a side-effect, we observe that older interpretability methods for static embeddings - while more diverse and mature than those available for their dynamic counterparts - are underutilized in studying newer contextualized representations.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":13,"sentence":"Consequently, we introduce simple and fully general methods for converting from contextualized representations to static lookup-table embeddings which we apply to 5 popular pretrained models and 9 sets of pretrained weights.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":13,"sentence":"Our analysis of the resulting static embeddings notably reveals that pooling over many contexts significantly improves representational quality under intrinsic evaluation.","offset":4,"pro":0.5714285714285714,"labels":"CLN"},{"idx":13,"sentence":"Complementary to analyzing representational quality, we consider social biases encoded in pretrained representations with respect to gender, race/ethnicity, and religion and find that bias is encoded disparately across pretrained models and internal layers even for models with the same training data.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":13,"sentence":"Concerningly, we find dramatic inconsistencies between social bias estimators for word embeddings.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":14,"sentence":"Attention mechanisms are ubiquitous components in neural architectures applied to natural language processing.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"In addition to yielding gains in predictive accuracy, attention weights are often claimed to confer interpretability, purportedly useful both for providing insights to practitioners and for explaining why a model makes its decisions to stakeholders.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":14,"sentence":"We call the latter use of attention mechanisms into question by demonstrating a simple method for training models to produce deceptive attention masks.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":14,"sentence":"Our method diminishes the total weight assigned to designated impermissible tokens, even when the models can be shown to nevertheless rely on these features to drive predictions.","offset":3,"pro":0.42857142857142855,"labels":"RST"},{"idx":14,"sentence":"Across multiple models and tasks, our approach manipulates attention weights while paying surprisingly little cost in accuracy.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":14,"sentence":"Through a human study, we show that our manipulated attention-based explanations deceive people into thinking that predictions from a model biased against gender minorities do not rely on the gender.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":14,"sentence":"Consequently, our results cast doubt on attention's reliability as a tool for auditing algorithms in the context of fairness and accountability.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":15,"sentence":"We propose a general framework to study language emergence through signaling games with neural agents.","offset":0,"pro":0,"labels":"PUR"},{"idx":15,"sentence":"Using a continuous latent space, we are able to (i) train using backpropagation, (ii) show that discrete messages nonetheless naturally emerge.","offset":1,"pro":0.3333333333333333,"labels":"RST"},{"idx":15,"sentence":"We explore whether categorical perception effects follow and show that the messages are not compositional.","offset":2,"pro":0.6666666666666666,"labels":"PUR"},{"idx":16,"sentence":"Although models using contextual word embeddings have achieved state-of-the-art results on a host of NLP tasks, little is known about exactly what information these embeddings encode about the context words that they are understood to reflect.","offset":0,"pro":0,"labels":"GAP"},{"idx":16,"sentence":"To address this question, we introduce a suite of probing tasks that enable fine-grained testing of contextual embeddings for encoding of information about surrounding words.","offset":1,"pro":0.25,"labels":"PUR"},{"idx":16,"sentence":"We apply these tasks to examine the popular BERT, ELMo and GPT contextual encoders, and find that each of our tested information types is indeed encoded as contextual information across tokens, often with near-perfect recoverability-but the encoders vary in which features they distribute to which tokens, how nuanced their distributions are, and how robust the encoding of each feature is to distance.","offset":2,"pro":0.5,"labels":"RST"},{"idx":16,"sentence":"We discuss implications of these results for how different types of models break down and prioritize word-level context information when constructing token embeddings.","offset":3,"pro":0.75,"labels":"IMP"},{"idx":17,"sentence":"Videos convey rich information.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"Dynamic spatio-temporal relationships between people/objects, and diverse multimodal events are present in a video clip.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":17,"sentence":"Hence, it is important to develop automated models that can accurately extract such information from videos.","offset":2,"pro":0.2,"labels":"BAC"},{"idx":17,"sentence":"Answering questions on videos is one of the tasks which can evaluate such AI abilities.","offset":3,"pro":0.3,"labels":"BAC"},{"idx":17,"sentence":"In this paper, we propose a video question answering model which effectively integrates multi-modal input sources and finds the temporally relevant information to answer questions.","offset":4,"pro":0.4,"labels":"PUR"},{"idx":17,"sentence":"Specifically, we first employ dense image captions to help identify objects and their detailed salient regions and actions, and hence give the model useful extra information (in explicit textual format to allow easier matching) for answering questions.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":17,"sentence":"Moreover, our model is also comprised of dual-level attention (word/object and frame level), multi-head self/cross-integration for different sources (video and dense captions), and gates which pass more relevant information to the classifier.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":17,"sentence":"Finally, we also cast the frame selection problem as a multi-label classification task and introduce two loss functions, In-andOut Frame Score Margin (IOFSM) and Balanced Binary Cross-Entropy (BBCE), to better supervise the model with human importance annotations.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":17,"sentence":"We evaluate our model on the challenging TVQA dataset, where each of our model components provides significant gains, and our overall model outperforms the state-of-the-art by a large margin (74.09% versus 70.52%).","offset":8,"pro":0.8,"labels":"RST"},{"idx":17,"sentence":"We also present several word, object, and frame level visualization studies.","offset":9,"pro":0.9,"labels":"MTD"},{"idx":18,"sentence":"By describing the features and abstractions of our world, language is a crucial tool for human learning and a promising source of supervision for machine learning models.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"We use language to improve few-shot visual classification in the underexplored scenario where natural language task descriptions are available during training, but unavailable for novel tasks at test time.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":18,"sentence":"Existing models for this setting sample new descriptions at test time and use those to classify images.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":18,"sentence":"Instead, we propose language-shaped learning (LSL), an end-to-end model that regularizes visual representations to predict language.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":18,"sentence":"LSL is conceptually simpler, more data efficient, and outperforms baselines in two challenging few-shot domains.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":19,"sentence":"While much work on deep latent variable models of text uses continuous latent variables, discrete latent variables are interesting because they are more interpretable and typically more space efficient.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"We consider several approaches to learning discrete latent variable models for text in the case where exact marginalization over these variables is intractable.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":19,"sentence":"We compare the performance of the learned representations as features for low-resource document and sentence classification.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":19,"sentence":"Our best models outperform the previous best reported results with continuous representations in these low-resource settings, while learning significantly more compressed representations.","offset":3,"pro":0.6,"labels":"RST"},{"idx":19,"sentence":"Interestingly, we find that an amortized variant of Hard EM performs particularly well in the lowest-resource regimes.","offset":4,"pro":0.8,"labels":"RST"},{"idx":0,"sentence":"Various natural language processing tasks are structured prediction problems where outputs are constructed with multiple interdependent decisions.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"Past work has shown that domain knowledge, framed as constraints over the output space, can help improve predictive accuracy.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":0,"sentence":"However, designing good constraints often relies on domain expertise.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":0,"sentence":"In this paper, we study the problem of learning such constraints.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":0,"sentence":"We frame the problem as that of training a two-layer rectifier network to identify valid structures or substructures, and show a construction for converting a trained network into a system of linear constraints over the inference variables.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":0,"sentence":"Our experiments on several NLP tasks show that the learned constraints can improve the prediction accuracy, especially when the number of training examples is small.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":1,"sentence":"Recent models for unsupervised representation learning of text have employed a number of techniques to improve contextual word representations but have put little focus on discourse-level representations.","offset":0,"pro":0,"labels":"GAP"},{"idx":1,"sentence":"We propose Conpono, an inter-sentence objective for pretraining language models that models discourse coherence and the distance between sentences.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":1,"sentence":"Given an anchor sentence, our model is trained to predict the text k sentences away using a sampled-softmax objective where the candidates consist of neighboring sentences and sentences randomly sampled from the corpus.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":1,"sentence":"On the discourse representation benchmark DiscoEval, our model improves over the previous state-of-the-art by up to 13% and on average 4% absolute across 7 tasks.","offset":3,"pro":0.5,"labels":"RST"},{"idx":1,"sentence":"Our model is the same size as BERT-Base, but outperforms the much larger BERT-Large model and other more recent approaches that incorporate discourse.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":1,"sentence":"We also show that Conpono yields gains of 2%-6% absolute even for tasks that do not explicitly evaluate discourse: textual entailment (RTE), common sense reasoning (COPA) and reading comprehension (ReCoRD).","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":2,"sentence":"Many high-level procedural tasks can be decomposed into sequences of instructions that vary in their order and choice of tools.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"In the cooking domain, the web offers many, partially-overlapping, text and video recipes (i.e. procedures) that describe how to make the same dish (i.e. high-level task).","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":2,"sentence":"Aligning instructions for the same dish across different sources can yield descriptive visual explanations that are far richer semantically than conventional textual instructions, providing commonsense insight into how real-world procedures are structured.","offset":2,"pro":0.2857142857142857,"labels":"BAC"},{"idx":2,"sentence":"Learning to align these different instruction sets is challenging because: a) different recipes vary in their order of instructions and use of ingredients; and b) video instructions can be noisy and tend to contain far more information than text instructions.","offset":3,"pro":0.42857142857142855,"labels":"GAP"},{"idx":2,"sentence":"To address these challenges, we use an unsupervised alignment algorithm that learns pairwise alignments between instructions of different recipes for the same dish.","offset":4,"pro":0.5714285714285714,"labels":"PUR"},{"idx":2,"sentence":"We then use a graph algorithm to derive a joint alignment between multiple text and multiple video recipes for the same dish.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":2,"sentence":"We release the Microsoft Research Multimodal Aligned Recipe Corpus containing asciitilde150K pairwise alignments between recipes across 4262 dishes with rich commonsense information.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":3,"sentence":"We introduce a new large-scale NLI benchmark dataset, collected via an iterative, adversarial human-and-model-in-the-loop procedure.","offset":0,"pro":0,"labels":"PUR"},{"idx":3,"sentence":"We show that training models on this new dataset leads to state-of-the-art performance on a variety of popular NLI benchmarks, while posing a more difficult challenge with its new test set.","offset":1,"pro":0.25,"labels":"RST"},{"idx":3,"sentence":"Our analysis sheds light on the shortcomings of current state-of-the-art models, and shows that non-expert annotators are successful at finding their weaknesses.","offset":2,"pro":0.5,"labels":"CLN"},{"idx":3,"sentence":"The data collection method can be applied in a never-ending learning scenario, becoming a moving target for NLU, rather than a static benchmark that will quickly saturate.","offset":3,"pro":0.75,"labels":"IMP"},{"idx":4,"sentence":"Although measuring held-out accuracy has been the primary approach to evaluate generalization, it often overestimates the performance of NLP models, while alternative approaches for evaluating models either focus on individual tasks or on specific behaviors.","offset":0,"pro":0,"labels":"GAP"},{"idx":4,"sentence":"Inspired by principles of behavioral testing in software engineering, we introduce CheckList, a task-agnostic methodology for testing NLP models.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":4,"sentence":"CheckList includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation, as well as a software tool to generate a large and diverse number of test cases quickly.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":4,"sentence":"We illustrate the utility of CheckList with tests for three tasks, identifying critical failures in both commercial and state-of-art models.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":4,"sentence":"In a user study, a team responsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":4,"sentence":"In another user study, NLP practitioners with CheckList created twice as many tests, and found almost three times as many bugs as users without it.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":5,"sentence":"There is an increasing interest in studying natural language and computer code together, as large corpora of programming texts become readily available on the Internet.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"For example, StackOverflow currently has over 15 million programming related questions written by 8.5 million users.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":5,"sentence":"Meanwhile, there is still a lack of fundamental NLP techniques for identifying code tokens or software-related named entities that appear within natural language sentences.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":5,"sentence":"In this paper, we introduce a new named entity recognition (NER) corpus for the computer programming domain, consisting of 15,372 sentences annotated with 20 fine-grained entity types.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":5,"sentence":"We trained in-domain BERT representations (BERTOverflow) on 152 million sentences from StackOverflow, which lead to an absolute increase of +10 F1 score over off-the-shelf BERT.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":5,"sentence":"We also present the SoftNER model which achieves an overall 79.10 F-1 score for code and named entity recognition on StackOverflow data.","offset":5,"pro":0.625,"labels":"RST"},{"idx":5,"sentence":"Our SoftNER model incorporates a context-independent code token classifier with corpus-level features to improve the BERT-based tagging model.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":5,"sentence":"Our code and data are available at: https://github.com/jeniyat/StackOverflowNER/","offset":7,"pro":0.875,"labels":"CTN"},{"idx":6,"sentence":"We present the first human-annotated dialogue-based relation extraction (RE) dataset DialogRE, aiming to support the prediction of relation(s) between two arguments that appear in a dialogue.","offset":0,"pro":0,"labels":"PUR"},{"idx":6,"sentence":"We further offer DialogRE as a platform for studying cross-sentence RE as most facts span multiple sentences.","offset":1,"pro":0.16666666666666666,"labels":"CTN"},{"idx":6,"sentence":"We argue that speaker-related information plays a critical role in the proposed task, based on an analysis of similarities and differences between dialogue-based and traditional RE tasks.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":6,"sentence":"Considering the timeliness of communication in a dialogue, we design a new metric to evaluate the performance of RE methods in a conversational setting and investigate the performance of several representative RE methods on DialogRE.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":6,"sentence":"Experimental results demonstrate that a speaker-aware extension on the best-performing model leads to gains in both the standard and conversational evaluation settings.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":6,"sentence":"DialogRE is available at https://dataset.org/dialogre/.","offset":5,"pro":0.8333333333333334,"labels":"CTN"},{"idx":7,"sentence":"Commonly adopted metrics for extractive summarization focus on lexical overlap at the token level.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"In this paper, we present a facet-aware evaluation setup for better assessment of the information coverage in extracted summaries.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":7,"sentence":"Specifically, we treat each sentence in the reference summary as a facet, identify the sentences in the document that express the semantics of each facet as support sentences of the facet, and automatically evaluate extractive summarization methods by comparing the indices of extracted sentences and support sentences of all the facets in the reference summary.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":7,"sentence":"To facilitate this new evaluation setup, we construct an extractive version of the CNN/Daily Mail dataset and perform a thorough quantitative investigation, through which we demonstrate that facet-aware evaluation manifests better correlation with human judgment than ROUGE, enables fine-grained evaluation as well as comparative analysis, and reveals valuable insights of state-of-the-art summarization methods.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":7,"sentence":"Data can be found at url{https://github.com/morningmoni/FAR.","offset":4,"pro":0.8,"labels":"CTN"},{"idx":8,"sentence":"Automated generation of conversational dialogue using modern neural architectures has made notable advances.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"However, these models are known to have a drawback of often producing uninteresting, predictable responses; this is known as the diversity problem.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":8,"sentence":"We introduce a new strategy to address this problem, called Diversity-Informed Data Collection.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":8,"sentence":"Unlike prior approaches, which modify model architectures to solve the problem, this method uses dynamically computed corpus-level statistics to determine which conversational participants to collect data from.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":8,"sentence":"Diversity-Informed Data Collection produces significantly more diverse data than baseline data collection methods, and better results on two downstream tasks: emotion classification and dialogue generation.","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":8,"sentence":"This method is generalizable and can be used with other corpus-level metrics.","offset":5,"pro":0.8333333333333334,"labels":"CTN"},{"idx":9,"sentence":"We introduce S2ORC, a large corpus of 81.1M English-language academic papers spanning many academic disciplines.","offset":0,"pro":0,"labels":"PUR"},{"idx":9,"sentence":"The corpus consists of rich metadata, paper abstracts, resolved bibliographic references, as well as structured full text for 8.1M open access papers.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":9,"sentence":"Full text is annotated with automatically-detected inline mentions of citations, figures, and tables, each linked to their corresponding paper objects.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":9,"sentence":"In S2ORC, we aggregate papers from hundreds of academic publishers and digital archives into a unified source, and create the largest publicly-available collection of machine-readable academic text to date.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":9,"sentence":"We hope this resource will facilitate research and development of tools and tasks for text mining over academic text.","offset":4,"pro":0.8,"labels":"IMP"},{"idx":10,"sentence":"Automatic metrics are fundamental for the development and evaluation of machine translation systems.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"Judging whether, and to what extent, automatic metrics concur with the gold standard of human evaluation is not a straightforward problem.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":10,"sentence":"We show that current methods for judging metrics are highly sensitive to the translations used for assessment, particularly the presence of outliers, which often leads to falsely confident conclusions about a metric's efficacy.","offset":2,"pro":0.4,"labels":"RST"},{"idx":10,"sentence":"Finally, we turn to pairwise system ranking, developing a method for thresholding performance improvement under an automatic metric against human judgements, which allows quantification of type I versus type II errors incurred, i.e., insignificant human differences in system quality that are accepted, and significant human differences that are rejected.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":10,"sentence":"Together, these findings suggest improvements to the protocols for metric evaluation and system performance evaluation in machine translation.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":11,"sentence":"Generating a readable summary that describes the functionality of a program is known as source code summarization.","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"In this task, learning code representation by modeling the pairwise relationship between code tokens to capture their long-range dependencies is crucial.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":11,"sentence":"To learn code representation for summarization, we explore the Transformer model that uses a self-attention mechanism and has shown to be effective in capturing long-range dependencies.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":11,"sentence":"In this work, we show that despite the approach is simple, it outperforms the state-of-the-art techniques by a significant margin.","offset":3,"pro":0.5,"labels":"RST"},{"idx":11,"sentence":"We perform extensive analysis and ablation studies that reveal several important findings, e.g., the absolute encoding of source code tokens' position hinders, while relative encoding significantly improves the summarization performance.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":11,"sentence":"We have made our code publicly available to facilitate future research.","offset":5,"pro":0.8333333333333334,"labels":"CTN"},{"idx":12,"sentence":"Practical applications of abstractive summarization models are limited by frequent factual inconsistencies with respect to their input.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"Existing automatic evaluation metrics for summarization are largely insensitive to such errors.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":12,"sentence":"We propose QAGS (pronounced kags), an automatic evaluation protocol that is designed to identify factual inconsistencies in a generated summary.","offset":2,"pro":0.2222222222222222,"labels":"PUR"},{"idx":12,"sentence":"QAGS is based on the intuition that if we ask questions about a summary and its source, we will receive similar answers if the summary is factually consistent with the source.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":12,"sentence":"To evaluate QAGS, we collect human judgments of factual consistency on model-generated summaries for the CNN/DailyMail (Hermann et al., 2015) and XSUM (Narayan et al., 2018) summarization datasets.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":12,"sentence":"QAGS has substantially higher correlations with these judgments than other automatic evaluation metrics.","offset":5,"pro":0.5555555555555556,"labels":"RST"},{"idx":12,"sentence":"Also, QAGS offers a natural form of interpretability: The answers and questions generated while computing QAGS indicate which tokens of a summary are inconsistent and why.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":12,"sentence":"We believe QAGS is a promising tool in automatically generating usable and factually consistent text.","offset":7,"pro":0.7777777777777778,"labels":"CLN"},{"idx":12,"sentence":"Code for QAGS will be available at https://github.com/W4ngatang/qags.","offset":8,"pro":0.8888888888888888,"labels":"CTN"},{"idx":13,"sentence":"Recently BERT has been adopted for document encoding in state-of-the-art text summarization models.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"However, sentence-based extractive models often result in redundant or uninformative phrases in the extracted summaries.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":13,"sentence":"Also, long-range dependencies throughout a document are not well captured by BERT, which is pre-trained on sentence pairs instead of documents.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":13,"sentence":"To address these issues, we present a discourse-aware neural summarization model - DiscoBert.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":13,"sentence":"DiscoBert extracts sub-sentential discourse units (instead of sentences) as candidates for extractive selection on a finer granularity.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":13,"sentence":"To capture the long-range dependencies among discourse units, structural discourse graphs are constructed based on RST trees and coreference mentions, encoded with Graph Convolutional Networks.","offset":5,"pro":0.7142857142857143,"labels":"PUR"},{"idx":13,"sentence":"Experiments show that the proposed model outperforms state-of-the-art methods by a significant margin on popular summarization benchmarks compared to other BERT-base models.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":14,"sentence":"Automatic sentence summarization produces a shorter version of a sentence, while preserving its most important information.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"A good summary is characterized by language fluency and high information overlap with the source sentence.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":14,"sentence":"We model these two aspects in an unsupervised objective function, consisting of language modeling and semantic similarity metrics.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":14,"sentence":"We search for a high-scoring summary by discrete optimization.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":14,"sentence":"Our proposed method achieves a new state-of-the art for unsupervised sentence summarization according to ROUGE scores.","offset":4,"pro":0.5714285714285714,"labels":"CLN"},{"idx":14,"sentence":"Additionally, we demonstrate that the commonly reported ROUGE F1 metric is sensitive to summary length.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":14,"sentence":"Since this is unwillingly exploited in recent work, we emphasize that future evaluation should explicitly group summarization systems by output length brackets.","offset":6,"pro":0.8571428571428571,"labels":"IMP"},{"idx":15,"sentence":"We present a new summarization task, generating summaries of novel chapters using summary/chapter pairs from online study guides.","offset":0,"pro":0,"labels":"PUR"},{"idx":15,"sentence":"This is a harder task than the news summarization task, given the chapter length as well as the extreme paraphrasing and generalization found in the summaries.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":15,"sentence":"We focus on extractive summarization, which requires the creation of a gold-standard set of extractive summaries.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":15,"sentence":"We present a new metric for aligning reference summary sentences with chapter sentences to create gold extracts and also experiment with different alignment methods.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":15,"sentence":"Our experiments demonstrate significant improvement over prior alignment approaches for our task as shown through automatic metrics and a crowd-sourced pyramid analysis.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":16,"sentence":"Neural abstractive summarization models are prone to generate content inconsistent with the source document, i.e. unfaithful.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"Existing automatic metrics do not capture such mistakes effectively.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":16,"sentence":"We tackle the problem of evaluating faithfulness of a generated summary given its source document.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":16,"sentence":"We first collected human annotations of faithfulness for outputs from numerous models on two datasets.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":16,"sentence":"We find that current models exhibit a trade-off between abstractiveness and faithfulness: outputs with less word overlap with the source document are more likely to be unfaithful.","offset":4,"pro":0.5,"labels":"RST"},{"idx":16,"sentence":"Next, we propose an automatic question answering (QA) based metric for faithfulness, FEQA, which leverages recent advances in reading comprehension.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":16,"sentence":"Given question-answer pairs generated from the summary, a QA model extracts answers from the document; non-matched answers indicate unfaithful information in the summary.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":16,"sentence":"Among metrics based on word overlap, embedding similarity, and learned language understanding models, our QA-based metric has significantly higher correlation with human faithfulness scores, especially on highly abstractive summaries.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":17,"sentence":"Abstractive summarisation is notoriously hard to evaluate since standard word-overlap-based metrics are insufficient.","offset":0,"pro":0,"labels":"GAP"},{"idx":17,"sentence":"We introduce a new evaluation metric which is based on fact-level content weighting, i.e. relating the facts of the document to the facts of the summary.","offset":1,"pro":0.25,"labels":"PUR"},{"idx":17,"sentence":"We fol- low the assumption that a good summary will reflect all relevant facts, i.e. the ones present in the ground truth (human-generated refer- ence summary).","offset":2,"pro":0.5,"labels":"MTD"},{"idx":17,"sentence":"We confirm this hypothe- sis by showing that our weightings are highly correlated to human perception and compare favourably to the recent manual highlight- based metric of Hardy et al. (2019).","offset":3,"pro":0.75,"labels":"CLN"},{"idx":18,"sentence":"Current summarization systems only produce plain, factual headlines, far from the practical needs for the exposure and memorableness of the articles.","offset":0,"pro":0,"labels":"GAP"},{"idx":18,"sentence":"We propose a new task, Stylistic Headline Generation (SHG), to enrich the headlines with three style options (humor, romance and clickbait), thus attracting more readers.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":18,"sentence":"With no style-specific article-headline pair (only a standard headline summarization dataset and mono-style corpora), our method TitleStylist generates stylistic headlines by combining the summarization and reconstruction tasks into a multitasking framework.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":18,"sentence":"We also introduced a novel parameter sharing scheme to further disentangle the style from text.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":18,"sentence":"Through both automatic and human evaluation, we demonstrate that TitleStylist can generate relevant, fluent headlines with three target styles: humor, romance, and clickbait.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":18,"sentence":"The attraction score of our model generated headlines outperforms the state-of-the-art summarization model by 9.68%, even outperforming human-written references.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":19,"sentence":"Sequence-to-sequence models for abstractive summarization have been studied extensively, yet the generated summaries commonly suffer from fabricated content, and are often found to be near-extractive.","offset":0,"pro":0,"labels":"GAP"},{"idx":19,"sentence":"We argue that, to address these issues, the summarizer should acquire semantic interpretation over input, e.g., via structured representation, to allow the generation of more informative summaries.","offset":1,"pro":0.125,"labels":"PUR"},{"idx":19,"sentence":"In this paper, we present ASGARD, a novel framework for Abstractive Summarization with Graph-Augmentation and semantic-driven RewarD.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":19,"sentence":"We propose the use of dual encoders-a sequential document encoder and a graph-structured encoder-to maintain the global context and local characteristics of entities, complementing each other.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":19,"sentence":"We further design a reward based on a multiple choice cloze test to drive the model to better capture entity interactions.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":19,"sentence":"Results show that our models produce significantly higher ROUGE scores than a variant without knowledge graph as input on both New York Times and CNN/Daily Mail datasets.","offset":5,"pro":0.625,"labels":"RST"},{"idx":19,"sentence":"We also obtain better or comparable performance compared to systems that are fine-tuned from large pretrained language models.","offset":6,"pro":0.75,"labels":"RST"},{"idx":19,"sentence":"Human judges further rate our model outputs as more informative and containing fewer unfaithful errors.","offset":7,"pro":0.875,"labels":"MTD"},{"idx":0,"sentence":"Neural abstractive summarization models are able to generate summaries which have high overlap with human references.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"However, existing models are not optimized for factual correctness, a critical metric in real-world applications.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":0,"sentence":"In this work, we develop a general framework where we evaluate the factual correctness of a generated summary by fact-checking it automatically against its reference using an information extraction module.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":0,"sentence":"We further propose a training strategy which optimizes a neural summarization model with a factual correctness reward via reinforcement learning.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":0,"sentence":"We apply the proposed method to the summarization of radiology reports, where factual correctness is a key requirement.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":0,"sentence":"On two separate datasets collected from hospitals, we show via both automatic and human evaluation that the proposed approach substantially improves the factual correctness and overall quality of outputs over a competitive neural summarization system, producing radiology summaries that approach the quality of human-authored ones.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":1,"sentence":"This paper describes the Critical Role Dungeons and Dragons Dataset (CRD3) and related analyses.","offset":0,"pro":0,"labels":"PUR"},{"idx":1,"sentence":"Critical Role is an unscripted, live-streamed show where a fixed group of people play Dungeons and Dragons, an open-ended role-playing game.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":1,"sentence":"The dataset is collected from 159 Critical Role episodes transcribed to text dialogues, consisting of 398,682 turns.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":1,"sentence":"It also includes corresponding abstractive summaries collected from the Fandom wiki.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":1,"sentence":"The dataset is linguistically unique in that the narratives are generated entirely through player collaboration and spoken interaction.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":1,"sentence":"For each dialogue, there are a large number of turns, multiple abstractive summaries with varying levels of detail, and semantic ties to the previous dialogues.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":1,"sentence":"In addition, we provide a data augmentation method that produces 34,243 summary-dialogue chunk pairs to support current neural ML approaches, and we provide an abstractive summarization benchmark and evaluation.","offset":6,"pro":0.8571428571428571,"labels":"MTD"},{"idx":2,"sentence":"This work presents a new approach to unsupervised abstractive summarization based on maximizing a combination of coverage and fluency for a given length constraint.","offset":0,"pro":0,"labels":"PUR"},{"idx":2,"sentence":"It introduces a novel method that encourages the inclusion of key terms from the original document into the summary: key terms are masked out of the original document and must be filled in by a coverage model using the current generated summary.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":2,"sentence":"A novel unsupervised training procedure leverages this coverage model along with a fluency model to generate and score summaries.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":2,"sentence":"When tested on popular news summarization datasets, the method outperforms previous unsupervised methods by more than 2 R-1 points, and approaches results of competitive supervised methods.","offset":3,"pro":0.6,"labels":"RST"},{"idx":2,"sentence":"Our model attains higher levels of abstraction with copied passages roughly two times shorter than prior work, and learns to compress and merge sentences without supervision.","offset":4,"pro":0.8,"labels":"RST"},{"idx":3,"sentence":"Opinion summarization is the task of automatically creating summaries that reflect subjective information expressed in multiple documents, such as product reviews.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"While the majority of previous work has focused on the extractive setting, i.e., selecting fragments from input reviews to produce a summary, we let the model generate novel sentences and hence produce abstractive summaries.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":3,"sentence":"Recent progress in summarization has seen the development of supervised models which rely on large quantities of document-summary pairs.","offset":2,"pro":0.2222222222222222,"labels":"BAC"},{"idx":3,"sentence":"Since such training data is expensive to acquire, we instead consider the unsupervised setting, in other words, we do not use any summaries in training.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":3,"sentence":"We define a generative model for a review collection which capitalizes on the intuition that when generating a new review given a set of other reviews of a product, we should be able to control the amount of novelty going into the new review or, equivalently, vary the extent to which it deviates from the input.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":3,"sentence":"At test time, when generating summaries, we force the novelty to be minimal, and produce a text reflecting consensus opinions.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":3,"sentence":"We capture this intuition by defining a hierarchical variational autoencoder model.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":3,"sentence":"Both individual reviews and the products they correspond to are associated with stochastic latent codes, and the review generator (decoder) has direct access to the text of input reviews through the pointer-generator mechanism.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":3,"sentence":"Experiments on Amazon and Yelp datasets, show that setting at test time the review's latent code to its mean, allows the model to produce fluent and coherent summaries reflecting common opinions.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":4,"sentence":"Human speakers have an extensive toolkit of ways to express themselves.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"In this paper, we engage with an idea largely absent from discussions of meaning in natural language understanding-namely, that the way something is expressed reflects different ways of conceptualizing or construing the information being conveyed.","offset":1,"pro":0.25,"labels":"PUR"},{"idx":4,"sentence":"We first define this phenomenon more precisely, drawing on considerable prior work in theoretical cognitive semantics and psycholinguistics.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":4,"sentence":"We then survey some dimensions of construed meaning and show how insights from construal could inform theoretical and practical work in NLP.","offset":3,"pro":0.75,"labels":"MTD"},{"idx":5,"sentence":"The success of the large neural language models on many NLP tasks is exciting.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"However, we find that these successes sometimes lead to hype in which these models are being described as understanding language or capturing meaning.","offset":1,"pro":0.25,"labels":"GAP"},{"idx":5,"sentence":"In this position paper, we argue that a system trained only on form has a priori no way to learn meaning.","offset":2,"pro":0.5,"labels":"PUR"},{"idx":5,"sentence":"In keeping with the ACL 2020 theme of Taking Stock of Where We've Been and Where We're Going, we argue that a clear understanding of the distinction between form and meaning will help guide the field towards better science around natural language understanding.","offset":3,"pro":0.75,"labels":"PUR"},{"idx":6,"sentence":"We extracted information from the ACL Anthology (AA) and Google Scholar (GS) to examine trends in citations of NLP papers.","offset":0,"pro":0,"labels":"PUR"},{"idx":6,"sentence":"We explore questions such as: how well cited are papers of different types (journal articles, conference papers, demo papers, etc.)? how well cited are papers from different areas of within NLP? etc. Notably, we show that only about 56% of the papers in AA are cited ten or more times.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":6,"sentence":"CL Journal has the most cited papers, but its citation dominance has lessened in recent years.","offset":2,"pro":0.4,"labels":"BAC"},{"idx":6,"sentence":"On average, long papers get almost three times as many citations as short papers; and papers on sentiment classification, anaphora resolution, and entity recognition have the highest median citations.","offset":3,"pro":0.6,"labels":"BAC"},{"idx":6,"sentence":"The analyses presented here, and the associated dataset of NLP papers mapped to citations, have a number of uses including: understanding how the field is growing and quantifying the impact of different types of papers.","offset":4,"pro":0.8,"labels":"BAC"},{"idx":7,"sentence":"This position paper describes and critiques the Pretraining-Agnostic Identically Distributed (PAID) evaluation paradigm, which has become a central tool for measuring progress in natural language understanding.","offset":0,"pro":0,"labels":"PUR"},{"idx":7,"sentence":"This paradigm consists of three stages: (1) pre-training of a word prediction model on a corpus of arbitrary size; (2) fine-tuning (transfer learning) on a training set representing a classification task; (3) evaluation on a test set drawn from the same distribution as that training set.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":7,"sentence":"This paradigm favors simple, low-bias architectures, which, first, can be scaled to process vast amounts of data, and second, can capture the fine-grained statistical properties of a particular data set, regardless of whether those properties are likely to generalize to examples of the task outside the data set.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":7,"sentence":"This contrasts with humans, who learn language from several orders of magnitude less data than the systems favored by this evaluation paradigm, and generalize to new tasks in a consistent way.","offset":3,"pro":0.6,"labels":"GAP"},{"idx":7,"sentence":"We advocate for supplementing or replacing PAID with paradigms that reward architectures that generalize as quickly and robustly as humans.","offset":4,"pro":0.8,"labels":"PUR"},{"idx":8,"sentence":"Legal Artificial Intelligence (LegalAI) focuses on applying the technology of artificial intelligence, especially natural language processing, to benefit tasks in the legal domain.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"In recent years, LegalAI has drawn increasing attention rapidly from both AI researchers and legal professionals, as LegalAI is beneficial to the legal system for liberating legal professionals from a maze of paperwork.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":8,"sentence":"Legal professionals often think about how to solve tasks from rule-based and symbol-based methods, while NLP researchers concentrate more on data-driven and embedding methods.","offset":2,"pro":0.2857142857142857,"labels":"BAC"},{"idx":8,"sentence":"In this paper, we introduce the history, the current state, and the future directions of research in LegalAI.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":8,"sentence":"We illustrate the tasks from the perspectives of legal professionals and NLP researchers and show several representative applications in LegalAI.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":8,"sentence":"We conduct experiments and provide an in-depth analysis of the advantages and disadvantages of existing works to explore possible future directions.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":8,"sentence":"You can find the implementation of our work from https://github.com/thunlp/CLAIM.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":9,"sentence":"While pretrained models such as BERT have shown large gains across natural language understanding tasks, their performance can be improved by further training the model on a data-rich intermediate task, before fine-tuning it on a target task.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"However, it is still poorly understood when and why intermediate-task training is beneficial for a given target task.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":9,"sentence":"To investigate this, we perform a large-scale study on the pretrained RoBERTa model with 110 intermediate-target task combinations.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":9,"sentence":"We further evaluate all trained models with 25 probing tasks meant to reveal the specific skills that drive transfer.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":9,"sentence":"We observe that intermediate tasks requiring high-level inference and reasoning abilities tend to work best.","offset":4,"pro":0.5,"labels":"RST"},{"idx":9,"sentence":"We also observe that target task performance is strongly correlated with higher-level abilities such as coreference resolution.","offset":5,"pro":0.625,"labels":"RST"},{"idx":9,"sentence":"However, we fail to observe more granular correlations between probing and target task performance, highlighting the need for further work on broad-coverage probing benchmarks.","offset":6,"pro":0.75,"labels":"CLN"},{"idx":9,"sentence":"We also observe evidence that the forgetting of knowledge learned during pretraining may limit our analysis, highlighting the need for further work on transfer learning methods in these settings.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":10,"sentence":"An increasing number of natural language processing papers address the effect of bias on predictions, introducing mitigation techniques at different parts of the standard NLP pipeline (data and models).","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"However, these works have been conducted individually, without a unifying framework to organize efforts within the field.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":10,"sentence":"This situation leads to repetitive approaches, and focuses overly on bias symptoms/effects, rather than on their origins, which could limit the development of effective countermeasures.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":10,"sentence":"In this paper, we propose a unifying predictive bias framework for NLP.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":10,"sentence":"We summarize the NLP literature and suggest general mathematical definitions of predictive bias.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":10,"sentence":"We differentiate two consequences of bias: outcome disparities and error disparities, as well as four potential origins of biases: label bias, selection bias, model overamplification, and semantic bias.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":10,"sentence":"Our framework serves as an overview of predictive bias in NLP, integrating existing work into a single structure, and providing a conceptual baseline for improved frameworks.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":11,"sentence":"Pre-trained visually grounded language models such as ViLBERT, LXMERT, and UNITER have achieved significant performance improvement on vision-and-language tasks but what they learn during pre-training remains unclear.","offset":0,"pro":0,"labels":"GAP"},{"idx":11,"sentence":"In this work, we demonstrate that certain attention heads of a visually grounded language model actively ground elements of language to image regions.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":11,"sentence":"Specifically, some heads can map entities to image regions, performing the task known as entity grounding.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":11,"sentence":"Some heads can even detect the syntactic relations between non-entity words and image regions, tracking, for example, associations between verbs and regions corresponding to their arguments.","offset":3,"pro":0.5,"labels":"BAC"},{"idx":11,"sentence":"We denote this ability as syntactic grounding.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":11,"sentence":"We verify grounding both quantitatively and qualitatively, using Flickr30K Entities as a testbed.","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":12,"sentence":"Throughout a conversation, participants make choices that can orient the flow of the interaction.","offset":0,"pro":0,"labels":"MTD"},{"idx":12,"sentence":"Such choices are particularly salient in the consequential domain of crisis counseling, where a difficulty for counselors is balancing between two key objectives: advancing the conversation towards a resolution, and empathetically addressing the crisis situation.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":12,"sentence":"In this work, we develop an unsupervised methodology to quantify how counselors manage this balance.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":12,"sentence":"Our main intuition is that if an utterance can only receive a narrow range of appropriate replies, then its likely aim is to advance the conversation forwards, towards a target within that range.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":12,"sentence":"Likewise, an utterance that can only appropriately follow a narrow range of possible utterances is likely aimed backwards at addressing a specific situation within that range.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":12,"sentence":"By applying this intuition, we can map each utterance to a continuous orientation axis that captures the degree to which it is intended to direct the flow of the conversation forwards or backwards.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":12,"sentence":"This unsupervised method allows us to characterize counselor behaviors in a large dataset of crisis counseling conversations, where we show that known counseling strategies intuitively align with this axis.","offset":6,"pro":0.75,"labels":"CLN"},{"idx":12,"sentence":"We also illustrate how our measure can be indicative of a conversation's progress, as well as its effectiveness.","offset":7,"pro":0.875,"labels":"RST"},{"idx":13,"sentence":"Natural disasters (e.g., hurricanes) affect millions of people each year, causing widespread destruction in their wake.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"People have recently taken to social media websites (e.g., Twitter) to share their sentiments and feelings with the larger community.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":13,"sentence":"Consequently, these platforms have become instrumental in understanding and perceiving emotions at scale.","offset":2,"pro":0.2857142857142857,"labels":"BAC"},{"idx":13,"sentence":"In this paper, we introduce HurricaneEmo, an emotion dataset of 15,000 English tweets spanning three hurricanes: Harvey, Irma, and Maria.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":13,"sentence":"We present a comprehensive study of fine-grained emotions and propose classification tasks to discriminate between coarse-grained emotion groups.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":13,"sentence":"Our best BERT model, even after task-guided pre-training which leverages unlabeled Twitter data, achieves only 68% accuracy (averaged across all groups).","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":13,"sentence":"HurricaneEmo serves not only as a challenging benchmark for models but also as a valuable resource for analyzing emotions in disaster-centric domains.","offset":6,"pro":0.8571428571428571,"labels":"BAC"},{"idx":14,"sentence":"Not all documents are equally important.","offset":0,"pro":0,"labels":"GAP"},{"idx":14,"sentence":"Language processing is increasingly finding use as a supplement for questionnaires to assess psychological attributes of consenting individuals, but most approaches neglect to consider whether all documents of an individual are equally informative.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":14,"sentence":"In this paper, we present a novel model that uses message-level attention to learn the relative weight of users' social media posts for assessing their five factor personality traits.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":14,"sentence":"We demonstrate that models with message-level attention outperform those with word-level attention, and ultimately yield state-of-the-art accuracies for all five traits by using both word and message attention in combination with past approaches (an average increase in Pearson r of 2.5%).","offset":3,"pro":0.6,"labels":"RST"},{"idx":14,"sentence":"In addition, examination of the high-signal posts identified by our model provides insight into the relationship between language and personality, helping to inform future work.","offset":4,"pro":0.8,"labels":"IMP"},{"idx":15,"sentence":"People vary in their ability to make accurate predictions about the future.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"Prior studies have shown that some individuals can predict the outcome of future events with consistently better accuracy.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":15,"sentence":"This leads to a natural question: what makes some forecasters better than others?","offset":2,"pro":0.2222222222222222,"labels":"BAC"},{"idx":15,"sentence":"In this paper we explore connections between the language people use to describe their predictions and their forecasting skill.","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":15,"sentence":"Datasets from two different forecasting domains are explored: (1) geopolitical forecasts from Good Judgment Open, an online prediction forum and (2) a corpus of company earnings forecasts made by financial analysts.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":15,"sentence":"We present a number of linguistic metrics which are computed over text associated with people's predictions about the future including: uncertainty, readability, and emotion.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":15,"sentence":"By studying linguistic factors associated with predictions, we are able to shed some light on the approach taken by skilled forecasters.","offset":6,"pro":0.6666666666666666,"labels":"RST"},{"idx":15,"sentence":"Furthermore, we demonstrate that it is possible to accurately predict forecasting skill using a model that is based solely on language.","offset":7,"pro":0.7777777777777778,"labels":"CLN"},{"idx":15,"sentence":"This could potentially be useful for identifying accurate predictions or potentially skilled forecasters earlier.","offset":8,"pro":0.8888888888888888,"labels":"IMP"},{"idx":16,"sentence":"Many applications of computational social science aim to infer causal conclusions from non-experimental data.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"Such observational data often contains confounders, variables that influence both potential causes and potential effects.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":16,"sentence":"Unmeasured or latent confounders can bias causal estimates, and this has motivated interest in measuring potential confounders from observed text.","offset":2,"pro":0.4,"labels":"BAC"},{"idx":16,"sentence":"For example, an individual's entire history of social media posts or the content of a news article could provide a rich measurement of multiple confounders.Yet, methods and applications for this problem are scattered across different communities and evaluation practices are inconsistent.This review is the first to gather and categorize these examples and provide a guide to data-processing and evaluation decisions.","offset":3,"pro":0.6,"labels":"GAP"},{"idx":16,"sentence":"Despite increased attention on adjusting for confounding using text, there are still many open problems, which we highlight in this paper.","offset":4,"pro":0.8,"labels":"GAP"},{"idx":17,"sentence":"Ideal point models analyze lawmakers' votes to quantify their political positions, or ideal points.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"But votes are not the only way to express a political position.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":17,"sentence":"Lawmakers also give speeches, release press statements, and post tweets.","offset":2,"pro":0.2222222222222222,"labels":"BAC"},{"idx":17,"sentence":"In this paper, we introduce the text-based ideal point model (TBIP), an unsupervised probabilistic topic model that analyzes texts to quantify the political positions of its authors.","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":17,"sentence":"We demonstrate the TBIP with two types of politicized text data: U.S. Senate speeches and senator tweets.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":17,"sentence":"Though the model does not analyze their votes or political affiliations, the TBIP separates lawmakers by party, learns interpretable politicized topics, and infers ideal points close to the classical vote-based ideal points.","offset":5,"pro":0.5555555555555556,"labels":"CLN"},{"idx":17,"sentence":"One benefit of analyzing texts, as opposed to votes, is that the TBIP can estimate ideal points of anyone who authors political texts, including non-voting actors.","offset":6,"pro":0.6666666666666666,"labels":"BAC"},{"idx":17,"sentence":"To this end, we use it to study tweets from the 2020 Democratic presidential candidates.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":17,"sentence":"Using only the texts of their tweets, it identifies them along an interpretable progressive-to-moderate spectrum.","offset":8,"pro":0.8888888888888888,"labels":"MTD"},{"idx":18,"sentence":"While national politics often receive the spotlight, the overwhelming majority of legislation proposed, discussed, and enacted is done at the state level.","offset":0,"pro":0,"labels":"GAP"},{"idx":18,"sentence":"Despite this fact, there is little awareness of the dynamics that lead to adopting these policies.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":18,"sentence":"In this paper, we take the first step towards a better understanding of these processes and the underlying dynamics that shape them, using data-driven methods.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":18,"sentence":"We build a new large-scale dataset, from multiple data sources, connecting state bills and legislator information, geographical information about their districts, and donations and donors' information.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":18,"sentence":"We suggest a novel task, predicting the legislative body's vote breakdown for a given bill, according to different criteria of interest, such as gender, rural-urban and ideological splits.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":18,"sentence":"Finally, we suggest a shared relational embedding model, representing the interactions between the text of the bill and the legislative context in which it is presented.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":18,"sentence":"Our experiments show that providing this context helps improve the prediction over strong text-based models.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":19,"sentence":"Understanding human preferences, along with cultural and social nuances, lives at the heart of natural language understanding.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"Concretely, we present a new task and corpus for learning alignments between machine and human preferences.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":19,"sentence":"Our newly introduced problem is concerned with predicting the preferable options from two sentences describing scenarios that may involve social and cultural situations.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":19,"sentence":"Our problem is framed as a natural language inference task with crowd-sourced preference votes by human players, obtained from a gamified voting platform.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":19,"sentence":"We benchmark several state-of-the-art neural models, along with BERT and friends on this task.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":19,"sentence":"Our experimental results show that current state-of-the-art NLP models still leave much room for improvement.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":0,"sentence":"Understanding discourse structures of news articles is vital to effectively contextualize the occurrence of a news event.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"To enable computational modeling of news structures, we apply an existing theory of functional discourse structure for news articles that revolves around the main event and create a human-annotated corpus of 802 documents spanning over four domains and three media sources.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":0,"sentence":"Next, we propose several document-level neural-network models to automatically construct news content structures.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":0,"sentence":"Finally, we demonstrate that incorporating system predicted news structures yields new state-of-the-art performance for event coreference resolution.","offset":3,"pro":0.6,"labels":"CLN"},{"idx":0,"sentence":"The news documents we annotated are openly available and the annotations are publicly released for future research.","offset":4,"pro":0.8,"labels":"CTN"},{"idx":1,"sentence":"Pragmatic inferences often subtly depend on the presence or absence of linguistic features.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"For example, the presence of a partitive construction (of the) increases the strength of a so-called scalar inference: listeners perceive the inference that Chris did not eat all of the cookies to be stronger after hearing Chris ate some of the cookies than after hearing the same utterance without a partitive, Chris ate some cookies.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":1,"sentence":"In this work, we explore to what extent neural network sentence encoders can learn to predict the strength of scalar inferences.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":1,"sentence":"We first show that an LSTM-based sentence encoder trained on an English dataset of human inference strength ratings is able to predict ratings with high accuracy (r = 0.78).","offset":3,"pro":0.5,"labels":"MTD"},{"idx":1,"sentence":"We then probe the model's behavior using manually constructed minimal sentence pairs and corpus data.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":1,"sentence":"We first that the model inferred previously established associations between linguistic features and inference strength, suggesting that the model learns to use linguistic features to predict pragmatic inferences.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":2,"sentence":"Implicit relation classification on Penn Discourse TreeBank (PDTB) 2.0 is a common benchmark task for evaluating the understanding of discourse relations.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"However, the lack of consistency in preprocessing and evaluation poses challenges to fair comparison of results in the literature.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":2,"sentence":"In this work, we highlight these inconsistencies and propose an improved evaluation protocol.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":2,"sentence":"Paired with this protocol, we report strong baseline results from pretrained sentence encoders, which set the new state-of-the-art for PDTB 2.0.","offset":3,"pro":0.5,"labels":"RST"},{"idx":2,"sentence":"Furthermore, this work is the first to explore fine-grained relation classification on PDTB 3.0.","offset":4,"pro":0.6666666666666666,"labels":"CTN"},{"idx":2,"sentence":"We expect our work to serve as a point of comparison for future work, and also as an initiative to discuss models of larger context and possible data augmentations for downstream transferability.","offset":5,"pro":0.8333333333333334,"labels":"IMP"},{"idx":3,"sentence":"We propose PeTra, a memory-augmented neural network designed to track entities in its memory slots.","offset":0,"pro":0,"labels":"PUR"},{"idx":3,"sentence":"PeTra is trained using sparse annotation from the GAP pronoun resolution dataset and outperforms a prior memory model on the task while using a simpler architecture.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":3,"sentence":"We empirically compare key modeling choices, finding that we can simplify several aspects of the design of the memory module while retaining strong performance.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":3,"sentence":"To measure the people tracking capability of memory models, we (a) propose a new diagnostic evaluation based on counting the number of unique entities in text, and (b) conduct a small scale human evaluation to compare evidence of people tracking in the memory logs of PeTra relative to a previous approach.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":3,"sentence":"PeTra is highly effective in both evaluations, demonstrating its ability to track people in its memory despite being trained with limited annotation.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":4,"sentence":"Zero pronoun recovery and resolution aim at recovering the dropped pronoun and pointing out its anaphoric mentions, respectively.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"We propose to better explore their interaction by solving both tasks together, while the previous work treats them separately.","offset":1,"pro":0.25,"labels":"PUR"},{"idx":4,"sentence":"For zero pronoun resolution, we study this task in a more realistic setting, where no parsing trees or only automatic trees are available, while most previous work assumes gold trees.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":4,"sentence":"Experiments on two benchmarks show that joint modeling significantly outperforms our baseline that already beats the previous state of the arts.","offset":3,"pro":0.75,"labels":"RST"},{"idx":5,"sentence":"Hate speech classifiers trained on imbalanced datasets struggle to determine if group identifiers like gay or black are used in offensive or prejudiced ways.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"Such biases manifest in false positives when these identifiers are present, due to models' inability to learn the contexts which constitute a hateful usage of identifiers.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":5,"sentence":"We extract post-hoc explanations from fine-tuned BERT classifiers to detect bias towards identity terms.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":5,"sentence":"Then, we propose a novel regularization technique based on these explanations that encourages models to learn from the context of group identifiers in addition to the identifiers themselves.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":5,"sentence":"Our approach improved over baselines in limiting false positives on out-of-domain data while maintaining and in cases improving in-domain performance.","offset":4,"pro":0.8,"labels":"RST"},{"idx":6,"sentence":"Word embeddings derived from human-generated corpora inherit strong gender bias which can be further amplified by downstream models.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"Some commonly adopted debiasing approaches, including the seminal Hard Debias algorithm, apply post-processing procedures that project pre-trained word embeddings into a subspace orthogonal to an inferred gender subspace.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":6,"sentence":"We discover that semantic-agnostic corpus regularities such as word frequency captured by the word embeddings negatively impact the performance of these algorithms.","offset":2,"pro":0.4,"labels":"GAP"},{"idx":6,"sentence":"We propose a simple but effective technique, Double Hard Debias, which purifies the word embeddings against such corpus regularities prior to inferring and removing the gender subspace.","offset":3,"pro":0.6,"labels":"PUR"},{"idx":6,"sentence":"Experiments on three bias mitigation benchmarks show that our approach preserves the distributional semantics of the pre-trained word embeddings while reducing gender bias to a significantly larger degree than prior approaches.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":7,"sentence":"We survey 146 papers analyzing bias in NLP systems, finding that their motivations are often vague, inconsistent, and lacking in normative reasoning, despite the fact that analyzing bias is an inherently normative process.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"We further find that these papers' proposed quantitative techniques for measuring or mitigating bias are poorly matched to their motivations and do not engage with the relevant literature outside of NLP.","offset":1,"pro":0.25,"labels":"BAC"},{"idx":7,"sentence":"Based on these findings, we describe the beginnings of a path forward by proposing three recommendations that should guide work analyzing bias in NLP systems.","offset":2,"pro":0.5,"labels":"PUR"},{"idx":7,"sentence":"These recommendations rest on a greater recognition of the relationships between language and social hierarchies, encouraging researchers and practitioners to articulate their conceptualizations of bias---i.e., what kinds of system behaviors are harmful, in what ways, to whom, and why, as well as the normative reasoning underlying these statements-and to center work around the lived experiences of members of communities affected by NLP systems, while interrogating and reimagining the power relations between technologists and such communities.","offset":3,"pro":0.75,"labels":"MTD"},{"idx":8,"sentence":"Warning: this paper contains content that may be offensive or upsetting.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"Language has the power to reinforce stereotypes and project social biases onto others.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":8,"sentence":"At the core of the challenge is that it is rarely what is stated explicitly, but rather the implied meanings, that frame people's judgments about others.","offset":2,"pro":0.2,"labels":"BAC"},{"idx":8,"sentence":"For example, given a statement that we shouldn't lower our standards to hire more women, most listeners will infer the implicature intended by the speaker - that women (candidates) are less qualified.","offset":3,"pro":0.3,"labels":"BAC"},{"idx":8,"sentence":"Most semantic formalisms, to date, do not capture such pragmatic implications in which people express social biases and power differentials in language.","offset":4,"pro":0.4,"labels":"GAP"},{"idx":8,"sentence":"We introduce Social Bias Frames, a new conceptual formalism that aims to model the pragmatic frames in which people project social biases and stereotypes onto others.","offset":5,"pro":0.5,"labels":"PUR"},{"idx":8,"sentence":"In addition, we introduce the Social Bias Inference Corpus to support large-scale modelling and evaluation with 150k structured annotations of social media posts, covering over 34k implications about a thousand demographic groups.","offset":6,"pro":0.6,"labels":"PUR"},{"idx":8,"sentence":"We then establish baseline approaches that learn to recover Social Bias Frames from unstructured text.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":8,"sentence":"We find that while state-of-the-art neural models are effective at high-level categorization of whether a given statement projects unwanted social bias (80% F1), they are not effective at spelling out more detailed explanations in terms of Social Bias Frames.","offset":8,"pro":0.8,"labels":"RST"},{"idx":8,"sentence":"Our study motivates future work that combines structured pragmatic inference with commonsense reasoning on social implications.","offset":9,"pro":0.9,"labels":"IMP"},{"idx":9,"sentence":"Building equitable and inclusive NLP technologies demands consideration of whether and how social attitudes are represented in ML models.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"In particular, representations encoded in models often inadvertently perpetuate undesirable social biases from the data on which they are trained.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":9,"sentence":"In this paper, we present evidence of such undesirable biases towards mentions of disability in two different English language models: toxicity prediction and sentiment analysis.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":9,"sentence":"Next, we demonstrate that the neural embeddings that are the critical first step in most NLP pipelines similarly contain undesirable biases towards mentions of disability.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":9,"sentence":"We end by highlighting topical biases in the discourse about disability which may contribute to the observed model biases; for instance, gun violence, homelessness, and drug addiction are over-represented in texts discussing mental illness.","offset":4,"pro":0.8,"labels":"MTD"},{"idx":10,"sentence":"As natural language processing methods are increasingly deployed in real-world scenarios such as healthcare, legal systems, and social science, it becomes necessary to recognize the role they potentially play in shaping social biases and stereotypes.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"Previous work has revealed the presence of social biases in widely used word embeddings involving gender, race, religion, and other social constructs.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":10,"sentence":"While some methods were proposed to debias these word-level embeddings, there is a need to perform debiasing at the sentence-level given the recent shift towards new contextualized sentence representations such as ELMo and BERT.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":10,"sentence":"In this paper, we investigate the presence of social biases in sentence-level representations and propose a new method, Sent-Debias, to reduce these biases.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":10,"sentence":"We show that Sent-Debias is effective in removing biases, and at the same time, preserves performance on sentence-level downstream tasks such as sentiment analysis, linguistic acceptability, and natural language understanding.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":10,"sentence":"We hope that our work will inspire future research on characterizing and removing social biases from widely adopted sentence representations for fairer NLP.","offset":5,"pro":0.8333333333333334,"labels":"IMP"},{"idx":11,"sentence":"Knowledge Graph Completion (KGC) aims at automatically predicting missing links for large-scale knowledge graphs.","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"A vast number of state-of-the-art KGC techniques have got published at top conferences in several research fields, including data mining, machine learning, and natural language processing.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":11,"sentence":"However, we notice that several recent papers report very high performance, which largely outperforms previous state-of-the-art methods.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":11,"sentence":"In this paper, we find that this can be attributed to the inappropriate evaluation protocol used by them and propose a simple evaluation protocol to address this problem.","offset":3,"pro":0.42857142857142855,"labels":"RST"},{"idx":11,"sentence":"The proposed protocol is robust to handle bias in the model, which can substantially affect the final results.","offset":4,"pro":0.5714285714285714,"labels":"CLN"},{"idx":11,"sentence":"We conduct extensive experiments and report performance of several existing methods using our protocol.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":11,"sentence":"The reproducible code has been made publicly available.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":12,"sentence":"A range of studies have concluded that neural word prediction models can distinguish grammatical from ungrammatical sentences with high accuracy.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"However, these studies are based primarily on monolingual evidence from English.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":12,"sentence":"To investigate how these models' ability to learn syntax varies by language, we introduce CLAMS (Cross-Linguistic Assessment of Models on Syntax), a syntactic evaluation suite for monolingual and multilingual models.","offset":2,"pro":0.2222222222222222,"labels":"PUR"},{"idx":12,"sentence":"CLAMS includes subject-verb agreement challenge sets for English, French, German, Hebrew and Russian, generated from grammars we develop.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":12,"sentence":"We use CLAMS to evaluate LSTM language models as well as monolingual and multilingual BERT.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":12,"sentence":"Across languages, monolingual LSTMs achieved high accuracy on dependencies without attractors, and generally poor accuracy on agreement across object relative clauses.","offset":5,"pro":0.5555555555555556,"labels":"RST"},{"idx":12,"sentence":"On other constructions, agreement accuracy was generally higher in languages with richer morphology.","offset":6,"pro":0.6666666666666666,"labels":"RST"},{"idx":12,"sentence":"Multilingual models generally underperformed monolingual models.","offset":7,"pro":0.7777777777777778,"labels":"CLN"},{"idx":12,"sentence":"Multilingual BERT showed high syntactic accuracy on English, but noticeable deficiencies in other languages.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":13,"sentence":"Algorithmic approaches to interpreting machine learning models have proliferated in recent years.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"We carry out human subject tests that are the first of their kind to isolate the effect of algorithmic explanations on a key aspect of model interpretability, simulatability, while avoiding important confounding experimental factors.","offset":1,"pro":0.125,"labels":"PUR"},{"idx":13,"sentence":"A model is simulatable when a person can predict its behavior on new inputs.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":13,"sentence":"Through two kinds of simulation tests involving text and tabular data, we evaluate five explanations methods: (1) LIME, (2) Anchor, (3) Decision Boundary, (4) a Prototype model, and (5) a Composite approach that combines explanations from each method.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":13,"sentence":"Clear evidence of method effectiveness is found in very few cases: LIME improves simulatability in tabular classification, and our Prototype method is effective in counterfactual simulation tests.","offset":4,"pro":0.5,"labels":"RST"},{"idx":13,"sentence":"We also collect subjective ratings of explanations, but we do not find that ratings are predictive of how helpful explanations are.","offset":5,"pro":0.625,"labels":"RST"},{"idx":13,"sentence":"Our results provide the first reliable and comprehensive estimates of how explanations influence simulatability across a variety of explanation methods and data domains.","offset":6,"pro":0.75,"labels":"CLN"},{"idx":13,"sentence":"We show that (1) we need to be careful about the metrics we use to evaluate explanation methods, and (2) there is significant room for improvement in current methods.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":14,"sentence":"Modern deep learning models for NLP are notoriously opaque.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"This has motivated the development of methods for interpreting such models, e.g., via gradient-based saliency maps or the visualization of attention weights.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":14,"sentence":"Such approaches aim to provide explanations for a particular model prediction by highlighting important words in the corresponding input text.","offset":2,"pro":0.2,"labels":"BAC"},{"idx":14,"sentence":"While this might be useful for tasks where decisions are explicitly influenced by individual tokens in the input, we suspect that such highlighting is not suitable for tasks where model decisions should be driven by more complex reasoning.","offset":3,"pro":0.3,"labels":"GAP"},{"idx":14,"sentence":"In this work, we investigate the use of influence functions for NLP, providing an alternative approach to interpreting neural text classifiers.","offset":4,"pro":0.4,"labels":"PUR"},{"idx":14,"sentence":"Influence functions explain the decisions of a model by identifying influential training examples.","offset":5,"pro":0.5,"labels":"BAC"},{"idx":14,"sentence":"Despite the promise of this approach, influence functions have not yet been extensively evaluated in the context of NLP, a gap addressed by this work.","offset":6,"pro":0.6,"labels":"GAP"},{"idx":14,"sentence":"We conduct a comparison between influence functions and common word-saliency methods on representative tasks.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":14,"sentence":"As suspected, we find that influence functions are particularly useful for natural language inference, a task in which saliency maps' may not have clear interpretation.","offset":8,"pro":0.8,"labels":"RST"},{"idx":14,"sentence":"Furthermore, we develop a new quantitative measure based on influence functions that can reveal artifacts in training data.","offset":9,"pro":0.9,"labels":"MTD"},{"idx":15,"sentence":"Recent work has found evidence that Multilingual BERT (mBERT), a transformer-based multilingual masked language model, is capable of zero-shot cross-lingual transfer, suggesting that some aspects of its representations are shared cross-lingually.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"To better understand this overlap, we extend recent work on finding syntactic trees in neural networks' internal representations to the multilingual setting.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":15,"sentence":"We show that subspaces of mBERT representations recover syntactic tree distances in languages other than English, and that these subspaces are approximately shared across languages.","offset":2,"pro":0.4,"labels":"RST"},{"idx":15,"sentence":"Motivated by these results, we present an unsupervised analysis method that provides evidence mBERT learns representations of syntactic dependency labels, in the form of clusters which largely agree with the Universal Dependencies taxonomy.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":15,"sentence":"This evidence suggests that even without explicit supervision, multilingual masked language models learn certain linguistic universals.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":16,"sentence":"Generating explanations for neural networks has become crucial for their applications in real-world with respect to reliability and trustworthiness.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"In natural language processing, existing methods usually provide important features which are words or phrases selected from an input text as an explanation, but ignore the interactions between them.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":16,"sentence":"It poses challenges for humans to interpret an explanation and connect it to model prediction.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":16,"sentence":"In this work, we build hierarchical explanations by detecting feature interactions.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":16,"sentence":"Such explanations visualize how words and phrases are combined at different levels of the hierarchy, which can help users understand the decision-making of black-box models.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":16,"sentence":"The proposed method is evaluated with three neural text classifiers (LSTM, CNN, and BERT) on two benchmark datasets, via both automatic and human evaluations.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":16,"sentence":"Experiments show the effectiveness of the proposed method in providing explanations that are both faithful to models and interpretable to humans.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":17,"sentence":"Neural module networks (NMNs) are a popular approach for modeling compositionality: they achieve high accuracy when applied to problems in language and vision, while reflecting the compositional structure of the problem in the network architecture.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"However, prior work implicitly assumed that the structure of the network modules, describing the abstract reasoning process, provides a faithful explanation of the model's reasoning; that is, that all modules perform their intended behaviour.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":17,"sentence":"In this work, we propose and conduct a systematic evaluation of the intermediate outputs of NMNs on NLVR2 and DROP, two datasets which require composing multiple reasoning steps.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":17,"sentence":"We find that the intermediate outputs differ from the expected output, illustrating that the network structure does not provide a faithful explanation of model behaviour.","offset":3,"pro":0.6,"labels":"RST"},{"idx":17,"sentence":"To remedy that, we train the model with auxiliary supervision and propose particular choices for module architecture that yield much better faithfulness, at a minimal cost to accuracy.","offset":4,"pro":0.8,"labels":"MTD"},{"idx":18,"sentence":"Selecting input features of top relevance has become a popular method for building self-explaining models.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"In this work, we extend this selective rationalization approach to text matching, where the goal is to jointly select and align text pieces, such as tokens or sentences, as a justification for the downstream prediction.","offset":1,"pro":0.125,"labels":"PUR"},{"idx":18,"sentence":"Our approach employs optimal transport (OT) to find a minimal cost alignment between the inputs.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":18,"sentence":"However, directly applying OT often produces dense and therefore uninterpretable alignments.","offset":3,"pro":0.375,"labels":"GAP"},{"idx":18,"sentence":"To overcome this limitation, we introduce novel constrained variants of the OT problem that result in highly sparse alignments with controllable sparsity.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":18,"sentence":"Our model is end-to-end differentiable using the Sinkhorn algorithm for OT and can be trained without any alignment annotations.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":18,"sentence":"We evaluate our model on the StackExchange, MultiNews, e-SNLI, and MultiRC datasets.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":18,"sentence":"Our model achieves very sparse rationale selections with high fidelity while preserving prediction accuracy compared to strong attention baseline models.","offset":7,"pro":0.875,"labels":"RST"},{"idx":19,"sentence":"Complex compositional reading comprehension datasets require performing latent sequential decisions that are learned via supervision from the final answer.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"A large combinatorial space of possible decision paths that result in the same answer, compounded by the lack of intermediate supervision to help choose the right path, makes the learning particularly hard for this task.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":19,"sentence":"In this work, we study the benefits of collecting intermediate reasoning supervision along with the answer during data collection.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":19,"sentence":"We find that these intermediate annotations can provide two-fold benefits.","offset":3,"pro":0.5,"labels":"RST"},{"idx":19,"sentence":"First, we observe that for any collection budget, spending a fraction of it on intermediate annotations results in improved model performance, for two complex compositional datasets: DROP and Quoref.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":19,"sentence":"Second, these annotations encourage the model to learn the correct latent reasoning steps, helping combat some of the biases introduced during the data collection process.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":0,"sentence":"We propose the task of unsupervised morphological paradigm completion.","offset":0,"pro":0,"labels":"PUR"},{"idx":0,"sentence":"We further introduce a system for the task, which generates morphological paradigms via the following steps: (i) EDIT TREE retrieval, (ii) additional lemma retrieval, (iii) paradigm size discovery, and (iv) inflection generation.","offset":1,"pro":0.14285714285714285,"labels":"MTD"},{"idx":0,"sentence":"We perform an evaluation on 14 typologically diverse languages.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":0,"sentence":"Our system outperforms trivial baselines with ease and, for some languages, even obtains a higher accuracy than minimally supervised systems.","offset":3,"pro":0.42857142857142855,"labels":"RST"},{"idx":0,"sentence":"From a natural language processing (NLP) perspective, this is a challenging unsupervised task, and high-performing systems have the potential to improve tools for low-resource languages or to assist linguistic annotators. ","offset":4,"pro":0.5714285714285714,"labels":"PUR"},{"idx":0,"sentence":"From a cognitive science perspective, this can shed light on how children acquire morphological knowledge. ","offset":5,"pro":0.7142857142857143,"labels":"PUR"},{"idx":0,"sentence":"Given only raw text and a lemma list, the task consists of generating the morphological paradigms, i.e., all inflected forms, of the lemmas. ","offset":6,"pro":0.8571428571428571,"labels":"PUR"},{"idx":1,"sentence":"Natural Questions is a new challenging machine reading comprehension benchmark with two-grained answers, which are a long answer (typically a paragraph) and a short answer (one or more entities inside the long answer).","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"Despite the effectiveness of existing methods on this benchmark, they treat these two sub-tasks individually during training while ignoring their dependencies.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":1,"sentence":"To address this issue, we present a novel multi-grained machine reading comprehension framework that focuses on modeling documents at their hierarchical nature, which are different levels of granularity: documents, paragraphs, sentences, and tokens.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":1,"sentence":"We utilize graph attention networks to obtain different levels of representations so that they can be learned simultaneously.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":1,"sentence":"The long and short answers can be extracted from paragraph-level representation and token-level representation, respectively.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":1,"sentence":"In this way, we can model the dependencies between the two-grained answers to provide evidence for each other.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":1,"sentence":"We jointly train the two sub-tasks, and our experiments show that our approach significantly outperforms previous systems at both long and short answer criteria.","offset":6,"pro":0.8571428571428571,"labels":"MTD"},{"idx":2,"sentence":"Question Answering (QA) has shown great success thanks to the availability of large-scale datasets and the effectiveness of neural models.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"Recent research works have attempted to extend these successes to the settings with few or no labeled data available.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":2,"sentence":"In this work, we introduce two approaches to improve unsupervised QA.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":2,"sentence":"First, we harvest lexically and syntactically divergent questions from Wikipedia to automatically construct a corpus of question-answer pairs (named as RefQA).","offset":3,"pro":0.375,"labels":"MTD"},{"idx":2,"sentence":"Second, we take advantage of the QA model to extract more appropriate answers, which iteratively refines data over RefQA.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":2,"sentence":"We conduct experiments on SQuAD 1.1, and NewsQA by fine-tuning BERT without access to manually annotated data.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":2,"sentence":"Our approach outperforms previous unsupervised approaches by a large margin, and is competitive with early supervised models.","offset":6,"pro":0.75,"labels":"RST"},{"idx":2,"sentence":"We also show the effectiveness of our approach in the few-shot learning setting.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":3,"sentence":"This paper focuses on generating multi-hop reasoning questions from the raw text in a low resource circumstance.","offset":0,"pro":0,"labels":"PUR"},{"idx":3,"sentence":"Specifically, we first build a multi-hop generation model and guide it to satisfy the logical rationality by the reasoning chain extracted from a given text.","offset":1,"pro":0.1111111111111111,"labels":"MTD"},{"idx":3,"sentence":"With latent patterns as a prior, we can regularize the generation model and produce the optimal results.","offset":2,"pro":0.2222222222222222,"labels":"MTD"},{"idx":3,"sentence":"Experimental results on the HotpotQA data set demonstrate the effectiveness of our model.","offset":3,"pro":0.3333333333333333,"labels":"CLN"},{"idx":3,"sentence":"Moreover, we apply the generated results to the task of machine reading comprehension and achieve significant performance improvements.","offset":4,"pro":0.4444444444444444,"labels":"CLN"},{"idx":3,"sentence":"Such questions have to be syntactically valid and need to logically correlate with the answers by deducing over multiple relations on several sentences in the text. ","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":3,"sentence":"Since the labeled data is limited and insufficient for training, we propose to learn the model with the help of a large scale of unlabeled data that is much easier to obtain. ","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":3,"sentence":"Such data contains rich expressive forms of the questions with structural patterns on syntax and semantics. ","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":3,"sentence":"These patterns can be estimated by the neural hidden semi-Markov model using latent variables. ","offset":8,"pro":0.8888888888888888,"labels":"MTD"},{"idx":4,"sentence":"Recent studies have revealed that reading comprehension (RC) systems learn to exploit annotation artifacts and other biases in current datasets.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"This prevents the community from reliably measuring the progress of RC systems.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":4,"sentence":"To address this issue, we introduce R4C, a new task for evaluating RC systems' internal reasoning.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":4,"sentence":"R4C requires giving not only answers but also derivations: explanations that justify predicted answers.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":4,"sentence":"We create and publicly release the R4C dataset, the first, quality-assured dataset consisting of 4.6k questions, each of which is annotated with 3 reference derivations (i.e. 13.8k derivations).","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":4,"sentence":"Experiments show that our automatic evaluation metrics using multiple reference derivations are reliable, and that R4C assesses different skills from an existing benchmark.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":4,"sentence":"We present a reliable, crowdsourced framework for scalably annotating RC datasets with derivations. ","offset":6,"pro":0.8571428571428571,"labels":"MTD"},{"idx":5,"sentence":"In this paper, we study machine reading comprehension (MRC) on long texts: where a model takes as inputs a lengthy document and a query, extracts a text span from the document as an answer.","offset":0,"pro":0,"labels":"PUR"},{"idx":5,"sentence":"State-of-the-art models (e.g., BERT) tend to use a stack of transformer layers that are pre-trained from a large number of unlabeled language corpora to encode the joint contextual information of query and document.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":5,"sentence":"However, these transformer models can only take as input a fixed-length (e.g., 512) text.","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":5,"sentence":"To deal with even longer text inputs, previous approaches usually chunk them into equally-spaced segments and predict answers based on each segment independently without considering the information from other segments.","offset":3,"pro":0.3333333333333333,"labels":"BAC"},{"idx":5,"sentence":"As a result, they may form segments that fail to cover complete answers or retain insufficient contexts around the correct answer required for question answering.","offset":4,"pro":0.4444444444444444,"labels":"GAP"},{"idx":5,"sentence":"Moreover, they are less capable of answering questions that need cross-segment information.","offset":5,"pro":0.5555555555555556,"labels":"GAP"},{"idx":5,"sentence":"We propose to let a model learn to chunk in a more flexible way via reinforcement learning: a model can decide the next segment that it wants to process in either direction.","offset":6,"pro":0.6666666666666666,"labels":"PUR"},{"idx":5,"sentence":"We also apply recurrent mechanisms to enable information to flow across segments.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":5,"sentence":"Experiments on three MRC tasks - CoQA, QuAC, and TriviaQA - demonstrate the effectiveness of our proposed recurrent chunking mechanisms: we can obtain segments that are more likely to contain complete answers and at the same time provide sufficient contexts around the ground truth answers for better predictions.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":6,"sentence":"Reading long documents to answer open-domain questions remains challenging in natural language understanding.","offset":0,"pro":0,"labels":"GAP"},{"idx":6,"sentence":"In this paper, we introduce a new model, called RikiNet, which reads Wikipedia pages for natural question answering.","offset":1,"pro":0.125,"labels":"PUR"},{"idx":6,"sentence":"RikiNet contains a dynamic paragraph dual-attention reader and a multi-level cascaded answer predictor.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":6,"sentence":"The reader dynamically represents the document and question by utilizing a set of complementary attention mechanisms.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":6,"sentence":"The representations are then fed into the predictor to obtain the span of the short answer, the paragraph of the long answer, and the answer type in a cascaded manner.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":6,"sentence":"On the Natural Questions (NQ) dataset, a single RikiNet achieves 74.3 F1 and 57.9 F1 on long-answer and short-answer tasks.","offset":5,"pro":0.625,"labels":"RST"},{"idx":6,"sentence":"To our best knowledge, it is the first single model that outperforms the single human performance.","offset":6,"pro":0.75,"labels":"CTN"},{"idx":6,"sentence":"Furthermore, an ensemble RikiNet obtains 76.1 F1 and 61.3 F1 on long-answer and short-answer tasks, achieving the best performance on the official NQ leaderboard.","offset":7,"pro":0.875,"labels":"RST"},{"idx":7,"sentence":"We propose variable-in-situ logico-semantic graphs to bridge the gap between semantic graph and logical form parsing.","offset":0,"pro":0,"labels":"PUR"},{"idx":7,"sentence":"The new type of graph-based meaning representation allows us to include analysis for scope-related phenomena, such as quantification, negation and modality, in a way that is consistent with the state-of-the-art underspecification approach.","offset":1,"pro":0.14285714285714285,"labels":"MTD"},{"idx":7,"sentence":"Moreover, the well-formedness of such a graph is clear, since model-theoretic interpretation is available.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":7,"sentence":"We demonstrate the effectiveness of this new perspective by developing a new state-of-the-art semantic parser for English Resource Semantics.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":7,"sentence":"At the core of this parser is a novel neural graph rewriting system which combines the strengths of Hyperedge Replacement Grammar, a knowledge-intensive model, and Graph Neural Networks, a data-intensive model.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":7,"sentence":"Our parser achieves an accuracy of 92.39% in terms of elementary dependency match, which is a 2.88 point improvement over the best data-driven model in the literature.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":7,"sentence":"The output of our parser is highly coherent: at least 91% graphs are valid, in that they allow at least one sound scope-resolved logical form.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":8,"sentence":"This paper is concerned with semantic parsing for English as a second language (ESL).","offset":0,"pro":0,"labels":"PUR"},{"idx":8,"sentence":"Motivated by the theoretical emphasis on the learning challenges that occur at the syntax-semantics interface during second language acquisition, we formulate the task based on the divergence between literal and intended meanings.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":8,"sentence":"We combine the complementary strengths of English Resource Grammar, a linguistically-precise hand-crafted deep grammar, and TLE, an existing manually annotated ESL UD-TreeBank with a novel reranking model.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":8,"sentence":"Experiments demonstrate that in comparison to human annotations, our method can obtain a very promising SemBanking quality.","offset":3,"pro":0.5,"labels":"RST"},{"idx":8,"sentence":"By means of the newly created corpus, we evaluate state-of-the-art semantic parsing as well as grammatical error correction models.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":8,"sentence":"The evaluation profiles the performance of neural NLP techniques for handling ESL data and suggests some research directions.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":9,"sentence":"Semantic dependency parsing, which aims to find rich bi-lexical relationships, allows words to have multiple dependency heads, resulting in graph-structured representations.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"We propose an approach to semi-supervised learning of semantic dependency parsers based on the CRF autoencoder framework.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":9,"sentence":"Our encoder is a discriminative neural semantic dependency parser that predicts the latent parse graph of the input sentence.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":9,"sentence":"Our decoder is a generative neural model that reconstructs the input sentence conditioned on the latent parse graph.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":9,"sentence":"Our model is arc-factored and therefore parsing and learning are both tractable.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":9,"sentence":"Experiments show our model achieves significant and consistent improvement over the supervised baseline.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":10,"sentence":"One daunting problem for semantic parsing is the scarcity of annotation.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"Aiming to reduce nontrivial human labor, we propose a two-stage semantic parsing framework, where the first stage utilizes an unsupervised paraphrase model to convert an unlabeled natural language utterance into the canonical utterance.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":10,"sentence":"The downstream naive semantic parser accepts the intermediate output and returns the target logical form.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":10,"sentence":"Furthermore, the entire training process is split into two phases: pre-training and cycle learning.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":10,"sentence":"Three tailored self-supervised tasks are introduced throughout training to activate the unsupervised paraphrase model.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":10,"sentence":"Experimental results on benchmarks Overnight and GeoGranno demonstrate that our framework is effective and compatible with supervised training.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":11,"sentence":"Discourse representation tree structure (DRTS) parsing is a novel semantic parsing task which has been concerned most recently.","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"State-of-the-art performance can be achieved by a neural sequence-to-sequence model, treating the tree construction as an incremental sequence generation problem.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":11,"sentence":"Structural information such as input syntax and the intermediate skeleton of the partial output has been ignored in the model, which could be potentially useful for the DRTS parsing.","offset":2,"pro":0.4,"labels":"GAP"},{"idx":11,"sentence":"In this work, we propose a structural-aware model at both the encoder and decoder phase to integrate the structural information, where graph attention network (GAT) is exploited for effectively modeling.","offset":3,"pro":0.6,"labels":"PUR"},{"idx":11,"sentence":"Experimental results on a benchmark dataset show that our proposed model is effective and can obtain the best performance in the literature.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":12,"sentence":"We tackle the task of Term Set Expansion (TSE): given a small seed set of example terms from a semantic class, finding more members of that class.","offset":0,"pro":0,"labels":"PUR"},{"idx":12,"sentence":"We harness the power of neural masked language models (MLM) and propose a novel TSE algorithm, which combines the pattern-based and distributional approaches.","offset":1,"pro":0.125,"labels":"PUR"},{"idx":12,"sentence":"Due to the small size of the seed set, fine-tuning methods are not effective, calling for more creative use of the MLM.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":12,"sentence":"The gist of the idea is to use the MLM to first mine for informative patterns with respect to the seed set, and then to obtain more members of the seed class by generalizing these patterns.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":12,"sentence":"Our method outperforms state-of-the-art TSE algorithms.","offset":4,"pro":0.5,"labels":"RST"},{"idx":12,"sentence":"Implementation is available at: https://github.com/ guykush/TermSetExpansion-MPB/","offset":5,"pro":0.625,"labels":"CTN"},{"idx":12,"sentence":"The task is of great practical utility, and also of theoretical utility as it requires generalization from few examples. ","offset":6,"pro":0.75,"labels":"PUR"},{"idx":12,"sentence":"Previous approaches to the TSE task can be characterized as either distributional or pattern-based. ","offset":7,"pro":0.875,"labels":"GAP"},{"idx":13,"sentence":"Recently, the character-word lattice structure has been proved to be effective for Chinese named entity recognition (NER) by incorporating the word information.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"However, since the lattice structure is complex and dynamic, the lattice-based models are hard to fully utilize the parallel computation of GPUs and usually have a low inference speed.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":13,"sentence":"In this paper, we propose FLAT: Flat-LAttice Transformer for Chinese NER, which converts the lattice structure into a flat structure consisting of spans.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":13,"sentence":"Each span corresponds to a character or latent word and its position in the original lattice.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":13,"sentence":"With the power of Transformer and well-designed position encoding, FLAT can fully leverage the lattice information and has an excellent parallel ability.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":13,"sentence":"Experiments on four datasets show FLAT outperforms other lexicon-based models in performance and efficiency.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":14,"sentence":"Entity embeddings, which represent different aspects of each entity with a single vector like word embeddings, are a key component of neural entity linking models.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"Existing entity embeddings are learned from canonical Wikipedia articles and local contexts surrounding target entities.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":14,"sentence":"Such entity embeddings are effective, but too distinctive for linking models to learn contextual commonality.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":14,"sentence":"We propose a simple yet effective method, FGS2EE, to inject fine-grained semantic information into entity embeddings to reduce the distinctiveness and facilitate the learning of contextual commonality.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":14,"sentence":"FGS2EE first uses the embeddings of semantic type words to generate semantic embeddings, and then combines them with existing entity embeddings through linear aggregation.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":14,"sentence":"Extensive experiments show the effectiveness of such embeddings.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":14,"sentence":"Based on our entity embeddings, we achieved new sate-of-the-art performance on entity linking.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":15,"sentence":"Our experiments are conducted using the cross-lingual test collection produced within the CLEF eHealth information retrieval tasks in 2013-2015 containing English documents and queries in several European languages.","offset":0,"pro":0,"labels":"MTD"},{"idx":15,"sentence":"We exploit the Statistical Machine Translation (SMT) and Neural Machine Translation (NMT) paradigms and train several domain-specific and task-specific machine translation systems to translate the non-English queries into English (for the QT approach) and the English documents to all the query languages (for the DT approach).","offset":1,"pro":0.2,"labels":"MTD"},{"idx":15,"sentence":"The results show that the quality of QT by SMT is sufficient enough to outperform the retrieval results of the DT approach for all the languages.","offset":2,"pro":0.4,"labels":"RST"},{"idx":15,"sentence":"NMT then further boosts translation quality and retrieval quality for both QT and DT for most languages, but still, QT provides generally better retrieval results than DT.","offset":3,"pro":0.6,"labels":"RST"},{"idx":15,"sentence":"We present a thorough comparison of two principal approaches to Cross-Lingual Information Retrieval: document translation (DT) and query translation (QT). ","offset":4,"pro":0.8,"labels":"PUR"},{"idx":16,"sentence":"Mitigating the problem requires a large labeled dataset, which is expensive and time-consuming to obtain.","offset":0,"pro":0,"labels":"GAP"},{"idx":16,"sentence":"In this paper, we develop a deep, end-to-end model that learns to effectively classify mismatches and to generate hard mismatched examples to improve the classifier.","offset":1,"pro":0.125,"labels":"PUR"},{"idx":16,"sentence":"We train the model end-to-end by introducing a latent variable into the cross-entropy loss that alternates between using the real and generated samples.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":16,"sentence":"This not only makes the classifier more robust but also boosts the overall ranking performance.","offset":3,"pro":0.375,"labels":"RST"},{"idx":16,"sentence":"Our model achieves a relative gain compared to baselines by over 26% in F-score, and over 17% in Area Under PR curve.","offset":4,"pro":0.5,"labels":"RST"},{"idx":16,"sentence":"On live search traffic, our model gains significant improvement in multiple countries.","offset":5,"pro":0.625,"labels":"RST"},{"idx":16,"sentence":"Showing items that do not match search query intent degrades customer experience in e-commerce. ","offset":6,"pro":0.75,"labels":"GAP"},{"idx":16,"sentence":"hese mismatches result from counterfactual biases of the ranking algorithms toward noisy behavioral signals such as clicks and purchases in the search logs. ","offset":7,"pro":0.875,"labels":"GAP"},{"idx":17,"sentence":"Prior work has explored directly regularizing the output distributions of probabilistic models to alleviate peaky (i.e. over-confident) predictions, a common sign of overfitting.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"This class of techniques, of which label smoothing is one, has a connection to entropy regularization.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":17,"sentence":"Despite the consistent success of label smoothing across architectures and data sets in language generation tasks, two problems remain open: (1) there is little understanding of the underlying effects entropy regularizers have on models, and (2) the full space of entropy regularization techniques is largely unexplored.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":17,"sentence":"We introduce a parametric family of entropy regularizers, which includes label smoothing as a special case, and use it to gain a better understanding of the relationship between the entropy of a model and its performance on language generation tasks.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":17,"sentence":"We also find that variance in model performance can be explained largely by the resulting entropy of the model.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":17,"sentence":"Lastly, we find that label smoothing provably does not allow for sparsity in an output distribution, an undesirable property for language generation models, and therefore advise the use of other entropy regularization methods in its place.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":18,"sentence":"Self-attention mechanisms have made striking state-of-the-art (SOTA) progress in various sequence learning tasks, standing on the multi-headed dot product attention by attending to all the global contexts at different locations.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"The subsidiary content-based SDU gates allow for the information flow of modulated latent embeddings through skipped connections, leading to a clear margin of convergence speed with gradient descent algorithms.","offset":1,"pro":0.25,"labels":"MTD"},{"idx":18,"sentence":"Through a pseudo information highway, we introduce a gated component self-dependency units (SDU) that incorporates LSTM-styled gating units to replenish internal semantic importance within the multi-dimensional latent space of individual representations. ","offset":2,"pro":0.5,"labels":"PUR"},{"idx":18,"sentence":"We may unveil the role of gating mechanism to aid in the context-based Transformer modules, with hypothesizing that SDU gates, especially on shallow layers, could push it faster to step towards suboptimal points during the optimization process. ","offset":3,"pro":0.75,"labels":"CLN"},{"idx":19,"sentence":"Knowledge graph (KG) embeddings learn low- dimensional representations of entities and relations to predict missing facts.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"KGs often exhibit hierarchical and logical patterns which must be preserved in the embedding space.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":19,"sentence":"For hierarchical data, hyperbolic embedding methods have shown promise for high-fidelity and parsimonious representations.","offset":2,"pro":0.2222222222222222,"labels":"BAC"},{"idx":19,"sentence":"However, existing hyperbolic embedding methods do not account for the rich logical patterns in KGs.","offset":3,"pro":0.3333333333333333,"labels":"GAP"},{"idx":19,"sentence":"In this work, we introduce a class of hyperbolic KG embedding models that simultaneously capture hierarchical and logical patterns.","offset":4,"pro":0.4444444444444444,"labels":"PUR"},{"idx":19,"sentence":"Our approach combines hyperbolic reflections and rotations with attention to model complex relational patterns.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":19,"sentence":"Experimental results on standard KG benchmarks show that our method improves over previous Euclidean- and hyperbolic-based efforts by up to 6.1% in mean reciprocal rank (MRR) in low dimensions.","offset":6,"pro":0.6666666666666666,"labels":"RST"},{"idx":19,"sentence":"Furthermore, we observe that different geometric transformations capture different types of relations while attention- based transformations generalize to multiple relations.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":19,"sentence":"In high dimensions, our approach yields new state-of-the-art MRRs of 49.6% on WN18RR and 57.7% on YAGO3-10.","offset":8,"pro":0.8888888888888888,"labels":"RST"},{"idx":0,"sentence":"Structural heterogeneity between knowledge graphs is an outstanding challenge for entity alignment.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"This paper presents Neighborhood Matching Network (NMN), a novel entity alignment framework for tackling the structural heterogeneity challenge.","offset":1,"pro":0.125,"labels":"PUR"},{"idx":0,"sentence":"NMN estimates the similarities between entities to capture both the topological structure and the neighborhood difference.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":0,"sentence":"It provides two innovative components for better learning representations for entity alignment.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":0,"sentence":"It first uses a novel graph sampling method to distill a discriminative neighborhood for each entity.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":0,"sentence":"It then adopts a cross-graph neighborhood matching module to jointly encode the neighborhood difference for a given entity pair.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":0,"sentence":"Such strategies allow NMN to effectively construct matching-oriented entity representations while ignoring noisy neighbors that have a negative impact on the alignment task.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":0,"sentence":"Extensive experiments performed on three entity alignment datasets show that NMN can well estimate the neighborhood similarity in more tough cases and significantly outperforms 12 previous state-of-the-art methods.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":1,"sentence":"Recent neural models for relation extraction with distant supervision alleviate the impact of irrelevant sentences in a bag by learning importance weights for the sentences.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"Efforts thus far have focused on improving extraction accuracy but little is known about their explanability.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":1,"sentence":"In this work we annotate a test set with ground-truth sentence-level explanations to evaluate the quality of explanations afforded by the relation extraction models.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":1,"sentence":"We demonstrate that replacing the entity mentions in the sentences with their fine-grained entity types not only enhances extraction accuracy but also improves explanation.","offset":3,"pro":0.5,"labels":"CLN"},{"idx":1,"sentence":"We also propose to automatically generate distractor sentences to augment the bags and train the model to ignore the distractors.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":1,"sentence":"Evaluations on the widely used FB-NYT dataset show that our methods achieve new state-of-the-art accuracy while improving model explanability.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":2,"sentence":"We propose a novel approach using representation learning for tackling the problem of extracting structured information from form-like document images.","offset":0,"pro":0,"labels":"PUR"},{"idx":2,"sentence":"We propose an extraction system that uses knowledge of the types of the target fields to generate extraction candidates and a neural network architecture that learns a dense representation of each candidate based on neighboring words in the document.","offset":1,"pro":0.3333333333333333,"labels":"PUR"},{"idx":2,"sentence":"These learned representations are not only useful in solving the extraction task for unseen document templates from two different domains but are also interpretable, as we show using loss cases.","offset":2,"pro":0.6666666666666666,"labels":"CLN"},{"idx":3,"sentence":"To better tackle the named entity recognition (NER) problem on languages with little/no labeled data, cross-lingual NER must effectively leverage knowledge learned from source languages with rich labeled data.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"Previous works on cross-lingual NER are mostly based on label projection with pairwise texts or direct model transfer.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":3,"sentence":"However, such methods either are not applicable if the labeled data in the source languages is unavailable, or do not leverage information contained in unlabeled data in the target language.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":3,"sentence":"In this paper, we propose a teacher-student learning method to address such limitations, where NER models in the source languages are used as teachers to train a student model on unlabeled data in the target language.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":3,"sentence":"The proposed method works for both single-source and multi-source cross-lingual NER.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":3,"sentence":"For the latter, we further propose a similarity measuring method to better weight the supervision from different teacher models.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":3,"sentence":"Extensive experiments for 3 target languages on benchmark datasets well demonstrate that our method outperforms existing state-of-the-art methods for both single-source and multi-source cross-lingual NER.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":4,"sentence":"Opinion entity extraction is a fundamental task in fine-grained opinion mining.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"Related studies generally extract aspects and/or opinion expressions without recognizing the relations between them.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":4,"sentence":"However, the relations are crucial for downstream tasks, including sentiment classification, opinion summarization, etc. In this paper, we explore Aspect-Opinion Pair Extraction (AOPE) task, which aims at extracting aspects and opinion expressions in pairs.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":4,"sentence":"To deal with this task, we propose Synchronous Double-channel Recurrent Network (SDRN) mainly consisting of an opinion entity extraction unit, a relation detection unit, and a synchronization unit.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":4,"sentence":"The opinion entity extraction unit and the relation detection unit are developed as two channels to extract opinion entities and relations simultaneously.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":4,"sentence":"Furthermore, within the synchronization unit, we design Entity Synchronization Mechanism (ESM) and Relation Synchronization Mechanism (RSM) to enhance the mutual benefit on the above two channels.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":4,"sentence":"To verify the performance of SDRN, we manually build three datasets based on SemEval 2014 and 2015 benchmarks.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":4,"sentence":"Extensive experiments demonstrate that SDRN achieves state-of-the-art performances.","offset":7,"pro":0.875,"labels":"RST"},{"idx":5,"sentence":"We use coherence relations inspired by computational models of discourse to study the information needs and goals of image captioning.","offset":0,"pro":0,"labels":"MTD"},{"idx":5,"sentence":"Using an annotation protocol specifically devised for capturing image-caption coherence relations, we annotate 10,000 instances from publicly-available image-caption pairs.","offset":1,"pro":0.25,"labels":"MTD"},{"idx":5,"sentence":"We introduce a new task for learning inferences in imagery and text, coherence relation prediction, and show that these coherence annotations can be exploited to learn relation classifiers as an intermediary step, and also train coherence-aware, controllable image captioning models.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":5,"sentence":"The results show a dramatic improvement in the consistency and quality of the generated captions with respect to information needs specified via coherence relations.","offset":3,"pro":0.75,"labels":"RST"},{"idx":6,"sentence":"In human cognition, world knowledge supports the perception of object colours: knowing that trees are typically green helps to perceive their colour in certain contexts.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"We go beyond previous studies on colour terms using isolated colour swatches and study visual grounding of colour terms in realistic objects.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":6,"sentence":"Our models integrate processing of visual information and object-specific knowledge via hard-coded (late) or learned (early) fusion.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":6,"sentence":"We find that both models consistently outperform a bottom-up baseline that predicts colour terms solely from visual inputs, but show interesting differences when predicting atypical colours of so-called colour diagnostic objects.","offset":3,"pro":0.6,"labels":"RST"},{"idx":6,"sentence":"Our models also achieve promising results when tested on new object categories not seen during training.","offset":4,"pro":0.8,"labels":"RST"},{"idx":7,"sentence":"Given an untrimmed video and a text query, natural language video localization (NLVL) is to locate a matching span from the video that semantically corresponds to the query.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"Existing solutions formulate NLVL either as a ranking task and apply multimodal matching architecture, or as a regression task to directly regress the target video span.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":7,"sentence":"In this work, we address NLVL task with a span-based QA approach by treating the input video as text passage.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":7,"sentence":"We propose a video span localizing network (VSLNet), on top of the standard span-based QA framework, to address NLVL.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":7,"sentence":"The proposed VSLNet tackles the differences between NLVL and span-based QA through a simple and yet effective query-guided highlighting (QGH) strategy.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":7,"sentence":"The QGH guides VSLNet to search for matching video span within a highlighted region.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":7,"sentence":"Through extensive experiments on three benchmark datasets, we show that the proposed VSLNet outperforms the state-of-the-art methods; and adopting span-based QA framework is a promising direction to solve NLVL.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":8,"sentence":"Visual referring expression recognition is a challenging task that requires natural language understanding in the context of an image.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"We critically examine RefCOCOg, a standard benchmark for this task, using a human study and show that 83.7% of test instances do not require reasoning on linguistic structure, i.e., words are enough to identify the target object, the word order doesn't matter.","offset":1,"pro":0.14285714285714285,"labels":"RST"},{"idx":8,"sentence":"To measure the true progress of existing models, we split the test set into two sets, one which requires reasoning on linguistic structure and the other which doesn't.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":8,"sentence":"Additionally, we create an out-of-distribution dataset Ref-Adv by asking crowdworkers to perturb in-domain examples such that the target object changes.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":8,"sentence":"Using these datasets, we empirically show that existing methods fail to exploit linguistic structure and are 12% to 23% lower in performance than the established progress for this task.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":8,"sentence":"We also propose two methods, one based on contrastive learning and the other based on multi-task learning, to increase the robustness of ViLBERT, the current state-of-the-art model for this task.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":8,"sentence":"Our datasets are publicly available at https://github.com/aws/aws-refcocog-adv.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":9,"sentence":"Multi-head attentive neural architectures have achieved state-of-the-art results on a variety of natural language processing tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"Evidence has shown that they are overparameterized; attention heads can be pruned without significant performance loss.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":9,"sentence":"In this work, we instead reallocate them-the model learns to activate different heads on different inputs.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":9,"sentence":"Drawing connections between multi-head attention and mixture of experts, we propose the mixture of attentive experts model (MAE).","offset":3,"pro":0.375,"labels":"PUR"},{"idx":9,"sentence":"MAE is trained using a block coordinate descent algorithm that alternates between updating (1) the responsibilities of the experts and (2) their parameters.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":9,"sentence":"Experiments on machine translation and language modeling show that MAE outperforms strong baselines on both tasks.","offset":5,"pro":0.625,"labels":"RST"},{"idx":9,"sentence":"Particularly, on the WMT14 English to German translation dataset, MAE improves over transformer-base by 0.8 BLEU, with a comparable number of parameters.","offset":6,"pro":0.75,"labels":"RST"},{"idx":9,"sentence":"Our analysis shows that our model learns to specialize different experts to different inputs.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":10,"sentence":"Aspect-based sentiment classification is a popular task aimed at identifying the corresponding emotion of a specific aspect.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"One sentence may contain various sentiments for different aspects.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":10,"sentence":"Many sophisticated methods such as attention mechanism and Convolutional Neural Networks (CNN) have been widely employed for handling this challenge.","offset":2,"pro":0.2222222222222222,"labels":"BAC"},{"idx":10,"sentence":"Recently, semantic dependency tree implemented by Graph Convolutional Networks (GCN) is introduced to describe the inner connection between aspects and the associated emotion words.","offset":3,"pro":0.3333333333333333,"labels":"BAC"},{"idx":10,"sentence":"But the improvement is limited due to the noise and instability of dependency trees.","offset":4,"pro":0.4444444444444444,"labels":"GAP"},{"idx":10,"sentence":"To this end, we propose a dependency graph enhanced dual-transformer network (named DGEDT) by jointly considering the flat representations learnt from Transformer and graph-based representations learnt from the corresponding dependency graph in an iterative interaction manner.","offset":5,"pro":0.5555555555555556,"labels":"PUR"},{"idx":10,"sentence":"Specifically, a dual-transformer structure is devised in DGEDT to support mutual reinforcement between the flat representation learning and graph-based representation learning.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":10,"sentence":"The idea is to allow the dependency graph to guide the representation learning of the transformer encoder and vice versa.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":10,"sentence":"The results on five datasets demonstrate that the proposed DGEDT outperforms all state-of-the-art alternatives with a large margin.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":11,"sentence":"We propose Differentiable Window, a new neural module and general purpose component for dynamic window selection.","offset":0,"pro":0,"labels":"MTD"},{"idx":11,"sentence":"While universally applicable, we demonstrate a compelling use case of utilizing Differentiable Window to improve standard attention modules by enabling more focused attentions over the input regions.","offset":1,"pro":0.2,"labels":"CLN"},{"idx":11,"sentence":"We propose two variants of Differentiable Window, and integrate them within the Transformer architecture in two novel ways.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":11,"sentence":"We evaluate our proposed approach on a myriad of NLP tasks, including machine translation, sentiment analysis, subject-verb agreement and language modeling.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":11,"sentence":"Our experimental results demonstrate consistent and sizable improvements across all tasks.","offset":4,"pro":0.8,"labels":"RST"},{"idx":12,"sentence":"Despite achieving prominent performance on many important tasks, it has been reported that neural networks are vulnerable to adversarial examples.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"Previously studies along this line mainly focused on semantic tasks such as sentiment analysis, question answering and reading comprehension.","offset":1,"pro":0.25,"labels":"BAC"},{"idx":12,"sentence":"In this study, we show that adversarial examples also exist in dependency parsing: we propose two approaches to study where and how parsers make mistakes by searching over perturbations to existing texts at sentence and phrase levels, and design algorithms to construct such examples in both of the black-box and white-box settings.","offset":2,"pro":0.5,"labels":"PUR"},{"idx":12,"sentence":"Our experiments with one of state-of-the-art parsers on the English Penn Treebank (PTB) show that up to 77% of input examples admit adversarial perturbations, and we also show that the robustness of parsing models can be improved by crafting high-quality adversaries and including them in the training stage, while suffering little to no performance drop on the clean input data.","offset":3,"pro":0.75,"labels":"RST"},{"idx":13,"sentence":"It is commonly believed that knowledge of syntactic structure should improve language modeling.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"However, effectively and computationally efficiently incorporating syntactic structure into neural language models has been a challenging topic.","offset":1,"pro":0.25,"labels":"GAP"},{"idx":13,"sentence":"In this paper, we make use of a multi-task objective, i.e., the models simultaneously predict words as well as ground truth parse trees in a form called syntactic distances, where information between these two separate objectives shares the same intermediate representation.","offset":2,"pro":0.5,"labels":"PUR"},{"idx":13,"sentence":"Experimental results on the Penn Treebank and Chinese Treebank datasets show that when ground truth parse trees are provided as additional training signals, the model is able to achieve lower perplexity and induce trees with better quality.","offset":3,"pro":0.75,"labels":"RST"},{"idx":14,"sentence":"Neural architecture search (NAS) has advanced significantly in recent years but most NAS systems restrict search to learning architectures of a recurrent or convolutional cell.","offset":0,"pro":0,"labels":"GAP"},{"idx":14,"sentence":"In this paper, we extend the search space of NAS.","offset":1,"pro":0.125,"labels":"PUR"},{"idx":14,"sentence":"In particular, we present a general approach to learn both intra-cell and inter-cell architectures (call it ESS).","offset":2,"pro":0.25,"labels":"MTD"},{"idx":14,"sentence":"For a better search result, we design a joint learning method to perform intra-cell and inter-cell NAS simultaneously.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":14,"sentence":"We implement our model in a differentiable architecture search system.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":14,"sentence":"For recurrent neural language modeling, it outperforms a strong baseline significantly on the PTB and WikiText data, with a new state-of-the-art on PTB.","offset":5,"pro":0.625,"labels":"RST"},{"idx":14,"sentence":"Moreover, the learned architectures show good transferability to other systems.","offset":6,"pro":0.75,"labels":"CLN"},{"idx":14,"sentence":"E.g., they improve state-of-the-art systems on the CoNLL and WNUT named entity recognition (NER) tasks and CoNLL chunking task, indicating a promising line of research on large-scale pre-learned architectures.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":15,"sentence":"As NLP models become larger, executing a trained model requires significant computational resources incurring monetary and environmental costs.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"To better respect a given inference budget, we propose a modification to contextual representation fine-tuning which, during inference, allows for an early (and fast) exit from neural network calculations for simple instances, and late (and accurate) exit for hard instances.","offset":1,"pro":0.125,"labels":"PUR"},{"idx":15,"sentence":"To achieve this, we add classifiers to different layers of BERT and use their calibrated confidence scores to make early exit decisions.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":15,"sentence":"We test our proposed modification on five different datasets in two tasks: three text classification datasets and two natural language inference benchmarks.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":15,"sentence":"Our method presents a favorable speed/accuracy tradeoff in almost all cases, producing models which are up to five times faster than the state of the art, while preserving their accuracy.","offset":4,"pro":0.5,"labels":"RST"},{"idx":15,"sentence":"Our method also requires almost no additional training resources (in either time or parameters) compared to the baseline BERT model.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":15,"sentence":"Finally, our method alleviates the need for costly retraining of multiple models at different levels of efficiency; we allow users to control the inference speed/accuracy tradeoff using a single trained model, by setting a single variable at inference time.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":15,"sentence":"We publicly release our code.","offset":7,"pro":0.875,"labels":"CTN"},{"idx":16,"sentence":"Polysynthetic languages have exceptionally large and sparse vocabularies, thanks to the number of morpheme slots and combinations in a word.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"This complexity, together with a general scarcity of written data, poses a challenge to the development of natural language technologies.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":16,"sentence":"To address this challenge, we offer linguistically-informed approaches for bootstrapping a neural morphological analyzer, and demonstrate its application to Kunwinjku, a polysynthetic Australian language.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":16,"sentence":"We generate data from a finite state transducer to train an encoder-decoder model.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":16,"sentence":"We improve the model by hallucinating missing linguistic structure into the training data, and by resampling from a Zipf distribution to simulate a more natural distribution of morphemes.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":16,"sentence":"The best model accounts for all instances of reduplication in the test set and achieves an accuracy of 94.7% overall, a 10 percentage point improvement over the FST baseline.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":16,"sentence":"This process demonstrates the feasibility of bootstrapping a neural morph analyzer from minimal resources.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":17,"sentence":"Fully supervised neural approaches have achieved significant progress in the task of Chinese word segmentation (CWS).","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"Nevertheless, the performance of supervised models always drops gravely if the domain shifts due to the distribution gap across domains and the out of vocabulary (OOV) problem.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":17,"sentence":"In order to simultaneously alleviate the issues, this paper intuitively couples distant annotation and adversarial training for cross-domain CWS.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":17,"sentence":"1) We rethink the essence of Chinese words and design an automatic distant annotation mechanism, which does not need any supervision or pre-defined dictionaries on the target domain.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":17,"sentence":"The method could effectively explore domain-specific words and distantly annotate the raw texts for the target domain.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":17,"sentence":"2) We further develop a sentence-level adversarial training procedure to perform noise reduction and maximum utilization of the source domain information.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":17,"sentence":"Experiments on multiple real-world datasets across various domains show the superiority and robustness of our model, significantly outperforming previous state-of-the-arts cross-domain CWS methods.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":18,"sentence":"This paper describes a language-independent model for fully unsupervised morphological analysis that exploits a universal framework leveraging morphological typology.","offset":0,"pro":0,"labels":"PUR"},{"idx":18,"sentence":"By modeling morphological processes including suffixation, prefixation, infixation, and full and partial reduplication with constrained stem change rules, our system effectively constrains the search space and offers a wide coverage in terms of morphological typology.","offset":1,"pro":0.25,"labels":"RST"},{"idx":18,"sentence":"The system is tested on nine typologically and genetically diverse languages, and shows superior performance over leading systems.","offset":2,"pro":0.5,"labels":"CLN"},{"idx":18,"sentence":"We also investigate the effect of an oracle that provides only a handful of bits per language to signal morphological type.","offset":3,"pro":0.75,"labels":"MTD"},{"idx":19,"sentence":"The noun lexica of many natural languages are divided into several declension classes with characteristic morphological properties.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"Class membership is far from deterministic, but the phonological form of a noun and/or its meaning can often provide imperfect clues.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":19,"sentence":"Here, we investigate the strength of those clues.","offset":2,"pro":0.2222222222222222,"labels":"PUR"},{"idx":19,"sentence":"More specifically, we operationalize this by measuring how much information, in bits, we can glean about declension class from knowing the form and/or meaning of nouns.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":19,"sentence":"We know that form and meaning are often also indicative of grammatical gender-which, as we quantitatively verify, can itself share information with declension class-so we also control for gender.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":19,"sentence":"We find for two Indo-European languages (Czech and German) that form and meaning respectively share significant amounts of information with class (and contribute additional information above and beyond gender).","offset":5,"pro":0.5555555555555556,"labels":"RST"},{"idx":19,"sentence":"The three-way interaction between class, form, and meaning (given gender) is also significant.","offset":6,"pro":0.6666666666666666,"labels":"RST"},{"idx":19,"sentence":"Our study is important for two reasons: First, we introduce a new method that provides additional quantitative support for a classic linguistic finding that form and meaning are relevant for the classification of nouns into declensions.","offset":7,"pro":0.7777777777777778,"labels":"CTN"},{"idx":19,"sentence":"Secondly, we show not only that individual declensions classes vary in the strength of their clues within a language, but also that these variations themselves vary across languages.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":0,"sentence":"Recent advances in pre-trained multilingual language models lead to state-of-the-art results on the task of quality estimation (QE) for machine translation.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"A carefully engineered ensemble of such models won the QE shared task at WMT19.","offset":1,"pro":0.25,"labels":"RST"},{"idx":0,"sentence":"Our in-depth analysis, however, shows that the success of using pre-trained language models for QE is over-estimated due to three issues we observed in current QE datasets: (i) The distributions of quality scores are imbalanced and skewed towards good quality scores; (iii) QE models can perform well on these datasets while looking at only source or translated sentences; (iii) They contain statistical artifacts that correlate well with human-annotated QE labels.","offset":2,"pro":0.5,"labels":"RST"},{"idx":0,"sentence":"Our findings suggest that although QE models might capture fluency of translated sentences and complexity of source sentences, they cannot model adequacy of translations effectively.","offset":3,"pro":0.75,"labels":"CLN"},{"idx":1,"sentence":"While natural language understanding (NLU) is advancing rapidly, today's technology differs from human-like language understanding in fundamental ways, notably in its inferior efficiency, interpretability, and generalization.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"This work proposes an approach to representation and learning based on the tenets of embodied cognitive linguistics (ECL).","offset":1,"pro":0.25,"labels":"PUR"},{"idx":1,"sentence":"According to ECL, natural language is inherently executable (like programming languages), driven by mental simulation and metaphoric mappings over hierarchical compositions of structures and schemata learned through embodied interaction.","offset":2,"pro":0.5,"labels":"BAC"},{"idx":1,"sentence":"This position paper argues that the use of grounding by metaphoric reasoning and simulation will greatly benefit NLU systems, and proposes a system architecture along with a roadmap towards realizing this vision.","offset":3,"pro":0.75,"labels":"PUR"},{"idx":2,"sentence":"Language technologies contribute to promoting multilingualism and linguistic diversity around the world.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"However, only a very small number of the over 7000 languages of the world are represented in the rapidly evolving language technologies and applications.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":2,"sentence":"In this paper we look at the relation between the types of languages, resources, and their representation in NLP conferences to understand the trajectory that different languages have followed over time.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":2,"sentence":"Our quantitative investigation underlines the disparity between languages, especially in terms of their resources, and calls into question the language agnostic status of current models and systems.","offset":3,"pro":0.6,"labels":"CLN"},{"idx":2,"sentence":"Through this paper, we attempt to convince the ACL community to prioritise the resolution of the predicaments highlighted here, so that no language is left behind.","offset":4,"pro":0.8,"labels":"PUR"},{"idx":3,"sentence":"In this paper, we trace the history of neural networks applied to natural language understanding tasks, and identify key contributions which the nature of language has made to the development of neural network architectures.","offset":0,"pro":0,"labels":"PUR"},{"idx":3,"sentence":"We focus on the importance of variable binding and its instantiation in attention-based models, and argue that Transformer is not a sequence model but an induced-structure model.","offset":1,"pro":0.3333333333333333,"labels":"PUR"},{"idx":3,"sentence":"This perspective leads to predictions of the challenges facing research in deep learning architectures for natural language understanding.","offset":2,"pro":0.6666666666666666,"labels":"CLN"},{"idx":4,"sentence":"Corpus query systems exist to address the multifarious information needs of any person interested in the content of annotated corpora.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"In this role they play an important part in making those resources usable for a wider audience.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":4,"sentence":"Over the past decades, several such query systems and languages have emerged, varying greatly in their expressiveness and technical details.","offset":2,"pro":0.4,"labels":"BAC"},{"idx":4,"sentence":"This paper offers a broad overview of the history of corpora and corpus query tools.","offset":3,"pro":0.6,"labels":"PUR"},{"idx":4,"sentence":"It focusses strongly on the query side and hints at exciting directions for future development.","offset":4,"pro":0.8,"labels":"RST"},{"idx":5,"sentence":"Recent studies in dialogue state tracking (DST) leverage historical information to determine states which are generally represented as slot-value pairs.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"However, most of them have limitations to efficiently exploit relevant context due to the lack of a powerful mechanism for modeling interactions between the slot and the dialogue history.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":5,"sentence":"Besides, existing methods usually ignore the slot imbalance problem and treat all slots indiscriminately, which limits the learning of hard slots and eventually hurts overall performance.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":5,"sentence":"In this paper, we propose to enhance the DST through employing a contextual hierarchical attention network to not only discern relevant information at both word level and turn level but also learn contextual representations.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":5,"sentence":"We further propose an adaptive objective to alleviate the slot imbalance problem by dynamically adjust weights of different slots during training.","offset":4,"pro":0.6666666666666666,"labels":"PUR"},{"idx":5,"sentence":"Experimental results show that our approach reaches 52.68% and 58.55% joint accuracy on MultiWOZ 2.0 and MultiWOZ 2.1 datasets respectively and achieves new state-of-the-art performance with considerable improvements (+1.24% and +5.98%).","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":6,"sentence":"Current state-of-the-art neural dialogue models learn from human conversations following the data-driven paradigm.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"As such, a reliable training corpus is the crux of building a robust and well-behaved dialogue model.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":6,"sentence":"However, due to the open-ended nature of human conversations, the quality of user-generated training data varies greatly, and effective training samples are typically insufficient while noisy samples frequently appear.","offset":2,"pro":0.2,"labels":"GAP"},{"idx":6,"sentence":"This impedes the learning of those data-driven neural dialogue models.","offset":3,"pro":0.3,"labels":"GAP"},{"idx":6,"sentence":"Therefore, effective dialogue learning requires not only more reliable learning samples, but also fewer noisy samples.","offset":4,"pro":0.4,"labels":"GAP"},{"idx":6,"sentence":"In this paper, we propose a data manipulation framework to proactively reshape the data distribution towards reliable samples by augmenting and highlighting effective learning samples as well as reducing the effect of inefficient samples simultaneously.","offset":5,"pro":0.5,"labels":"PUR"},{"idx":6,"sentence":"In particular, the data manipulation model selectively augments the training samples and assigns an importance weight to each instance to reform the training data.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":6,"sentence":"Note that, the proposed data manipulation framework is fully data-driven and learnable.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":6,"sentence":"It not only manipulates training samples to optimize the dialogue generation model, but also learns to increase its manipulation skills through gradient descent with validation samples.","offset":8,"pro":0.8,"labels":"MTD"},{"idx":6,"sentence":"Extensive experiments show that our framework can improve the dialogue generation performance with respect to various automatic evaluation metrics and human judgments.","offset":9,"pro":0.9,"labels":"CLN"},{"idx":7,"sentence":"Recent studies have shown remarkable success in end-to-end task-oriented dialog system.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"However, most neural models rely on large training data, which are only available for a certain number of task domains, such as navigation and scheduling.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":7,"sentence":"This makes it difficult to scalable for a new domain with limited labeled data.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":7,"sentence":"However, there has been relatively little research on how to effectively use data from all domains to improve the performance of each domain and also unseen domains.","offset":3,"pro":0.375,"labels":"GAP"},{"idx":7,"sentence":"To this end, we investigate methods that can make explicit use of domain knowledge and introduce a shared-private network to learn shared and specific knowledge.","offset":4,"pro":0.5,"labels":"PUR"},{"idx":7,"sentence":"In addition, we propose a novel Dynamic Fusion Network (DF-Net) which automatically exploit the relevance between the target domain and each domain.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":7,"sentence":"Results show that our models outperforms existing methods on multi-domain dialogue, giving the state-of-the-art in the literature.","offset":6,"pro":0.75,"labels":"RST"},{"idx":7,"sentence":"Besides, with little training data, we show its transferability by outperforming prior best model by 13.9% on average.","offset":7,"pro":0.875,"labels":"RST"},{"idx":8,"sentence":"Training a task-oriented dialogue agent with reinforcement learning is prohibitively expensive since it requires a large volume of interactions with users.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"Human demonstrations can be used to accelerate learning progress.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":8,"sentence":"However, how to effectively leverage demonstrations to learn dialogue policy remains less explored.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":8,"sentence":"In this paper, we present S{^{2Agent that efficiently learns dialogue policy from demonstrations through policy shaping and reward shaping.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":8,"sentence":"We use an imitation model to distill knowledge from demonstrations, based on which policy shaping estimates feedback on how the agent should act in policy space.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":8,"sentence":"Reward shaping is then incorporated to bonus state-actions similar to demonstrations explicitly in value space encouraging better exploration.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":8,"sentence":"The effectiveness of the proposed S{^{2Agentt is demonstrated in three dialogue domains and a challenging domain adaptation task with both user simulator evaluation and human evaluation.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":9,"sentence":"Dialogue state tracker is responsible for inferring user intentions through dialogue history.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"Previous methods have difficulties in handling dialogues with long interaction context, due to the excessive information.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":9,"sentence":"We propose a Dialogue State Tracker with Slot Attention and Slot Information Sharing (SAS) to reduce redundant information's interference and improve long dialogue context tracking.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":9,"sentence":"Specially, we first apply a Slot Attention to learn a set of slot-specific features from the original dialogue and then integrate them using a slot information sharing module.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":9,"sentence":"Our model yields a significantly improved performance compared to previous state-of the-art models on the MultiWOZ dataset.","offset":4,"pro":0.8,"labels":"RST"},{"idx":10,"sentence":"Automatic evaluation of open-domain dialogue response generation is very challenging because there are many appropriate responses for a given context.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"Existing evaluation models merely compare the generated response with the ground truth response and rate many of the appropriate responses as inappropriate if they deviate from the ground truth.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":10,"sentence":"One approach to resolve this problem is to consider the similarity of the generated response with the conversational context.","offset":2,"pro":0.2222222222222222,"labels":"BAC"},{"idx":10,"sentence":"In this paper, we propose an automatic evaluation model based on that idea and learn the model parameters from an unlabeled conversation corpus.","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":10,"sentence":"Our approach considers the speakers in defining the different levels of similar context.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":10,"sentence":"We use a Twitter conversation corpus that contains many speakers and conversations to test our evaluation model.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":10,"sentence":"Experiments show that our model outperforms the other existing evaluation metrics in terms of high correlation with human annotation scores.","offset":6,"pro":0.6666666666666666,"labels":"RST"},{"idx":10,"sentence":"We also show that our model trained on Twitter can be applied to movie dialogues without any additional training.","offset":7,"pro":0.7777777777777778,"labels":"CLN"},{"idx":10,"sentence":"We provide our code and the learned parameters so that they can be used for automatic evaluation of dialogue response generation models.","offset":8,"pro":0.8888888888888888,"labels":"CTN"},{"idx":11,"sentence":"Due to its great importance in deep natural language understanding and various down-stream applications, text-level parsing of discourse rhetorical structure (DRS) has been drawing more and more attention in recent years.","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"However, all the previous studies on text-level discourse parsing adopt bottom-up approaches, which much limit the DRS determination on local information and fail to well benefit from global information of the overall discourse.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":11,"sentence":"In this paper, we justify from both computational and perceptive points-of-view that the top-down architecture is more suitable for text-level DRS parsing.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":11,"sentence":"On the basis, we propose a top-down neural architecture toward text-level DRS parsing.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":11,"sentence":"In particular, we cast discourse parsing as a recursive split point ranking task, where a split point is classified to different levels according to its rank and the elementary discourse units (EDUs) associated with it are arranged accordingly.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":11,"sentence":"In this way, we can determine the complete DRS as a hierarchical tree structure via an encoder-decoder with an internal stack.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":11,"sentence":"Experimentation on both the English RST-DT corpus and the Chinese CDTB corpus shows the great effectiveness of our proposed top-down approach towards text-level DRS parsing.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":12,"sentence":"An in-depth exploration of protein-protein interactions (PPI) is essential to understand the metabolism in addition to the regulations of biological entities like proteins, carbohydrates, and many more.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"Most of the recent PPI tasks in BioNLP domain have been carried out solely using textual data.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":12,"sentence":"In this paper, we argue that incorporating multimodal cues can improve the automatic identification of PPI.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":12,"sentence":"As a first step towards enabling the development of multimodal approaches for PPI identification, we have developed two multi-modal datasets which are extensions and multi-modal versions of two popular benchmark PPI corpora (BioInfer and HRPD50).","offset":3,"pro":0.375,"labels":"MTD"},{"idx":12,"sentence":"Besides, existing textual modalities, two new modalities, 3D protein structure and underlying genomic sequence, are also added to each instance.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":12,"sentence":"Further, a novel deep multi-modal architecture is also implemented to efficiently predict the protein interactions from the developed datasets.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":12,"sentence":"A detailed experimental analysis reveals the superiority of the multi-modal approach in comparison to the strong baselines including unimodal approaches and state-of the-art methods over both the generated multi-modal datasets.","offset":6,"pro":0.75,"labels":"CLN"},{"idx":12,"sentence":"The developed multi-modal datasets are available for use at https://github.com/sduttap16/MM{_PPI{_NLP.","offset":7,"pro":0.875,"labels":"CTN"},{"idx":13,"sentence":"In this paper, we propose a novel bipartite flat-graph network (BiFlaG) for nested named entity recognition (NER), which contains two subgraph modules: a flat NER module for outermost entities and a graph module for all the entities located in inner layers.","offset":0,"pro":0,"labels":"PUR"},{"idx":13,"sentence":"Bidirectional LSTM (BiLSTM) and graph convolutional network (GCN) are adopted to jointly learn flat entities and their inner dependencies.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":13,"sentence":"Different from previous models, which only consider the unidirectional delivery of information from innermost layers to outer ones (or outside-to-inside), our model effectively captures the bidirectional interaction between them.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":13,"sentence":"We first use the entities recognized by the flat NER module to construct an entity graph, which is fed to the next graph module.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":13,"sentence":"The richer representation learned from graph module carries the dependencies of inner entities and can be exploited to improve outermost entity predictions.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":13,"sentence":"Experimental results on three standard nested NER datasets demonstrate that our BiFlaG outperforms previous state-of-the-art models.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":14,"sentence":"Knowledge graph (KG) entity typing aims at inferring possible missing entity type instances in KG, which is a very significant but still under-explored subtask of knowledge graph completion.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"In this paper, we propose a novel approach for KG entity typing which is trained by jointly utilizing local typing knowledge from existing entity type assertions and global triple knowledge in KGs.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":14,"sentence":"Specifically, we present two distinct knowledge-driven effective mechanisms of entity type inference.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":14,"sentence":"Accordingly, we build two novel embedding models to realize the mechanisms.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":14,"sentence":"Afterward, a joint model via connecting them is used to infer missing entity type instances, which favors inferences that agree with both entity type instances and triple knowledge in KGs.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":14,"sentence":"Experimental results on two real-world datasets (Freebase and YAGO) demonstrate the effectiveness of our proposed mechanisms and models for improving KG entity typing.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":14,"sentence":"The source code and data of this paper can be obtained from: https://github.com/Adam1679/ConnectE .","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":15,"sentence":"Continual relation learning aims to continually train a model on new data to learn incessantly emerging novel relations while avoiding catastrophically forgetting old relations.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"Some pioneering work has proved that storing a handful of historical relation examples in episodic memory and replaying them in subsequent training is an effective solution for such a challenging problem.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":15,"sentence":"However, these memory-based methods usually suffer from overfitting the few memorized examples of old relations, which may gradually cause inevitable confusion among existing relations.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":15,"sentence":"Inspired by the mechanism in human long-term memory formation, we introduce episodic memory activation and reconsolidation (EMAR) to continual relation learning.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":15,"sentence":"Every time neural models are activated to learn both new and memorized data, EMAR utilizes relation prototypes for memory reconsolidation exercise to keep a stable understanding of old relations.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":15,"sentence":"The experimental results show that EMAR could get rid of catastrophically forgetting old relations and outperform the state-of-the-art continual learning models.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":16,"sentence":"One great challenge in neural sequence labeling is the data sparsity problem for rare entity words and phrases.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"Most of test set entities appear only few times and are even unseen in training corpus, yielding large number of out-of-vocabulary (OOV) and low-frequency (LF) entities during evaluation.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":16,"sentence":"In this work, we propose approaches to address this problem.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":16,"sentence":"For OOV entities, we introduce local context reconstruction to implicitly incorporate contextual information into their representations.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":16,"sentence":"For LF entities, we present delexicalized entity identification to explicitly extract their frequency-agnostic and entity-type-specific representations.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":16,"sentence":"Extensive experiments on multiple benchmark datasets show that our model has significantly outperformed all previous methods and achieved new start-of-the-art results.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":16,"sentence":"Notably, our methods surpass the model fine-tuned on pre-trained language models without external resource.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":17,"sentence":"Interpretable rationales for model predictions play a critical role in practical applications.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"In this study, we develop models possessing interpretable inference process for structured prediction.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":17,"sentence":"Specifically, we present a method of instance-based learning that learns similarities between spans.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":17,"sentence":"At inference time, each span is assigned a class label based on its similar spans in the training set, where it is easy to understand how much each training instance contributes to the predictions.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":17,"sentence":"Through empirical analysis on named entity recognition, we demonstrate that our method enables to build models that have high interpretability without sacrificing performance.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":18,"sentence":"Electronic Medical Records (EMRs) have become key components of modern medical care systems.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"Despite the merits of EMRs, many doctors suffer from writing them, which is time-consuming and tedious.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":18,"sentence":"We believe that automatically converting medical dialogues to EMRs can greatly reduce the burdens of doctors, and extracting information from medical dialogues is an essential step.","offset":2,"pro":0.25,"labels":"BAC"},{"idx":18,"sentence":"To this end, we annotate online medical consultation dialogues in a window-sliding style, which is much easier than the sequential labeling annotation.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":18,"sentence":"We then propose a Medical Information Extractor (MIE) towards medical dialogues.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":18,"sentence":"MIE is able to extract mentioned symptoms, surgeries, tests, other information and their corresponding status.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":18,"sentence":"To tackle the particular challenges of the task, MIE uses a deep matching architecture, taking dialogue turn-interaction into account.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":18,"sentence":"The experimental results demonstrate MIE is a promising solution to extract medical information from doctor-patient dialogues.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":19,"sentence":"Named Entity Recognition (NER) is a fundamental task in Natural Language Processing, concerned with identifying spans of text expressing references to entities.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"NER research is often focused on flat entities only (flat NER), ignoring the fact that entity references can be nested, as in [Bank of [China]] (Finkel and Manning, 2009).","offset":1,"pro":0.2,"labels":"BAC"},{"idx":19,"sentence":"In this paper, we use ideas from graph-based dependency parsing to provide our model a global view on the input via a biaffine model (Dozat and Manning, 2017).","offset":2,"pro":0.4,"labels":"PUR"},{"idx":19,"sentence":"The biaffine model scores pairs of start and end tokens in a sentence which we use to explore all spans, so that the model is able to predict named entities accurately.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":19,"sentence":"We show that the model works well for both nested and flat NER through evaluation on 8 corpora and achieving SoTA performance on all of them, with accuracy gains of up to 2.2 percentage points.","offset":4,"pro":0.8,"labels":"RST"},{"idx":0,"sentence":"Pre-trained language models have shown remarkable success in improving various downstream NLP tasks due to their ability to capture dependencies in textual data and generate natural responses.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"In this paper, we leverage the power of pre-trained language models for improving video-grounded dialogue, which is very challenging and involves complex features of different dynamics: (1) Video features which can extend across both spatial and temporal dimensions; and (2) Dialogue features which involve semantic dependencies over multiple dialogue turns.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":0,"sentence":"We propose a framework by extending GPT-2 models to tackle these challenges by formulating video-grounded dialogue tasks as a sequence-to-sequence task, combining both visual and textual representation into a structured sequence, and fine-tuning a large pre-trained GPT-2 network.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":0,"sentence":"Our framework allows fine-tuning language models to capture dependencies across multiple modalities over different levels of information: spatio-temporal level in video and token-sentence level in dialogue context.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":0,"sentence":"We achieve promising improvement on the Audio-Visual Scene-Aware Dialogues (AVSD) benchmark from DSTC7, which supports a potential direction in this line of research.","offset":4,"pro":0.8,"labels":"RST"},{"idx":1,"sentence":"The task of named entity recognition (NER) is normally divided into nested NER and flat NER depending on whether named entities are nested or not.Models are usually separately developed for the two tasks, since sequence labeling models, the most widely used backbone for flat NER, are only able to assign a single label to a particular token, which is unsuitable for nested NER where a token may be assigned several labels.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"In this paper, we propose a unified framework that is capable of handling both flat and nested NER tasks.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":1,"sentence":"Instead of treating the task of NER as a sequence labeling problem, we propose to formulate it as a machine reading comprehension (MRC) task.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":1,"sentence":"For example, extracting entities with the per label is formalized as extracting answer spans to the question which person is mentioned in the text''.This formulation naturally tackles the entity overlapping issue in nested NER: the extraction of two overlapping entities with different categories requires answering two independent questions.","offset":3,"pro":0.42857142857142855,"labels":"BAC"},{"idx":1,"sentence":"Additionally, since the query encodes informative prior knowledge, this strategy facilitates the process of entity extraction, leading to better performances for not only nested NER, but flat NER.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":1,"sentence":"We conduct experiments on both nested and flat NER datasets.Experiment results demonstrate the effectiveness of the proposed formulation.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":1,"sentence":"We are able to achieve a vast amount of performance boost over current SOTA models on nested NER datasets, i.e., +1.28, +2.55, +5.44, +6.37,respectively on ACE04, ACE05, GENIA and KBP17, along with SOTA results on flat NER datasets, i.e., +0.24, +1.95, +0.21, +1.49 respectively on English CoNLL 2003, English OntoNotes 5.0, Chinese MSRA and Chinese OntoNotes 4.0.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":2,"sentence":"Unlike widely used Named Entity Recognition (NER) data sets in generic domains, biomedical NER data sets often contain mentions consisting of discontinuous spans.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"Conventional sequence tagging techniques encode Markov assumptions that are efficient but preclude recovery of these mentions.","offset":1,"pro":0.25,"labels":"BAC"},{"idx":2,"sentence":"We propose a simple, effective transition-based model with generic neural encoding for discontinuous NER.","offset":2,"pro":0.5,"labels":"PUR"},{"idx":2,"sentence":"Through extensive experiments on three biomedical data sets, we show that our model can effectively recognize discontinuous mentions without sacrificing the accuracy on continuous mentions.","offset":3,"pro":0.75,"labels":"RST"},{"idx":3,"sentence":"While traditional systems for Open Information Extraction were statistical and rule-based, recently neural models have been introduced for the task.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"Our work builds upon CopyAttention, a sequence generation OpenIE model (Cui et. al. 18).","offset":1,"pro":0.14285714285714285,"labels":"MTD"},{"idx":3,"sentence":"Our analysis reveals that CopyAttention produces a constant number of extractions per sentence, and its extracted tuples often express redundant information.","offset":2,"pro":0.2857142857142857,"labels":"RST"},{"idx":3,"sentence":"We present IMoJIE, an extension to CopyAttention, which produces the next extraction conditioned on all previously extracted tuples.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":3,"sentence":"This approach overcomes both shortcomings of CopyAttention, resulting in a variable number of diverse extractions per sentence.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":3,"sentence":"We train IMoJIE on training data bootstrapped from extractions of several non-neural systems, which have been automatically filtered to reduce redundancy and noise.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":3,"sentence":"IMoJIE outperforms CopyAttention by about 18 F1 pts, and a BERT-based strong baseline by 2 F1 pts, establishing a new state of the art for the task.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":4,"sentence":"Event Detection (ED) is a fundamental task in automatically structuring texts.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"Due to the small scale of training data, previous methods perform poorly on unseen/sparsely labeled trigger words and are prone to overfitting densely labeled trigger words.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":4,"sentence":"To address the issue, we propose a novel Enrichment Knowledge Distillation (EKD) model to leverage external open-domain trigger knowledge to reduce the in-built biases to frequent trigger words in annotations.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":4,"sentence":"Experiments on benchmark ACE2005 show that our model outperforms nine strong baselines, is especially effective for unseen/sparsely labeled trigger words.","offset":3,"pro":0.6,"labels":"CLN"},{"idx":4,"sentence":"The source code is released on https://github.com/shuaiwa16/ekd.git.","offset":4,"pro":0.8,"labels":"CTN"},{"idx":5,"sentence":"Exploiting sentence-level labels, which are easy to obtain, is one of the plausible methods to improve low-resource named entity recognition (NER), where token-level labels are costly to annotate.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"Current models for jointly learning sentence and token labeling are limited to binary classification.","offset":1,"pro":0.25,"labels":"GAP"},{"idx":5,"sentence":"We present a joint model that supports multi-class classification and introduce a simple variant of self-attention that allows the model to learn scaling factors.","offset":2,"pro":0.5,"labels":"PUR"},{"idx":5,"sentence":"Our model produces 3.78%, 4.20%, 2.08% improvements in F1 over the BiLSTM-CRF baseline on e-commerce product titles in three different low-resource languages: Vietnamese, Thai, and Indonesian, respectively.","offset":3,"pro":0.75,"labels":"RST"},{"idx":6,"sentence":"Cross-domain NER is a challenging yet practical problem.","offset":0,"pro":0,"labels":"GAP"},{"idx":6,"sentence":"Entity mentions can be highly different across domains.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":6,"sentence":"However, the correlations between entity types can be relatively more stable across domains.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":6,"sentence":"We investigate a multi-cell compositional LSTM structure for multi-task learning, modeling each entity type using a separate cell state.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":6,"sentence":"With the help of entity typed units, cross-domain knowledge transfer can be made in an entity type level.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":6,"sentence":"Theoretically, the resulting distinct feature distributions for each entity type make it more powerful for cross-domain transfer.","offset":5,"pro":0.7142857142857143,"labels":"BAC"},{"idx":6,"sentence":"Empirically, experiments on four few-shot and zero-shot datasets show our method significantly outperforms a series of multi-task learning methods and achieves the best results.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":7,"sentence":"This paper presents Pyramid, a novel layered model for Nested Named Entity Recognition (nested NER).","offset":0,"pro":0,"labels":"PUR"},{"idx":7,"sentence":"In our approach, token or text region embeddings are recursively inputted into L flat NER layers, from bottom to top, stacked in a pyramid shape.","offset":1,"pro":0.125,"labels":"MTD"},{"idx":7,"sentence":"Each time an embedding passes through a layer of the pyramid, its length is reduced by one.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":7,"sentence":"Its hidden state at layer l represents an l-gram in the input text, which is labeled only if its corresponding text region represents a complete entity mention.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":7,"sentence":"We also design an inverse pyramid to allow bidirectional interaction between layers.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":7,"sentence":"The proposed method achieves state-of-the-art F1 scores in nested NER on ACE-2004, ACE-2005, GENIA, and NNE, which are 80.27, 79.42, 77.78, and 93.70 with conventional embeddings, and 87.74, 86.34, 79.31, and 94.68 with pre-trained contextualized embeddings.","offset":5,"pro":0.625,"labels":"RST"},{"idx":7,"sentence":"In addition, our model can be used for the more general task of Overlapping Named Entity Recognition.","offset":6,"pro":0.75,"labels":"CTN"},{"idx":7,"sentence":"A preliminary experiment confirms the effectiveness of our method in overlapping NER.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":8,"sentence":"The goal of Knowledge graph embedding (KGE) is to learn how to represent the low dimensional vectors for entities and relations based on the observed triples.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"The conventional shallow models are limited to their expressiveness.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":8,"sentence":"ConvE (Dettmers et al., 2018) takes advantage of CNN and improves the expressive power with parameter efficient operators by increasing the interactions between head and relation embeddings.","offset":2,"pro":0.2222222222222222,"labels":"BAC"},{"idx":8,"sentence":"However, there is no structural information in the embedding space of ConvE, and the performance is still limited by the number of interactions.","offset":3,"pro":0.3333333333333333,"labels":"GAP"},{"idx":8,"sentence":"The recent KBGAT (Nathani et al., 2019) provides another way to learn embeddings by adaptively utilizing structural information.","offset":4,"pro":0.4444444444444444,"labels":"BAC"},{"idx":8,"sentence":"In this paper, we take the benefits of ConvE and KBGAT together and propose a Relation-aware Inception network with joint local-global structural information for knowledge graph Embedding (ReInceptionE).","offset":5,"pro":0.5555555555555556,"labels":"PUR"},{"idx":8,"sentence":"Specifically, we first explore the Inception network to learn query embedding, which aims to further increase the interactions between head and relation embeddings.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":8,"sentence":"Then, we propose to use a relation-aware attention mechanism to enrich the query embedding with the local neighborhood and global entity information.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":8,"sentence":"Experimental results on both WN18RR and FB15k-237 datasets demonstrate that ReInceptionE achieves competitive performance compared with state-of-the-art methods.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":9,"sentence":"Distant supervision based methods for entity and relation extraction have received increasing popularity due to the fact that these methods require light human annotation efforts.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"In this paper, we consider the problem of shifted label distribution, which is caused by the inconsistency between the noisy-labeled training set subject to external knowledge graph and the human-annotated test set, and exacerbated by the pipelined entity-then-relation extraction manner with noise propagation.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":9,"sentence":"We propose a joint extraction approach to address this problem by re-labeling noisy instances with a group of cooperative multiagents.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":9,"sentence":"To handle noisy instances in a fine-grained manner, each agent in the cooperative group evaluates the instance by calculating a continuous confidence score from its own perspective; To leverage the correlations between these two extraction tasks, a confidence consensus module is designed to gather the wisdom of all agents and re-distribute the noisy training set with confidence-scored labels.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":9,"sentence":"Further, the confidences are used to adjust the training losses of extractors.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":9,"sentence":"Experimental results on two real-world datasets verify the benefits of re-labeling noisy instance, and show that the proposed model significantly outperforms the state-of-the-art entity and relation extraction methods.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":10,"sentence":"Recently, many works have tried to augment the performance of Chinese named entity recognition (NER) using word lexicons.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"As a representative, Lattice-LSTM has achieved new benchmark results on several public Chinese NER datasets.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":10,"sentence":"However, Lattice-LSTM has a complex model architecture.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":10,"sentence":"This limits its application in many industrial areas where real-time NER responses are needed.","offset":3,"pro":0.375,"labels":"GAP"},{"idx":10,"sentence":"In this work, we propose a simple but effective method for incorporating the word lexicon into the character representations.","offset":4,"pro":0.5,"labels":"PUR"},{"idx":10,"sentence":"This method avoids designing a complicated sequence modeling architecture, and for any neural NER model, it requires only subtle adjustment of the character representation layer to introduce the lexicon information.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":10,"sentence":"Experimental studies on four benchmark Chinese NER datasets show that our method achieves an inference speed up to 6.15 times faster than those of state-of-the-art methods, along with a better performance.","offset":6,"pro":0.75,"labels":"RST"},{"idx":10,"sentence":"The experimental results also show that the proposed method can be easily incorporated with pre-trained models like BERT.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":11,"sentence":"In this paper, we propose a new adversarial augmentation method for Neural Machine Translation (NMT).","offset":0,"pro":0,"labels":"PUR"},{"idx":11,"sentence":"The main idea is to minimize the vicinal risk over virtual sentences sampled from two vicinity distributions, in which the crucial one is a novel vicinity distribution for adversarial sentences that describes a smooth interpolated embedding space centered around observed training sentence pairs.","offset":1,"pro":0.25,"labels":"MTD"},{"idx":11,"sentence":"We then discuss our approach, AdvAug, to train NMT models using the embeddings of virtual sentences in sequence-to-sequence learning.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":11,"sentence":"Experiments on Chinese-English, English-French, and English-German translation benchmarks show that AdvAug achieves significant improvements over theTransformer (up to 4.9 BLEU points), and substantially outperforms other data augmentation techniques (e.g.back-translation) without using extra corpora.","offset":3,"pro":0.75,"labels":"RST"},{"idx":12,"sentence":"The advent of context-aware NMT has resulted in promising improvements in the overall translation quality and specifically in the translation of discourse phenomena such as pronouns.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"Previous works have mainly focused on the use of past sentences as context with a focus on anaphora translation.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":12,"sentence":"In this work, we investigate the effect of future sentences as context by comparing the performance of a contextual NMT model trained with the future context to the one trained with the past context.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":12,"sentence":"Our experiments and evaluation, using generic and pronoun-focused automatic metrics, show that the use of future context not only achieves significant improvements over the context-agnostic Transformer, but also demonstrates comparable and in some cases improved performance over its counterpart trained on past context.","offset":3,"pro":0.6,"labels":"CLN"},{"idx":12,"sentence":"We also perform an evaluation on a targeted cataphora test suite and report significant gains over the context-agnostic Transformer in terms of BLEU.","offset":4,"pro":0.8,"labels":"RST"},{"idx":13,"sentence":"Although neural machine translation (NMT) has achieved significant progress in recent years, most previous NMT models only depend on the source text to generate translation.","offset":0,"pro":0,"labels":"GAP"},{"idx":13,"sentence":"Inspired by the success of template-based and syntax-based approaches in other fields, we propose to use extracted templates from tree structures as soft target templates to guide the translation procedure.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":13,"sentence":"In order to learn the syntactic structure of the target sentences, we adopt constituency-based parse tree to generate candidate templates.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":13,"sentence":"We incorporate the template information into the encoder-decoder framework to jointly utilize the templates and source text.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":13,"sentence":"Experiments show that our model significantly outperforms the baseline models on four benchmarks and demonstrates the effectiveness of soft target templates.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":14,"sentence":"In this paper, we show that neural machine translation (NMT) systems trained on large back-translated data overfit some of the characteristics of machine-translated texts.","offset":0,"pro":0,"labels":"PUR"},{"idx":14,"sentence":"Such NMT systems better translate human-produced translations, i.e., translationese, but may largely worsen the translation quality of original texts.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":14,"sentence":"Our analysis reveals that adding a simple tag to back-translations prevents this quality degradation and improves on average the overall translation quality by helping the NMT system to distinguish back-translated data from original parallel data during training.","offset":2,"pro":0.4,"labels":"CLN"},{"idx":14,"sentence":"We also show that, in contrast to high-resource configurations, NMT systems trained in low-resource settings are much less vulnerable to overfit back-translations.","offset":3,"pro":0.6,"labels":"CLN"},{"idx":14,"sentence":"We conclude that the back-translations in the training data should always be tagged especially when the origin of the text to be translated is unknown.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":15,"sentence":"Speech translation (ST) aims to learn transformations from speech in the source language to the text in the target language.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"Previous works show that multitask learning improves the ST performance, in which the recognition decoder generates the text of the source language, and the translation decoder obtains the final translations based on the output of the recognition decoder.","offset":1,"pro":0.3333333333333333,"labels":"BAC"},{"idx":15,"sentence":"Because whether the output of the recognition decoder has the correct semantics is more critical than its accuracy, we propose to improve the multitask ST model by utilizing word embedding as the intermediate.","offset":2,"pro":0.6666666666666666,"labels":"PUR"},{"idx":16,"sentence":"Measuring the scholarly impact of a document without citations is an important and challenging problem.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"Existing approaches such as Document Influence Model (DIM) are based on dynamic topic models, which only consider the word frequency change.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":16,"sentence":"In this paper, we use both frequency changes and word semantic shifts to measure document influence by developing a neural network framework.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":16,"sentence":"Our model has three steps.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":16,"sentence":"Firstly, we train the word embeddings for different time periods.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":16,"sentence":"Subsequently, we propose an unsupervised method to align vectors for different time periods.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":16,"sentence":"Finally, we compute the influence value of documents.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":16,"sentence":"Our experimental results show that our model outperforms DIM.","offset":7,"pro":0.875,"labels":"RST"},{"idx":17,"sentence":"Neural sequence to sequence text generation has been proved to be a viable approach to paraphrase generation.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"Despite promising results, paraphrases generated by these models mostly suffer from lack of quality and diversity.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":17,"sentence":"To address these problems, we propose a novel retrieval-based method for paraphrase generation.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":17,"sentence":"Our model first retrieves a paraphrase pair similar to the input sentence from a pre-defined index.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":17,"sentence":"With its novel editor module, the model then paraphrases the input sequence by editing it using the extracted relations between the retrieved pair of sentences.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":17,"sentence":"In order to have fine-grained control over the editing process, our model uses the newly introduced concept of Micro Edit Vectors.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":17,"sentence":"It both extracts and exploits these vectors using the attention mechanism in the Transformer architecture.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":17,"sentence":"Experimental results show the superiority of our paraphrase generation method in terms of both automatic metrics, and human evaluation of relevance, grammaticality, and diversity of generated paraphrases.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":18,"sentence":"We study the problem of multilingual masked language modeling, i.e. the training of a single model on concatenated text from multiple languages, and present a detailed study of several factors that influence why these models are so effective for cross-lingual transfer.","offset":0,"pro":0,"labels":"PUR"},{"idx":18,"sentence":"We show, contrary to what was previously hypothesized, that transfer is possible even when there is no shared vocabulary across the monolingual corpora and also when the text comes from very different domains.","offset":1,"pro":0.2,"labels":"RST"},{"idx":18,"sentence":"The only requirement is that there are some shared parameters in the top layers of the multi-lingual encoder.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":18,"sentence":"To better understand this result, we also show that representations from monolingual BERT models in different languages can be aligned post-hoc quite effectively, strongly suggesting that, much like for non-contextual word embeddings, there are universal latent symmetries in the learned embedding spaces.","offset":3,"pro":0.6,"labels":"CLN"},{"idx":18,"sentence":"For multilingual masked language modeling, these symmetries are automatically discovered and aligned during the joint training process.","offset":4,"pro":0.8,"labels":"MTD"},{"idx":19,"sentence":"Pre-trained language models like BERT have proven to be highly performant.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"However, they are often computationally expensive in many practical scenarios, for such heavy models can hardly be readily implemented with limited resources.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":19,"sentence":"To improve their efficiency with an assured model performance, we propose a novel speed-tunable FastBERT with adaptive inference time.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":19,"sentence":"The speed at inference can be flexibly adjusted under varying demands, while redundant calculation of samples is avoided.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":19,"sentence":"Moreover, this model adopts a unique self-distillation mechanism at fine-tuning, further enabling a greater computational efficacy with minimal loss in performance.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":19,"sentence":"Our model achieves promising results in twelve English and Chinese datasets.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":19,"sentence":"It is able to speed up by a wide range from 1 to 12 times than BERT if given different speedup thresholds to make a speed-performance tradeoff.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":0,"sentence":"Answer retrieval is to find the most aligned answer from a large set of candidates given a question.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"Learning vector representations of questions/answers is the key factor.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":0,"sentence":"Question-answer alignment and question/answer semantics are two important signals for learning the representations.","offset":2,"pro":0.25,"labels":"BAC"},{"idx":0,"sentence":"Existing methods learned semantic representations with dual encoders or dual variational auto-encoders.","offset":3,"pro":0.375,"labels":"BAC"},{"idx":0,"sentence":"The semantic information was learned from language models or question-to-question (answer-to-answer) generative processes.","offset":4,"pro":0.5,"labels":"BAC"},{"idx":0,"sentence":"However, the alignment and semantics were too separate to capture the aligned semantics between question and answer.","offset":5,"pro":0.625,"labels":"GAP"},{"idx":0,"sentence":"In this work, we propose to cross variational auto-encoders by generating questions with aligned answers and generating answers with aligned questions.","offset":6,"pro":0.75,"labels":"PUR"},{"idx":0,"sentence":"Experiments show that our method outperforms the state-of-the-art answer retrieval method on SQuAD.","offset":7,"pro":0.875,"labels":"RST"},{"idx":1,"sentence":"Many natural language questions require qualitative, quantitative or logical comparisons between two entities or events.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"This paper addresses the problem of improving the accuracy and consistency of responses to comparison questions by integrating logic rules and neural models.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":1,"sentence":"Our method leverages logical and linguistic knowledge to augment labeled training data and then uses a consistency-based regularizer to train the model.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":1,"sentence":"Improving the global consistency of predictions, our approach achieves large improvements over previous methods in a variety of question answering (QA) tasks, including multiple-choice qualitative reasoning, cause-effect reasoning, and extractive machine reading comprehension.","offset":3,"pro":0.42857142857142855,"labels":"CLN"},{"idx":1,"sentence":"In particular, our method significantly improves the performance of RoBERTa-based models by 1-5% across datasets.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":1,"sentence":"We advance state of the art by around 5-8% on WIQA and QuaRel and reduce consistency violations by 58% on HotpotQA.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":1,"sentence":"We further demonstrate that our approach can learn effectively from limited data.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":2,"sentence":"Automatic question generation (QG) has shown promise as a source of synthetic training data for question answering (QA).","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"In this paper we ask: Is textual diversity in QG beneficial for downstream QA?","offset":1,"pro":0.25,"labels":"PUR"},{"idx":2,"sentence":"Using top-p nucleus sampling to derive samples from a transformer-based question generator, we show that diversity-promoting QG indeed provides better QA training than likelihood maximization approaches such as beam search.","offset":2,"pro":0.5,"labels":"CLN"},{"idx":2,"sentence":"We also show that standard QG evaluation metrics such as BLEU, ROUGE and METEOR are inversely correlated with diversity, and propose a diversity-aware intrinsic measure of overall QG quality that correlates well with extrinsic evaluation on QA.","offset":3,"pro":0.75,"labels":"CLN"},{"idx":3,"sentence":"We address the problem of extractive question answering using document-level distant super-vision, pairing questions and relevant documents with answer strings.","offset":0,"pro":0,"labels":"PUR"},{"idx":3,"sentence":"We compare previously used probability space and distant supervision assumptions (assumptions on the correspondence between the weak answer string labels and possible answer mention spans).","offset":1,"pro":0.2,"labels":"MTD"},{"idx":3,"sentence":"We show that these assumptions interact, and that different configurations provide complementary benefits.","offset":2,"pro":0.4,"labels":"CLN"},{"idx":3,"sentence":"We demonstrate that a multi-objective model can efficiently combine the advantages of multiple assumptions and outperform the best individual formulation.","offset":3,"pro":0.6,"labels":"CLN"},{"idx":3,"sentence":"Our approach outperforms previous state-of-the-art models by 4.3 points in F1 on TriviaQA-Wiki and 1.7 points in Rouge-L on NarrativeQA summaries.","offset":4,"pro":0.8,"labels":"RST"},{"idx":4,"sentence":"We introduce SCDE, a dataset to evaluate the performance of computational models through sentence prediction.","offset":0,"pro":0,"labels":"PUR"},{"idx":4,"sentence":"SCDE is a human created sentence cloze dataset, collected from public school English examinations.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":4,"sentence":"Our task requires a model to fill up multiple blanks in a passage from a shared candidate set with distractors designed by English teachers.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":4,"sentence":"Experimental results demonstrate that this task requires the use of non-local, discourse-level context beyond the immediate sentence neighborhood.","offset":3,"pro":0.42857142857142855,"labels":"RST"},{"idx":4,"sentence":"The blanks require joint solving and significantly impair each other's context.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":4,"sentence":"Furthermore, through ablations, we show that the distractors are of high quality and make the task more challenging.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":4,"sentence":"Our experiments show that there is a significant performance gap between advanced models (72%) and humans (87%), encouraging future models to bridge this gap.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":5,"sentence":"To avoid giving wrong answers, question answering (QA) models need to know when to abstain from answering.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"Moreover, users often ask questions that diverge from the model's training data, making errors more likely and thus abstention more critical.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":5,"sentence":"In this work, we propose the setting of selective question answering under domain shift, in which a QA model is tested on a mixture of in-domain and out-of-domain data, and must answer (i.e., not abstain on) as many questions as possible while maintaining high accuracy.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":5,"sentence":"Abstention policies based solely on the model's softmax probabilities fare poorly, since models are overconfident on out-of-domain inputs.","offset":3,"pro":0.375,"labels":"BAC"},{"idx":5,"sentence":"Instead, we train a calibrator to identify inputs on which the QA model errs, and abstain when it predicts an error is likely.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":5,"sentence":"Crucially, the calibrator benefits from observing the model's behavior on out-of-domain data, even if from a different domain than the test data.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":5,"sentence":"We combine this method with a SQuAD-trained QA model and evaluate on mixtures of SQuAD and five other QA datasets.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":5,"sentence":"Our method answers 56% of questions while maintaining 80% accuracy; in contrast, directly using the model's probabilities only answers 48% at 80% accuracy.","offset":7,"pro":0.875,"labels":"RST"},{"idx":6,"sentence":"Large transformer-based language models have been shown to be very effective in many classification tasks.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"However, their computational complexity prevents their use in applications requiring the classification of a large set of candidates.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":6,"sentence":"While previous works have investigated approaches to reduce model size, relatively little attention has been paid to techniques to improve batch throughput during inference.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":6,"sentence":"In this paper, we introduce the Cascade Transformer, a simple yet effective technique to adapt transformer-based models into a cascade of rankers.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":6,"sentence":"Each ranker is used to prune a subset of candidates in a batch, thus dramatically increasing throughput at inference time.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":6,"sentence":"Partial encodings from the transformer model are shared among rerankers, providing further speed-up.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":6,"sentence":"When compared to a state-of-the-art transformer model, our approach reduces computation by 37% with almost no impact on accuracy, as measured on two English Question Answering datasets.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":7,"sentence":"We introduce a novel approach to transformers that learns hierarchical representations in multiparty dialogue.","offset":0,"pro":0,"labels":"PUR"},{"idx":7,"sentence":"First, three language modeling tasks are used to pre-train the transformers, token- and utterance-level language modeling and utterance order prediction, that learn both token and utterance embeddings for better understanding in dialogue contexts.","offset":1,"pro":0.25,"labels":"MTD"},{"idx":7,"sentence":"Then, multi-task learning between the utterance prediction and the token span prediction is applied to fine-tune for span-based question answering (QA).","offset":2,"pro":0.5,"labels":"MTD"},{"idx":7,"sentence":"Our approach is evaluated on the FriendsQA dataset and shows improvements of 3.8% and 1.4% over the two state-of-the-art transformer models, BERT and RoBERTa, respectively.","offset":3,"pro":0.75,"labels":"RST"},{"idx":8,"sentence":"Empirical research in Natural Language Processing (NLP) has adopted a narrow set of principles for assessing hypotheses, relying mainly on p-value computation, which suffers from several known issues.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"While alternative proposals have been well-debated and adopted in other fields, they remain rarely discussed or used within the NLP community.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":8,"sentence":"We address this gap by contrasting various hypothesis assessment techniques, especially those not commonly used in the field (such as evaluations based on Bayesian inference).","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":8,"sentence":"Since these statistical techniques differ in the hypotheses they can support, we argue that practitioners should first decide their target hypothesis before choosing an assessment method.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":8,"sentence":"This is crucial because common fallacies, misconceptions, and misinterpretation surrounding hypothesis assessment methods often stem from a discrepancy between what one would like to claim versus what the method used actually assesses.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":8,"sentence":"Our survey reveals that these issues are omnipresent in the NLP research community.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":8,"sentence":"As a step forward, we provide best practices and guidelines tailored to NLP research, as well as an easy-to-use package for Bayesian assessment of hypotheses, complementing existing tools.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":9,"sentence":"We present STARC (Structured Annotations for Reading Comprehension), a new annotation framework for assessing reading comprehension with multiple choice questions.","offset":0,"pro":0,"labels":"PUR"},{"idx":9,"sentence":"Our framework introduces a principled structure for the answer choices and ties them to textual span annotations.","offset":1,"pro":0.125,"labels":"MTD"},{"idx":9,"sentence":"The framework is implemented in OneStopQA, a new high-quality dataset for evaluation and analysis of reading comprehension in English.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":9,"sentence":"We use this dataset to demonstrate that STARC can be leveraged for a key new application for the development of SAT-like reading comprehension materials: automatic annotation quality probing via span ablation experiments.","offset":3,"pro":0.375,"labels":"CLN"},{"idx":9,"sentence":"We further show that it enables in-depth analyses and comparisons between machine and human reading comprehension behavior, including error distributions and guessing ability.","offset":4,"pro":0.5,"labels":"CLN"},{"idx":9,"sentence":"Our experiments also reveal that the standard multiple choice dataset in NLP, RACE, is limited in its ability to measure reading comprehension.","offset":5,"pro":0.625,"labels":"RST"},{"idx":9,"sentence":"47% of its questions can be guessed by machines without accessing the passage, and 18% are unanimously judged by humans as not having a unique correct answer.","offset":6,"pro":0.75,"labels":"RST"},{"idx":9,"sentence":"OneStopQA provides an alternative test set for reading comprehension which alleviates these shortcomings and has a substantially higher human ceiling performance.","offset":7,"pro":0.875,"labels":"CTN"},{"idx":10,"sentence":"In this paper, we present the first comprehensive categorization of essential commonsense knowledge for answering the Winograd Schema Challenge (WSC).","offset":0,"pro":0,"labels":"PUR"},{"idx":10,"sentence":"For each of the questions, we invite annotators to first provide reasons for making correct decisions and then categorize them into six major knowledge categories.","offset":1,"pro":0.14285714285714285,"labels":"MTD"},{"idx":10,"sentence":"By doing so, we better understand the limitation of existing methods (i.e., what kind of knowledge cannot be effectively represented or inferred with existing methods) and shed some light on the commonsense knowledge that we need to acquire in the future for better commonsense reasoning.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":10,"sentence":"Moreover, to investigate whether current WSC models can understand the commonsense or they simply solve the WSC questions based on the statistical bias of the dataset, we leverage the collected reasons to develop a new task called WinoWhy, which requires models to distinguish plausible reasons from very similar but wrong reasons for all WSC questions.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":10,"sentence":"Experimental results prove that even though pre-trained language representation models have achieved promising progress on the original WSC dataset, they are still struggling at WinoWhy.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":10,"sentence":"Further experiments show that even though supervised models can achieve better performance, the performance of these models can be sensitive to the dataset distribution.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":10,"sentence":"WinoWhy and all codes are available at: https://github.com/HKUST-KnowComp/WinoWhy.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":11,"sentence":"In online debates, users express different levels of agreement/disagreement with one another's arguments and ideas.","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"Often levels of agreement/disagreement are implicit in the text, and must be predicted to analyze collective opinions.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":11,"sentence":"Existing stance detection methods predict the polarity of a post's stance toward a topic or post, but don't consider the stance's degree of intensity.","offset":2,"pro":0.2,"labels":"GAP"},{"idx":11,"sentence":"We introduce a new research problem, stance polarity and intensity prediction in response relationships between posts.","offset":3,"pro":0.3,"labels":"PUR"},{"idx":11,"sentence":"This problem is challenging because differences in stance intensity are often subtle and require nuanced language understanding.","offset":4,"pro":0.4,"labels":"GAP"},{"idx":11,"sentence":"Cyber argumentation research has shown that incorporating both stance polarity and intensity data in online debates leads to better discussion analysis.","offset":5,"pro":0.5,"labels":"GAP"},{"idx":11,"sentence":"We explore five different learning models: Ridge-M regression, Ridge-S regression, SVR-RF-R, pkudblab-PIP, and T-PAN-PIP for predicting stance polarity and intensity in argumentation.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":11,"sentence":"These models are evaluated using a new dataset for stance polarity and intensity prediction collected using a cyber argumentation platform.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":11,"sentence":"The SVR-RF-R model performs best for prediction of stance polarity with an accuracy of 70.43% and intensity with RMSE of 0.596.","offset":8,"pro":0.8,"labels":"RST"},{"idx":11,"sentence":"This work is the first to train models for predicting a post's stance polarity and intensity in one combined value in cyber argumentation with reasonably good accuracy.","offset":9,"pro":0.9,"labels":"CTN"},{"idx":12,"sentence":"Recent neural network models have achieved impressive performance on sentiment classification in English as well as other languages.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"Their success heavily depends on the availability of a large amount of labeled data or parallel corpus.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":12,"sentence":"In this paper, we investigate an extreme scenario of cross-lingual sentiment classification, in which the low-resource language does not have any labels or parallel corpus.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":12,"sentence":"We propose an unsupervised cross-lingual sentiment classification model named multi-view encoder-classifier (MVEC) that leverages an unsupervised machine translation (UMT) system and a language discriminator.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":12,"sentence":"Unlike previous language model (LM) based fine-tuning approaches that adjust parameters solely based on the classification error on training data, we employ the encoder-decoder framework of a UMT as a regularization component on the shared network parameters.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":12,"sentence":"In particular, the cross-lingual encoder of our model learns a shared representation, which is effective for both reconstructing input sentences of two languages and generating more representative views from the input for classification.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":12,"sentence":"Extensive experiments on five language pairs verify that our model significantly outperforms other models for 8/11 sentiment classification tasks.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":13,"sentence":"We present an efficient annotation framework for argument quality, a feature difficult to be measured reliably as per previous work.","offset":0,"pro":0,"labels":"PUR"},{"idx":13,"sentence":"A stochastic transitivity model is combined with an effective sampling strategy to infer high-quality labels with low effort from crowdsourced pairwise judgments.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":13,"sentence":"The model's capabilities are showcased by compiling Webis-ArgQuality-20, an argument quality corpus that comprises scores for rhetorical, logical, dialectical, and overall quality inferred from a total of 41,859 pairwise judgments among 1,271 arguments.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":13,"sentence":"With up to 93% cost savings, our approach significantly outperforms existing annotation procedures.","offset":3,"pro":0.6,"labels":"RST"},{"idx":13,"sentence":"Furthermore, novel insight into argument quality is provided through statistical analysis, and a new aggregation method to infer overall quality from individual quality dimensions is proposed.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":14,"sentence":"This paper studies the task of comparative preference classification (CPC).","offset":0,"pro":0,"labels":"PUR"},{"idx":14,"sentence":"Given two entities in a sentence, our goal is to classify whether the first (or the second) entity is preferred over the other or no comparison is expressed at all between the two entities.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":14,"sentence":"Existing works either do not learn entity-aware representations well and fail to deal with sentences involving multiple entity pairs or use sequential modeling approaches that are unable to capture long-range dependencies between the entities.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":14,"sentence":"Some also use traditional machine learning approaches that do not generalize well.","offset":3,"pro":0.5,"labels":"GAP"},{"idx":14,"sentence":"This paper proposes a novel Entity-aware Dependency-based Deep Graph Attention Network (ED-GAT) that employs a multi-hop graph attention over a dependency graph sentence representation to leverage both the semantic information from word embeddings and the syntactic information from the dependency graph to solve the problem.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":14,"sentence":"Empirical evaluation shows that the proposed model achieves the state-of-the-art performance in comparative preference classification.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":15,"sentence":"We present OpinionDigest, an abstractive opinion summarization framework, which does not rely on gold-standard summaries for training.","offset":0,"pro":0,"labels":"PUR"},{"idx":15,"sentence":"The framework uses an Aspect-based Sentiment Analysis model to extract opinion phrases from reviews, and trains a Transformer model to reconstruct the original reviews from these extractions.","offset":1,"pro":0.14285714285714285,"labels":"MTD"},{"idx":15,"sentence":"At summarization time, we merge extractions from multiple reviews and select the most popular ones.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":15,"sentence":"The selected opinions are used as input to the trained Transformer model, which verbalizes them into an opinion summary.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":15,"sentence":"OpinionDigest can also generate customized summaries, tailored to specific user needs, by filtering the selected opinions according to their aspect and/or sentiment.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":15,"sentence":"Automatic evaluation on Yelp data shows that our framework outperforms competitive baselines.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":15,"sentence":"Human studies on two corpora verify that OpinionDigest produces informative summaries and shows promising customization capabilities.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":16,"sentence":"Affective tasks such as sentiment analysis, emotion classification, and sarcasm detection have been popular in recent years due to an abundance of user-generated data, accurate computational linguistic models, and a broad range of relevant applications in various domains.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"At the same time, many studies have highlighted the importance of text preprocessing, as an integral step to any natural language processing prediction model and downstream task.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":16,"sentence":"While preprocessing in affective systems is well-studied, preprocessing in word vector-based models applied to affective systems, is not.","offset":2,"pro":0.4,"labels":"GAP"},{"idx":16,"sentence":"To address this limitation, we conduct a comprehensive analysis of the role of preprocessing techniques in affective analysis based on word vector models.","offset":3,"pro":0.6,"labels":"PUR"},{"idx":16,"sentence":"Our analysis is the first of its kind and provides useful insights of the importance of each preprocessing technique when applied at the training phase, commonly ignored in pretrained word vector models, and/or at the downstream task phase.","offset":4,"pro":0.8,"labels":"CTN"},{"idx":17,"sentence":"Generative dialogue systems tend to produce generic responses, which often leads to boring conversations.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"For alleviating this issue, Recent studies proposed to retrieve and introduce knowledge facts from knowledge graphs.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":17,"sentence":"While this paradigm works to a certain extent, it usually retrieves knowledge facts only based on the entity word itself, without considering the specific dialogue context.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":17,"sentence":"Thus, the introduction of the context-irrelevant knowledge facts can impact the quality of generations.","offset":3,"pro":0.375,"labels":"GAP"},{"idx":17,"sentence":"To this end, this paper proposes a novel commonsense knowledge-aware dialogue generation model, ConKADI.","offset":4,"pro":0.5,"labels":"PUR"},{"idx":17,"sentence":"We design a Felicitous Fact mechanism to help the model focus on the knowledge facts that are highly relevant to the context; furthermore, two techniques, Context-Knowledge Fusion and Flexible Mode Fusion are proposed to facilitate the integration of the knowledge in the ConKADI.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":17,"sentence":"We collect and build a large-scale Chinese dataset aligned with the commonsense knowledge for dialogue generation.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":17,"sentence":"Extensive evaluations over both an open-released English dataset and our Chinese dataset demonstrate that our approach ConKADI outperforms the state-of-the-art approach CCM, in most experiments.","offset":7,"pro":0.875,"labels":"RST"},{"idx":18,"sentence":"Maintaining a consistent personality in conversations is quite natural for human beings, but is still a non-trivial task for machines.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"The persona-based dialogue generation task is thus introduced to tackle the personality-inconsistent problem by incorporating explicit persona text into dialogue generation models.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":18,"sentence":"Despite the success of existing persona-based models on generating human-like responses, their one-stage decoding framework can hardly avoid the generation of inconsistent persona words.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":18,"sentence":"In this work, we introduce a three-stage framework that employs a generate-delete-rewrite mechanism to delete inconsistent words from a generated response prototype and further rewrite it to a personality-consistent one.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":18,"sentence":"We carry out evaluations by both human and automatic metrics.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":18,"sentence":"Experiments on the Persona-Chat dataset show that our approach achieves good performance.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":19,"sentence":"Training the generative models with minimal corpus is one of the critical challenges for building open-domain dialogue systems.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"Existing methods tend to use the meta-learning framework which pre-trains the parameters on all non-target tasks then fine-tunes on the target task.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":19,"sentence":"However, fine-tuning distinguishes tasks from the parameter perspective but ignores the model-structure perspective, resulting in similar dialogue models for different tasks.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":19,"sentence":"In this paper, we propose an algorithm that can customize a unique dialogue model for each task in the few-shot setting.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":19,"sentence":"In our approach, each dialogue model consists of a shared module, a gating module, and a private module.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":19,"sentence":"The first two modules are shared among all the tasks, while the third one will differentiate into different network structures to better capture the characteristics of the corresponding task.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":19,"sentence":"The extensive experiments on two datasets show that our method outperforms all the baselines in terms of task consistency, response quality, and diversity.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":0,"sentence":"The curve-face gear pair is a new type of face gear pair with a variable transmission ratio.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"The determination rule of the limiting points on the tooth surface was analyzed by introducing the generating method and tooth surface characteristics of this gear tooth.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":0,"sentence":"Through the corresponding limiting angles and the distribution law of the contact line, the gear tooth surface was discretized, and the theoretical data points of the tooth surface were obtained.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":0,"sentence":"The variations in gear tooth surface in a meshing cycle and under different parameters were analyzed.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":0,"sentence":"By comparing the virtual tooth model and the corresponding rolling experiment, the correctness of the tooth surface discrete algorithm of the curve-face gear was verified.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":0,"sentence":"Concerning the complexity of its tooth surface, based on the coordinate transformation principle and the spatial meshing theory, a discrete algorithm of this tooth surface was proposed. ","offset":5,"pro":0.8333333333333334,"labels":"PUR"},{"idx":1,"sentence":"Engineered products often have more social impacts than are realized.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"In this paper, we show the extent to which different social impacts in 11 impact categories are co-present in 150 products and how this can help engineers and others during the product development process.","offset":1,"pro":0.1111111111111111,"labels":"PUR"},{"idx":1,"sentence":"The product review resulted in 13,200 data points that were divided into two data sets, one with 8800 data points from which a social impact probability table was created.","offset":2,"pro":0.2222222222222222,"labels":"MTD"},{"idx":1,"sentence":"The remaining data points were then used to validate the table.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":1,"sentence":"All data points were then combined to create a final social impact probability table.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":1,"sentence":"A simple method for predicting social impact is also created in order to assist engineers when developing products with social impacts in mind.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":1,"sentence":"Specifically, we show how social impact categories not previously considered can be identified. ","offset":6,"pro":0.6666666666666666,"labels":"PUR"},{"idx":1,"sentence":"This table provides insight for how various social impact categories correlate and can assist engineers in expanding their views to include additional social impact objectives and thus achieve a design with broader social impact or a design with minimized unwanted negative social impact. ","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":1,"sentence":"A product review was conducted to bring this to light. ","offset":8,"pro":0.8888888888888888,"labels":"PUR"},{"idx":2,"sentence":"This work develops a methodology for sensor placement and dynamic sensor scheduling decisions for digital twins.","offset":0,"pro":0,"labels":"PUR"},{"idx":2,"sentence":"The digital twin data assimilation is posed as a classification problem, and predictive models are used to train optimal classification trees that represent the map from observed data to estimated digital twin states.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":2,"sentence":"In addition to providing a rapid digital twin updating capability, the resulting classification trees yield an interpretable mathematical representation that can be queried to inform sensor placement and sensor scheduling decisions.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":2,"sentence":"The proposed approach is demonstrated for a structural digital twin of a 12 ft wingspan unmanned aerial vehicle.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":2,"sentence":"Offline, training data are generated by simulating scenarios using predictive reduced-order models of the vehicle in a range of structural states.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":2,"sentence":"These training data can be further augmented using experimental or other historical data.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":2,"sentence":"In operation, the trained classifier is applied to observational data from the physical vehicle, enabling rapid adaptation of the digital twin in response to changes in structural health.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":2,"sentence":"Within this context, we study the performance of the optimal tree classifiers and demonstrate how they enable explainable structural assessments from sparse sensor measurements and also inform optimal sensor placement. ","offset":7,"pro":0.875,"labels":"RST"},{"idx":3,"sentence":"Function is defined as the ensemble of tasks that enable the product to complete the designed purpose.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"Functional tools, such as functional modeling, offer decision guidance in the early phase of product design, where explicit design decisions are yet to be made.","offset":1,"pro":0.09090909090909091,"labels":"BAC"},{"idx":3,"sentence":"Function-based design data is often sparse and grounded in individual interpretation.","offset":2,"pro":0.18181818181818182,"labels":"BAC"},{"idx":3,"sentence":"As such, function-based design tools can benefit from automatic function classification to increase data fidelity and provide function representation models that enable function-based intelligent design agents.","offset":3,"pro":0.2727272727272727,"labels":"BAC"},{"idx":3,"sentence":"Function-based design data is commonly stored in manually generated design repositories.","offset":4,"pro":0.36363636363636365,"labels":"BAC"},{"idx":3,"sentence":"These design repositories are a collection of expert knowledge and interpretations of function in product design bounded by function-flow and component taxonomies.","offset":5,"pro":0.45454545454545453,"labels":"BAC"},{"idx":3,"sentence":"In this work, we represent a structured taxonomy-based design repository as assembly-flow graphs, then leverage a graph neural network (GNN) model to perform automatic function classification.","offset":6,"pro":0.5454545454545454,"labels":"PUR"},{"idx":3,"sentence":"We support automated function classification by learning from repository data to establish the ground truth of component function assignment.","offset":7,"pro":0.6363636363636364,"labels":"MTD"},{"idx":3,"sentence":"Experimental results show that our GNN model achieves a micro-average F-1-score of 0.617 for tier 1 (broad), 0.624 for tier 2, and 0.415 for tier 3 (specific) functions.","offset":8,"pro":0.7272727272727273,"labels":"RST"},{"idx":3,"sentence":"Given the imbalance of data features and the subjectivity in the definition of product function, the results are encouraging.","offset":9,"pro":0.8181818181818182,"labels":"CLN"},{"idx":3,"sentence":"Our efforts in this paper can be a starting point for more sophisticated applications in knowledge-based CAD systems and Design-for-X consideration in function-based design.","offset":10,"pro":0.9090909090909091,"labels":"CTN"},{"idx":4,"sentence":"Scientific and engineering problems often require the use of artificial intelligence to aid understanding and the search for promising designs.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"While Gaussian processes (GP) stand out as easy-to-use and interpretable learners, they have difficulties in accommodating big data sets, categorical inputs, and multiple responses, which has become a common challenge for a growing number of data-driven design applications.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":4,"sentence":"In this paper, we propose a GP model that utilizes latent variables and functions obtained through variational inference to address the aforementioned challenges simultaneously.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":4,"sentence":"The method is built upon the latent-variable Gaussian process (LVGP) model where categorical factors are mapped into a continuous latent space to enable GP modeling of mixed-variable data sets.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":4,"sentence":"By extending variational inference to LVGP models, the large training data set is replaced by a small set of inducing points to address the scalability issue.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":4,"sentence":"Output response vectors are represented by a linear combination of independent latent functions, forming a flexible kernel structure to handle multiple responses that might have distinct behaviors.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":4,"sentence":"Comparative studies demonstrate that the proposed method scales well for large data sets with over 10(4) data points, while outperforming state-of-the-art machine learning methods without requiring much hyperparameter tuning.","offset":6,"pro":0.75,"labels":"CLN"},{"idx":4,"sentence":"In addition, an interpretable latent space is obtained to draw insights into the effect of categorical factors, such as those associated with building blocks. ","offset":7,"pro":0.875,"labels":"CLN"},{"idx":5,"sentence":"Ongoing work within the engineering design research community seeks to develop automated design methods and tools that enhance the natural capabilities of designers in developing highly innovative concepts.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"Central to this vision is the ability to first obtain a deep understanding of the underlying behavior and process dynamics that predict successful performance in early-stage concept generation.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":5,"sentence":"The objective of this research is to better understand the predictive factors that lead to improved performance during concept generation.","offset":2,"pro":0.2222222222222222,"labels":"PUR"},{"idx":5,"sentence":"To accomplish this, we leverage an existing large-scale dataset containing hundreds of early-stage design concepts; each concept contains detailed ratings regarding its overall feasibility, usefulness, and novelty, as well as when in the ideation session the idea was recorded.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":5,"sentence":"Surprisingly, results indicate that there is no effect of idea fluency or timing on the quality of the output when using a holistic evaluation mechanism, such as the innovation measure, instead of a single measure such as novelty.","offset":4,"pro":0.4444444444444444,"labels":"RST"},{"idx":5,"sentence":"Thus, exceptional concepts can be achieved by all participant segments independent of idea fluency.","offset":5,"pro":0.5555555555555556,"labels":"CLN"},{"idx":5,"sentence":"In particular, this work focuses on the impact of idea fluency and timing of early-stage design concepts and their effect on overall measures of ideation session success. ","offset":6,"pro":0.6666666666666666,"labels":"PUR"},{"idx":5,"sentence":"Furthermore, in early-stage concept generation sessions, highest-rated concepts have an equal probability of occurring early and late in a session. ","offset":7,"pro":0.7777777777777778,"labels":"CLN"},{"idx":5,"sentence":"Taken together, these findings can be used to improve performance in ideation by effectively determining when and which types of design interventions future design tools might suggest. ","offset":8,"pro":0.8888888888888888,"labels":"IMP"},{"idx":6,"sentence":"Design optimization studies that model competition with other products in the market often use a small set of products to represent all competitors.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"We investigate the effect of competitor product representation on profit-maximizing design solutions.","offset":1,"pro":0.125,"labels":"PUR"},{"idx":6,"sentence":"We derive first-order optimality conditions and show that optimal design (but not price) is independent of competitors when using logit and nested logit models (where preferences are homogeneous).","offset":2,"pro":0.25,"labels":"CLN"},{"idx":6,"sentence":"However, this relationship differs in the case of random-coefficients logit models (where preferences are heterogeneous), and we demonstrate that profit-maximizing design solutions using latent-class or mixed-logit models can (but need not always) depend on the representation of competing products.","offset":3,"pro":0.375,"labels":"CLN"},{"idx":6,"sentence":"We discuss factors that affect the magnitude of the difference between models with elemental and composite representations of competitors, including preference heterogeneity, cost function curvature, and competitor set specification.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":6,"sentence":"We present correction factors that ensure models using subsets or composite representation of competitors have optimal design solutions that match those of disaggregated elemental models.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":6,"sentence":"Specifically, we study the implications of replacing a large set of disaggregated elemental competitor products with a subset of competitor products or composite products. ","offset":6,"pro":0.75,"labels":"PUR"},{"idx":6,"sentence":"While optimal designs using logit and nested logit models are not affected by ad hoc modeling decisions of competitor representation, the independence of optimal designs from competitors when using these models raises questions of when these models are appropriate to use. ","offset":7,"pro":0.875,"labels":"CLN"},{"idx":7,"sentence":"To systematically investigate the contact constraint relationships and the influences of fit clearances on the kinematic performances of a double roller tripod joint (DRTJ), a method for the kinematic analysis of DRTJs is proposed based on the principle of conjugate surfaces.","offset":0,"pro":0,"labels":"PUR"},{"idx":7,"sentence":"In the proposed method, the constraint relations between rollers and tracks as well as between rollers and trunnions are first derived based on the principle of conjugate surfaces.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":7,"sentence":"Then, according to the constructed constraint relationships, the kinematic analysis model of a DRTJ considering the influences of fit clearances is established.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":7,"sentence":"Next, the effectiveness of the proposed method is validated by measuring the relative displacements and angles between rollers and tracks via experiment.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":7,"sentence":"The fit clearances have little influence on the kinematic performances of the DRTJ, thus proper fit clearances between rollers and tracks as well as between rollers and trunnions can be designed to improve the lubricating conditions of the DRTJ.","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":7,"sentence":"Finally, kinematic analyses are carried out and the main results show that the relative pitch angle between rollers and tracks are always kept as zeroes under any working condition by designing the shapes of rollers' outer surfaces to be semi-toroid. ","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":8,"sentence":"Prototyping plays a pivotal role in the engineering design process.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"Prototypes represent physical or digital manifestations of design ideas, and as such act as effective communication tools for designers.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":8,"sentence":"While the benefits of prototyping are well-documented in research, the fundamental ways in which the construction of a prototype affects designers' reflection on and evaluation of their design outcomes and processes are not well understood.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":8,"sentence":"The relationships between prototypes, designers' communication strategies, and recollection of design processes is of particular interest in this work, as preliminary research suggests that novice designers tend to struggle to clearly articulate the decisions made during the design process.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":8,"sentence":"This work serves to extend prior work and build foundational knowledge by exploring the evaluation of design outcomes and decisions, and communication strategies used by novice designers during prototyping tasks.","offset":4,"pro":0.5714285714285714,"labels":"PUR"},{"idx":8,"sentence":"A controlled in situ study was conducted with 45 undergraduate engineering students.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":8,"sentence":"Results from qualitative analyses suggest that a number of rhetorical patterns emerged in students' communications, suggesting that a complicated relationship exists between prototyping and communication.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":9,"sentence":"This article discusses passive mechanical architectures for accuracy correction during assembly operations.","offset":0,"pro":0,"labels":"PUR"},{"idx":9,"sentence":"A specific solution for passive force generation with thresholds is presented.","offset":1,"pro":0.25,"labels":"MTD"},{"idx":9,"sentence":"Then, the kinematics of several rotational and translational mechanisms is presented.","offset":2,"pro":0.5,"labels":"MTD"},{"idx":9,"sentence":"Afterwards, a case study is introduced and the design of a prototype is presented together with some tests performed on a spark plug assembly task.","offset":3,"pro":0.75,"labels":"MTD"},{"idx":10,"sentence":"The importance-performance analysis (IPA) is a widely used technique to guide strategic planning for the improvement of customer satisfaction.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"Compared with surveys, numerous online reviews can be easily collected at a lower cost.","offset":1,"pro":0.09090909090909091,"labels":"BAC"},{"idx":10,"sentence":"Online reviews provide a promising source for the IPA.","offset":2,"pro":0.18181818181818182,"labels":"BAC"},{"idx":10,"sentence":"This paper proposes an approach for conducting the IPA from online reviews for product design.","offset":3,"pro":0.2727272727272727,"labels":"PUR"},{"idx":10,"sentence":"Product attributes from online reviews are first identified by latent Dirichlet allocation.","offset":4,"pro":0.36363636363636365,"labels":"MTD"},{"idx":10,"sentence":"The performance of the identified attributes is subsequently estimated by the aspect-based sentiment analysis of IBM Watson.","offset":5,"pro":0.45454545454545453,"labels":"MTD"},{"idx":10,"sentence":"Finally, the importance of the identified attributes is estimated by evaluating the effect of sentiments of each product attribute on the overall rating using an explainable deep neural network.","offset":6,"pro":0.5454545454545454,"labels":"MTD"},{"idx":10,"sentence":"A Shapley additive explanation-based method is proposed to estimate the importance values of product attributes with a low variance by combining the effect of the input features from multiple optimal neural networks with a high performance.","offset":7,"pro":0.6363636363636364,"labels":"MTD"},{"idx":10,"sentence":"A case study of smartphones is presented to demonstrate the proposed approach.","offset":8,"pro":0.7272727272727273,"labels":"MTD"},{"idx":10,"sentence":"The performance and importance estimates of the proposed approach are compared with those of previous sentiment analysis and neural network-based method, and the results exhibit that the former can perform IPA more reliably.","offset":9,"pro":0.8181818181818182,"labels":"CLN"},{"idx":10,"sentence":"The proposed approach uses minimal manual operation and can support companies to take decisions rapidly and effectively, compared with survey-based methods.","offset":10,"pro":0.9090909090909091,"labels":"CLN"},{"idx":11,"sentence":"The data-driven approach is emerging as a promising method for the topological design of multiscale structures with greater efficiency.","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"However, existing data-driven methods mostly focus on a single class of microstructures without considering multiple classes to accommodate spatially varying desired properties.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":11,"sentence":"The key challenge is the lack of an inherent ordering or distance'} measure between different classes of microstructures in meeting a range of properties.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":11,"sentence":"To overcome this hurdle, we extend the newly developed latent-variable Gaussian process (LVGP) models to create multi-response LVGP (MR-LVGP) models for the microstructure libraries of metamaterials, taking both qualitative microstructure concepts and quantitative microstructure design variables as mixed-variable inputs.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":11,"sentence":"The MR-LVGP model embeds the mixed variables into a continuous design space based on their collective effects on the responses, providing substantial insights into the interplay between different geometrical classes and material parameters of microstructures.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":11,"sentence":"With this model, we can easily obtain a continuous and differentiable transition between different microstructure concepts that can render gradient information for multiscale topology optimization.","offset":5,"pro":0.625,"labels":"RST"},{"idx":11,"sentence":"We demonstrate its benefits through multiscale topology optimization with aperiodic microstructures.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":11,"sentence":"Design examples reveal that considering multiclass microstructures can lead to improved performance due to the consistent load-transfer paths for micro- and macro-structures.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":12,"sentence":"To promote a new mode of production and a changed lifestyle in a relatively closed, underdeveloped community, critical interventions should be explored, and, as appropriate, implemented.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"We explore the possibility of second-season cultivation to improve the villagers' social-economic status in both the short term and the long term.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":12,"sentence":"Different scenarios are identified and explored so that decision support can be provided to social entrepreneurs (SEs). ","offset":2,"pro":0.3333333333333333,"labels":"BAC"},{"idx":12,"sentence":"Here, agent-based modeling (ABM) is used to simulate villagers' acceptance of second-season cultivation, growing two crops a year instead of one. ","offset":3,"pro":0.5,"labels":"BAC"},{"idx":12,"sentence":"The proposed method of capturing and making use of critical factors in influencing individuals' behavior in a community can be used in other projects. ","offset":4,"pro":0.6666666666666666,"labels":"IMP"},{"idx":12,"sentence":"Our focus in here is on the method, rather than the specific results. ","offset":5,"pro":0.8333333333333334,"labels":"PUR"},{"idx":13,"sentence":"Strategy dynamics are hypothesized to be a structural factor of interactive multi-actor design problems that influence collective performance and behaviors of design actors.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"In these tasks, design actor pairs work collectively to maximize their individual values while managing the trade-offs between aligning with or deviating from a mutually beneficial collective strategy.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":13,"sentence":"Results from a human subject design experiment indicate cognizant actors generally follow normative predictions for some strategy dynamics (harmony and coexistence) but not strictly for others (bistability and defection).","offset":2,"pro":0.3333333333333333,"labels":"CLN"},{"idx":13,"sentence":"Cumulative link model regression analysis shows that a greed factor contributing to strategy dynamics has a stronger effect on collective efficiency and equality of individual outcomes compared to a fear factor.","offset":3,"pro":0.5,"labels":"CLN"},{"idx":13,"sentence":"Results of this study provide an initial description of strategy dynamics in engineering design and help to frame future work to mitigate potential unfavorable effects of their underlying strategy dynamics through social constructs or mechanism design.","offset":4,"pro":0.6666666666666666,"labels":"CTN"},{"idx":13,"sentence":"Using a bi-level model of collective decision processes based on design optimization and strategy selection, we formulate a series of two-actor parameter design tasks that exhibit four strategy dynamics (harmony, coexistence, bistability, and defection) associated with low and high levels of structural fear and greed. ","offset":5,"pro":0.8333333333333334,"labels":"PUR"},{"idx":14,"sentence":"A new method for optimizing the layout of device-routing systems is presented.","offset":0,"pro":0,"labels":"PUR"},{"idx":14,"sentence":"In addition to geometric considerations, this method supports optimization based on system behavior by including physics-based objectives and constraints.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":14,"sentence":"Multiple physics domains are modeled using lumped parameter and finite element models.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":14,"sentence":"A geometric projection for devices of arbitrary polygonal shape is developed along with sensitivity analysis.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":14,"sentence":"Two thermal-fluid systems are optimized to demonstrate the use of this method.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":14,"sentence":"Gradient-based topology optimization techniques are used to simultaneously optimize both device locations and routing paths of device interconnects. ","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":15,"sentence":"Engineering design involves information acquisition decisions such as selecting designs in the design space for testing, selecting information sources, and deciding when to stop design exploration.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"Existing literature has established normative models for these decisions, but there is lack of knowledge about how human designers make these decisions and which strategies they use.","offset":1,"pro":0.1,"labels":"GAP"},{"idx":15,"sentence":"This knowledge is important for accurately modeling design decisions, identifying sources of inefficiencies, and improving the design process.","offset":2,"pro":0.2,"labels":"BAC"},{"idx":15,"sentence":"Therefore, the primary objective in this study is to identify models that provide the best description of a designer's information acquisition decisions when multiple information sources are present and the total budget is limited.","offset":3,"pro":0.3,"labels":"PUR"},{"idx":15,"sentence":"We conduct a controlled human subject experiment with two independent variables: the amount of fixed budget and the monetary incentive proportional to the saved budget.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":15,"sentence":"By using the experimental observations, we perform Bayesian model comparison on various simple heuristic models and expected utility (EU)-based models.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":15,"sentence":"As expected, the subjects' decisions are better represented by the heuristic models than the EU-based models.","offset":6,"pro":0.6,"labels":"CLN"},{"idx":15,"sentence":"While the EU-based models result in better net payoff, the heuristic models used by the subjects generate better design performance.","offset":7,"pro":0.7,"labels":"RST"},{"idx":15,"sentence":"The net payoff using heuristic models is closer to the EU-based models in experimental treatments where the budget is low and there is incentive for saving the budget.","offset":8,"pro":0.8,"labels":"RST"},{"idx":15,"sentence":"This indicates the potential for nudging designers' decisions toward maximizing the net payoff by setting the fixed budget at low values and providing monetary incentives proportional to saved budget.","offset":9,"pro":0.9,"labels":"CLN"},{"idx":16,"sentence":"Kinematic reliability is an essential index that assesses the performance of the mechanism associating with uncertainties.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"This study proposes a novel approach to kinematic reliability analysis for planar parallel manipulators based on error propagation on plane motion groups and clipped Gaussian in terms of joint clearance, input uncertainty, and manufacturing imperfection.","offset":1,"pro":0.125,"labels":"PUR"},{"idx":16,"sentence":"First, the linear relationship between the local pose distortion coming from the passive joint and that caused by other error sources, which are all represented by the exponential coordinate, are established by means of the Baker-Campbell-Hausdorff formula.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":16,"sentence":"Then, the second-order nonparametric formulas of error propagation on independent and dependent plane motion groups are derived in closed form for analytically determining the mean and covariance of the pose error distribution of the end-effector.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":16,"sentence":"On this basis, the kinematic reliability, i.e., the probability of the pose error within the specified safe region, is evaluated by a fast algorithm.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":16,"sentence":"Compared to the previous methods, the proposed approach has a significantly high precision for both cases with small and large errors under small and large safe bounds, which is also very efficient.","offset":5,"pro":0.625,"labels":"CLN"},{"idx":16,"sentence":"Finally, the effectiveness and advantages of the proposed approach are verified by comparing with the Monte Carlo simulation method.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":16,"sentence":"Additionally, it is available for arbitrarily distributed errors and can analyze the kinematic reliability only regarding either position or orientation as well. ","offset":7,"pro":0.875,"labels":"CLN"},{"idx":17,"sentence":"In this paper, a dynamic model is developed to study the influence of sliding friction on the dynamic characteristics of the planetary gear set by including the time-varying mesh stiffness (TVMS), sliding friction forces and torques.","offset":0,"pro":0,"labels":"PUR"},{"idx":17,"sentence":"An improved analytical model is proposed to calculate the TVMS with sliding friction.","offset":1,"pro":0.1,"labels":"MTD"},{"idx":17,"sentence":"The explicit analytical expressions of the sliding friction forces and torques are also derived.","offset":2,"pro":0.2,"labels":"MTD"},{"idx":17,"sentence":"Three kinds of different models are applied to investigate the influence of sliding friction: (1) the basic model: sliding friction is neglected in the dynamic model; (2) the improved model I: only the sliding friction forces and torques are considered in the dynamic model; and (3) the improved model II: both the influence of sliding friction on the TVMS and the sliding friction forces and torques are introduced into the dynamic model.","offset":3,"pro":0.3,"labels":"MTD"},{"idx":17,"sentence":"The planetary gear set with three equally spaced planet gears is applied to analyze the dynamic characteristics under sliding friction.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":17,"sentence":"The simulation results show that the dynamic characteristics can be enhanced or disturbed by sliding friction.","offset":5,"pro":0.5,"labels":"RST"},{"idx":17,"sentence":"In the end, the dynamic model is validated by the experiments.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":17,"sentence":"Therefore, the influence of sliding friction is non-negligible when investigating the dynamic characteristics of the planetary gear set.","offset":7,"pro":0.7,"labels":"CLN"},{"idx":17,"sentence":"The developed dynamic model provides a feasible dynamic research scheme for the planetary gear set with sliding friction.","offset":8,"pro":0.8,"labels":"CLN"},{"idx":17,"sentence":"Acting as an important internal excitation, sliding friction can cause the vibration and noise of the planetary gear set. ","offset":9,"pro":0.9,"labels":"GAP"},{"idx":18,"sentence":"This paper proposes an inverse structural modification method for the assignment of anti-resonances in undamped vibrating systems by modifying the inertial and elastic properties of the existing degrees offreedom of the original system.","offset":0,"pro":0,"labels":"PUR"},{"idx":18,"sentence":"Hence, no additional degrees of freedom are added to the system.","offset":1,"pro":0.14285714285714285,"labels":"MTD"},{"idx":18,"sentence":"The problem is formulated as an eigenstructure assignment approach since such a novel formulation is suitable for complex systems, such as those modeled through finite elements.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":18,"sentence":"Assignment is cast as a constrained non -convex non-linear minimization problem and the proposed solving strategy is based on the homotopy optimization approach.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":18,"sentence":"The method effectiveness is shown through three meaningful test cases.","offset":4,"pro":0.5714285714285714,"labels":"CLN"},{"idx":18,"sentence":"Indeed, these systems are difficult to handle with the methods already proposed in the literature. ","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":18,"sentence":"Additionally, the proposed approach is suitable for both point and cross receptances. ","offset":6,"pro":0.8571428571428571,"labels":"MTD"},{"idx":19,"sentence":"Using the topology optimization can be an effective means of synthesizing planar rigid-body linkage mechanisms to generate desired motion, as it does not require a baseline mechanism for a specific topology.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"While most earlier studies were mainly concerned with the formulation and implementation of topology optimization -based synthesis in a fixed grid, this study aims to realize the simultaneous shape and topology optimization of planar linkage mechanisms using a low -resolution spring -connected rigid block model.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":19,"sentence":"Here, we demonstrate the effectiveness of simultaneous optimization over a higher-resolution fixed-grid rigid block-based topology optimization process.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":19,"sentence":"When shape optimization to change the block shapes is combined with topology optimization to synthesize the mechanism, the use of low -resolution discretized models improves the computation efficiency considerably and helps to yield compact mechanisms with less complexity, making them more amenable to fabrication.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":19,"sentence":"After verifying the effectiveness of the simultaneous shape and topology optimization process with several benchmark problems, we apply the method to synthesize a mechanism which guides a planar version of a human's gait trajectory.","offset":4,"pro":0.8,"labels":"MTD"},{"idx":0,"sentence":"While psychological safety is a consistent, generalizable, and multilevel predictor of outcomes in team performance across fields that can positively impact the creative process, there have been limited investigations of psychological safety in the engineering domain.","offset":0,"pro":0,"labels":"GAP"},{"idx":0,"sentence":"Without this knowledge, we do not know whether fostering psychological safety in a team environment is important for specific engineering design outputs from concept generation and screening practices.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":0,"sentence":"This study provides one of the first attempts at addressing this research gap through an empirical study with 69 engineering design student teams over the course of 4- and 8-week design projects.","offset":2,"pro":0.2222222222222222,"labels":"PUR"},{"idx":0,"sentence":"Specifically, we sought to identify the role of psychological safety on the number and quality (judged by goodness) of ideas generated.","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":0,"sentence":"In addition, we explored the role of psychological safety on ownership bias and goodness in the concept screening process.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":0,"sentence":"The results of the study identified that while psychological safety was negatively related to the number of ideas a team developed, it was positively related to the quality (goodness) of the ideas developed.","offset":5,"pro":0.5555555555555556,"labels":"RST"},{"idx":0,"sentence":"This result indicates that while psychological safety may not increase team productivity in terms of the number of ideas produced, it may impact team effectiveness in coming up with viable candidate ideas to move forward in the design process.","offset":6,"pro":0.6666666666666666,"labels":"CLN"},{"idx":0,"sentence":"In addition, there was no relationship between psychological safety and ownership bias during concept screening.","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":0,"sentence":"These findings provide quantitative evidence on the role of psychological safety on engineering team idea production and identify areas for further study.","offset":8,"pro":0.8888888888888888,"labels":"CTN"},{"idx":1,"sentence":"A new three-dimensional structural optimization is presented based on the level set method to obtain favorable designs for wire-fed metal additive manufacturing with uniform wall thickness.","offset":0,"pro":0,"labels":"MTD"},{"idx":1,"sentence":"By exploiting the signed distance nature of a level set function, a structure under design is always defined as a thin domain with uniform thickness without employing any constrains or penalty functionals.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":1,"sentence":"The boundary surfaces of a thin-walled domain are defined as the surfaces with level set values of +/- t/2(t: wall thickness).","offset":2,"pro":0.25,"labels":"MTD"},{"idx":1,"sentence":"Design velocity can be represented in terms of curvatures of the zero-level-set surface, extended to level set grids in the narrow band.","offset":3,"pro":0.375,"labels":"BAC"},{"idx":1,"sentence":"Therefore, the calculation of accurate curvatures on the zero-level set is crucial for correct design sensitivities.","offset":4,"pro":0.5,"labels":"BAC"},{"idx":1,"sentence":"In this investigation, mean and Gaussian curvatures at a point on the triangle mesh of the discretized zero-level set are calculated by spatial averages over the Voronoi cell of the point, by which the sensitivity of a material volume can be calculated with optimal accuracy.","offset":5,"pro":0.625,"labels":"PUR"},{"idx":1,"sentence":"To address the high computational cost by a dense regular mesh for representing thin walls, degrees of freedom in void regions is mostly removed.","offset":6,"pro":0.75,"labels":"PUR"},{"idx":1,"sentence":"Design examples of beams and a T-joint structure with uniform thickness are presented to verify the effectiveness of the proposed method.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":2,"sentence":"This paper presents VISION (Visual Interaction tool for Seeking Inspiration based on Nonnegative Matrix Factorization), a computational design-by-analogy (DbA) tool that enables designers to visually explore a space of analogical inspiration for creative idea generation.","offset":0,"pro":0,"labels":"PUR"},{"idx":2,"sentence":"While many currently available DbA tools use a query-based approach for retrieving analogies (i.e., input keywords or functions to return a set of relevant results), VISION allows designers to retrieve a collection of design analogies that are related to topics of interest and explore a space of potential inspiration, the way one would gather books of particular topics from multiple shelves at the library to find potential resources.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":2,"sentence":"Two cognitive engineering design studies were conducted to evaluate the efficacy of VISION during the conceptual design process.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":2,"sentence":"In the first study, conducted in a controlled-lab setting, VISION was evaluated based on its effect on the quantity, quality, novelty, and direct physical similarity ratings of design outcomes.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":2,"sentence":"In the second study, conducted in a graduate engineering design class, VISION was evaluated based on designers' abilities to retrieve analogies from different domains and analogies that are different from already existing design solutions.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":2,"sentence":"Studies show that VISION could provide an alternative to the query-based search that many DbA computational support systems use and open up new opportunities for designers to benefit from computationally supported analogies.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":3,"sentence":"It is generally understood that intractable computational intensity stemming from repeatedly calling performance function when evaluating the contribution of joint focal elements hinders the application of evidence theory in practical engineering.","offset":0,"pro":0,"labels":"GAP"},{"idx":3,"sentence":"In order to promote the practicability of evidence theory for the reliability evaluation of engineering structures, an efficient reliability analysis method based on the active learning Kriging model is proposed in this study.","offset":1,"pro":0.1111111111111111,"labels":"PUR"},{"idx":3,"sentence":"To start with, a basic variable is selected according to basic probability assignment (BPA) of evidence variables to divide the evidence space into sub-evidence spaces.","offset":2,"pro":0.2222222222222222,"labels":"MTD"},{"idx":3,"sentence":"Intersection points between the performance function and the sub-evidence spaces are then determined by solving the univariate root-finding problem.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":3,"sentence":"Sample points are randomly identified to enhance the accuracy of the subsequently established surrogate model.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":3,"sentence":"Initial Kriging model with high approximation accuracy is subsequently established through these intersection points and additional sample points generated by Latin hypercube sampling.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":3,"sentence":"An active learning function is employed to sequentially refine the Kriging model with minimal sample points.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":3,"sentence":"As a result, belief (Bel) measure and plausibility (Pl) measure are derived efficiently via the surrogate model in the evidence-theory-based reliability analysis.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":3,"sentence":"The currently proposed analysis method is exemplified with three numerical examples to demonstrate the efficiency and is applied to reliability analysis of positioning accuracy for an industrial robot.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":4,"sentence":"Bayesian optimization (BO) is a low-cost global optimization tool for expensive black-box objective functions, where we learn from prior evaluated designs, update a posterior surrogate Gaussian process model, and select new designs for future evaluation using an acquisition function.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"This research focuses upon developing a BO model with multiple black-box objective functions.","offset":1,"pro":0.1111111111111111,"labels":"PUR"},{"idx":4,"sentence":"In the standard multi-objective (MO) optimization problem, the weighted Tchebycheff method is efficiently used to find both convex and non-convex Pareto frontiers.","offset":2,"pro":0.2222222222222222,"labels":"BAC"},{"idx":4,"sentence":"This approach requires knowledge of utopia values before we start optimization.","offset":3,"pro":0.3333333333333333,"labels":"BAC"},{"idx":4,"sentence":"However, in the BO framework, since the functions are expensive to evaluate, it is very expensive to obtain the utopia values as a prior knowledge.","offset":4,"pro":0.4444444444444444,"labels":"GAP"},{"idx":4,"sentence":"Therefore, in this paper, we develop a MO-BO framework where we calibrate with multiple linear regression (MLR) models to estimate the utopia value for each objective as a function of design input variables; the models are updated iteratively with sampled training data from the proposed MO-BO.","offset":5,"pro":0.5555555555555556,"labels":"PUR"},{"idx":4,"sentence":"These iteratively estimated mean utopia values are used to formulate the weighted Tchebycheff MO acquisition function.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":4,"sentence":"The proposed approach is implemented in two numerical test examples and one engineering design problem of optimizing thin tube geometries under constant loading of temperature and pressure, with minimizing the risk of creep-fatigue failure and design cost, along with risk-based and manufacturing constraints.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":4,"sentence":"Finally, the model accuracy with frequentist, Bayesian and without MLR-based calibration are compared to true Pareto solutions.","offset":8,"pro":0.8888888888888888,"labels":"MTD"},{"idx":5,"sentence":"Flexure oscillators are promising time bases, thanks to their high quality factor and monolithic design compatible with microfabrication.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"In mechanical watchmaking, they could advantageously replace the traditional balance and hairspring oscillator, leading to improvements in timekeeping accuracy, autonomy, and assembly.","offset":1,"pro":0.09090909090909091,"labels":"BAC"},{"idx":5,"sentence":"As MEMS oscillators, their performance can rival that of the well-established quartz oscillator.","offset":2,"pro":0.18181818181818182,"labels":"BAC"},{"idx":5,"sentence":"However, their inherent nonlinear elastic behavior can introduce a variation of their frequency with amplitude called isochronism defect, a major obstacle to accurate timekeeping in mechanical watches.","offset":3,"pro":0.2727272727272727,"labels":"GAP"},{"idx":5,"sentence":"Previous research has focused on addressing this issue by controlling the elastic properties of flexure oscillators.","offset":4,"pro":0.36363636363636365,"labels":"BAC"},{"idx":5,"sentence":"Yet, these oscillators exhibit other amplitude-related frequency variations caused by changes of inertia with amplitude.","offset":5,"pro":0.45454545454545453,"labels":"GAP"},{"idx":5,"sentence":"In this article, we not only improve existing models by taking into account inertia effects but also present a new way of using them to adjust the isochronism defect.","offset":6,"pro":0.5454545454545454,"labels":"PUR"},{"idx":5,"sentence":"This results in a better understanding of flexure oscillators and an alternative way of tuning isochronism by acting on inertia instead of stiffness.","offset":7,"pro":0.6363636363636364,"labels":"CTN"},{"idx":5,"sentence":"This also opens the door to promising architectures such as the new rotation-dilation coupled oscillator (RDCO) whose symmetry has the advantage of minimizing the influence of linear accelerations on its frequency (the other major limitation of flexure oscillators).","offset":8,"pro":0.7272727272727273,"labels":"CTN"},{"idx":5,"sentence":"We derive analytical models for the isochronism of this oscillator, show a dimensioning with compensating inertia and stiffness variations, and present a practical method for post-fabrication isochronism tuning by displacing masses.","offset":9,"pro":0.8181818181818182,"labels":"MTD"},{"idx":5,"sentence":"The models are validated by finite element method (FEM) and mockups serve as preliminary proof-of-concept.","offset":10,"pro":0.9090909090909091,"labels":"MTD"},{"idx":6,"sentence":"Increasingly strict emission and fuel economy standards stimulate the researches on hybrid electric vehicle techniques in the automobile industry and one of the most important techniques is the design of powertrain configurations.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"In this paper, a theoretical design methodology for hybrid electric vehicle powertrain configurations is proposed to find the configurations with excellent performance in a large pool of configurations.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":6,"sentence":"There are two main parts in a powertrain configuration, power/coupling devices (engine, electric machine, wheel, and planetary gear set) and mechanical connections between these devices.","offset":2,"pro":0.2857142857142857,"labels":"BAC"},{"idx":6,"sentence":"Different connections will lead to configurations having different performances.","offset":3,"pro":0.42857142857142855,"labels":"GAP"},{"idx":6,"sentence":"This paper divides all connections in configurations into three categories and a novel matrix representation method is developed to express these kinds of connections to reflect system dynamics and physical structure of configurations.","offset":4,"pro":0.5714285714285714,"labels":"PUR"},{"idx":6,"sentence":"With the support of the matrix representation method, configuration selections from large pools can automatically be completed by computer and manual calculation and comparison can be avoided, which saves much energy and time.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":6,"sentence":"Finally, the proposed method is vigorously verified by simulations.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":7,"sentence":"Precise time series prediction serves as an important role in constructing a digital twin (DT).","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"The various internal and external interferences result in highly nonlinear and stochastic time series.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":7,"sentence":"Although artificial neural networks (ANNs) are often used to forecast time series because of their strong self-learning and nonlinear fitting capabilities, it is a challenging and time-consuming task to obtain the optimal ANN architecture.","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":7,"sentence":"This paper proposes a hybrid time series prediction model based on an ensemble empirical mode decomposition (EEMD), long short-term memory (LSTM) neural networks, and Bayesian optimization (BO).","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":7,"sentence":"To improve the predictability of stochastic and nonstationary time series, the EEMD method is implemented to decompose the original time series into several components (each component is a single-frequency and stationary signal) and a residual signal.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":7,"sentence":"The decomposed signals are used to train the neural networks, in which the hyperparameters are fine-tuned by the BO algorithm.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":7,"sentence":"The following time series data are predicted by summating all the predictions of the decomposed signals based on the trained neural networks.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":7,"sentence":"To evaluate the performance of the proposed EEMD-BO-LSTM neural networks, this paper conducts two case studies (the wind speed prediction and the wave height prediction) and implements a comprehensive comparison between the proposed method and other approaches including the persistence model, autoregressive integrated moving average (ARIMA) model, LSTM neural networks, BO-LSTM neural networks, and EEMD-LSTM neural networks.","offset":7,"pro":0.7777777777777778,"labels":"PUR"},{"idx":7,"sentence":"The results show an improved prediction accuracy using the proposed method by multiple accuracy metrics.","offset":8,"pro":0.8888888888888888,"labels":"RST"},{"idx":8,"sentence":"Designers' choices of methods are well known to shape project outcomes.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"However, questions remain about why design teams select particular methods and how teams' decision-making strategies are influenced by project- and process-based factors.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":8,"sentence":"In this mixed-methods study, we analyze novice design teams' decision-making strategies underlying 297 selections of human-centered design methods over the course of three semester-long project-based engineering design courses.","offset":2,"pro":0.2222222222222222,"labels":"PUR"},{"idx":8,"sentence":"We propose a framework grounded in 100+ factors sourced from new product development literature that classifies design teams' method selection strategy as either Agent- (A), Outcome- (O), or Process- (P) driven, with eight further subclassifications.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":8,"sentence":"Coding method selections with this framework, we uncover three insights about design team method selection.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":8,"sentence":"First, we identify fewer outcomes-based selection strategies across all phases and innovation types.","offset":5,"pro":0.5555555555555556,"labels":"RST"},{"idx":8,"sentence":"Second, we observe a shift in decision-making strategy from user-focused outcomes in earlier phases to product-based outcomes in later phases.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":8,"sentence":"Third, we observe that decision-making strategy produces a greater heterogeneity of method selections as compared to the class average as a whole or project type alone.","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":8,"sentence":"These findings provide a deeper understanding of designers' method selection behavior and have implications for effective management of design teams, development of automated design support tools to aid design teams, and curation of design method repositories.","offset":8,"pro":0.8888888888888888,"labels":"IMP"},{"idx":9,"sentence":"Design activity can be supported using inspirational stimuli (e.g., analogies, patents) by helping designers overcome impasses or in generating solutions with more positive characteristics during ideation.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"Design researchers typically generate inspirational stimuli a priori in order to investigate their impact.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":9,"sentence":"However, for a chosen stimulus to possess maximal utility, it should automatically reflect the current and ongoing progress of the designer.","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":9,"sentence":"In this work, designers receive computationally selected inspirational stimuli midway through an ideation session in response to the contents of their current solution.","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":9,"sentence":"Sourced from a broad database of related example solutions, the semantic similarity between the content of the current design and concepts within the database determines which potential stimulus is received.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":9,"sentence":"Designers receive a particular stimulus based on three experimental conditions: a semantically near stimulus, a semantically far stimulus, or no stimulus (control).","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":9,"sentence":"Results indicate that adaptive inspirational stimuli can be determined using latent semantic analysis (LSA) and that semantic similarity measures are a promising approach for real-time monitoring of the design process.","offset":6,"pro":0.6666666666666666,"labels":"CLN"},{"idx":9,"sentence":"The ability to achieve differentiable near versus far stimuli was validated using both semantic cosine similarity values and participant self-response ratings.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":9,"sentence":"As a further contribution, this work also explores the impact of different types of adaptive inspirational stimuli on design outcomes using a newly introduced design innovation.","offset":8,"pro":0.8888888888888888,"labels":"PUR"},{"idx":10,"sentence":"Nowadays, increasing awareness of environmental protection has evoked the adoption of green technologies in design and manufacturing.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"As a revolutionizing manufacturing technology that produces components in a layer-by-layer fashion, additive manufacturing (AM) has followed this trend.","offset":1,"pro":0.07692307692307693,"labels":"BAC"},{"idx":10,"sentence":"Among a variety of AM processes, fused filament fabrication (FFF) is one of the most commonly used technologies.","offset":2,"pro":0.15384615384615385,"labels":"BAC"},{"idx":10,"sentence":"However, AM (including FFF) is inherently energy expensive and energy inefficient compared with the conventional manufacturing.","offset":3,"pro":0.23076923076923078,"labels":"GAP"},{"idx":10,"sentence":"Thus, an urgent investigation is needed to reduce the energy consumption for AM production.","offset":4,"pro":0.3076923076923077,"labels":"BAC"},{"idx":10,"sentence":"On the other hand, part geometric accuracy is an important aspect for the quality of additively manufactured components.","offset":5,"pro":0.38461538461538464,"labels":"BAC"},{"idx":10,"sentence":"It is not meaningful to improve AM's energy consumption performance with compromised part geometric accuracy.","offset":6,"pro":0.46153846153846156,"labels":"GAP"},{"idx":10,"sentence":"Therefore, it is necessary to jointly consider energy consumption as well as part geometric accuracy in the AM process design.","offset":7,"pro":0.5384615384615384,"labels":"GAP"},{"idx":10,"sentence":"This study applies the statistical regression approach to model AM energy consumption and part geometric accuracy.","offset":8,"pro":0.6153846153846154,"labels":"PUR"},{"idx":10,"sentence":"The nondominated sorting genetic algorithm II (NSGA-II) and the technique for order of preference by similarity to ideal solution (TOPSIS) method together are used to locate the compromised optimal solution for AM process parameter settings.","offset":9,"pro":0.6923076923076923,"labels":"MTD"},{"idx":10,"sentence":"The effectiveness of the proposed approach is demonstrated through a case study developed with the FFF process and a specific part design.","offset":10,"pro":0.7692307692307693,"labels":"MTD"},{"idx":10,"sentence":"The results of this study are significant to both AM energy consumption and part geometric accuracy in terms of qualitative and quantitative analyses.","offset":11,"pro":0.8461538461538461,"labels":"CLN"},{"idx":10,"sentence":"Furthermore, the study can potentially guide the future AM sustainability model development and be extended to future AM process improvement.","offset":12,"pro":0.9230769230769231,"labels":"IMP"},{"idx":11,"sentence":"We address a central issue that arises within element-based topology optimization.","offset":0,"pro":0,"labels":"PUR"},{"idx":11,"sentence":"To achieve a sufficiently well-defined material interface, one requires a highly refined finite element mesh; however, this leads to an increased computational cost due to the solution of the finite element analysis problem.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":11,"sentence":"By generating an optimal structure on a coarse mesh and using an artificial neural network to map this coarse solution to a refined mesh, we can greatly reduce computational time.","offset":2,"pro":0.2857142857142857,"labels":"RST"},{"idx":11,"sentence":"This approach resulted in time savings of up to 85\\% for test cases considered.","offset":3,"pro":0.42857142857142855,"labels":"RST"},{"idx":11,"sentence":"This significant advantage in computational time also preserves the structural integrity when compared with a fine -mesh optimization with limited error.","offset":4,"pro":0.5714285714285714,"labels":"CLN"},{"idx":11,"sentence":"Along with the savings in computational time, the boundary edges become more refined during the process, allowing for a sharp transition from solid to void.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":11,"sentence":"This improved boundary edge can be leveraged to improve the manufacturability of the optimized designs.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":12,"sentence":"The field of aerial robotics has advanced rapidly, but the design knowledge has not yet been codified into reusable design guidelines.","offset":0,"pro":0,"labels":"GAP"},{"idx":12,"sentence":"Design guidelines have been developed for many mechanical design areas to advance the field itself and help novice designers benefit from past expert knowledge more easily.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":12,"sentence":"We used an inductive approach and collected 90 aerial robot examples by reviewing recent work in aerial robotics and studying the key motivations, features, functionalities, and potential design contradictions.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":12,"sentence":"Then, design guidelines are derived by identifying patterns and grouping them by the problem they solve and the innovation made to solve it iteratively.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":12,"sentence":"From this, we find 35 unique design examples that can be grouped into either 14 design guidelines for more sensing, battery, mission, or actuation efficiency; or to improve the desired functionality in an aerial robot such as reducing complexity or improving how the robot can interact with objects or its environment.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":12,"sentence":"The derived guidelines are validated for thematic saturation using convergence analysis and its utility through a qualitative design study involving novices and experienced designers working on two design problems.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":12,"sentence":"The design guidelines presented in this research can support the design of future innovative aerial robots.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":13,"sentence":"This paper proposes a detailed methodology for constructing an additive manufacturing (AM) digital twin for the laser powder bed fusion (LPBF) process.","offset":0,"pro":0,"labels":"PUR"},{"idx":13,"sentence":"An important aspect of the proposed digital twin is the incorporation of model uncertainty and process variability.","offset":1,"pro":0.09090909090909091,"labels":"BAC"},{"idx":13,"sentence":"A virtual representation of the LPBF process is first constructed using a physics-based model.","offset":2,"pro":0.18181818181818182,"labels":"MTD"},{"idx":13,"sentence":"To enable faster computation required in uncertainty analysis and decision-making, the physics-based model is replaced by a cheaper surrogate model.","offset":3,"pro":0.2727272727272727,"labels":"MTD"},{"idx":13,"sentence":"A two-step surrogate model is proposed when the quantity of interest is not directly observable during manufacturing.","offset":4,"pro":0.36363636363636365,"labels":"MTD"},{"idx":13,"sentence":"The data collected from the monitoring sensors are used for diagnosis (of current part quality) and passed on to the virtual representation for model updating.","offset":5,"pro":0.45454545454545453,"labels":"MTD"},{"idx":13,"sentence":"The model updating consists of Bayesian calibration of the uncertain parameters and the discrepancy term representing the model prediction error.","offset":6,"pro":0.5454545454545454,"labels":"MTD"},{"idx":13,"sentence":"The resulting digital twin is thus tailored for the particular individual part being produced and is used for probabilistic process parameter optimization (initial, before starting the printing) and online, real-time adjustment of the LPBF process parameters, in order to control the porosity in the manufactured part.","offset":7,"pro":0.6363636363636364,"labels":"MTD"},{"idx":13,"sentence":"A robust design optimization formulation is used to minimize the mean and standard deviation of the difference between the target porosity and the predicted porosity.","offset":8,"pro":0.7272727272727273,"labels":"MTD"},{"idx":13,"sentence":"The proposed methodology includes validation of the digital twin in two stages.","offset":9,"pro":0.8181818181818182,"labels":"MTD"},{"idx":13,"sentence":"Validation of the initial model in the digital twin is performed using available data, whereas data collected during manufacturing are used to validate the overall digital twin.","offset":10,"pro":0.9090909090909091,"labels":"MTD"},{"idx":14,"sentence":"Reliability analysis is a core element in engineering design and can be performed with physical models (limit-state functions).","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"Reliability analysis becomes computationally expensive when the dimensionality of input random variables is high.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":14,"sentence":"This work develops a high-dimensional reliability analysis method through a new dimension reduction strategy so that the contributions of unimportant input variables are also accommodated after dimension reduction.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":14,"sentence":"Dimension reduction is performed with the first iteration of the first-order reliability method (FORM), which identifies important and unimportant input variables.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":14,"sentence":"Then a higher order reliability analysis is performed in the reduced space of only important input variables.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":14,"sentence":"The reliability obtained in the reduced space is then integrated with the contributions of unimportant input variables, resulting in the final reliability prediction that accounts for both types of input variables.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":14,"sentence":"Consequently, the new reliability method is more accurate than the traditional method which fixes unimportant input variables at their means.","offset":6,"pro":0.75,"labels":"CLN"},{"idx":14,"sentence":"The accuracy is demonstrated by three examples.","offset":7,"pro":0.875,"labels":"RST"},{"idx":15,"sentence":"Industry 4.0, as the fourth industrial revolution, represents significant challenges and numerous innovation opportunities for future product realization.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"A critical area of Industry 4.0 is the advancement of new design theories, design methods, and design principles to drive and enable the revolution with designers, engineers, teams, and organizations.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":15,"sentence":"This paper focuses on the advancement of a design theory and design principles for a growing manufacturing capability for Industry 4.0: additive manufacturing (AM).","offset":2,"pro":0.25,"labels":"PUR"},{"idx":15,"sentence":"With high degrees of freedom, the field and use of AM requires design guidance and highly practical knowledge for supporting ideation processes, enabling understanding of capabilities, and creating a basis to innovative with the technology.","offset":3,"pro":0.375,"labels":"BAC"},{"idx":15,"sentence":"Some design principles for AM exist in the literature; however, designers seek more fundamental and practical design guidelines for successfully creating and building their customized design artefacts, especially as Industry 4.0 moves forward.","offset":4,"pro":0.5,"labels":"GAP"},{"idx":15,"sentence":"In this study, a crowdsourced repository for additively manufacturable components is used as the source of design data, within an empirical study, to extract practical design principles for AM.","offset":5,"pro":0.625,"labels":"PUR"},{"idx":15,"sentence":"A total of 23 crowdsourced design principles for AM are extracted and clustered according to level specificity: (i) design for manufacturing, (ii) design for digital manufacturing, (iii) design for AM, and (iv) design for fused deposition modeling.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":15,"sentence":"These 23 AM design principles, as a foundation for AM design and Industry 4.0, are provided in a common framework; expressed for ready use by designers, developers, and researchers; and illustrated through some contemporary designs.","offset":7,"pro":0.875,"labels":"MTD"},{"idx":16,"sentence":"Cyber-physical-social systems (CPSS) with highly integrated functions of sensing, actuation, computation, and communication are becoming the mainstream consumer and commercial products.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"The performance of CPSS heavily relies on the information sharing between devices.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":16,"sentence":"Given the extensive data collection and sharing, security and privacy are of major concerns.","offset":2,"pro":0.2,"labels":"BAC"},{"idx":16,"sentence":"Thus, one major challenge of designing those CPSS is how to incorporate the perception of trust in product and systems design.","offset":3,"pro":0.3,"labels":"BAC"},{"idx":16,"sentence":"Recently, a trust quantification method was proposed to measure the trustworthiness of CPSS by quantitative metrics of ability, benevolence, and integrity.","offset":4,"pro":0.4,"labels":"BAC"},{"idx":16,"sentence":"The CPSS network architecture can be optimized by choosing a subnet such that the trust metrics are maximized.","offset":5,"pro":0.5,"labels":"BAC"},{"idx":16,"sentence":"The combinatorial network optimization problem, however, is computationally challenging.","offset":6,"pro":0.6,"labels":"GAP"},{"idx":16,"sentence":"Most of the available global optimization algorithms for solving such problems are heuristic methods.","offset":7,"pro":0.7,"labels":"BAC"},{"idx":16,"sentence":"In this paper, a surrogate-based discrete Bayesian optimization method is developed to perform network design, where the most trustworthy CPSS network with respect to a reference node is formed to collaborate and share information with.","offset":8,"pro":0.8,"labels":"PUR"},{"idx":16,"sentence":"The applications of ability and benevolence metrics in design optimization of CPSS architecture are demonstrated.","offset":9,"pro":0.9,"labels":"MTD"},{"idx":17,"sentence":"Parametric optimization solves optimization problems as a function of uncontrollable or unknown parameters.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"Such an approach allows an engineer to gather more information than traditional optimization procedures during design.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":17,"sentence":"Existing methods for parametric optimization of computationally or monetarily expensive functions can be too time-consuming or impractical to solve.","offset":2,"pro":0.2,"labels":"GAP"},{"idx":17,"sentence":"Therefore, new methods for the parametric optimization of expensive functions need to be explored.","offset":3,"pro":0.3,"labels":"GAP"},{"idx":17,"sentence":"This work proposes a novel algorithm that leverages the advantages of two existing optimization algorithms.","offset":4,"pro":0.4,"labels":"PUR"},{"idx":17,"sentence":"This new algorithm is called the efficient parametric optimization (EPO) algorithm.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":17,"sentence":"EPO enables adaptive sampling of a high-fidelity design space using an inexpensive low-fidelity response surface model.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":17,"sentence":"Such an approach largely reduces the required number of expensive high-fidelity computations.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":17,"sentence":"The proposed method is benchmarked using analytic test problems and used to evaluate a case study requiring finite element analysis.","offset":8,"pro":0.8,"labels":"MTD"},{"idx":17,"sentence":"Results show that EPO performs as well as or better than the existing alternative, Predictive Parameterized Pareto Genetic Algorithm (P3GA), for these problems given an allowable number of function evaluations.","offset":9,"pro":0.9,"labels":"RST"},{"idx":18,"sentence":"Topology optimization has been proved to be an efficient tool for structural design.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"In recent years, the focus of structural topology optimization has been shifting from single material continuum structures to multimaterial and multiscale structures.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":18,"sentence":"This paper aims at devising a numerical scheme for designing bionic structures by combining a two-stage parametric level set topology optimization with the conformal mapping method.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":18,"sentence":"At the first stage, the macro-structural topology and the effective material properties are optimized simultaneously.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":18,"sentence":"At the second stage, another structural topology optimization is carried out to identify the exact layout of the metamaterial at the mesoscale.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":18,"sentence":"The achieved structure and metamaterial designs are further synthesized to form a multiscale structure using conformal mapping, which mimics the bionic structures with orderly chaos.","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":19,"sentence":"A large amount of energy from power plants, vehicles, oil refining, and steel or glass making process is released to the atmosphere as waste heat.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"The thermoelectric generator (TEG) provides a way to reutilize this portion of energy by converting temperature differences into electricity using Seebeck phenomenon.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":19,"sentence":"Because the figures of merit zT of the thermoelectric materials are temperature-dependent, it is not feasible to achieve high efficiency of the thermoelectric conversion using only one single thermoelectric material in a wide temperature range.","offset":2,"pro":0.2,"labels":"GAP"},{"idx":19,"sentence":"To address this challenge, the authors propose a method based on topology optimization to optimize the layouts of functional graded TEGs consisting of multiple materials.","offset":3,"pro":0.3,"labels":"PUR"},{"idx":19,"sentence":"The multimaterial TEG is optimized using the solid isotropic material with penalization (SIMP) method.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":19,"sentence":"Instead of dummy materials, both the P-type and N-type electric conductors are optimally distributed with two different practical thermoelectric materials.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":19,"sentence":"Specifically, Bi2Te3 and Zn4Sb3 are selected for the P-type element while Bi2Te3 and CoSb3 are employed for the N-type element.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":19,"sentence":"Two optimization scenarios with relatively regular domains are first considered with one optimizing on both the P-type and N-type elements simultaneously, and the other one only on single P-type element.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":19,"sentence":"The maximum conversion efficiency could reach 9.61\\% and 12.34\\% respectively in the temperature range from 25 degrees C to 400 degrees C. CAD models are reconstructed based on the optimization results for numerical verification.","offset":8,"pro":0.8,"labels":"RST"},{"idx":19,"sentence":"A good agreement between the performance of the CAD model and optimization result is achieved, which demonstrates the effectiveness of the proposed method.","offset":9,"pro":0.9,"labels":"CLN"},{"idx":0,"sentence":"Surrogate models can be used to approximate complex systems at a reduced cost and are widely used when data generation is expensive or time consuming.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"The accuracy of these models is dependent on the samples used to create them.","offset":1,"pro":0.08333333333333333,"labels":"BAC"},{"idx":0,"sentence":"Therefore, proper sample selection within the parameter space is paramount.","offset":2,"pro":0.16666666666666666,"labels":"BAC"},{"idx":0,"sentence":"Numerous design of experiments (DOE) methodologies have been developed with the aim of identifying the optimal sample set to capture the system of interest.","offset":3,"pro":0.25,"labels":"BAC"},{"idx":0,"sentence":"Adaptive sampling techniques are a subclass of DOE methods that identify optimal locations for new samples by leveraging response information from existing samples.","offset":4,"pro":0.3333333333333333,"labels":"BAC"},{"idx":0,"sentence":"By exploiting knowledge of the system, adaptive sampling methods have been demonstrated to significantly reduce the number of samples required to build a surrogate model of a given accuracy.","offset":5,"pro":0.4166666666666667,"labels":"BAC"},{"idx":0,"sentence":"However, utilizing the response information of the previous samples adds a computational cost associated with determining the ideal sample locations.","offset":6,"pro":0.5,"labels":"GAP"},{"idx":0,"sentence":"Additionally, this cost typically grows with the sample count.","offset":7,"pro":0.5833333333333334,"labels":"GAP"},{"idx":0,"sentence":"This article presents techniques to reduce the cost associated with the adaptive sampling procedure so that the cost savings provided by adaptive sampling are maximized.","offset":8,"pro":0.6666666666666666,"labels":"PUR"},{"idx":0,"sentence":"A new K-fold cross-validation (KFCV)-Voronoi adaptive sampling technique is proposed to reduce the sample selection costs by adding a global KFCV filter to the cross-validation (CV)-Voronoi technique.","offset":9,"pro":0.75,"labels":"MTD"},{"idx":0,"sentence":"The costs are further reduced through an innovative Voronoi batch sampling technique that is demonstrated to outperform naive batch sampling.","offset":10,"pro":0.8333333333333334,"labels":"MTD"},{"idx":0,"sentence":"The proposed adaptive sampling acceleration techniques are evaluated using benchmark functions of increasing dimension and aerodynamic loading data.","offset":11,"pro":0.9166666666666666,"labels":"MTD"},{"idx":1,"sentence":"This paper describes the design, functional testing, and user feedback for a tractor specialized for small farms in low-resource settings, particularly India.","offset":0,"pro":0,"labels":"PUR"},{"idx":1,"sentence":"The presented tractor is unique in its ability to compete with draft animals' physical dimensions, pulling performance, and sale price, while retaining key tractor advantages such as compatibility with modern tools, low maintenance costs, and reduced drudgery.","offset":1,"pro":0.1111111111111111,"labels":"MTD"},{"idx":1,"sentence":"This tractor features motorcycle-like controls and seating, inline drive wheels, stabilization via an outrigger arm or a specially developed, novel balance board attachment, and the ability to attach implements ahead or behind the rear axle.","offset":2,"pro":0.2222222222222222,"labels":"MTD"},{"idx":1,"sentence":"The design was created to address unmet farmer requirements identified during interviews with Indian farming stakeholders.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":1,"sentence":"A prototype of the tractor demonstrated the completion of key farming operations in a Massachusetts farm where expert user feedback was obtained.","offset":4,"pro":0.4444444444444444,"labels":"RST"},{"idx":1,"sentence":"In-person interviews on the tractor's usefulness were then conducted with 24 small and marginal Indian farmers in Karnataka, Gujarat, and Tamil Nadu.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":1,"sentence":"The tractor was described to the farmers with help of pictures, videos, and local experts.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":1,"sentence":"Farmers generally reported that the prototype tractor would meet their needs and suggested being willing to purchase the vehicle for 123,000 INR, about 22\\% higher than the price target for which the tractor was designed.","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":1,"sentence":"The interviewed farmers reported an average likelihood of 4.8/5 that they would use the vehicle for planting, inter-cultivation, and spraying, and an average likelihood of 3.8/5 that they would use the tractor for primary or secondary tillage.","offset":8,"pro":0.8888888888888888,"labels":"RST"},{"idx":2,"sentence":"A highly accurate digital-twin spiral bevel gear or hypoid gear is often required for dynamic analysis or stress analysis for gear transmission.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"However, a highly accurate digital-twin solid model is not always available because the final hypoid gear is completed by the gear manufacturer.","offset":1,"pro":0.1,"labels":"GAP"},{"idx":2,"sentence":"This study constructs a digital-twin from a sample hypoid gear.","offset":2,"pro":0.2,"labels":"PUR"},{"idx":2,"sentence":"The tooth surface of sample gear is digitized as topographical grids using a coordinate measuring machine (CMM) or a gear measurement center.","offset":3,"pro":0.3,"labels":"MTD"},{"idx":2,"sentence":"The geometric parameters (the surface position vector, the normal vector, the principal curvatures, and the corresponding principal directions) for the measured tooth surface (MTS) are then extracted using numerical differential geometry (NDG).","offset":4,"pro":0.4,"labels":"MTD"},{"idx":2,"sentence":"The machine settings and the cutter parameters for the sample hypoid gear are obtained by minimizing the topographical error between the replicated digital-twin and the sample gear using optimization software.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":2,"sentence":"The initial estimation for the machine settings and the cutter parameters is calculated using an explicit form of the modified-roll motion (MRM), which decreases numerical divergence and time that is required for calculation.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":2,"sentence":"The machine settings, the cutting tool parameters, and the auxiliary flank modification (AFM) motion are used as the design variables.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":2,"sentence":"A numerical example is presented to verify the proposed methodology.","offset":8,"pro":0.8,"labels":"MTD"},{"idx":2,"sentence":"The numerical results show that the replicated digital-twin that is developed using the proposed method is sufficiently accurate for industrial applications.","offset":9,"pro":0.9,"labels":"CLN"},{"idx":3,"sentence":"We present a new sampling method for the multi-scale design of polycrystalline materials, which improves the computational time efficiency compared to the existing computational approaches.","offset":0,"pro":0,"labels":"PUR"},{"idx":3,"sentence":"The solution strategy aims to find microstructure designs that optimize component-scale mechanical properties.","offset":1,"pro":0.1111111111111111,"labels":"PUR"},{"idx":3,"sentence":"The microstructure is represented with a probabilistic texture descriptor that quantifies the volume fractions of different crystallographic orientations.","offset":2,"pro":0.2222222222222222,"labels":"BAC"},{"idx":3,"sentence":"However, the original microstructure design space is high-dimensional and thus optimization in this domain is not favorable.","offset":3,"pro":0.3333333333333333,"labels":"GAP"},{"idx":3,"sentence":"Instead, we generate property closures, which are the reduced spaces of volume-averaged material properties that are computed in terms of the microstructural texture descriptors.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":3,"sentence":"We observe that the traditional design approaches which are based on sampling in the original microstructure space and sampling on the property closure are inefficient as they lead to highly concentrated design samples in the solution space.","offset":5,"pro":0.5555555555555556,"labels":"GAP"},{"idx":3,"sentence":"Therefore, we introduce a new sampling method in the property closure, which creates simplexes using the triangulation of the property hull and then generating samples for each simplex.","offset":6,"pro":0.6666666666666666,"labels":"PUR"},{"idx":3,"sentence":"Example problems include the optimization of Galfenol and alpha-titanium microstructures to improve non-linear material properties.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":3,"sentence":"The new sampling approach is shown to obtain better solutions while decreasing the required computational time compared to the previous microstructure design methods.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":4,"sentence":"Due to the benefits associated with additive manufacturing (AM), there are increasingly more opportunities to leverage AM to enable the fabrication of components that were previously made using conventional techniques such as subtractive manufacturing or casting.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"To support this transition, it is critical to be able to rigorously evaluate the technical and economic feasibility of additively manufacturing an existing component design.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":4,"sentence":"In order to support this evaluation, this paper presents a novel feasibility analysis that performs a multi-criteria assessment of AM readiness.","offset":2,"pro":0.2,"labels":"PUR"},{"idx":4,"sentence":"Along with the development of these assessments, we also present a novel scoring approach for qualitatively and quantitatively evaluating the feasibility of each component assessment.","offset":3,"pro":0.3,"labels":"MTD"},{"idx":4,"sentence":"This scoring approach, which leverages preference models from physical programing, introduces a flexible set of feasibility levels to assess the manufacturability capabilities of AM technologies.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":4,"sentence":"It also allows for the integration of a designer's preferences toward the AM assessments, supporting the decision whether to utilize AM technologies or not.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":4,"sentence":"The presented feasibility analysis allows for both technical and economic benefits since it suggests only using AM for those products whose feasibility results are within suitable ranges.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":4,"sentence":"The details of the approach are illustrated using four sample parts with varying geometries.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":4,"sentence":"Experimental validation is also performed to demonstrate the robustness of the evaluation.","offset":8,"pro":0.8,"labels":"MTD"},{"idx":4,"sentence":"Results obtained show the capability and generalizability of these approaches to analyze intricate geometries and provide useful decision support in AM feasibility analysis.","offset":9,"pro":0.9,"labels":"CLN"},{"idx":5,"sentence":"Students in design courses work on projects that are influenced by ambiguity, gender orientation, and domain relatedness.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"This study investigates the impacts of these factors on student self-efficacy in order to increase retention in engineering disciplines.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":5,"sentence":"From a comprehensive literature review and feedback from engineering experts, an instrument is developed to assess student perceptions on tolerance to ambiguity (STA), project gender orientation (PGO), and project domain relatedness (PDR).","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":5,"sentence":"Statistical analyses are conducted to examine the influence of STA, PGO, and PDR on student self-efficacy and collective efficacy.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":5,"sentence":"Results indicate that an increase in the gender orientation of the project decreases student self-efficacy.","offset":4,"pro":0.5714285714285714,"labels":"CLN"},{"idx":5,"sentence":"Furthermore, gender bias of the design project diminishes student tolerance to deal with ambiguous situations.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":5,"sentence":"Therefore, instructors should consider choosing more gender-neutral projects or make appropriate adjustments in project descriptions to minimize gender bias.","offset":6,"pro":0.8571428571428571,"labels":"IMP"},{"idx":6,"sentence":"Feasibility robust optimization techniques solve optimization problems with uncertain parameters that appear only in their constraint functions.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"Solving such problems requires finding an optimal solution that is feasible for all realizations of the uncertain parameters.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":6,"sentence":"This paper presents a new feasibility robust optimization approach involving uncertain parameters defined on continuous domains.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":6,"sentence":"The proposed approach is based on an integration of two techniques: (i) a sampling-based scenario generation scheme and (ii) a local robust optimization approach.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":6,"sentence":"An analysis of the computational cost of this integrated approach is performed to provide worst-case bounds on its computational cost.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":6,"sentence":"The proposed approach is applied to several non-convex engineering test problems and compared against two existing robust optimization approaches.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":6,"sentence":"The results show that the proposed approach can efficiently find a robust optimal solution across the test problems, even when existing methods for non-convex robust optimization are unable to find a robust optimal solution.","offset":6,"pro":0.75,"labels":"RST"},{"idx":6,"sentence":"A scalable test problem is solved by the approach, demonstrating that its computational cost scales with problem size as predicted by an analysis of the worst-case computational cost bounds.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":7,"sentence":"Optimization of dynamic engineering systems generally requires problem formulations that account for the coupling between embodiment design and control system design simultaneously.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"Such formulations are commonly known as combined optimal design and control (co-design) problems, and their application to deterministic systems is well established in the literature through a variety of methods.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":7,"sentence":"However, an issue that has not been addressed in the co-design literature is the impact of the inherent uncertainties within a dynamic system on its integrated design solution.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":7,"sentence":"Accounting for these uncertainties transforms the standard, deterministic co-design problem into a stochastic one, thus requiring appropriate stochastic optimization approaches for its solution.","offset":3,"pro":0.375,"labels":"GAP"},{"idx":7,"sentence":"This paper serves as the starting point for research on stochastic co-design problems by proposing and solving a novel problem formulation based on robust design optimization (RDO) principles.","offset":4,"pro":0.5,"labels":"PUR"},{"idx":7,"sentence":"Specifically, a co-design method known as multidisciplinary dynamic system design optimization (MDSDO) is used as the basis for an RDO problem formulation and implementation.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":7,"sentence":"The robust objective and inequality constraints are computed per usual as functions of their first-order-approximated means and variances, whereas analysis-based equality constraints are evaluated deterministically at the means of the random decision variables.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":7,"sentence":"The proposed stochastic co-design problem formulation is then implemented for two case studies, with the results indicating the importance of the robust approach on the integrated design solutions and performance measures.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":8,"sentence":"Planning the shortest collision-free path among scattered obstacles is an NP-complete problem.","offset":0,"pro":0,"labels":"GAP"},{"idx":8,"sentence":"As reviewed in this paper, a variety of deterministic as well as heuristic methods have been developed to address different instances of the problem.","offset":1,"pro":0.09090909090909091,"labels":"BAC"},{"idx":8,"sentence":"The focus of the deterministic methods is primarily on the optimality of the final solution and has been applied exclusively to regular shapes such as spheres or cubes.","offset":2,"pro":0.18181818181818182,"labels":"BAC"},{"idx":8,"sentence":"Nevertheless, due to the problem's intrinsic complexities (especially in 3D), researchers mainly resort to heuristics that offer acceptable (yet possibly suboptimal) solutions with reasonable resources.","offset":3,"pro":0.2727272727272727,"labels":"GAP"},{"idx":8,"sentence":"Therefore, less attention has been given to further the state-of-the-art in deterministic methods, which for 3D problems primarily focuses on approximating the solution.","offset":4,"pro":0.36363636363636365,"labels":"GAP"},{"idx":8,"sentence":"However, with the advancements in high-performance computing, we believe it is time to focus on solution quality.","offset":5,"pro":0.45454545454545453,"labels":"GAP"},{"idx":8,"sentence":"As such, this study aims to further the efforts in deterministic optimization methods for 3D path planning by overcoming some challenges of the existing methods and improving the optimality of the solution.","offset":6,"pro":0.5454545454545454,"labels":"PUR"},{"idx":8,"sentence":"The proposed approach is rooted in visibility-based planning methods where the obstacle-free space is modeled as a connectivity graph to be searched for the shortest path.","offset":7,"pro":0.6363636363636364,"labels":"MTD"},{"idx":8,"sentence":"The advantage of the proposed method in constructing the representative graph is that it does not make approximations to identify the graph nodes, unlike the existing methods.","offset":8,"pro":0.7272727272727273,"labels":"MTD"},{"idx":8,"sentence":"Nor does it limit the objects' geometries to specific shapes such as blocks or spheres.","offset":9,"pro":0.8181818181818182,"labels":"MTD"},{"idx":8,"sentence":"The capability of the method in finding the shortest collision-free paths in environments cluttered with convex polyhedra is demonstrated using sample test problems.","offset":10,"pro":0.9090909090909091,"labels":"CLN"},{"idx":9,"sentence":"Undetected fatigue and overload damages at the key locations of the crane boom are among the biggest threats in construction, leading to structural failure.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"Thus, the structural health of the crane boom should be monitored in real time to ensure that it works under the designed load capacity.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":9,"sentence":"In this study, we developed a lightweight digital twin by the multifidelity surrogate (MFS) model to improve the real-time monitoring and prediction accuracy of the structural safety of a crane boom.","offset":2,"pro":0.2222222222222222,"labels":"PUR"},{"idx":9,"sentence":"Digital twin technology, which can establish real-time mapping between the physical space and the digital space, has a promising potential for online monitoring and analysis of structures, equipment, and even human bodies.","offset":3,"pro":0.3333333333333333,"labels":"BAC"},{"idx":9,"sentence":"By combining the MFS model and sensor data, the lightweight digital twin can dynamically mirror the crane boom postures and predict its structural performance in real time.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":9,"sentence":"In this study, the structural analysis of the crane boom is limited to the linear elastic stage of materials.","offset":5,"pro":0.5555555555555556,"labels":"PUR"},{"idx":9,"sentence":"Numerical experiments showed that the accuracy of the lightweight digital twin was enhanced compared with that established by the single-fidelity surrogate model, and the computational cost of the lightweight digital twin was decreased with respect to the digital twin built by the numerical method.","offset":6,"pro":0.6666666666666666,"labels":"RST"},{"idx":9,"sentence":"Meanwhile, the uncertainty from the physical space was analyzed to enhance the reliability of the lightweight digital twin.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":9,"sentence":"Thus, the lightweight digital twin developed in our work can ensure accurate safety prediction and design optimization for crane booms.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":10,"sentence":"How to enlarge the output displacement is a key issue in the research field of microgrippers.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"It is difficult to further enlarge the output displacement for the traditional displacement transmission mechanism (DTM).","offset":1,"pro":0.09090909090909091,"labels":"GAP"},{"idx":10,"sentence":"In this research, a two-stage amplification cylinder-driven DTM based on the compliant mechanisms is designed to realize the displacement output expansion.","offset":2,"pro":0.18181818181818182,"labels":"PUR"},{"idx":10,"sentence":"The opening and closing of the clamping jaws is driven by the air cylinder to enlarge the output displacement of the microgripper.","offset":3,"pro":0.2727272727272727,"labels":"MTD"},{"idx":10,"sentence":"According to the analysis of statics model of the mechanism, the relationship between the output displacement of the microgripper and the driving pressure of the cylinder is established.","offset":4,"pro":0.36363636363636365,"labels":"MTD"},{"idx":10,"sentence":"The magnification of the microgripper is obtained using a dynamic model.","offset":5,"pro":0.45454545454545453,"labels":"MTD"},{"idx":10,"sentence":"Moreover, based on the finite element analysis, the mechanical structure parameters are optimized.","offset":6,"pro":0.5454545454545454,"labels":"MTD"},{"idx":10,"sentence":"The microgripper was fabricated by utilizing wire electro discharge machining (WEDM) technique, and then a series of experiments were carried out to obtain the relationship between the displacement and the driving pressure.","offset":7,"pro":0.6363636363636364,"labels":"MTD"},{"idx":10,"sentence":"It is found that the maximum output displacement measured is 1190.4 mu m under the pressure of 0-0.6 MPa, corresponding to the magnification of 47.63.","offset":8,"pro":0.7272727272727273,"labels":"RST"},{"idx":10,"sentence":"Compared with the results of finite element analysis and theoretical calculation, the test results have a discrepancy of 2.39\\% and 6.62\\%, respectively.","offset":9,"pro":0.8181818181818182,"labels":"RST"},{"idx":10,"sentence":"The microgripper has successfully grasped a variety of micro-parts with irregular shapes, and parallel grasping can be achieved, demonstrating the potential application of this design in the field of micromanipulation.","offset":10,"pro":0.9090909090909091,"labels":"CLN"},{"idx":11,"sentence":"Bayesian optimization (BO) is an efficient and flexible global optimization framework that is applicable to a very wide range of engineering applications.","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"To leverage the capability of the classical BO, many extensions, including multi-objective, multi-fidelity, parallelization, and latent-variable modeling, have been proposed to address the limitations of the classical BO framework.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":11,"sentence":"In this work, we propose a novel multi-objective BO formalism, called srMO-BO-3GP, to solve multi-objective optimization problems in a sequential setting.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":11,"sentence":"Three different Gaussian processes (GPs) are stacked together, where each of the GPs is assigned with a different task.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":11,"sentence":"The first GP is used to approximate a single-objective computed from the multi-objective definition, the second GP is used to learn the unknown constraints, and the third one is used to learn the uncertain Pareto frontier.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":11,"sentence":"At each iteration, a multi-objective augmented Tchebycheff function is adopted to convert multi-objective to single-objective, where the regularization with a regularized ridge term is also introduced to smooth the single-objective function.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":11,"sentence":"Finally, we couple the third GP along with the classical BO framework to explore the convergence and diversity of the Pareto frontier by the acquisition function for exploitation and exploration.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":11,"sentence":"The proposed framework is demonstrated using several numerical benchmark functions, as well as a thermomechanical finite element model for flip-chip package design optimization.","offset":7,"pro":0.875,"labels":"MTD"},{"idx":12,"sentence":"Multiobjective design optimization studies typically derive Pareto sets or use a scalar substitute function to capture design trade-offs, leaving it up to the designer's intuition to use this information for design refinements and decision-making.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"Understanding the causality of trade-offs more deeply, beyond simple postoptimality parametric studies, would be particularly valuable in configuration design problems to guide configuration redesign.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":12,"sentence":"This article presents the method of multiobjective monotonicity analysis to identify root causes for the existence of trade-offs and the particular shape of Pareto sets.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":12,"sentence":"This analysis process involves reducing optimization models through constraint activity identification to a point where dependencies specific to the Pareto set and the constraints that cause them are revealed.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":12,"sentence":"The insights gained can then be used to target configuration design changes.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":12,"sentence":"We demonstrate the proposed approach in the preliminary design of a medical device for oral drug delivery.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":13,"sentence":"While the modeling analysis of the kinetostatic behavior of underactuated tendon-driven robotic fingers has been largely addressed in the literature, tendon routing is often not considered by these theoretical models.","offset":0,"pro":0,"labels":"GAP"},{"idx":13,"sentence":"The tendon routing path plays a fundamental role in defining joint torques, and subsequently, the force vectors produced by the phalanges.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":13,"sentence":"However, dynamic tendon behavior is difficult to predict and is influenced by many external factors including tendon friction, the shape of the grasped object, the initial pose of the fingers, and finger contact points.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":13,"sentence":"In this paper, we present an experimental comparison of the force performance of nine fingers, with different tendon routing configurations.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":13,"sentence":"We use the concept of force-isotropy, in which forces are equal and distributed on each phalanx as the optimum condition for an adaptive grasp.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":13,"sentence":"Our results show only some of the finger designs surveyed exhibited a partial adaptive behavior, showing distributed force for the proximal and distal phalanxes throughout grasping cycles, while other routings resulted in only a single phalanx remaining in contact with the object.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":14,"sentence":"The capabilities of additive manufacturing (AM) open up designers'solution space and enable them to build designs previously impossible through traditional manufacturing (TM).","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"To leverage this design freedom, designers must emphasize opportunistic design for AM (DfAM), i.e., design techniques that leverage AM capabilities.","offset":1,"pro":0.0625,"labels":"BAC"},{"idx":14,"sentence":"Additionally, designers must also emphasize restrictive DfAM, i.e., design considerations that account for AM limitations, to ensure that their designs can be successfully built.","offset":2,"pro":0.125,"labels":"MTD"},{"idx":14,"sentence":"Therefore, designers must adopt a dual'} design mindset-emphasizing both, opportunistic and restrictive DfAM-when designing for AM.","offset":3,"pro":0.1875,"labels":"GAP"},{"idx":14,"sentence":"However, to leverage AM capabilities, designers must not only generate creative ideas for AM but also select these creative ideas during the concept selection stage.","offset":4,"pro":0.25,"labels":"GAP"},{"idx":14,"sentence":"Design educators must specifically emphasize selecting creative ideas in DfAM, as ideas perceived as infeasible through the traditional design for manufacturing lens may now be feasible with AM.","offset":5,"pro":0.3125,"labels":"MTD"},{"idx":14,"sentence":"This emphasis could prevent creative but feasible ideas from being discarded due to their perceived infeasibility.","offset":6,"pro":0.375,"labels":"GAP"},{"idx":14,"sentence":"While several studies have discussed the role of DfAM in encouraging creative idea generation, there is a need to investigate concept selection in DfAM.","offset":7,"pro":0.4375,"labels":"GAP"},{"idx":14,"sentence":"In this paper, we investigated the effects of four variations in DfAM education: (1) restrictive, (2) opportunistic, (3) restrictive followed by opportunistic (R-O), and (4) opportunistic followed by restrictive (O-R), on students' concept selection process.","offset":8,"pro":0.5,"labels":"PUR"},{"idx":14,"sentence":"We compared the creativity of the concepts generated by students to the creativity of the concepts they selected.","offset":9,"pro":0.5625,"labels":"MTD"},{"idx":14,"sentence":"The creativity of designs was measured on four dimensions: (1) uniqueness, (2) usefulness, (3) technical goodness, and (4) overall creativity.","offset":10,"pro":0.625,"labels":"MTD"},{"idx":14,"sentence":"We also performed qualitative analyses to gain insight into the rationale provided by students when making their design decisions.","offset":11,"pro":0.6875,"labels":"MTD"},{"idx":14,"sentence":"From the results, we see that only teams from the restrictive and dual O-R groups selected ideas of higher uniqueness and overall creativity.","offset":12,"pro":0.75,"labels":"RST"},{"idx":14,"sentence":"In contrast, teams from the dual R-O DfAM group selected ideas of lower uniqueness compared with the mean uniqueness of ideas generated.","offset":13,"pro":0.8125,"labels":"RST"},{"idx":14,"sentence":"Finally, we see that students trained in opportunistic DfAM emphasized minimizing build material the most, whereas those trained only in restrictive DfAM emphasized minimizing build time.","offset":14,"pro":0.875,"labels":"RST"},{"idx":14,"sentence":"These results highlight the need for DfAM education to encourage AM designers to not just generate creative ideas but also have the courage to select them for the next stage of design.","offset":15,"pro":0.9375,"labels":"CLN"},{"idx":15,"sentence":"Design variety metrics measure how much a design space is explored.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"This article proposes that a generalized class of entropy metrics based on Sharma-Mittal entropy offers advantages over existing methods to measure design variety.","offset":1,"pro":0.2,"labels":"PUR"},{"idx":15,"sentence":"We show that an exemplar metric from Sharma-Mittal entropy, namely, the Herfindahl-Hirschman index for design (HHID) has the following desirable advantages over existing metrics: (a) more accuracy: it better aligns with human ratings compared to existing and commonly used tree-based metrics for two new datasets; (b) higher sensitivity: it has higher sensitivity compared to existing methods when distinguishing between the variety of sets; (c) allows efficient optimization: it is a submodular function, which enables one to optimize design variety using a polynomial time greedy algorithm; and (d) generalizes to multiple metrics: many existing metrics can be derived by changing the parameters of this metric, which allows a researcher to fit the metric to better represent variety for new domains.","offset":2,"pro":0.4,"labels":"CLN"},{"idx":15,"sentence":"This article also contributes a procedure for comparing metrics used to measure variety via constructing ground truth datasets from pairwise comparisons.","offset":3,"pro":0.6,"labels":"CTN"},{"idx":15,"sentence":"Overall, our results shed light on some qualities that good design variety metrics should possess and the nontrivial challenges associated with collecting the data needed to measure those qualities.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":16,"sentence":"Optimization of dynamic engineering systems requires an integrated approach that accounts for the coupling between embodiment design and control system design, simultaneously.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"Generally known as combined design and control optimization (co-design), these methods offer superior system's performance and reduced costs.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":16,"sentence":"Despite the widespread use of co-design approaches in the literature, not much work has been done to address the issue of uncertainty in co-design problem formulations.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":16,"sentence":"This is problematic as all engineering models contain some level of uncertainty that might negatively affect the system's performance, if overlooked.","offset":3,"pro":0.375,"labels":"GAP"},{"idx":16,"sentence":"While in our previous study we developed a robust co-design approach, a more rigorous evaluation of probabilistic constraints is required to obtain the targeted reliability levels for probabilistic constraints.","offset":4,"pro":0.5,"labels":"GAP"},{"idx":16,"sentence":"Therefore, we propose and implement a novel stochastic co-design approach based on the principles of reliability-based design optimization (RBDO) to explicitly account for uncertainties from design decision variables and problem parameters.","offset":5,"pro":0.625,"labels":"PUR"},{"idx":16,"sentence":"In particular, a reliability-based, multidisciplinary dynamic system design optimization (RB-MDSDO) formulation is developed using the sequential optimization and reliability assessment (SORA) algorithm, such that the analysis-type dynamic equality constraints are satisfied at the mean values of random variables, as well as their most probable points (MPPs).","offset":6,"pro":0.75,"labels":"MTD"},{"idx":16,"sentence":"The proposed approach is then implemented for two case studies, and the results were benchmarked through Monte Carlo simulation (MCS) to indicate the impact of including reliability measures in co-design formulations.","offset":7,"pro":0.875,"labels":"MTD"},{"idx":17,"sentence":"Predicting the behavior or response for complicated dynamical systems during their operation may require high-fidelity and computationally costly simulations.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"Because of the high computational cost, such simulations are generally done offline.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":17,"sentence":"The offline simulation data can then be combined with sensors measurement data for online, operational prediction of the system's behavior.","offset":2,"pro":0.2222222222222222,"labels":"BAC"},{"idx":17,"sentence":"In this paper, a generic online data-driven approach is proposed for the prediction of spatio-temporal behavior of dynamical systems using their simulation data combined with sparse, noisy sensors measurement data.","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":17,"sentence":"The approach relies on an offline-online approach and is based on an integration of dimension reduction, surrogate modeling, and data assimilation techniques.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":17,"sentence":"A step-by-step application of the proposed approach is demonstrated by a simple numerical example.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":17,"sentence":"The performance of the approach is also evaluated by a case study which involves predicting aeroelastic response of a joined-wing aircraft in which sensors are sparsely placed on its wing.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":17,"sentence":"Through this case study, it is shown that the results obtained from the proposed spatio-temporal prediction technique have comparable accuracy to those from the high-fidelity simulation, while at the same time significant reduction in computational expense is achieved.","offset":7,"pro":0.7777777777777778,"labels":"CLN"},{"idx":17,"sentence":"It is also shown that, for the case study, the proposed approach has a prediction accuracy that is relatively robust to the sensors' locations.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":18,"sentence":"We introduce a method to analyze and modify a shape to make it manufacturable for a given additive manufacturing (AM) process.","offset":0,"pro":0,"labels":"PUR"},{"idx":18,"sentence":"Different AM technologies, process parameters, or materials introduce geometric constraints on what is manufacturable or not.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":18,"sentence":"Given an input 3D model and minimum printable feature size dictated by the manufacturing process characteristics and parameters, our algorithm generates a corrected geometry that is printable with the intended AM process.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":18,"sentence":"A key issue in model correction for manufacturability is the identification of critical features that are affected by the printing process.","offset":3,"pro":0.42857142857142855,"labels":"BAC"},{"idx":18,"sentence":"To address this challenge, we propose a topology aware approach to construct the allowable space for a print head to traverse during the 3D printing process.","offset":4,"pro":0.5714285714285714,"labels":"PUR"},{"idx":18,"sentence":"Combined with our build orientation optimization algorithm, the amount of modifications performed on the shape is kept at minimum while providing an accurate approximation of the as-manufactured part.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":18,"sentence":"We demonstrate our method on a variety of 3D models and validate it by 3D printing the results.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":19,"sentence":"This paper presents the design, fabrication, and testing of a novel single stack-based piezoelectric energy harvester (PEH) for harvesting energy from three-degree-of-freedom (3-DOF) force excitation.","offset":0,"pro":0,"labels":"PUR"},{"idx":19,"sentence":"One uniqueness lies in that the 3-DOF energy harvesting is implemented by using one piezoelectric stack.","offset":1,"pro":0.1,"labels":"MTD"},{"idx":19,"sentence":"To scavenge energy from the 3-DOF input force, the proposed PEH is constructed with several force transmission mechanisms and slider mechanisms.","offset":2,"pro":0.2,"labels":"PUR"},{"idx":19,"sentence":"The direction of the input force is first changed by the force transmission mechanisms, and the redundant force components are eliminated by the slider mechanisms.","offset":3,"pro":0.3,"labels":"MTD"},{"idx":19,"sentence":"The transmitted force is then amplified by a two-stage force amplifier mechanism to improve the electric power output.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":19,"sentence":"The key parameters were found by establishing an analytical model of the proposed PEH.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":19,"sentence":"The best output performance of the PEH is achieved by selecting and optimally designing the key parameters.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":19,"sentence":"A prototype harvester was fabricated, and several experimental studies were conducted to verify the device performance.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":19,"sentence":"Results show the effectiveness of the developed 3-DOF PEH under the input force applied in x-axis, y-axis or z-axis.","offset":8,"pro":0.8,"labels":"CLN"},{"idx":19,"sentence":"Furthermore, the issues that affect the practical application are discussed.","offset":9,"pro":0.9,"labels":"MTD"},{"idx":0,"sentence":"When designing for additive manufacturing (AM), designers often need assistance in breaking out of their conventional manufacturing mind-set.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"Previously, the authors derived design heuristics for AM (DHAM) to assist designers in doing this during the early phases of the design process.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":0,"sentence":"This study proposes a set of 25 multimodal cards and objects to accompany each of the design heuristics for AM and studies their effect through a series of controlled, novice user studies conducted using both teams and individuals who redesign a city E-Bike.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":0,"sentence":"The resulting AM concepts are analyzed in terms of the quantity of design modifications relevant for AM, AM flexibility, novelty, and variety.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":0,"sentence":"It is found that the DHAM cards and objects increase the inclusion of AM concepts, AM modifications, and the unique capabilities of AM in the concepts generated by both individuals and teams.","offset":4,"pro":0.5,"labels":"CLN"},{"idx":0,"sentence":"They also increase the creativity of the concepts generated by both individuals and teams, as measured through a series of defined metrics.","offset":5,"pro":0.625,"labels":"CLN"},{"idx":0,"sentence":"Furthermore, the objects in combination with the cards are more effective at stimulating the generation of a wider variety of designs than the cards alone.","offset":6,"pro":0.75,"labels":"CLN"},{"idx":0,"sentence":"Future work will focus on studying the use of the DHAM cards and objects in an industrial setting.","offset":7,"pro":0.875,"labels":"IMP"},{"idx":1,"sentence":"This paper proposes a new parametric level set method for topology optimization based on deep neural network (DNN).","offset":0,"pro":0,"labels":"PUR"},{"idx":1,"sentence":"In this method, the fully connected DNN is incorporated into the conventional level set methods to construct an effective approach for structural topology optimization.","offset":1,"pro":0.1,"labels":"MTD"},{"idx":1,"sentence":"The implicit function of level set is described by fully connected DNNs.","offset":2,"pro":0.2,"labels":"MTD"},{"idx":1,"sentence":"A DNN-based level set optimization method is proposed, where the Hamilton-Jacobi partial differential equations (PDEs) are transformed into parametrized ordinary differential equations (ODEs).","offset":3,"pro":0.3,"labels":"MTD"},{"idx":1,"sentence":"The zero-level set of implicit function is updated through updating the weights and biases of networks.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":1,"sentence":"The parametrized reinitialization is applied periodically to prevent the implicit function from being too steep or too flat in the vicinity of its zero-level set.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":1,"sentence":"The proposed method is implemented in the framework of minimum compliance, which is a well-known benchmark for topology optimization.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":1,"sentence":"In practice, designers desire to have multiple design options, where they can choose a better conceptual design base on their design experience.","offset":7,"pro":0.7,"labels":"BAC"},{"idx":1,"sentence":"One of the major advantages of the DNN-based level set method is capable to generate diverse and competitive designs with different network architectures.","offset":8,"pro":0.8,"labels":"BAC"},{"idx":1,"sentence":"Several numerical examples are presented to verify the effectiveness of the proposed DNN-based level set method.","offset":9,"pro":0.9,"labels":"CLN"},{"idx":2,"sentence":"Cooperative three-dimensional (3D) printing (C3DP)-a representative realization of cooperative manufacturing (CM)-is a novel approach that utilizes multiple mobile 3D printing robots for additive manufacturing (AM).","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"It makes the make-span much shorter compared with traditional 3D printing due to parallel printing.","offset":1,"pro":0.09090909090909091,"labels":"BAC"},{"idx":2,"sentence":"In C3DP, collision-free scheduling is critical to the realization of cooperation and parallel operation among mobile printers.","offset":2,"pro":0.18181818181818182,"labels":"BAC"},{"idx":2,"sentence":"In the extant literature, there is a lack of methods to schedule multi-robot C3DP with limited resources.","offset":3,"pro":0.2727272727272727,"labels":"GAP"},{"idx":2,"sentence":"This study addresses this gap with two methods.","offset":4,"pro":0.36363636363636365,"labels":"PUR"},{"idx":2,"sentence":"The first method, dynamic dependency list algorithm (DDLA), uses a constraint-satisfaction approach to eliminate solutions that could result in collisions between robots and collisions between robots with already-printed materials.","offset":5,"pro":0.45454545454545453,"labels":"MTD"},{"idx":2,"sentence":"The second method, modified genetic algorithm (GA), uses chromosomes to represent chunk assignments and utilizes GA operators, such as the crossover and mutation, to generate diverse print schedules while maintaining the dependencies between chunks.","offset":6,"pro":0.5454545454545454,"labels":"MTD"},{"idx":2,"sentence":"Three case studies, including two large rectangular bars in different scales and a foldable sport utility vehicle (SUV), are used to demonstrate the effectiveness and performance of the two methods.","offset":7,"pro":0.6363636363636364,"labels":"MTD"},{"idx":2,"sentence":"The results show that both methods can effectively generate valid print schedules using a specified number of robots while attempting to minimize the make-span.","offset":8,"pro":0.7272727272727273,"labels":"CLN"},{"idx":2,"sentence":"The results also show that both methods generate a print schedule with equal print time for the first two case studies with homogeneous chunks.","offset":9,"pro":0.8181818181818182,"labels":"CLN"},{"idx":2,"sentence":"In contrast, the modified GA outperforms the DDLA in the third case study, where the chunks are heterogeneous in volume and require different times to print.","offset":10,"pro":0.9090909090909091,"labels":"RST"},{"idx":3,"sentence":"The arc-toothed cylindrical worm has an arc tooth profile in a section, which may be the axial section, the normal section, or an offsetting plane of the worm helical surface.","offset":0,"pro":0,"labels":"MTD"},{"idx":3,"sentence":"The meshing principle for a gearing containing such a worm is established.","offset":1,"pro":0.1,"labels":"PUR"},{"idx":3,"sentence":"The normal vector of instantaneous contact line is determined in the natural frame and the meshing performance parameters are obtained without the help of the curvature parameters of the worm helical surface to ensure the established meshing principle is concise and practical.","offset":2,"pro":0.2,"labels":"MTD"},{"idx":3,"sentence":"The numerical results show that the worm working length can be beyond half of the thread length and the meshing zone of the worm pair can cover most of the worm gear tooth surface.","offset":3,"pro":0.3,"labels":"RST"},{"idx":3,"sentence":"The instantaneous contact lines are uniformly distributed and the worm pair forms double-line contact.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":3,"sentence":"The numerical outcomes of the induced principal curvature show that the contact stress level between the teeth is higher in the middle of the worm gear tooth surface and near its dedendum.","offset":5,"pro":0.5,"labels":"CLN"},{"idx":3,"sentence":"The forming condition of the lubricating oil film is poorer in the middle of the worm gear tooth surface and from addendum to dedendum as demonstrated by the numerical results of the sliding angle.","offset":6,"pro":0.6,"labels":"RST"},{"idx":3,"sentence":"The normal arc-toothed worm lathed by an offsetting cutter is recommended to apply in industry after various researches and analyses.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":3,"sentence":"The cutting geometric condition of the worm is investigated quantitatively.","offset":8,"pro":0.8,"labels":"MTD"},{"idx":3,"sentence":"It is discovered that the rule of the cutter working relief angle changes along the cutting edge during lathing the worm.","offset":9,"pro":0.9,"labels":"CLN"},{"idx":4,"sentence":"Existing research on synthesis methods for single degree-of-freedom (DOF) six-bar linkages mainly include four or five exact poses.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"However, an ideal trajectory cannot be synthesized using only five exact poses, thus, it is necessary to introduce additional poses to constrain the trajectory.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":4,"sentence":"If more exact poses are introduced, then the linkage may have no solution.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":4,"sentence":"Therefore, the constraints of the approximate pose are considered to make the trajectory conform to the desired trajectory.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":4,"sentence":"This paper successfully introduces mixed poses into a six-bar linkage, based on Z (Z < 5) exact poses and K approximate poses of a given error range, and a new synthesis method for single DOF six-bar linkages is proposed.","offset":4,"pro":0.5,"labels":"PUR"},{"idx":4,"sentence":"The solution domain of the linkages synthesized by this method is wide and can be adjusted by controlling the error of the approximate poses, which reduces the difficulty of selecting the solution, ensures theoretical feasibility, and enables the trajectory of the final linkage to more closely match the ideal trajectory.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":4,"sentence":"Finally, for the coordinated training of multiple joints in human limbs, a rehabilitation device is designed based on the above six-bar linkage, and a prototype is developed and tested.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":4,"sentence":"The test results reveal the accuracy of the proposed method and the effectiveness of rehabilitation training.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":5,"sentence":"Design researchers have long sought to understand the mechanisms that support creative idea development.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"However, one of the key challenges faced by the design community is how to effectively measure the nebulous construct of creativity.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":5,"sentence":"The social science and engineering communities have adopted two vastly different approaches to solving this problem, both of which have been deployed throughout engineering design research.","offset":2,"pro":0.2857142857142857,"labels":"BAC"},{"idx":5,"sentence":"The goal of this paper was to compare and contrast these two approaches using design ratings of nearly 1000 engineering design ideas.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":5,"sentence":"The results of this study identify that while these two methods provide similar ratings of idea quality, there was a statistically significant negative relationship between these methods for ratings of idea novelty.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":5,"sentence":"In addition, the results show discrepancies in the reliability and consistency of global ratings of creativity.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":5,"sentence":"The results of this study guide the deployment of idea ratings in engineering design research and evidence.","offset":6,"pro":0.8571428571428571,"labels":"IMP"},{"idx":6,"sentence":"Additive manufacturing (AM) offers designers access to the entire volume of an artifact during its build operation, including the embedding of foreign objects, like sensors, motors, and actuators, into the artifact to produce multifunctional products from the build tray.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"However, the application of embedding requires extensive designer expertise in AM.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":6,"sentence":"This research aims to develop a tool to automate design decisions for in situ embedding, eliminating the need for ad hoc design decisions made by experts.","offset":2,"pro":0.2222222222222222,"labels":"PUR"},{"idx":6,"sentence":"Two unique approaches are proposed in this work: shadow projection and voxel simulation.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":6,"sentence":"Both of these approaches follow a three-stage methodology to achieve design automation by (1) identifying the optimum orientation for the object, (2) designing cavity, and finally (3) designing the shape converter for a flush surface at the paused layer.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":6,"sentence":"The two approaches differ in Stages 2 and 3.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":6,"sentence":"Where the shadow projection approach employs a series of point cloud manipulation to geometry of the embedded object, the voxel simulation approach simulates the process of insertion of the embedding geometry into the part voxel by voxel.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":6,"sentence":"While both proposed approaches are successful in automating design for embedding complex geometries, they result in tradeoffs between final designs and the time for computation.","offset":7,"pro":0.7777777777777778,"labels":"GAP"},{"idx":6,"sentence":"Computational experiment with six test cases shows that designers must strategically choose from one of the approaches to efficiently automate the digital design for embedding.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":7,"sentence":"This paper applies stress-based shape optimization to microstructures, a scarcely explored topic in the literature.","offset":0,"pro":0,"labels":"PUR"},{"idx":7,"sentence":"As the actual stresses arising at the macroscopic structure are scale separated, the microstrucural stress is considered herein as the state of a representative volume element (RVE) after applying test unit strain load cases, not related to the macroscale loads.","offset":1,"pro":0.14285714285714285,"labels":"MTD"},{"idx":7,"sentence":"The three stress states in 2D are aggregated via p-norm functions, which are used for stress minimization.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":7,"sentence":"A stress-based level set method is applied.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":7,"sentence":"The method linearizes the objective and constraint functions and solves an optimization problem at every iteration to obtain the boundary velocities.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":7,"sentence":"The Ersatz material approach is used to compute the stiffness of the elements sliced by the boundary.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":7,"sentence":"A single hole inclusion microstructure is optimized for minimum stress in order to verify the methodology.","offset":6,"pro":0.8571428571428571,"labels":"MTD"},{"idx":8,"sentence":"Constant-force mechanisms (CFMs) can produce an almost invariant output force over a limited range of input displacement.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"Without using additional sensor and force controller, adjustable CFMs can passively produce an adjustable constant output force to interact with the working environment.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":8,"sentence":"In the literature, one-dimensional CFMs have been developed for various applications.","offset":2,"pro":0.2222222222222222,"labels":"BAC"},{"idx":8,"sentence":"This paper presents the design of a novel CFM that can produce adjustable constant force in two dimensions.","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":8,"sentence":"Because an adjustable constant force can be produced in each radial direction, the proposed adjustable CFM can be used in applications that require two-dimensional force regulation.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":8,"sentence":"In this paper, the design formulation and simulation results are presented and discussed.","offset":5,"pro":0.5555555555555556,"labels":"PUR"},{"idx":8,"sentence":"Equations to minimize the output force variation are given to choose the design parameters optimally.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":8,"sentence":"A prototype of the two-dimensional CFM is tested to demonstrate the effectiveness and accuracy of adjustable force regulation.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":8,"sentence":"This novel CFM is expected to be used in machines or robots to interact friendly with the environment.","offset":8,"pro":0.8888888888888888,"labels":"IMP"},{"idx":9,"sentence":"Conceptual design of spatial compliant mechanisms with distinct input and output ports may be hard because of its complex interconnected topology and is currently accomplished by computationally intensive automated techniques.","offset":0,"pro":0,"labels":"GAP"},{"idx":9,"sentence":"This paper proposes a user insightful method for generating conceptual compliant topology solutions.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":9,"sentence":"The method builds on recent advances where the compliant mechanism deformation is represented as load flow in its constituent members.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":9,"sentence":"The nature of load flow enables functional decomposition of compliant mechanisms into maximally decoupled building blocks, namely, a transmitter member and a constraint member.","offset":3,"pro":0.42857142857142855,"labels":"BAC"},{"idx":9,"sentence":"The proposed design methodology seeks to synthesize spatial compliant designs by systematically combining transmitter-constraint members first, identifying kinematically feasible transmitter load paths between input(s) and output(s), and then selecting appropriate constraints that enforce the load path.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":9,"sentence":"The paper proposes four design steps to generate feasible solutions and four additional guidelines to optimize load paths and constraint orientations.","offset":5,"pro":0.7142857142857143,"labels":"PUR"},{"idx":9,"sentence":"The method is applied with equal ease to three spatial complaint mechanism examples that belong to single-input single-output, multiple-input single output, and single-input multiple-output mechanisms.","offset":6,"pro":0.8571428571428571,"labels":"MTD"},{"idx":10,"sentence":"To solve challenging optimization problems with time-consuming objective and constraints, a novel efficient Kriging-based constrained optimization (EKCO) algorithm is proposed in this paper.","offset":0,"pro":0,"labels":"PUR"},{"idx":10,"sentence":"The EKCO mainly consists of three sampling phases.","offset":1,"pro":0.08333333333333333,"labels":"MTD"},{"idx":10,"sentence":"In phase I of EKCO, considering the significance of constraints, feasible region is constructed via employing a feasible region sampling (FRS) criterion.","offset":2,"pro":0.16666666666666666,"labels":"MTD"},{"idx":10,"sentence":"The FRS criterion can avoid the local clustering phenomenon of sample points.","offset":3,"pro":0.25,"labels":"MTD"},{"idx":10,"sentence":"Therefore, phase I is also a global sampling process for the objective function in the feasible region.","offset":4,"pro":0.3333333333333333,"labels":"MTD"},{"idx":10,"sentence":"However, the objective function may be higher-order nonlinear than constraints.","offset":5,"pro":0.4166666666666667,"labels":"GAP"},{"idx":10,"sentence":"In phase II, by maximizing the prediction variance of the surrogate objective, more accurate objective function in the feasible region can be obtained.","offset":6,"pro":0.5,"labels":"MTD"},{"idx":10,"sentence":"After global sampling, to accelerate the convergence of EKCO, an objective local sampling criterion is introduced in phase III.","offset":7,"pro":0.5833333333333334,"labels":"MTD"},{"idx":10,"sentence":"The verification of the EKCO algorithm is examined on 18 benchmark problems by several recently published surrogate-based optimization algorithms.","offset":8,"pro":0.6666666666666666,"labels":"MTD"},{"idx":10,"sentence":"The results indicate that the sampling efficiency of EKCO is higher than or comparable with that of the recently published algorithms while maintaining the high accuracy of the optimal solution, and the adaptive ability of the proposed algorithm also be validated.","offset":9,"pro":0.75,"labels":"CLN"},{"idx":10,"sentence":"To verify the ability of EKCO to solve practical engineering problems, an optimization design problem of aeronautical structure is presented.","offset":10,"pro":0.8333333333333334,"labels":"PUR"},{"idx":10,"sentence":"The result indicates EKCO can find a better feasible design than the initial design with limited sample points, which demonstrates practicality of EKCO.","offset":11,"pro":0.9166666666666666,"labels":"CLN"},{"idx":11,"sentence":"This work presents a method for generating concept designs for coupled multiphysics problems by employing generative adversarial networks (GANs).","offset":0,"pro":0,"labels":"PUR"},{"idx":11,"sentence":"Since the optimal designs of multiphysics problems often contain a combination of features that can be found in the single-physics solutions, we investigate the feasibility of learning the optimal design from the single-physics solutions, to produce concept designs for problems that are governed by a combination of these single physics.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":11,"sentence":"We employ GANs to produce optimal topologies similar to the results of level set topology optimization (LSTO) by finding a mapping between the sensitivity fields of specific boundary conditions, and the optimal topologies.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":11,"sentence":"To find this mapping, we perform image-to-image translation GAN training with a combination of structural, heat conduction, and a relatively smaller number of coupled structural and heat conduction data.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":11,"sentence":"We observe that the predicted topologies using GAN for coupled multiphysics problems are very similar to those generated by level set topology optimization, which can then be used as the concept designs for further detailed design.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":11,"sentence":"We show that using a combination of multiple single-physics data in the training improves the prediction of GAN for multiphysics problems.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":11,"sentence":"We provide several examples to demonstrate this.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":12,"sentence":"Multi-fidelity surrogate modeling has been extensively used in engineering design to achieve a balance between computational efficiency and prediction accuracy.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"Sequential sampling strategies have been investigated to improve the computational efficiency of surrogate-assisted design optimization.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":12,"sentence":"The existing sequential sampling approaches, however, are dedicated to either deterministic multi-fidelity design optimization or robust design under uncertainty using single-fidelity models.","offset":2,"pro":0.2,"labels":"GAP"},{"idx":12,"sentence":"This paper proposes a sequential sampling method for robust design optimization based on multi-fidelity modeling.","offset":3,"pro":0.3,"labels":"PUR"},{"idx":12,"sentence":"The proposed method considers both design variable uncertainty and interpolation uncertainty during the sequential sampling.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":12,"sentence":"An extended upper confidence boundary (EUCB) function is developed to determine both the sampling locations and the fidelity levels of the sequential samples.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":12,"sentence":"In the EUCB function, the cost ratio between high- and low-fidelity models and the sampling density are considered.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":12,"sentence":"Moreover, the EUCB function is extended to handle constrained robust design optimization problems by combining the probability of feasibility.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":12,"sentence":"The performance of the proposed approach is verified using two analytical examples and an engineering case.","offset":8,"pro":0.8,"labels":"MTD"},{"idx":12,"sentence":"Results show that the proposed sequential approach is more efficient than the one-shot sampling method for robust design optimization problems.","offset":9,"pro":0.9,"labels":"CLN"},{"idx":13,"sentence":"The design of cleaning and maintenance (CaM) robots is generally limited by their fixed morphologies, resulting in limited functions and modes of operation.","offset":0,"pro":0,"labels":"GAP"},{"idx":13,"sentence":"Contrary to fixed shape robots, the design of reconfigurable robots presents unique challenges in designing their system, subsystems, and functionalities with the scope for innovative operational scenarios and achieving high performance in multiple modalities without compromise.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":13,"sentence":"This paper proposes a heuristic framework using three layers, namely input, formulation, and output layer, for designing reconfigurable robots with the aid of established transformation principles including expand/collapse, expose/cover, and fuse/divide observed in several products, services, and systems.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":13,"sentence":"We apply this heuristic framework approach to the novel design of a pavement CaM robotic system and subsystems, namely, (i) varying footprint, (ii) transmission, (iii) outer skin or cover, (iv) storage bin, (v) surface cleaning, and (vi) vacuum/suction and blowing.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":13,"sentence":"The advances in the design method using the heuristic approach are demonstrated by developing an innovative reconfigurable design for the CaM task.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":13,"sentence":"Kinematic analysis and control architecture enables the unique locomotion behavior and gaits, namely, (a) static reconfiguration and (b) reconfiguration while locomotion, supported by the control architecture.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":13,"sentence":"Experiments were conducted, and outcomes were discussed along with the failure mode analysis to support the design robustness and limitations through the observations made from the development to testing phase over one year.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":13,"sentence":"A detailed video demonstrating the design capabilities is linked.","offset":7,"pro":0.875,"labels":"CTN"},{"idx":14,"sentence":"Sudden changes in life and work patterns due to COVID-19 have affected customer requirements for the products.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"Under these circumstances, companies can achieve high profitability and customer satisfaction when they can efficiently identify and respond quickly to changing customer preferences caused by COVID-19.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":14,"sentence":"This article presents empirical research on dynamic changes in customer responses for product features caused by the spread of COVID-19 through sentiment analysis based on online reviews.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":14,"sentence":"A case study is conducted using new and refurbished smartphone reviews to investigate the dynamic changes in customer sentiment before/during COVID-19.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":14,"sentence":"The importance of the result is shown by comparing it to the actual market data.","offset":4,"pro":0.8,"labels":"MTD"},{"idx":15,"sentence":"Current legged landers are typical truss structures acting as one type of fundamental equipment for the close-range extraterrestrial exploration missions.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"Unluckily, the development process applying the current design framework always consumes a long time span searching for the final design, accompanying masses of trial and error with inefficiency and diseconomy.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":15,"sentence":"Its kernel reason is that the stages from concept to scheme employ the paradigm of structural-analysis-oriented redesign and untimely embed physical prototype experiments in masses of iterative design cycles.","offset":2,"pro":0.2857142857142857,"labels":"BAC"},{"idx":15,"sentence":"Furthermore, the current framework cannot support the creative development of future legged landers with novel functions and mechanisms.","offset":3,"pro":0.42857142857142855,"labels":"GAP"},{"idx":15,"sentence":"Herein, we present a complete computable design framework for speeding up the development of both current and future legged landers, highlighted by new mathematical models and new principles of forward-design paradigm and multi-mode synergistic design paradigm.","offset":4,"pro":0.5714285714285714,"labels":"PUR"},{"idx":15,"sentence":"It applies the numerical prototype simulation instead of the physical prototype experiment in most iterative processes.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":15,"sentence":"This work will facilitate the extraterrestrial exploration missions using the current legged landers (truss-based) and the future legged lander (robot-based).","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":16,"sentence":"This paper aims to present the topological structure design and kinematic analysis of a novel double-ring truss deployable satellite antenna mechanism.","offset":0,"pro":0,"labels":"PUR"},{"idx":16,"sentence":"First, a new topological scheme and a new rectangular prism deployable linkage unit are proposed for constructing the kind of antenna mechanisms.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":16,"sentence":"Second, the degree-of-freedom (DOF) of the deployable unit and the antenna mechanism are analyzed based on structure decomposition and screw theory.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":16,"sentence":"Third, the kinematic model of the double-ring truss deployable antenna mechanism is established based on its structural characteristics.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":16,"sentence":"Finally, a typical numerical example is used to illustrate the effectiveness of the designed mechanism and the established kinematic model.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":16,"sentence":"The new double-ring truss deployable antenna mechanism consists of the units with the better structural symmetry and has simpler joint axis layouts, comparing with the same type of most existing mechanisms.","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":17,"sentence":"As a kind of imprecise probabilistic model, probability-box (P-box) model can deal with both aleatory and epistemic uncertainties in parameters effectively.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"The P-box can generally be categorized into two classes, namely, parameterized P-box and non-parameterized P-box.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":17,"sentence":"Currently, the researches involving P-boxes mainly aim at the parameterized P-box, while the works handling the non-parameterized P-box are relatively inadequate.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":17,"sentence":"This paper proposes an efficient uncertainty propagation analysis method based on cumulative distribution function discretization (CDFD) for problems with non-parameterized P-boxes, through which the bounds of statistical moments and the cumulative distribution function (CDF) of a response function with non-parameterized P-box variables can be obtained.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":17,"sentence":"First, a series of linear programming models are established for acquiring the lower and upper bounds of the first four origin moments of the response function.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":17,"sentence":"Second, based on the bounds of the origin moments, the CDF bounds for the response function can be obtained using Johnson distributions fitting and an optimization approach based on percentiles.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":17,"sentence":"Finally, the accuracy and efficiency of the proposed method are verified by investigating two numerical examples.","offset":6,"pro":0.8571428571428571,"labels":"MTD"},{"idx":18,"sentence":"Designers often gather information, for instance through stakeholder or domain expert meetings, to understand their design problems and develop effective solutions.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"However, few previous studies have provided in-depth descriptions of novice engineering designers' approaches to conducting information gathering meetings.","offset":1,"pro":0.1,"labels":"GAP"},{"idx":18,"sentence":"In this preliminary study, we analyzed data from six capstone mechanical engineering design teams to identify the types of individuals from whom teams gathered information, when these meetings occurred, and how teams solicited information during meetings.","offset":2,"pro":0.2,"labels":"PUR"},{"idx":18,"sentence":"Teams in our study exhibited a range of information gathering behaviors that aligned with recommended practices, particularly in their early meetings.","offset":3,"pro":0.3,"labels":"MTD"},{"idx":18,"sentence":"We also observed relatively few instances of teams exhibiting behaviors that were less similar to recommended practices during their meetings.","offset":4,"pro":0.4,"labels":"RST"},{"idx":18,"sentence":"However, our findings revealed two key trends across teams that represented specific opportunities for improvement and that may reflect characteristic novice approaches to conducting information gathering meetings.","offset":5,"pro":0.5,"labels":"CLN"},{"idx":18,"sentence":"First, teams explored domain experts' perspectives in depth during meetings and met with additional domain experts to inform their projects.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":18,"sentence":"Teams' meetings with project partners contained few instances of deep exploratory information gathering behaviors in comparison.","offset":7,"pro":0.7,"labels":"RST"},{"idx":18,"sentence":"In addition, teams seemed to finalize design decisions during early design meetings and were less likely to conduct information gathering meetings during later design phases.","offset":8,"pro":0.8,"labels":"CLN"},{"idx":18,"sentence":"The comprehensive descriptions of novice mechanical engineering designers' approaches provided in our preliminary study provide an entry point for further investigations that can inform engineering training, tools, and pedagogy for conducting effective meetings.","offset":9,"pro":0.9,"labels":"IMP"},{"idx":19,"sentence":"This article presents a kinematic analysis and modification of a wrist mechanism of the DLR robot arm, which is based on antiparallelogram linkages.","offset":0,"pro":0,"labels":"PUR"},{"idx":19,"sentence":"This mechanism is modified to improve the range of motion (ROM), to reduce the parasitic motion, and to approximately perform the decoupled output motion.","offset":1,"pro":0.14285714285714285,"labels":"MTD"},{"idx":19,"sentence":"For these purposes, the elliptical rolling motion of an overconstrained antiparallelogram is first investigated in consideration of its structural modification.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":19,"sentence":"Also, a specific joint that has a relatively small movement is developed as a flexible hinge by further minimizing its angular displacement for design simplification.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":19,"sentence":"The axode analysis of the instantaneous screw axis for wrist movements is conducted to compare the rotational performance between the original and modified mechanisms.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":19,"sentence":"Moreover, their workspace qualities are evaluated through analyses of the workspace and the kinematic isotropy index.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":19,"sentence":"Finally, the improved DLR wrist of the final modification is prototyped, and its wide circumduction is demonstrated.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":0,"sentence":"This paper introduces a new technique, called state-parameterized nonlinear programming control (sp-NLPC), for designing a feedback controller that can stabilize intrinsically unstable nonlinear dynamical systems using parametric optimization.","offset":0,"pro":0,"labels":"PUR"},{"idx":0,"sentence":"The optimal control policy is approximated from P3GA output using radial basis function (RBF) metamodeling.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":0,"sentence":"The sp-NLPC technique requires fewer assumptions and is more data-efficient than alternative methods.","offset":2,"pro":0.2,"labels":"MTD"},{"idx":0,"sentence":"Two nonlinear systems (single and double inverted pendulums on a cart) are used as benchmarking problems.","offset":3,"pro":0.3,"labels":"MTD"},{"idx":0,"sentence":"Performance and computational efficiency are compared to several competing control design techniques.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":0,"sentence":"Results show that sp-NLPC outperforms and is more efficient than competing methods.","offset":5,"pro":0.5,"labels":"RST"},{"idx":0,"sentence":"A qualitative investigation on phase plane analysis for the controlled systems is presented to ensure stability.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":0,"sentence":"The approximating state-dependent solution for the control input lends itself to applications of control design for control co-design (CCD).","offset":7,"pro":0.7,"labels":"CLN"},{"idx":0,"sentence":"Such extensions are discussed as part of future work.","offset":8,"pro":0.8,"labels":"IMP"},{"idx":0,"sentence":"Stability-preserving constraints are included in the optimization problem solved offline by the predictive parameterized Pareto genetic algorithm (P3GA), a constrained nonlinear parametric optimization algorithm.","offset":9,"pro":0.9,"labels":"BAC"},{"idx":1,"sentence":"Product personalization will play a key role in the future of society by making these products available for everyone, everywhere.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"Personalized production requires the involvement of the customer in the design process.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":1,"sentence":"Thus, tools to identify which components and modules allow for customer interaction are needed, and to further assess the effects of customer interaction on the product design and the manufacturing system.","offset":2,"pro":0.2,"labels":"BAC"},{"idx":1,"sentence":"In this article, we propose a framework to analyze the extent of personalization a designer and a manufacturing firm can achieve in the context of efficient personalized production.","offset":3,"pro":0.3,"labels":"PUR"},{"idx":1,"sentence":"We study this as a dyad of product design and manufacturing system, which uses two main attributes: product modularity and manufacturing configuration complexity.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":1,"sentence":"To demonstrate our ideas, we use two applications.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":1,"sentence":"The first is a case study for gearbox, usually manufactured at high levels of efficiency under the mass production paradigm.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":1,"sentence":"The second case is an application for the footwear industry for the particular case of a sneaker, a sports shoe that is prone to be customized.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":1,"sentence":"These case studies show the flexibility of this framework to a wide set of industries.","offset":8,"pro":0.8,"labels":"MTD"},{"idx":1,"sentence":"In both cases, we compare a personalization scenario with a baseline scenario and provide managerial insights.","offset":9,"pro":0.9,"labels":"MTD"},{"idx":2,"sentence":"In this paper, a novel four degrees-of-freedom (4DOF) articulated parallel forceps mechanism with a large orientation workspace (+/- 90 deg in pitch and yaw, 360 deg in roll rotations) is presented for robotic minimally invasive surgery.","offset":0,"pro":0,"labels":"PUR"},{"idx":2,"sentence":"The proposed 3RSR-IUUP parallel mechanism utilizes a UUP center leg that can convert thrust motion of the 3RSR mechanism into gripping motion.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":2,"sentence":"This design eliminates the need for an additional gripper actuator, but also introduces the problem of unintentional gripper opening/ closing due to parasitic motion of the 3RSR mechanism.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":2,"sentence":"Here, position kinematics of the proposed mechanism, including the workspace, is analyzed in detail, and a solution to the parasitic motion problem is provided.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":2,"sentence":"Human-in-the-loop simulations with a haptic interface are also performed to confirm the feasibility of the proposed design.","offset":4,"pro":0.8,"labels":"MTD"},{"idx":3,"sentence":"We enhance the Bayesian optimization (BO) approach for simulation-based design of engineering systems consisting of multiple interconnected expensive simulation models.","offset":0,"pro":0,"labels":"PUR"},{"idx":3,"sentence":"The goal is to find the global optimum design with minimal model evaluation costs.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":3,"sentence":"A commonly used approach is to treat the whole system as a single expensive model and apply an existing BO algorithm.","offset":2,"pro":0.2857142857142857,"labels":"BAC"},{"idx":3,"sentence":"This approach is inefficient due to the need to evaluate all the component models in each iteration.","offset":3,"pro":0.42857142857142855,"labels":"GAP"},{"idx":3,"sentence":"We propose a multi-model BO approach that dynamically and selectively evaluates one component model per iteration based on the uncertainty quantification of linked emulators (metamodels) and the knowledge gradient of system response as the acquisition function.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":3,"sentence":"Building on our basic formulation, we further solve problems with constraints and feedback couplings that often occur in real complex engineering design by penalizing the objective emulator and reformulating the original problem into a decoupled one.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":3,"sentence":"The superior efficiency of our approach is demonstrated through solving two analytical problems and the design optimization of a multidisciplinary electronic packaging system.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":4,"sentence":"Topology optimization has been intensively studied and extensively applied in engineering design.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"However, the optimized results often take the form of a solid frame structure; hence, it is difficult to apply the topological results in the design of a thin-walled frame structure.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":4,"sentence":"Therefore, this paper proposes a novel bridging method to transform the topological results into a lightweight thin-walled frame structure while satisfying the stiffness and manufacturing requirements.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":4,"sentence":"First, the optimized topological results are obtained using the classical topology optimization method, which is smoothed to reduce structural complexity.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":4,"sentence":"Then, the initial thin-walled frame structure is created by referring to the smoothed topological results, in which the thin-walled cross section is designed according to the mechanical properties and manufacturing requirements.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":4,"sentence":"Furthermore, the size and shape of the thin-walled frame structure is optimized to minimize mass with the stiffness and manufacturing constraints.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":4,"sentence":"Finally, numerical examples demonstrate that the proposed method can reasonably design an optimized thin-walled frame structure from the topological results.","offset":6,"pro":0.8571428571428571,"labels":"RST"},{"idx":5,"sentence":"We present a resource for designing bistable developable mechanisms (BDMs) that reach their second stable positions while exterior or interior to a cylindrical surface.","offset":0,"pro":0,"labels":"PUR"},{"idx":5,"sentence":"Analysis of the necessary conditions to create extramobile and intramobile cylindrical BDMs is conducted through a series of three tests.","offset":1,"pro":0.14285714285714285,"labels":"MTD"},{"idx":5,"sentence":"These tests contain elements of both existing and new mechanism design tools, including a novel graphical method for identifying stable positions of linkages using a single dominant torsional spring, called the principle of reflection.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":5,"sentence":"These tests are applied to all possible mechanism cases and configurations to identify why certain configurations will always, sometimes, or never be a BDM.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":5,"sentence":"Two tables summarize these results as a guide when designing extramobile and intramobile BDMs.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":5,"sentence":"The results are compared and demonstrated with a numerical simulation of 30,000+ mechanisms, including several example mechanisms that illustrate the concepts discussed in the work.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":5,"sentence":"Discussion is then provided on the implication of these results.","offset":6,"pro":0.8571428571428571,"labels":"MTD"},{"idx":6,"sentence":"The authors report on the design, configure, and test of isotropic accelerometer strapdowns for high-precision inertial measurement unit (IMU) and folded MEMS configuration.","offset":0,"pro":0,"labels":"PUR"},{"idx":6,"sentence":"The biaxial low-g MEMS accelerometers are based on the Platonic solids.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":6,"sentence":"A Platonic strapdown is integrated into an embedded system for acceleration-array signal acquisition targeting rigid-body pose-and-twist estimation.","offset":2,"pro":0.4,"labels":"MTD"},{"idx":6,"sentence":"The electromechanical properties for dynamic sensitivity are tested on a haptic manipulator, which shows that the position estimation matches reasonably well the encoder readouts.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":6,"sentence":"The Platonic strapdown is promising in folded MEMS IMU with chip-level miniaturization and high estimation precision.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":7,"sentence":"Evaluating the social impacts of engineered products is critical to ensuring that products are having their intended positive impacts and learning how to improve product designs for a more positive social impact.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"Quantitative evaluation of product social impacts is made possible through the use of social impact indicators, which combine the user data in a meaningful way to give insight into the current social condition of an individual or population.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":7,"sentence":"Most existing methods for collecting these user data for social impact indicators require direct human interaction with users of a product (e.g., interviews, surveys, and observational studies).","offset":2,"pro":0.25,"labels":"BAC"},{"idx":7,"sentence":"These interactions produce high-fidelity data that help indicate the product impact but only at a single snapshot in time and are typically infrequently collected due to the large human resources and cost associated with obtaining them.","offset":3,"pro":0.375,"labels":"GAP"},{"idx":7,"sentence":"In this article, a framework is proposed that outlines how low-fidelity data often obtainable using remote sensors, satellites, or digital technology can be collected and correlated with high-fidelity, infrequently collected data to enable continuous, remote monitoring of engineered products via the user data.","offset":4,"pro":0.5,"labels":"PUR"},{"idx":7,"sentence":"These user data are critical to determining current social impact indicators that can be used in a posteriori social impact evaluation.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":7,"sentence":"We illustrate an application of this framework by demonstrating how it can be used to collect data for calculating several social impact indicators related to water hand pumps in Uganda.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":7,"sentence":"Key to this example is the use of a deep learning model to correlate user type (man, woman, or child statured) with the raw hand pump data obtained via an integrated motion unit sensor for 1200 hand pump users.","offset":7,"pro":0.875,"labels":"MTD"},{"idx":8,"sentence":"Long-lived systems are likely to experience many independent modifications during their lifecycles.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"Prior literature provides tools for predicting how a change in a fixed system is likely to propagate, but these tools do not address change propagation across multiple, independent modifications.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":8,"sentence":"The phenomenon of a modification consuming excess, thereby increasing the likelihood of change propagation in future modifications, is studied in this work as dynamic change probabilities (DCP).","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":8,"sentence":"This research builds on change propagation techniques, network theory, and excess to provide high-level guidance about how DCP may alter change propagation within a system over time.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":8,"sentence":"A sample of existing and synthetic systems are explored, as we show that the rate of change likelihood increase following a modification depends on the number of components (nodes), the dependencies between components (edges), and initial change propagation probability values (edge weights).","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":8,"sentence":"Results also show that excess placement in specific components, and the presence of system hubs (high-degree components), can mitigate the impact of excess consumption when multiple system modifications are made over time.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":9,"sentence":"All-electric geostationary orbit (GEO) satellite systems design is a challenging multidisciplinary design optimization (MDO) problem, which is computation -intensive due to the employment of expensive simulations.","offset":0,"pro":0,"labels":"GAP"},{"idx":9,"sentence":"In this paper, the all-electric GEO satellite MDO problem with multifidelity models is investigated.","offset":1,"pro":0.125,"labels":"PUR"},{"idx":9,"sentence":"The MDO problem involving six intercoupled disciplines is formulated to minimize the total mass of the satellite system subject to a number of engineering constraints.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":9,"sentence":"To reduce the computational cost of the multidisciplinary analysis (MDA) process, multifidelity transfer dynamics models and finite element analysis (FEA) models are developed for the geosynchronous transfer orbit (GTO) and structure disciplines, respectively.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":9,"sentence":"To effectively solve the all-electric GEO satellite MDO problem using multi-fidelity models, an adaptive Co-Kriging-based optimization framework is proposed.","offset":4,"pro":0.5,"labels":"PUR"},{"idx":9,"sentence":"In this framework, the samples from a high-fidelity MDA process are integrated with those from a low -fidelity MDA process to create a Co-Kriging metamodel with a moderate computational cost for optimization.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":9,"sentence":"Besides, for refining the Co-Kriging metamodels, a multi-objective adaptive infill sampling approach is developed to produce the infill sample points in terms of the expected improvement (El) and the probability offeasibility (PF) functions.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":9,"sentence":"Optimization results show that the proposed optimization framework can significantly reduce the total mass of satellite system with a limited computational budget, which demonstrates the effectiveness and practicality of the multifidelity modeling and adaptive Co-Kriging-based optimization framework for all-electric GEO satellite systems design.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":10,"sentence":"This paper presents a surrogate model-based computationally efficient optimization scheme for design problems with multiple, probabilistic objectives estimated through stochastic simulation.","offset":0,"pro":0,"labels":"PUR"},{"idx":10,"sentence":"It examines the extension of the previously developed MODU-AIM (Multi Objective Design under Uncertainty with Augmented Input Metamodels) algorithm, which performs well for bi-objective problem but encounters scalability difficulties for applications with more than two objectives.","offset":1,"pro":0.14285714285714285,"labels":"MTD"},{"idx":10,"sentence":"Computational efficiency is achieved by using a single surrogate model, adaptively refined within an iterative optimization setting, to simultaneously support the uncertainty quantification and the design optimization, and the MODU-AIM extension is established by replacing the originally used epsilon -constraint optimizer with a multi-objective evolutionary algorithm (MOEA).","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":10,"sentence":"This requires various modifications to accommodate MOEA's unique traits.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":10,"sentence":"For uncertainty quantification, a clustering -based importance sampling density selection is introduced to mitigate MOEA's lack of direct control on Pareto solution density.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":10,"sentence":"To address the potentially large solution set of MOEAs, both the termination criterion of the iterative optimization scheme and the design of experiment (DoE) strategy for refinement of the surrogate model are modified, leveraging efficient performance comparison indicators.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":10,"sentence":"The importance of each objective in the different parts of the Pareto front is further integrated in the DoE to improve the adaptive selection of experiments.","offset":6,"pro":0.8571428571428571,"labels":"MTD"},{"idx":11,"sentence":"In this paper, a novel 3-UPU (P and U stand for prismatic and universal joints, respectively) parallel mechanism (PM) and its variant PM are proposed.","offset":0,"pro":0,"labels":"PUR"},{"idx":11,"sentence":"Both of them have two rotational and one translational (2R1T) degrees offreedom (D0Fs) without involving any parasitic motion.","offset":1,"pro":0.1,"labels":"MTD"},{"idx":11,"sentence":"Mobility analysis shows that the three constraint forces provided by three limbs of the mechanism are located on the same plane and the mobile platform can translate perpendicular to this plane and rotate around any axis on it.","offset":2,"pro":0.2,"labels":"RST"},{"idx":11,"sentence":"Analysis of the mechanism's motion characteristics demonstrates that the mobile platform outputs either pure rotation or pure translation.","offset":3,"pro":0.3,"labels":"RST"},{"idx":11,"sentence":"Moreover, the rotational axis can be fixed during the rotation process, which means no parasitic motion is involved.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":11,"sentence":"The causes of the motion characteristics are analyzed by the combination of an overall Jacobian matrix, a statistical method, and a geometric method.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":11,"sentence":"The PMs only need to translate or rotate once to move from the initial configuration to the final configuration, which allows for easy control of speeds.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":11,"sentence":"The relationship between mechanism parameters and singularity is analyzed.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":11,"sentence":"A speed control methodfor the PMs is proposed and a prototype is designed and made.","offset":8,"pro":0.8,"labels":"MTD"},{"idx":11,"sentence":"Experiments are conducted to verify the determined motion characteristics, the speed control method, and the singularity analysis.","offset":9,"pro":0.9,"labels":"MTD"},{"idx":12,"sentence":"Three-dimensional spatial packaging of interconnected systems with physical interactions (SPI2) design plays a vital role in the functionality, operation, energy usage, and life cycle of practically all engineered systems, from chips to ships.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"SPI2 design problems are highly nonlinear, involving tightly constrained component placement, governed by coupled physical phenomena (thermal, hydraulic, electromagnetic, etc.), and involve energy and material transfer through intricate geometric interconnects.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":12,"sentence":"While many aspects of engineering system design have advanced rapidly in the last few decades through breakthroughs in computational support, SPI2 design has largely resisted automation and in practice requires at least some human-executed design steps.","offset":2,"pro":0.25,"labels":"BAC"},{"idx":12,"sentence":"SPI2 system reasoning and design decisions can quickly exceed human cognitive abilities at even moderate complexity levels, thwarting efforts to accelerate design cycles and tackle increasingly complex systems.","offset":3,"pro":0.375,"labels":"BAC"},{"idx":12,"sentence":"Existing design methods treat pieces of the SPI2 problem separately without a fundamental systems approach, are sometimes inefficient to evaluate various possible designs, and present barriers to effective adoption in practice.","offset":4,"pro":0.5,"labels":"GAP"},{"idx":12,"sentence":"This article explores a vision of a holistic SPI2 design approach needed to develop next-generation automated design methods capable of rapidly producing viable SPI2 design candidates.","offset":5,"pro":0.625,"labels":"PUR"},{"idx":12,"sentence":"We review several technical domains related to holistic SPI2 design, discuss existing knowledge gaps and practical challenges, examine exciting opportunities at the intersection of multiple domains that can enable comprehensive exploration of SPI2 design spaces, and present one viable two-stage SPI2 design automation framework.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":12,"sentence":"Holistic SPI2 design opens up a new direction of high industrial and societal relevance for the design research community.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":13,"sentence":"Including resilience in an overall systems optimization process is challenging because the space of hazard-mitigating features is complex, involving both inherent and active prevention and recovery measures.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"Many resilience optimization approaches have thus been put forward to optimize a system's resilience while systematically managing these complexities.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":13,"sentence":"However, there has been little study about when to apply or how to adapt architectures (or their underlying decomposition strategies) to new problems, which may be formulated differently.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":13,"sentence":"To resolve this problem, this article first reviews the literature to understand how choice of optimization architecture flows out of problem type and, based on this review, creates a conceptual framework for understanding these architectures in terms of their underlying decomposition strategies.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":13,"sentence":"To then better understand the applicability of alternating and bilevel decomposition strategies for resilience optimization, their performance is compared over two demonstration problems.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":13,"sentence":"These comparisons show that while both strategies can solve resilience optimization problem effectively, the alternating strategy is prone to adverse coupling relationships between design and resilience models, while the bilevel strategy is prone to increased computational costs from the use of gradient-based methods in the upper level.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":13,"sentence":"Thus, when considering how to solve a novel resilience optimization problem, the choice of decomposition strategy should flow out of problem coupling and efficiency characteristics.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":14,"sentence":"The usually high eigenfrequencies of miniaturized oscillators can be significantly lowered by reducing the stiffness through stiffness compensation.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"In this work, a mechanical design for a compliant ortho-planar mechanism is proposed in which the stiffness is compensated to such a degree that it can be identified as statically balanced.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":14,"sentence":"The mechanism was fabricated using laser micro-machining and subsequently preloaded through packaging.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":14,"sentence":"The statically balanced property of the mechanism was experimentally validated by a measurement of the force-deflection relation.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":14,"sentence":"A piezoelectric version of the design was fabricated for the purpose of energy harvesting from low-frequency motion.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":14,"sentence":"For a sub 1 Hz excitation, the device demonstrated an average power output of 21.7 mu W and an efficiency that compares favorably to piezoelectric energy harvesters reported in the literature.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":14,"sentence":"Therefore, it was found that stiffness compensation is a promising method for the design of piezoelectric energy harvesters for low-frequency motions.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":15,"sentence":"Additive manufacturing (AM) is used to produce load-bearing, safety-critical components in industries like aerospace, automotive, and medical devices.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"Designers can create AM components with complex internal features, organic topologies, and lattice structures to reduce part mass or part count.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":15,"sentence":"However, such complex features can make designs difficult or impossible to inspect using mature nondestructive testing (NDT) methods.","offset":2,"pro":0.2,"labels":"GAP"},{"idx":15,"sentence":"Professional organizations suggest designers keep quality assurance and quality control (QA/QC) in mind early in the design process.","offset":3,"pro":0.3,"labels":"BAC"},{"idx":15,"sentence":"The Design for Inspectability (DfI) framework is suggested as a way of meeting the need for early-stage QA/QC considerations.","offset":4,"pro":0.4,"labels":"BAC"},{"idx":15,"sentence":"This work presents a case study, where a group of designers considered one type of NDT, known as Pulse-Echo Ultrasonic (PEU) testing.","offset":5,"pro":0.5,"labels":"PUR"},{"idx":15,"sentence":"Using heuristics derived from relevant literature, designers were able to create designs with increased inspectability.","offset":6,"pro":0.6,"labels":"RST"},{"idx":15,"sentence":"This improved inspectability came at the cost of other design objectives, however, such as strength and mass.","offset":7,"pro":0.7,"labels":"RST"},{"idx":15,"sentence":"This implies that certain design objectives may be inversely related to increased inspectability, raising significant concerns for the field.","offset":8,"pro":0.8,"labels":"RST"},{"idx":15,"sentence":"This work marks the first step toward mapping out the trade-offs between inspection and performance objectives.","offset":9,"pro":0.9,"labels":"CTN"},{"idx":16,"sentence":"Compositionally graded alloys, a subclass of functionally graded materials (FGMs), utilize localized variations in composition with a single metal part to achieve higher performance than traditional single material parts.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"In previous work [Kirk, T., Galvan, E., Malak, R., and Arroyave, R., 2018, Computational Design of Gradient Paths in Additively Manufactured Functionally Graded Materials,'} J. Mech.","offset":1,"pro":0.08333333333333333,"labels":"GAP"},{"idx":16,"sentence":"Des., 140, p.","offset":2,"pro":0.16666666666666666,"labels":"GAP"},{"idx":16,"sentence":"111410.","offset":3,"pro":0.25,"labels":"GAP"},{"idx":16,"sentence":"10.1115/1.4040816], the authors presented a computational design methodology that avoids common issues which limit a gradient alloy's feasibility, such as deleterious phases, and optimizes for performance objectives.","offset":4,"pro":0.3333333333333333,"labels":"GAP"},{"idx":16,"sentence":"However, the previous methodology only samples the interior of a composition space, meaning designed gradients must include all elements in the space throughout the gradient.","offset":5,"pro":0.4166666666666667,"labels":"GAP"},{"idx":16,"sentence":"Because even small amounts of additional alloying elements can introduce new deleterious phases, this characteristic often neglects potentially simpler solutions to otherwise unsolvable problems and, consequently, discourages the addition of new elements to the state space.","offset":6,"pro":0.5,"labels":"GAP"},{"idx":16,"sentence":"The present work improves upon the previous methodology by introducing a sampling method that includes subspaces with fewer elements in the design search.","offset":7,"pro":0.5833333333333334,"labels":"PUR"},{"idx":16,"sentence":"The new method samples within an artificially expanded form of the state space and projects samples outside the true region to the nearest true subspace.","offset":8,"pro":0.6666666666666666,"labels":"MTD"},{"idx":16,"sentence":"This method is evaluated first by observing the sample distribution in each subspace of a 3D, 4D, and 5D state space.","offset":9,"pro":0.75,"labels":"MTD"},{"idx":16,"sentence":"Next, a parametric study in a synthetic 3D problem compares the performance of the new sampling scheme to the previous methodology.","offset":10,"pro":0.8333333333333334,"labels":"MTD"},{"idx":16,"sentence":"Lastly, the updated methodology is applied to design a gradient from stainless steel to equiatomic NiTi that has practical uses such as embedded shape memory actuation and for which the previous methodology fails to find a feasible path.","offset":11,"pro":0.9166666666666666,"labels":"MTD"},{"idx":17,"sentence":"Reliability-based mission planning of off-road autonomous ground vehicles (AGVs) aims to identify an optimal path under uncertain and deformable terrain environment, while satisfying specific mission mobility reliability (MMR) constraints.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"The repeated evaluation of MMR during path planning poses computational challenges for practical applications.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":17,"sentence":"This paper presents an efficient reliability-based mission planning using an outcrossing approach that has a similar computational complexity compared to deterministic mission planning.","offset":2,"pro":0.2222222222222222,"labels":"PUR"},{"idx":17,"sentence":"A Gaussian random field is employed to represent the spatially dependent uncertainty sources in the terrain environment.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":17,"sentence":"The latter are then used in conjunction with a vehicle mobility model to generate a stochastic mobility map.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":17,"sentence":"Based on the stochastic mobility map, outcrossing rate maps are generated using the outcrossing concept which is widely used in time-dependent reliability.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":17,"sentence":"Integration of the outcrossing rate map with a rapidly exploring random tree (RRT{*}) algorithm allows for efficient path planning of AGVs subject to MMR constraints.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":17,"sentence":"A reliable RRT{*} algorithm using the outcrossing approach (RRT{*}-OC) is developed to implement the proposed efficient reliability-based mission planning.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":17,"sentence":"Results of a case study with two scenarios verify the accuracy and efficiency of the proposed algorithm.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":18,"sentence":"There has been a plethora of design theory and methodology (DTM) research conducted to answer important questions centered around how ideas are developed and translated into successful products.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"Understanding this is vital because of the role creativity and innovation have in long-term economic success.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":18,"sentence":"However, most of these researches focused on U.S. samples, leaving to question if differences exist across cultural borders.","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":18,"sentence":"Answering this question is key to support a successful global economy.","offset":3,"pro":0.3333333333333333,"labels":"GAP"},{"idx":18,"sentence":"The current work provides a first step at answering this question by examining similarities and differences in generating concepts and screening practices between students in an emerging market, Morocco, and those in a more established market, the U.S., during a design thinking workshop.","offset":4,"pro":0.4444444444444444,"labels":"PUR"},{"idx":18,"sentence":"Our results show that while students in the U.S. sample produced more ideas than the Moroccan sample, there was no difference in the perceived quality of ideas generated (idea goodness).","offset":5,"pro":0.5555555555555556,"labels":"RST"},{"idx":18,"sentence":"In addition, while U.S. women were found to produce more ideas than U.S. men, there were no gender effects for students in the Moroccan sample.","offset":6,"pro":0.6666666666666666,"labels":"RST"},{"idx":18,"sentence":"Finally, the results show that ideas with low goodness had a higher probability of passing concept screening if it was evaluated by its owner regardless of the population studied-identifying the potential impact of ownership bias across cultures.","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":18,"sentence":"As a whole, these results suggest that key aspects of design theory and methodology research may in fact translate across cultures but also identified key areas for further investigation.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":19,"sentence":"This paper introduces an extended dynamic stiffness modeling approach for concurrent kinetostatic and dynamic analyses of planar flexure-hinge mechanisms with lumped compliance.","offset":0,"pro":0,"labels":"PUR"},{"idx":19,"sentence":"First, two novel dynamic stiffness matrices are derived for two types of flexure hinge connected to rigid bodies by shifting the end node to the mass center of rigid bodies considering the geometric effect of rigid motion.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":19,"sentence":"A straightforward modeling procedure is then proposed for the whole compliant mechanism based on d'Alembert's principle by selecting the displacements at both the mass center of rigid bodies and the rest end nodes of flexure hinges as the hybrid state variables.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":19,"sentence":"With the presented method, the statics and dynamics of flexure-hinge mechanisms with irregular-shaped rigid bodies in complex serial-parallel configurations can be analyzed in a concise form.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":19,"sentence":"The presented method is compared with other theoretical models, finite element simulation, and experiments for three case studies of a bridge-type compliant mechanism, a leveraged XY precision positioning stage, and a Scott-Russell-mechanism-based XY theta flexure manipulator.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":19,"sentence":"The results reveal the easy operation and well prediction accuracy of the presented method.","offset":5,"pro":0.8333333333333334,"labels":"RST"},{"idx":0,"sentence":"Deep generative models are proven to be a useful tool for automatic design synthesis and design space exploration.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"When applied in engineering design, existing generative models face three challenges: (1) generated designs lack diversity and do not cover all areas of the design space, (2) it is difficult to explicitly improve the overall performance or quality of generated designs, and (3) existing models generally do not generate novel designs, outside the domain of the training data.","offset":1,"pro":0.25,"labels":"GAP"},{"idx":0,"sentence":"In this article, we simultaneously address these challenges by proposing a new determinantal point process-based loss function for probabilistic modeling of diversity and quality.","offset":2,"pro":0.5,"labels":"PUR"},{"idx":0,"sentence":"With this new loss function, we develop a variant of the generative adversarial network, named performance augmented diverse generative adversarial network.","offset":3,"pro":0.75,"labels":"MTD"},{"idx":1,"sentence":"System reliability is quantified by the probability that a system performs its intended function in a period of time without failures.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"This work develops a time-dependent system reliability method that is extended from the component time-dependent reliability method using the envelope method and second-order reliability method.","offset":1,"pro":0.125,"labels":"PUR"},{"idx":1,"sentence":"The proposed method is efficient and is intended for series systems with limit-state functions whose input variables include random variables and time.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":1,"sentence":"The component reliability is estimated by the second-order component reliability method with an improve envelope approach, which produces a component reliability index.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":1,"sentence":"The covariance between component responses is estimated with the first-order approximations, which are available from the second-order approximations of the component reliability analysis.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":1,"sentence":"Then, the joint distribution of all the component responses is approximated by a multivariate normal distribution with its mean vector being component reliability indexes and covariance being those between component responses.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":1,"sentence":"The proposed method is demonstrated and evaluated by three examples.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":1,"sentence":"System reliability can be predicted if all the limit-state functions of the components of the system are available, and such a prediction is usually time consuming. ","offset":7,"pro":0.875,"labels":"GAP"},{"idx":2,"sentence":"A number of risk and resilience-based design methods have been put forward over the years that seek to provide designers the tools to reduce the effects of potential hazards in the early design phase.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"However, because of the associated high level of uncertainty and low-fidelity design representations, one might justifiably wonder if using a resilient design process in the early design phase will reliably produce useful results that would improve the realized design.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":2,"sentence":"This paper provides a testing framework for design processes that determines the validity of the process by quantifying the epistemic uncertainty in the assumptions used to make decisions.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":2,"sentence":"This framework uses this quantified uncertainty to test whether three metrics are within desirable bounds: the change in the design when uncertainty is considered, the increase in the expected value of the design, and the cost of choice-related uncertainty.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":2,"sentence":"This approach is illustrated using two examples to demonstrate how both discrete and continuous parametric uncertainty can be considered in the testing procedure.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":2,"sentence":"These examples show that early design process validity is sensitive to the level of uncertainty and magnitude of design changes, suggesting that while there is a justifiable decision-theoretic case to consider high-level, high-impact design changes during the early design phase, there is less of a case to choose between relatively similar design options because the cost of making the choice under high uncertainty is greater than the expected value improvement from choosing the better design.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":3,"sentence":"Increasingly complex engineering design challenges requires the diversification of knowledge required on design teams.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"In the context of open innovation, positioning key members within these teams or groups based on their estimated abilities leads to more impactful results since mass collaboration is fundamentally a sociotechnical system.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":3,"sentence":"Determining how each individual influences the overall design process requires an understanding of the predicted mapping between their technical competency and performance.","offset":2,"pro":0.2222222222222222,"labels":"BAC"},{"idx":3,"sentence":"This work explores this relationship through the use of predictive models composed of various algorithms.","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":3,"sentence":"With support of a dataset composed of documents related to the design performance of students working on their capstone design project in combination with textual descriptors representing individual technical aptitudes, correlations are explored as a method to predict overall project development performance.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":3,"sentence":"Each technical competency and project is represented as a distribution of topic knowledge to produce the performance metrics, which are referred to as topic competencies, since topic representations increase the ability to decompose and identify human-centric performance measures.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":3,"sentence":"Three methods of topic identification and five prediction models are compared based on their prediction accuracy.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":3,"sentence":"From this analysis, it is found that representing input variables as topics distributions and the resulting performance as a single indicator while using support vector regression provided the most accurate mapping between ability and performance.","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":3,"sentence":"With these findings, complex open innovation projects will benefit from increased knowledge of individual ability and how that correlates to their predicted performances.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":4,"sentence":"Short life cycle products are frequently replaced and discarded, even though they are resource-intensive products.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"Technological advances and rapid changes in demand have led manufacturers to develop their innovative next-generation products quickly, which not only enables multiple generations to coexist in the market but also speeds up the technological obsolescence of products.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":4,"sentence":"The low utilization rate of EoL products causes serious environmental problems such as e-waste and waste of natural resources.","offset":2,"pro":0.2,"labels":"GAP"},{"idx":4,"sentence":"To deal with the conflict between the technical evolution of products and the promotion of social benefits in solving environmental problems, this paper focuses on the impact of generational commonality effects on the overall production process including manufacturing and remanufacturing.","offset":3,"pro":0.3,"labels":"PUR"},{"idx":4,"sentence":"Diversity of collected end-of-life (EoL) and rapid technological obsolescence make the effective recovery of EoL products difficult. ","offset":4,"pro":0.4,"labels":"GAP"},{"idx":4,"sentence":"Generational commonality leads to an increase in the efficiency of manufacturing due to reducing related costs. ","offset":5,"pro":0.5,"labels":"PUR"},{"idx":4,"sentence":"Additionally, from the remanufacturing perspective, the interchangeability between generations can help collect the EoL products needed for remanufacturing. ","offset":6,"pro":0.6,"labels":"PUR"},{"idx":4,"sentence":"On the other hand, it causes a weakening of the level of performance and technology evolution between generations that significantly affect the demand for short life cycle products. ","offset":7,"pro":0.7,"labels":"PUR"},{"idx":4,"sentence":"Therefore, this study identifies these trade-offs of generational commonality levels in both manufacturing and remanufacturing based on a quantitative approach. ","offset":8,"pro":0.8,"labels":"MTD"},{"idx":4,"sentence":"This study finds how different pricing strategies, production plans, and recovery costs are based on the designs of a new generation with a different degree of generational commonality. ","offset":9,"pro":0.9,"labels":"RST"},{"idx":5,"sentence":"Prior to the development of sophisticated computer numerical control (CNC), both face milling (FM) and face hobbing (FH), the two most popular technologies for bevel gear production, required cradle-type machines with diverse and complicated mechanisms.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"In the last two decades, however, the gear industry has replaced these traditional machines with six-axis CNC bevel gear cutting machines that have superior efficiency and accuracy.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":5,"sentence":"One such machine is a vertical six-axis machine with a vertical spindle arrangement, which offers two industrially proven advantages: compact design and maximum machine stiffness.","offset":2,"pro":0.25,"labels":"BAC"},{"idx":5,"sentence":"The model also reduces manufacturing errors by applying an effective flank correction method based on a sensitivity analysis of how slight variations in the individual machine setting coefficients affect tooth geometry.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":5,"sentence":"We prove the model's efficacy by first using the proposed equations to derive the nonlinear coordinates for pinion and gear production and then conducting several cutting experiments on the gear and its correction.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":5,"sentence":"Although the numerical illustration used for this verification is based only on FM bevel gears produced by an SGDH cutting system, the model is, in fact, applicable in the production of both FM and FH bevel gears.","offset":5,"pro":0.625,"labels":"CLN"},{"idx":5,"sentence":"The technical details of this machine, however, remain undisclosed;","offset":6,"pro":0.75,"labels":"GAP"},{"idx":5,"sentence":"so, this paper proposes a mathematical model that uses inverse kinematics to derive the vertical machine's nonlinear six-axis coordinates from those of a traditional machine.","offset":7,"pro":0.875,"labels":"PUR"},{"idx":6,"sentence":"This study explores the value of visualizing the prototyping activities in a new product development process from idea to production.","offset":0,"pro":0,"labels":"PUR"},{"idx":6,"sentence":"Through a case study of a hardware startup, we present a retrospective and longitudinal study of their prototyping processes, from early idea to the introduction of several product generations to market.","offset":1,"pro":0.125,"labels":"MTD"},{"idx":6,"sentence":"We call the visualization technique ProtoMapping, and we use the qualitative and quantitative data captured by the ProtoMap to understand how prototyping strategies change over the course of product development processes.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":6,"sentence":"Specifically, we focus on the prototyping of parallel concepts, iterations of concepts, manufacturing processes used for prototypes, prototype media, prototype tests, as well as prototyping of isolated or integrated systems.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":6,"sentence":"On the basis of this longitudinal analysis, we present a number of insights that were possible to uncover with ProtoMapping.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":6,"sentence":"We observe how parallel prototypes of isolated systems can be used to explore the solution space and later be synthesized into prototypes for integrated systems.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":6,"sentence":"Furthermore, we understand how the need to scale up production can lead to increased prototyping activities, as several parts of a product must be redesigned.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":6,"sentence":"This study illustrates the utility of a retrospective visualization approach and serves as the first step in formulation of generalized guidelines for prototyping strategies in holistic product development.","offset":7,"pro":0.875,"labels":"CTN"},{"idx":7,"sentence":"This paper introduces the equations of motion of modular 2D snake robots moving in vertical plane employing Series Elastic Actuators (SEAs).","offset":0,"pro":0,"labels":"PUR"},{"idx":7,"sentence":"The kinematics of such 2D modular snake robot is presented in an efficient matrix form and Euler-Lagrange equations are constructed to model the robot.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":7,"sentence":"Moreover, using a spring-damper contact model, external contact forces, necessary for modeling pedal wave motion (undulation in the vertical plane) are taken into account, which unlike existing methods can be used to model the effect of multiple contact points.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":7,"sentence":"Using such a contact model, pedal wave motion of the robot is simulated and the torque signal measured by the elastic element from the simulation and experimentation are used to show the validity of the model.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":7,"sentence":"Moreover, pedal wave locomotion of such robot on uneven terrain is also modeled and an adaptive controller based on torque feedback in gait parameter's space with optimized control gain is proposed.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":7,"sentence":"The simulation and experimentation results showed the efficacy of the proposed controller as the robot successfully climbed over a stair-type obstacle without any prior knowledge about its location with at least 24.8\\% higher speed compared with non-adaptive motion.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":8,"sentence":"Three-dimensional printing systems have expanded the access to low cost, rapid methods for attaining physical prototypes or products.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"However, a cyber attack, system error, or operator error on a 3D-printing system may result in catastrophic situations, ranging from complete product failure, to small types of defects which weaken the structural integrity of the product.","offset":1,"pro":0.1,"labels":"GAP"},{"idx":8,"sentence":"Such defects can be introduced early-on via solid models or through G-codes for printer movements at a later stage.","offset":2,"pro":0.2,"labels":"BAC"},{"idx":8,"sentence":"Previous works have studied the use of image classifiers to predict defects in real-time and offline.","offset":3,"pro":0.3,"labels":"BAC"},{"idx":8,"sentence":"However, a major restriction in the functionality of these methods is the availability of a dataset capturing diverse attacks on printed entities or the printing process.","offset":4,"pro":0.4,"labels":"GAP"},{"idx":8,"sentence":"This paper introduces an image processing technique that analyzes the amplitude and phase variations of the print head platform arising through induced system manipulations.","offset":5,"pro":0.5,"labels":"PUR"},{"idx":8,"sentence":"The method uses an image sequence of the printing process to perform an offline spatio-temporal video decomposition to amplify changes attributable to a change in system parameters.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":8,"sentence":"The authors hypothesize that a change in the amplitude envelope and instantaneous phase response as a result of a change in the end-effector translational instructions to be correlated with an AM system compromise.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":8,"sentence":"Two case studies are presented, one verifies the hypothesis with statistical evidence in support of the method while the other studies the effectiveness of a conventional tensile test to identify system compromise.","offset":8,"pro":0.8,"labels":"MTD"},{"idx":8,"sentence":"The method has the potential to enhance the robustness of cyber-physical systems such as 3D printers.","offset":9,"pro":0.9,"labels":"CTN"},{"idx":9,"sentence":"In this work, we have developed a data-driven artificial intelligence (AI) solution to assist the ship hull design process.","offset":0,"pro":0,"labels":"PUR"},{"idx":9,"sentence":"Specifically, we have developed and implemented an AI-based multiple-input neural network model to realize the real-time prediction of the total resistance of the ship hull structure while avoiding the inconsistent estimates from different types of design input parameters.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":9,"sentence":"It is demonstrated that the developed AI-based machine learning algorithm as a prediction tool can assist the ship hull design process by accurately providing the total resistance of ship hulls in real time.","offset":2,"pro":0.3333333333333333,"labels":"CLN"},{"idx":9,"sentence":"Moreover, we have conducted design tasks to validate the proposed method, and the validation results show that a well-trained artificial neural network model can avoid the problem of different sensitivities due to the different degrees of influence of the input parameters on the output parameter.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":9,"sentence":"The proposed AI-based data-driven solution provides a real-time hydrodynamic performance calculation, which can predict the hyperdynamic performances of ship hulls based on their geometry modification parameters.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":9,"sentence":"This approach gives a consistent prediction in terms of accuracy when facing different geometry modification parameters, and it in turn provides a fast and accurate AI-based method to assist ship hull design to achieve an optimum forecast accuracy in the entire design space, making an advance to artificial intelligence assist design in naval architecture engineering.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":10,"sentence":"Systems-of-systems (SoS) often include multiple agents that interact in both cooperative and competitive modes.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"Moreover, they involve multiple resources, including energy, information, and bandwidth.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":10,"sentence":"If these resources are limited, agents need to decide how to share resources cooperatively to reach the system-level goal, while performing the tasks assigned to them autonomously.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":10,"sentence":"This paper takes a step toward addressing these challenges by proposing a dynamic two-tier learning framework, based on deep reinforcement learning that enables dynamic resource allocation while acknowledging the autonomy of systems constituents.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":10,"sentence":"The two-tier learning framework that decouples the learning process of the SoS constituents from that of the resource manager ensures that the autonomy and learning of the SoS constituents are not compromised as a result of interventions executed by the resource manager.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":10,"sentence":"We apply the proposed two-tier learning framework on a customized OpenAI Gym environment and compare the results of the proposed framework to baseline methods of resource allocation to show the superior performance of the two-tier learning scheme across a different set of SoS key parameters.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":10,"sentence":"We then use the results of this experiment and apply our heuristic inference method to interpret the decisions of the resource manager for a range of environment and agent parameters.","offset":6,"pro":0.8571428571428571,"labels":"MTD"},{"idx":11,"sentence":"Predicting system reliability is often a core task in systems design.","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"System reliability depends on component reliability and dependence of components.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":11,"sentence":"Component reliability can be predicted with a physics-based approach if the associated physical models are available.","offset":2,"pro":0.2,"labels":"BAC"},{"idx":11,"sentence":"If the models do not exist, component reliability may be estimated from data.","offset":3,"pro":0.3,"labels":"BAC"},{"idx":11,"sentence":"When both types of components coexist, their dependence is often unknown, and therefore, the component states are assumed independent by the traditional method, which can result in a large error.","offset":4,"pro":0.4,"labels":"GAP"},{"idx":11,"sentence":"This study proposes a new system reliability method to recover the missing component dependence, thereby leading to a more accurate estimate of the joint probability density function (PDF) of all the component states.","offset":5,"pro":0.5,"labels":"PUR"},{"idx":11,"sentence":"The method works for series systems whose load is shared by its components that may fail due to excessive loading.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":11,"sentence":"For components without physical models available, the load data are recorded upon failure, and equivalent physical models are created; the model parameters are estimated by the proposed Bayesian approach.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":11,"sentence":"Then models of all component states become available, and the dependence of component states, as well as their joint PDF, can be estimated.","offset":8,"pro":0.8,"labels":"MTD"},{"idx":11,"sentence":"Four examples are used to evaluate the proposed method, and the results indicate that the method can produce more accurate predictions of system reliability than the traditional method that assumes independent component states.","offset":9,"pro":0.9,"labels":"CLN"},{"idx":12,"sentence":"Requirement changes can result in substantial overdesign because of the way design margins are allocated at the beginning of the design process.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"We present a design optimization method for minimizing overdesign by making use of additive remanufacturing and recently defined constituents of design margins (buffer and excess).","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":12,"sentence":"The method can be used to obtain a set of design decisions for different changing requirement scenarios.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":12,"sentence":"We demonstrate our method by means of a turbine rear structure design problem where changes in the temperature loads are met by depositing different types of stiffeners on the outer casing.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":12,"sentence":"The results of the case study are visualized in a tradespace, which allows for comparison between sets of optimal, flexible, and robust designs.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":12,"sentence":"Results show that the optimized set of design decisions balances flexibility and robustness in a cost-effective manner.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":13,"sentence":"Industry 4.0 promises better control of the overall product development process; however, there is a lack of computational frameworks that can inject human factors engineering principles early in the design.","offset":0,"pro":0,"labels":"GAP"},{"idx":13,"sentence":"Thus, a smooth Industry 4.0 transformation requires keeping ergonomics in the loop, specifically to address the needs in the digitized prototyping process.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":13,"sentence":"In this paper, we explore a computational prototyping approach that focuses on various fidelity levels and different human-product interaction levels when conducting ergonomics assessments.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":13,"sentence":"Three computational prototyping strategies were explored, including (1) a digital sketchpad-based tool, (2) computer-aided design and digital human modeling-based approach, and (3) a combination of computer-aided design, digital human modeling, and surrogate modeling.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":13,"sentence":"These strategies are applied to six case studies to perform various ergonomics assessments (reach, vision, and lower-back).","offset":4,"pro":0.5,"labels":"MTD"},{"idx":13,"sentence":"The results from this study show that the designers need to consider the tradeoffs between the accuracy of ergonomic outcomes and resource availability when determining the fidelity level of prototypes.","offset":5,"pro":0.625,"labels":"CLN"},{"idx":13,"sentence":"Understanding the intricacies between the fidelity level, type of ergonomic assessment, and human-product interaction level helps designers in getting one step closer to digitizing human-centered prototyping and meeting Industry 4.0 objectives.","offset":6,"pro":0.75,"labels":"IMP"},{"idx":13,"sentence":"This shortage is particularly crucial for prototyping human-centered products where the stakes are high. ","offset":7,"pro":0.875,"labels":"GAP"},{"idx":14,"sentence":"Autonomous products, which perform many functions on their own with limited user input, require users to exhibit trust at an appropriate level before use.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"Research in product trust has thus far focused on the product characteristics: such as manipulating the product's design-for example, anthropomorphizing an autonomous vehicle-and measuring changes in the users' trust.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":14,"sentence":"This study flips the usual approach and instead manipulates users' mental state through priming, and then measures users' trust to an existing autonomous product, the Amazon Echo.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":14,"sentence":"While interacting with the Echo, users evaluated its performance and how well it met their expectations.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":14,"sentence":"Holistically, users' perceived performance of the Echo and age had significant effects on their trust of the product, but the affective primes showed no significant effect.","offset":4,"pro":0.5,"labels":"RST"},{"idx":14,"sentence":"However, for the subgroup of participants whose expectations of the product's performance were met: those who received either positive or negative prime were more likely to trust the product than those who saw neutral images; men were more likely to trust the product than others.","offset":5,"pro":0.625,"labels":"RST"},{"idx":14,"sentence":"The study demonstrates the importance of meeting users' expectations and highlights the potential to build trust by inducing emotions contextually.","offset":6,"pro":0.75,"labels":"CLN"},{"idx":14,"sentence":"In this study, we used visual stimuli (images) that evoked either positive or negative emotions as affective primes to influence users' trust before interacting with the Echo. ","offset":7,"pro":0.875,"labels":"MTD"},{"idx":15,"sentence":"In topology optimization using deep learning, the load and boundary conditions represented as vectors or sparse matrices often miss the opportunity to encode a rich view of the design problem, leading to less than ideal generalization results.","offset":0,"pro":0,"labels":"GAP"},{"idx":15,"sentence":"We propose a new data-driven topology optimization model called TopologyGAN that takes advantage of various physical fields computed on the original, unoptimized material domain, as inputs to the generator of a conditional generative adversarial network (cGAN).","offset":1,"pro":0.2,"labels":"PUR"},{"idx":15,"sentence":"Compared to a baseline cGAN, TopologyGAN achieves a nearly 3 x reduction in the mean squared error and a 2.5 x reduction in the mean absolute error on test problems involving previously unseen boundary conditions.","offset":2,"pro":0.4,"labels":"RST"},{"idx":15,"sentence":"Built on several existing network models, we also introduce a hybrid network called U-SE(Squeeze-and-Excitation)-ResNet for the generator that further increases the overall accuracy.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":15,"sentence":"We publicly share our full implementation and trained network.","offset":4,"pro":0.8,"labels":"CTN"},{"idx":16,"sentence":"Tolerance design is becoming increasingly important for electromechanical products.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"Reasonable tolerance design can reduce production costs and improve product performance.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":16,"sentence":"However, as the complexity of the coupling of tolerances and performance increases, it becomes difficult for designers to establish accurate tolerance design models, leading to experience-based design.","offset":2,"pro":0.2,"labels":"GAP"},{"idx":16,"sentence":"This study proposes a novel performance-oriented tolerance design method.","offset":3,"pro":0.3,"labels":"PUR"},{"idx":16,"sentence":"First, the main tolerance variables affecting the product performance are rapidly determined based on the proposed locally inferred sensitivity analysis method.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":16,"sentence":"Then, based on the improved approximate polynomial chaos expansion, a surrogate model of the product performance and main tolerance variables is established.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":16,"sentence":"Finally, the geometric tolerances of the electromechanical products are optimized based on the surrogate model with performance requirements.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":16,"sentence":"The proposed tolerance design method is computationally efficient and accurate, and it can be implemented with a small number of samples.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":16,"sentence":"To demonstrate its performance, the proposed method is validated with a spaceborne active-phased array antenna.","offset":8,"pro":0.8,"labels":"MTD"},{"idx":16,"sentence":"The optimal tolerance design of the antenna for the electrical performance requirements is performed successfully. ","offset":9,"pro":0.9,"labels":"RST"},{"idx":17,"sentence":"In a minimally invasive surgical (MIS) robot, the remote center of motion (RCM) mechanism is usually used to realize the constrained motion of the surgical instrument.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"In this paper, a novel design method for planar 2DOF RCM mechanisms is proposed based on closed-loop cable transmissions.","offset":1,"pro":0.125,"labels":"PUR"},{"idx":17,"sentence":"The concept is to utilize several coupled cable transmissions to constrain a serial kinematic chain.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":17,"sentence":"Through the analysis and determination of the transmission ratios for these cable transmissions, a class of planar 2DOF RCM mechanisms without any active or passive translational joints is obtained, which provides large workspace and low collision risk for the MIS robots.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":17,"sentence":"One of the resulting mechanisms is designed in detail and kinematically analyzed.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":17,"sentence":"To evaluate the influence of the elastic cables, a new error model for the proposed RCM mechanism is established through the static analysis and cable deformation analysis.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":17,"sentence":"Utilizing this model, the cable-induced error distributions of the tip and the RCM point are obtained, which show that these errors are within a relatively small range.","offset":6,"pro":0.75,"labels":"RST"},{"idx":17,"sentence":"Furthermore, the prototype of the proposed mechanism is built, and the accuracy experiments are conducted.","offset":7,"pro":0.875,"labels":"MTD"},{"idx":18,"sentence":"Hierarchical sensitivity analysis (HSA) of multilevel systems is to assess the effect of system's input uncertainties on the variations of system's performance through integrating the sensitivity indices of subsystems.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"However, it is difficult to deal with the engineering systems with complicated correlations among various variables across levels by using the existing hierarchical sensitivity analysis method based on variance decomposition.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":18,"sentence":"To overcome this limitation, a mapping-based hierarchical sensitivity analysis method is proposed to obtain sensitivity indices of multilevel systems with multidimensional correlations.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":18,"sentence":"For subsystems with dependent variables, a mapping-based sensitivity analysis, consisting of vine copula theory, Rosenblatt transformation, and polynomial chaos expansion (PCE) technique, is provided for obtaining the marginal sensitivity indices.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":18,"sentence":"The marginal sensitivity indices can allow us to distinguish between the mutual depend contribution and the independent contribution of an input to the response variance.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":18,"sentence":"Then, extended aggregation formulations for local variables and shared variables are developed to integrate the sensitivity indices of subsystems at each level so as to estimate the global effect of inputs on the response.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":18,"sentence":"Finally, this paper presents a computational framework that combines related techniques step by step.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":18,"sentence":"The effectiveness of the proposed mapping-based hierarchical sensitivity analysis (MHSA) method is verified by a mathematical example and a multiscale composite material.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":19,"sentence":"The design perspective of interfaces has strong implications on operator intuition and safety.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"Haptics enabled user interfaces can enhance operator skills and enhance interactivity.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":19,"sentence":"In this paper, an innovative method of haptic feedback in joysticks is presented for excavator control.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":19,"sentence":"Haptic illusion in the device is generated with the concept of the variable stiffness actuation mechanism.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":19,"sentence":"The stiffness in the device varies dynamically with the load and restricts the operator motion with a resistive torque in the range of 0-0.9 Nm.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":19,"sentence":"The user evaluation with the joystick showed an improvement of 40\\% in the volume of material removed and a significant drop in error rate related to force patterns and collisions.","offset":5,"pro":0.625,"labels":"RST"},{"idx":19,"sentence":"The force feedback (FFB) is rendered through haptic links, based on the effect of digging force at each joint. ","offset":6,"pro":0.75,"labels":"MTD"},{"idx":19,"sentence":"The haptic joystick aims to render high-fidelity kinesthetic feedback that can help to mitigate the operator error in loading operations. ","offset":7,"pro":0.875,"labels":"MTD"},{"idx":0,"sentence":"Engineers design for an inherently uncertain world.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"In the early stages of design processes, they commonly account for such uncertainty either by manually choosing a specific worst-case and multiplying uncertain parameters with safety factors or by using Monte Carlo simulations to estimate the probabilistic boundaries in which their design is feasible.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":0,"sentence":"The safety factors of this first practice are determined by industry and organizational standards, providing a limited account of uncertainty; the second practice is time intensive, requiring the development of separate testing infrastructure.","offset":2,"pro":0.25,"labels":"BAC"},{"idx":0,"sentence":"In theory, robust optimization provides an alternative, allowing set-based conceptualizations of uncertainty to be represented during model development as optimizable design parameters.","offset":3,"pro":0.375,"labels":"BAC"},{"idx":0,"sentence":"How these theoretical benefits translate to design practice has not previously been studied.","offset":4,"pro":0.5,"labels":"GAP"},{"idx":0,"sentence":"In this work, we analyzed the present use of geometric programs as design models in the aerospace industry to determine the current state-of-the-art, then conducted a human-subjects experiment to investigate how various mathematical representations of uncertainty affect design space exploration.","offset":5,"pro":0.625,"labels":"PUR"},{"idx":0,"sentence":"We found that robust optimization led to far more efficient explorations of possible designs with only small differences in an experimental participant's understanding of their model.","offset":6,"pro":0.75,"labels":"RST"},{"idx":0,"sentence":"Specifically, the Pareto frontier of a typical participant using robust optimization left less performance on the table.","offset":7,"pro":0.875,"labels":"RST"},{"idx":1,"sentence":"Networking complex sociotechnical systems into larger Systems of Systems (SoS) typically results in improved performance characteristics including sustainability, efficiency, and productivity.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"The response, or lack thereof, of many SoS to unexpected constituent system failures undermines their effectiveness in many cases.","offset":1,"pro":0.07692307692307693,"labels":"BAC"},{"idx":1,"sentence":"SoS performance after faults can be improved by improving the SoS's hard (physical design) or soft (human intervention) resilience.","offset":2,"pro":0.15384615384615385,"labels":"BAC"},{"idx":1,"sentence":"The current approaches to increase resilience are limited due to the cost and necessary of human response increasing non-linearly with SoS scale.","offset":3,"pro":0.23076923076923078,"labels":"GAP"},{"idx":1,"sentence":"The limitations of current approaches require a novel design approach to improve SoS network resilience.","offset":4,"pro":0.3076923076923077,"labels":"GAP"},{"idx":1,"sentence":"We hypothesize that biologically inspired network design can improve SoS resilience.","offset":5,"pro":0.38461538461538464,"labels":"PUR"},{"idx":1,"sentence":"To illustrate this, a systems dynamics model of a Forestry Industry is presented and an optimization search over potential hard and soft resilience approaches is compared to a biologically inspired network improvement.","offset":6,"pro":0.46153846153846156,"labels":"MTD"},{"idx":1,"sentence":"SoS network resilience is measured through the newly developed System of System Resilience Measurement (SoSRM).","offset":7,"pro":0.5384615384615384,"labels":"MTD"},{"idx":1,"sentence":"Our first result provides evidence that biologically inspired network design provides an approach to increase SoS resilience beyond hard and soft resilience improvements alone.","offset":8,"pro":0.6153846153846154,"labels":"CLN"},{"idx":1,"sentence":"Second, this work provides evidence that having a SoS constituent fulfill the ecosystem role of detrital actor increases resilience.","offset":9,"pro":0.6923076923076923,"labels":"CLN"},{"idx":1,"sentence":"Third, this paper documents the first case study using the new SoSRM metric to justify a design decision.","offset":10,"pro":0.7692307692307693,"labels":"MTD"},{"idx":1,"sentence":"Finally, this case study provides a counter-example to the theory that increased sustainability always results in increased resilience.","offset":11,"pro":0.8461538461538461,"labels":"MTD"},{"idx":1,"sentence":"By comparing biologically inspired network redesign and optimized traditional resilience improvements, this paper provides evidence that biologically inspired intervention may be the needed strategy to increase sociotechnical SoS network resilience, improve SoS performance, and overcome the limitations of traditional resilience improvement approaches.","offset":12,"pro":0.9230769230769231,"labels":"RST"},{"idx":2,"sentence":"Multi-state is a typical characteristic of engineered systems.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"Most existing studies of redundancy allocation problems (RAPs) for multi-state system (MSS) design assume that the state probabilities of redundant components are precisely known.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":2,"sentence":"However, due to lack of knowledge and/or ambiguous judgements from engineers/experts, the epistemic uncertainty associated with component states cannot be completely avoided and it is befitting to be represented as belief quantities.","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":2,"sentence":"In this paper, a multi-objective RAP is developed for MSS design under the belief function theory.","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":2,"sentence":"To address the epistemic uncertainty propagation from components to system reliability evaluation, an evidential network (EN) model is introduced to evaluate the reliability bounds of an MSS.","offset":4,"pro":0.4444444444444444,"labels":"PUR"},{"idx":2,"sentence":"The resulting multi-objective design optimization problem is resolved via a modified non-dominated sorting genetic algorithm II (NSGA-II), in which a set of new Pareto dominance criteria is put forth to compare any pair of feasible solutions under the belief function theory.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":2,"sentence":"A numerical case along with a SCADA system design is exemplified to demonstrate the efficiency of the EN model and the modified NSGA-II.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":2,"sentence":"As observed in our study, the EN model can properly handle the uncertainty propagation and achieve narrower reliability bounds than that of the existing methods.","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":2,"sentence":"More importantly, the original nested design optimization formulation can be simplified into a one-stage optimization model by the proposed method.","offset":8,"pro":0.8888888888888888,"labels":"CTN"},{"idx":3,"sentence":"This paper presents a computationally tractable approach for designing lattice structures for stiffness and strength.","offset":0,"pro":0,"labels":"PUR"},{"idx":3,"sentence":"Yielding in the mesostructure is determined by a worst-case stress analysis of the homogenization simulation data.","offset":1,"pro":0.125,"labels":"MTD"},{"idx":3,"sentence":"This provides a physically meaningful, generalizable, and conservative way to estimate structural failure in three-dimensional functionally graded lattice structures composed of any unit cell architectures.","offset":2,"pro":0.25,"labels":"CTN"},{"idx":3,"sentence":"Computational efficiency of the design framework is ensured by developing surrogate models for the unit cell stiffness and strength as a function of density.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":3,"sentence":"The surrogate models are then used in the coarse-scale analysis and synthesis.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":3,"sentence":"The proposed methodology further uses a compact representation of the material distribution via B-splines, which reduces the size of the design parameter space while ensuring a smooth density variation that is desirable for manufacturing.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":3,"sentence":"The proposed method is demonstrated in compliance with minimization studies using two types of unit cells with distinct mechanical properties.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":3,"sentence":"The effects of B-spline mesh refinement and the presence of a stress constraint on the optimization results are also investigated.","offset":7,"pro":0.875,"labels":"MTD"},{"idx":4,"sentence":"This paper proposed a new tooth surface modeling method for beveloid gear based on the real cutter surface using two orthogonal variables.","offset":0,"pro":0,"labels":"PUR"},{"idx":4,"sentence":"Then, the analytical mesh model with and without misalignments were derived and solved to study the influences of geometry design parameters on contact behaviors for paralleled beveloid gear pair.","offset":1,"pro":0.1111111111111111,"labels":"MTD"},{"idx":4,"sentence":"Loaded tooth contact analysis is used to validate the proposed mesh model by abaqus software, and the error is below 5\\%.","offset":2,"pro":0.2222222222222222,"labels":"MTD"},{"idx":4,"sentence":"Results suggest that the increase in pressure, cone, and helical angles enlarge the contact area for meshing without misalignments.","offset":3,"pro":0.3333333333333333,"labels":"CLN"},{"idx":4,"sentence":"The addendum coefficient has unsubstantial impacts on the contact behaviors.","offset":4,"pro":0.4444444444444444,"labels":"CLN"},{"idx":4,"sentence":"For meshing with axis error in the horizontal direction, the growth of pressure angle, cone angle, helical angle, and addendum coefficient improves the carrying capacity of single tooth.","offset":5,"pro":0.5555555555555556,"labels":"RST"},{"idx":4,"sentence":"But the transmission error deteriorates with the increase in pressure, cone, and helical angles.","offset":6,"pro":0.6666666666666666,"labels":"RST"},{"idx":4,"sentence":"All three types of misalignments have little influence on the size of the contact ellipse.","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":4,"sentence":"The growth of axis errors in horizontal and vertical directions significantly increases the transmission error, but the center distance error has a little influence on the transmission precision.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":5,"sentence":"Model validation methods have been widely used in engineering design to provide a quantified assessment of the agreement between simulation predictions and experimental observations.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"For the validation of simulation models with multiple correlated outputs, not only the uncertainty of the responses but also the correlation between them needs to be considered.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":5,"sentence":"Most of the existing validation methods for multiple correlated responses focus on the area metric, which only compares the overall area difference between the two cumulative probability distribution curves.","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":5,"sentence":"The differences in the distributions of the data sets are not fully utilized.","offset":3,"pro":0.3333333333333333,"labels":"GAP"},{"idx":5,"sentence":"In this paper, two covariance-overlap based model validation (COMV) methods are proposed for the validation of multiple correlated responses.","offset":4,"pro":0.4444444444444444,"labels":"PUR"},{"idx":5,"sentence":"The COMV method is used for a single validation site, while the covariance-overlap pooling based model validation (COPMV) method can pool the evidence from different validation sites into a scalar measure to give a global evaluation about the candidate model.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":5,"sentence":"The effectiveness and merits of the proposed methods are demonstrated by comparing with three different existing validation methods on three numerical examples and a practical engineering problem of a turbine blade validation example.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":5,"sentence":"The influence of sample size and the number of partitions in the proposed methods are also discussed.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":5,"sentence":"Results show that the proposed method shows better performance on the uncertainty estimation of different computational models, which is useful for practical engineering design problems with multiple correlated responses.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":6,"sentence":"Prototyping is a crucial part of new product development, and engineers and designers rely on prototyping to bring novel technologies to market.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"In recent years, tech-based startups like Tesla or Udacity have revolutionized their respective industries.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":6,"sentence":"However, many tech-based startups are unable to create a viable product with their available resources and fail before ever making it to market.","offset":2,"pro":0.3333333333333333,"labels":"GAP"},{"idx":6,"sentence":"In this work, we analyze survey responses from 34 startup representatives to investigate the relationship between prototyping practice, startup success, and perceived difficulty of startup tasks.","offset":3,"pro":0.5,"labels":"PUR"},{"idx":6,"sentence":"K-means cluster analysis shows three distinct groups, differentiated by (1) their amount of available funding, (2) their use of prototyping best practices, and (3) their reported difficulty in startup tasks.","offset":4,"pro":0.6666666666666666,"labels":"RST"},{"idx":6,"sentence":"High-performing startups reported having the highest funding, experiencing less difficulty in startup tasks, and using prototyping best practices more frequently than their peers.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":7,"sentence":"The traditional research on the dynamics of planetary gear transmission (PGT) is based on the assumption that the support is on the ground.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"However, the PGT inside the aircraft is spatially moved along with the airframe, which is not only subject to gravity, but also to additional inertia forces.","offset":1,"pro":0.08333333333333333,"labels":"GAP"},{"idx":7,"sentence":"These loads should affect the dynamic characteristics of the PGT.","offset":2,"pro":0.16666666666666666,"labels":"GAP"},{"idx":7,"sentence":"The PGT itself is a non-inertial system (NIS) and is called the internal non-inertial system (INIS).","offset":3,"pro":0.25,"labels":"BAC"},{"idx":7,"sentence":"By contrast, an airframe in the aerospace environment is named an external non-inertial system (ENIS).","offset":4,"pro":0.3333333333333333,"labels":"BAC"},{"idx":7,"sentence":"In order to investigate the dynamic behavior of the PGT in a compound NIS, the kinematic equations of various components in arbitrary spatial motion state of the airframe are deduced.","offset":5,"pro":0.4166666666666667,"labels":"PUR"},{"idx":7,"sentence":"Subsequently, the coupled dynamics model of PGT in NIS is improved.","offset":6,"pro":0.5,"labels":"MTD"},{"idx":7,"sentence":"The dynamic responses of PGT in different non-inertial conditions are compared based on the hovering motion of the airframe.","offset":7,"pro":0.5833333333333334,"labels":"MTD"},{"idx":7,"sentence":"The results indicate that INIS is the main factor affecting the trajectory of planet gear, while ENIS is the force source changing the trajectory of the central component.","offset":8,"pro":0.6666666666666666,"labels":"CLN"},{"idx":7,"sentence":"The aircraft's hovering motion makes the gravity effect become a relatively time-varying excitation, but the dominant factor is still the additional inertial forces.","offset":9,"pro":0.75,"labels":"GAP"},{"idx":7,"sentence":"The non-inertial effect during aerospace operation can significantly affect the bearing force, vibration and load sharing performance.","offset":10,"pro":0.8333333333333334,"labels":"BAC"},{"idx":7,"sentence":"It will lead to serious errors if the traditional research method is still used to obtain the dynamic behavior of PGT in the aerospace environment.","offset":11,"pro":0.9166666666666666,"labels":"GAP"},{"idx":8,"sentence":"Functional analysis aims to decompose the main functions of a product, which can be large and complex, into more manageable subfunctions.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"Recently, interest in explicitly considering users during functional modeling has grown to enhance analysis completeness, but how this addresses some commonly reported challenges for novice engineers is not clear.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":8,"sentence":"This research proposes and assesses a simple way to streamline user considerations in functional analysis as potential mitigation for some challenges (i.e., difficulty in identifying functions to explore design space and expressing the functions with expected syntax and solution neutrality) that novice designers frequently mention at the early design stage.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":8,"sentence":"Analysis of the results indicates that embedding requirements into a user workflow supports novice designers generate significantly more functions with correct syntax than starting with requirements only.","offset":3,"pro":0.42857142857142855,"labels":"CLN"},{"idx":8,"sentence":"In addition, the exploration space is prominently broader, especially at the higher levels measured by the geometry of the generated hierarchical models.","offset":4,"pro":0.5714285714285714,"labels":"CLN"},{"idx":8,"sentence":"These results suggest that strategically incorporating user considerations, even in a simple way, positively addresses the common challenges.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":8,"sentence":"This initial exploration and assessment could serve as an inspiration for future research on more efficient ways to streamline the functional decomposition process and, in turn, better support subsequent systematic conceptual design.","offset":6,"pro":0.8571428571428571,"labels":"IMP"},{"idx":9,"sentence":"Customization of manipulators having unconventional parameters and link shapes have gained attention to accomplish nonrepetitive tasks in a given cluttered environment.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"Adaptive modular and reconfigurable designs are being used to achieve customization and have provided time and cost-effective solutions.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":9,"sentence":"Major challenges are associated to provide the systematic approach on the design and realization of modular components considering connectivity and integration.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":9,"sentence":"This article focuses on the architectural synthesis of the modular links, optimized with respect to the dynamic torques while following a prescribed set of trajectories.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":9,"sentence":"The design methodology is proposed as an Architecture Prominent Sectioning-k strategy, which assumes a modular link as an equivalent system of k number of point masses, performing optimization to minimize the joint torques and map the resulting re-adjusted point masses into a possible architecture.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":9,"sentence":"The proposed strategy is general and can be applied to planar or spatial manipulators with n-DoF even with nonparallel and nonperpendicular jointed configurations.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":9,"sentence":"The design of optimal curved links is realized resulting from the optimized solution considering the dynamics of the modular configurations over primitive trajectories.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":9,"sentence":"The proposed modular library of unconventional curved link modules with joint modules have shown lesser requirement of the joint torques compared to the conventional straight links.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":10,"sentence":"This paper proposes a novel density-based method for structural design considering restrictions of multi-axis machining processes.","offset":0,"pro":0,"labels":"PUR"},{"idx":10,"sentence":"A new mathematical formulation based on Heaviside function is presented to transform the design field into a geometry which can be manufactured by multi-axis machining process.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":10,"sentence":"The formulation is developed for 5-axis machining, which can be also applied to 2.5D milling restriction.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":10,"sentence":"The filter techniques are incorporated to effectively control the minimum size of void region.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":10,"sentence":"The proposed method is demonstrated by solving the compliance minimization problem for different machinable freeform designs.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":10,"sentence":"The length to diameter (L:D) ratio geometric constraint is introduced to ensure the machinable design, where deep hole or narrow chamber features are avoided using proposed method.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":10,"sentence":"Several two- and three-dimensional numerical examples are presented and discussed in detail.","offset":6,"pro":0.8571428571428571,"labels":"MTD"},{"idx":11,"sentence":"Finite element-based crashworthiness optimization is extensively used to improve the safety of motor vehicles.","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"However, the responses of crash simulations are characterized by a high level of numerical noise, which can hamper the blind use of surrogate-based design optimization methods.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":11,"sentence":"It is therefore essential to account for the noise-induced uncertainty when performing optimization.","offset":2,"pro":0.2222222222222222,"labels":"BAC"},{"idx":11,"sentence":"For this purpose, a surrogate, referred to as non-deterministic kriging (NDK), can be used.","offset":3,"pro":0.3333333333333333,"labels":"BAC"},{"idx":11,"sentence":"It models the noise as a non-stationary stochastic process, which is added to a traditional deterministic kriging surrogate.","offset":4,"pro":0.4444444444444444,"labels":"BAC"},{"idx":11,"sentence":"Based on the NDK surrogate, this study proposes an optimization algorithm tailored to account for both epistemic uncertainty, due to the lack of data, and irreducible aleatory uncertainty, due to the simulation noise.","offset":5,"pro":0.5555555555555556,"labels":"PUR"},{"idx":11,"sentence":"The variances are included within an extension of the well-known expected improvement infill criterion referred to as modified augmented expected improvement (MAEI).","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":11,"sentence":"Because the proposed optimization scheme requires an estimate of the aleatory variance, it is approximated through a regression kriging, referred to as variance kriging, which is iteratively refined.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":11,"sentence":"The proposed algorithm is tested on a set of analytical functions and applied to the optimization of an occupant restraint system (ORS) during a crash.","offset":8,"pro":0.8888888888888888,"labels":"MTD"},{"idx":12,"sentence":"Combined plant and control design (control co-design (CCD)) methods are often used during product development to address the synergistic coupling between the plant and control parts of a dynamic system.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"Recently, a few studies have started applying CCD to stochastic dynamic systems.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":12,"sentence":"In their most rigorous approach, reliability-based design optimization (RBDO) principles have been used to ensure solution feasibility under uncertainty.","offset":2,"pro":0.2,"labels":"BAC"},{"idx":12,"sentence":"However, since existing reliability-based CCD (RBCCD) algorithms use all-at-once formulations, only most-probable-point (MPP) methods can be used as reliability analysis techniques.","offset":3,"pro":0.3,"labels":"GAP"},{"idx":12,"sentence":"Though effective for linear/quadratic RBCCD problems, the use of such methods for highly nonlinear RBCCD problems introduces solution error that could lead to system failure.","offset":4,"pro":0.4,"labels":"GAP"},{"idx":12,"sentence":"A multidisciplinary feasible (MDF) formulation for RBCCD problems would eliminate this issue by removing the dynamic equality constraints and instead enforcing them through forward simulation.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":12,"sentence":"Since the RBCCD problem structure would be similar to traditional RBDO problems, any of the well-established reliability analysis methods could be used.","offset":6,"pro":0.6,"labels":"BAC"},{"idx":12,"sentence":"Therefore, in this work, a novel reliability-based MDF formulation of multidisciplinary dynamic system design optimization has been proposed for RBCCD.","offset":7,"pro":0.7,"labels":"PUR"},{"idx":12,"sentence":"To quantify the uncertainty propagated by the random decision variables, Monte Carlo simulation has been applied to the generalized polynomial chaos expansion of the probabilistic constraints.","offset":8,"pro":0.8,"labels":"MTD"},{"idx":12,"sentence":"The proposed formulation is applied to two engineering test problems, with the results indicating the effectiveness of both the overall formulation as well as the reliability analysis technique for RBCCD.","offset":9,"pro":0.9,"labels":"CLN"},{"idx":13,"sentence":"An efficient scheme for the robust topology optimization considering hybrid bounded uncertainties (RTOHBU) is proposed for the graphene platelets (GPLs) reinforced functionally graded materials (FGMs).","offset":0,"pro":0,"labels":"PUR"},{"idx":13,"sentence":"By introducing the concept of the layer-wise FGMs, the properties of the GPLs reinforced FGMs are calculated based on the Halpin-Tsai micromechanics model.","offset":1,"pro":0.14285714285714285,"labels":"MTD"},{"idx":13,"sentence":"The practical boundedness of probabilistic variables is naturally ensured by utilizing a generalized Beta distribution in constructing the robust topology optimization model.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":13,"sentence":"To address the issue of lacking the information of critical loads in existing topology optimization approaches considering hybrid uncertainties, a gradient-attributed search is carried out at first based on the hypothesis of linear elasticity to determine the critical loads leading to the worst structural performance.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":13,"sentence":"Subsequently, the statistical characteristics of the objective structural performance under such critical loads are efficiently evaluated by integrating the univariate dimension reduction method and the Gauss-Laguerre quadrature, the accuracy of which is verified by the comparison analyses utilizing the results of Monte Carlo simulation as references.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":13,"sentence":"Furthermore, a novel realization vector set is constructed for the bounded probabilistic uncertainties to parallelize the sensitivity analysis and accelerate the optimization process.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":13,"sentence":"All the proposed innovations are integrated into the robust topology optimization scheme, the effectiveness and efficiency of which are verified by both numerical and realistic engineering examples.","offset":6,"pro":0.8571428571428571,"labels":"MTD"},{"idx":14,"sentence":"In this work, robust design optimization (RDO) is treated, motivated by the increasing desire to account for variability in the design phase.","offset":0,"pro":0,"labels":"PUR"},{"idx":14,"sentence":"The problem is formulated in a multi-objective setting with the objective of simultaneously minimizing the mean of the objective and its variance due to variability of design variables and/or parameters.","offset":1,"pro":0.1,"labels":"MTD"},{"idx":14,"sentence":"This allows the designer to choose its robustness level without the need to repeat the optimization as typically encountered when formulated as a single objective.","offset":2,"pro":0.2,"labels":"MTD"},{"idx":14,"sentence":"To account for the computational cost that is often encountered in RDO problems, the problem is fitted in a Bayesian optimization framework.","offset":3,"pro":0.3,"labels":"MTD"},{"idx":14,"sentence":"The use of surrogate modeling techniques to efficiently solve problems under uncertainty has effectively found its way in the optimization community leading to surrogate-assisted optimization-under-uncertainty schemes.","offset":4,"pro":0.4,"labels":"BAC"},{"idx":14,"sentence":"The Gaussian processes, the surrogates on which Bayesian optimization builds, are often considered cheap-to-sample black-boxes and are sampled to obtain the desired quantities of interest.","offset":5,"pro":0.5,"labels":"BAC"},{"idx":14,"sentence":"However, since the analytical formulation of these surrogates is known, an analytical treatment of the problem is available.","offset":6,"pro":0.6,"labels":"GAP"},{"idx":14,"sentence":"To obtain the quantities of interest without sampling an analytical uncertainty, propagation through the surrogate is presented.","offset":7,"pro":0.7,"labels":"PUR"},{"idx":14,"sentence":"The multi-objective Bayesian optimization framework and the analytical uncertainty quantification are linked together through the formulation of the robust expected improvement, obtaining the novel efficient robust global optimization scheme.","offset":8,"pro":0.8,"labels":"MTD"},{"idx":14,"sentence":"The method is tested on a series of test cases to examine its behavior for varying difficulties and validated on an aerodynamic test function which proves the effectiveness of the novel scheme.","offset":9,"pro":0.9,"labels":"CLN"},{"idx":15,"sentence":"Ideation methods have been extensively studied, and several ideation methods can be beneficial in different contexts, but it is not understood what makes a specific method work.","offset":0,"pro":0,"labels":"GAP"},{"idx":15,"sentence":"Previous work has shown that all the ideation methods comprise of 25 fundamental ideation mechanisms in two categories: idea implementation and idea promoting mechanisms.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":15,"sentence":"In this study, we try to understand how individual mechanisms affect idea generation outcomes.","offset":2,"pro":0.2,"labels":"PUR"},{"idx":15,"sentence":"We chose four idea promoting mechanisms: two from the process category (Classification and Combination) and two from the idea sources category (Building on Others and Stimulation).","offset":3,"pro":0.3,"labels":"MTD"},{"idx":15,"sentence":"These mechanisms were selected as they are examples of comparable mechanisms that could be integrated into any other ideation method.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":15,"sentence":"We conducted four experiments and assessed idea quantity, novelty, and originality.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":15,"sentence":"Our study showed that the chosen mechanisms increased ideation performance.","offset":6,"pro":0.6,"labels":"CLN"},{"idx":15,"sentence":"For the most part, the mechanisms are statistically equivalent, but we found evidence that classification outperforms combination in nonengineering concept generation exercise.","offset":7,"pro":0.7,"labels":"RST"},{"idx":15,"sentence":"We also found the building on others can be more useful than the type of stimulation used in engineering concept generation, but the difference was not found in nonengineering concept generation.","offset":8,"pro":0.8,"labels":"RST"},{"idx":15,"sentence":"Overall, we found evidence that all mechanisms improve ideation effectiveness and could be incorporated into any ideation method, but further studies are needed to build a more comprehensive understanding.","offset":9,"pro":0.9,"labels":"CLN"},{"idx":16,"sentence":"Mobile networks, constructed with simple linkages by tessellation, have great application potential in engineering as they could change their shapes according to the need of working state by one degree-of-freedom (DOF).","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"However, the existing one-DOF networks are always composed of bar-like links, and cooperated membranes should be designed and fabricated additionally, which makes the design and the realization more complicated.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":16,"sentence":"This paper is to construct a one-DOF network of Bennett linkages with identical square panels.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":16,"sentence":"Geometric conditions to construct the network are derived by investigating the kinematic compatibility, kinematics is carried out to show the relationships among all Bennett linkages, and the discussion on the design parameter shows the extensibility and the deploying performance, which is validated by two physical prototypes.","offset":3,"pro":0.6,"labels":"MTD"},{"idx":16,"sentence":"This work initials the construction of mobile networks with identical polygon-like links, which will simplify the fabrication and realization of deployable structures.","offset":4,"pro":0.8,"labels":"PUR"},{"idx":17,"sentence":"While a majority of accidents and malfunctions in complex engineered systems are attributed to human error, a closer inspection would reveal that such mishaps often emerge as a result of complex interactions between the human- and component-related vulnerabilities.","offset":0,"pro":0,"labels":"GAP"},{"idx":17,"sentence":"To fully understand and mitigate potential risks, the effects of such interactions between component failures and human errors (in addition to their independent effects) need to be considered early.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":17,"sentence":"Specifically, to facilitate risk-based design, severity of such failures need to be quantified early in the design process to determine overall risk and prioritize the most important hazards.","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":17,"sentence":"However, existing risk assessment methods either quantify the risk of component failures or human errors in isolation or are only applicable during later design stages.","offset":3,"pro":0.3333333333333333,"labels":"GAP"},{"idx":17,"sentence":"This work intends to overcome this limitation by introducing an expected cost model to the Human Error and Functional Failure Reasoning (HEFFR) framework to facilitate the quantification of the effects of human error and component failures acting in tandem.","offset":4,"pro":0.4444444444444444,"labels":"PUR"},{"idx":17,"sentence":"This approach will allow designers to assess the risk of hazards emerging from human- and component-related failures occurring in combination and identify worst-case fault scenarios.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":17,"sentence":"A coolant tank case study is used to demonstrate this approach.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":17,"sentence":"The results show that the proposed approach can help designers quantify the effects of human error and component failures acting alone and in tandem, identify worst-case scenarios, and improve human-product interactions.","offset":7,"pro":0.7777777777777778,"labels":"CLN"},{"idx":17,"sentence":"However, the underlying likelihood and cost models are subject to uncertainties which may affect the assessments.","offset":8,"pro":0.8888888888888888,"labels":"GAP"},{"idx":18,"sentence":"The assembly precision of wheel alignment parameters is vital to vehicle handling stability.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"Due to the vertical wheel displacement and compliant components in suspension systems, it is difficult to assemble qualified vehicles with proper wheel alignment parameters.","offset":1,"pro":0.1,"labels":"GAP"},{"idx":18,"sentence":"In the assembly shop of the automobile factory, the adjustment of wheel alignment parameters is the most time-consuming process because it relies on trial and error.","offset":2,"pro":0.2,"labels":"GAP"},{"idx":18,"sentence":"In order to provide a theoretical guidance to the precision control of wheel alignment parameters, this paper extends the theory of equilibrium equations of incremental forces (EEIF) to 3D compliant mechanisms.","offset":3,"pro":0.3,"labels":"PUR"},{"idx":18,"sentence":"Constraint equations of kinematic joints are adopted to express the spatial relationships of different parts.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":18,"sentence":"A couple of fixed and floating joint coordinate systems (CSs) are used together to represent deviations of compliant components.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":18,"sentence":"The impacts of suspension part deviations on vertical wheel displacement and assembly deformations are well illustrated by such approach.","offset":6,"pro":0.6,"labels":"CLN"},{"idx":18,"sentence":"Accuracy of the proposed method is verified by comparing with ADAMS simulation.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":18,"sentence":"The results show that the error rates of the 3D EEIF method are less than 5\\%.","offset":8,"pro":0.8,"labels":"RST"},{"idx":18,"sentence":"Furthermore, statistical assembly variation analysis of a Macpherson suspension is accomplished by using the proposed method and an optimized process strategy is put forward.","offset":9,"pro":0.9,"labels":"MTD"},{"idx":19,"sentence":"This paper presents a long short-term memory (LSTM)-based ensemble learning approach for time-dependent reliability analysis.","offset":0,"pro":0,"labels":"PUR"},{"idx":19,"sentence":"An LSTM network is first adopted to learn system dynamics for a specific setting with a fixed realization of time-independent random variables and stochastic processes.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":19,"sentence":"By randomly sampling the time-independent random variables, multiple LSTM networks can be trained and leveraged with the Gaussian process (GP) regression to construct a global surrogate model for the time-dependent limit state function.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":19,"sentence":"In detail, a set of augmented data is first generated by the LSTM networks and then utilized for GP modeling to estimate system responses under time-dependent uncertainties.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":19,"sentence":"With the GP models, the time-dependent system reliability can be approximated directly by sampling-based methods such as the Monte Carlo simulation (MCS).","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":19,"sentence":"Three case studies are introduced to demonstrate the efficiency and accuracy of the proposed approach.","offset":5,"pro":0.8333333333333334,"labels":"CLN"},{"idx":0,"sentence":"The paper presents a novel approach to applying Bayesian Optimization (BO) in predicting an unknown constraint boundary, also representing the discontinuity of an unknown function, for a feasibility check on the design space, thereby representing a classification tool to discern between a feasible and infeasible region.","offset":0,"pro":0,"labels":"PUR"},{"idx":0,"sentence":"Bayesian optimization is a low-cost black-box global optimization tool in the Sequential Design Methods where one learns and updates knowledge from prior evaluated designs, and proceeds to the selection of new designs for future evaluation.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":0,"sentence":"However, BO is best suited to problems with the assumption of a continuous objective function and does not guarantee true convergence when having a discontinuous design space.","offset":2,"pro":0.2,"labels":"GAP"},{"idx":0,"sentence":"This is because of the insufficient knowledge of the BO about the nature of the discontinuity of the unknown true function.","offset":3,"pro":0.3,"labels":"GAP"},{"idx":0,"sentence":"In this paper, we have proposed to predict the location of the discontinuity using a BO algorithm on an artificially projected continuous design space from the original discontinuous design space.","offset":4,"pro":0.4,"labels":"PUR"},{"idx":0,"sentence":"The proposed approach has been implemented in a thin tube design with the risk of creep-fatigue failure under constant loading of temperature and pressure.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":0,"sentence":"The stated risk depends on the location of the designs in terms of safe and unsafe regions, where the discontinuities lie at the transition between those regions; therefore, the discontinuity has also been treated as an unknown creep-fatigue failure constraint.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":0,"sentence":"The proposed BO algorithm has been trained to maximize sampling toward the unknown transition region, to act as a high accuracy classifier between safe and unsafe designs with minimal training cost.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":0,"sentence":"The converged solution has been validated for different design parameters with classification error rate and function evaluations at an average of <1\\% and similar to 150, respectively.","offset":8,"pro":0.8,"labels":"RST"},{"idx":0,"sentence":"Finally, the performance of our proposed approach in terms of training cost and classification accuracy of thin tube design is shown to be better than the existing machine learning (ML) algorithms such as Support Vector Machine (SVM), Random Forest (RF), and Boosting.","offset":9,"pro":0.9,"labels":"CLN"},{"idx":1,"sentence":"Additive manufacturing (AM) offers many advantages to make objects compared to traditional subtractive manufacturing methods.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"For example, complex geometries can be easily fabricated, and lightweight parts can be formed while maintaining the parts strength for the low carbon footprint, low material consumption and waste.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":1,"sentence":"But there are some areas for AM to improve in sustainability, reliability, productivity, robustness, material diversity, and part quality.","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":1,"sentence":"Life-cycle assessment studies have identified that the AM printing stage has a big impact on the life-cycle sustainability of 3D printed products.","offset":3,"pro":0.3333333333333333,"labels":"BAC"},{"idx":1,"sentence":"AM building parameters can be properly selected to improve the sustainability of AM.","offset":4,"pro":0.4444444444444444,"labels":"BAC"},{"idx":1,"sentence":"This paper explores the fused deposition modeling (FDM) process parameters for sustainability to reduce the process energy and material consumption.","offset":5,"pro":0.5555555555555556,"labels":"PUR"},{"idx":1,"sentence":"Investigated parameters include the printing layer height, number of shells, material infilling percentage, infilling type, and building orientation.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":1,"sentence":"Taguchi design of experiments approach and statistical analysis tools are used to find optimal parameter settings to improve the sustainability of the FDM process.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":1,"sentence":"Models formulated in this research can be easily extended to other AM processes.","offset":8,"pro":0.8888888888888888,"labels":"CTN"},{"idx":2,"sentence":"The multi-fidelity surrogate (MFS) model is designed to make use of a small amount of expensive but accurate high-fidelity (HF) information and a lot of inaccurate but cheap low-fidelity (LF) information.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"In this paper, a canonical correlation analysis (CCA)-based MFS model in which the least squares (LS) method is used to determine optimal parameters, named CCA-LS-MFS, is proposed.","offset":1,"pro":0.1,"labels":"PUR"},{"idx":2,"sentence":"The CCA-LS-MFS model consists of three stages.","offset":2,"pro":0.2,"labels":"MTD"},{"idx":2,"sentence":"The first stage is to construct two transition matrices of HF and LF samples using the CCA method.","offset":3,"pro":0.3,"labels":"MTD"},{"idx":2,"sentence":"Then, the discrepancy function between HF and LF models is constructed.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":2,"sentence":"In the third stage, parameters are determined by using the LS method.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":2,"sentence":"The correlation between HF and LF models, the cost ratio of HF to LF models, and the combination of HF and LF samples are explored.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":2,"sentence":"It is observed that the increase of the correlation between HF and LF models can highly improve the performance of the CCA-LS-MFS model.","offset":7,"pro":0.7,"labels":"RST"},{"idx":2,"sentence":"CCA-LS-MFS is capable of providing more robust performance than the other two baseline MFS models, especially when the HF and LF models are highly or weakly correlated, and is promising for being applied into the engineering problems with unclear correlation between HF and LF models.","offset":8,"pro":0.8,"labels":"CLN"},{"idx":2,"sentence":"In addition, it has been found that in case of given total budget and HF information, the cost ratio of HF to LF models plays an important role in prediction performance, which requires more research in the future work.","offset":9,"pro":0.9,"labels":"CLN"},{"idx":3,"sentence":"Technology adoption in low-income regions is among the key challenges facing international development projects.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"Nearly 40\\% of the world's population relies on open fires and rudimentary cooking devices exacerbating health outcomes, deforestation, and climatic impacts of inefficient biomass burning.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":3,"sentence":"Clean technology alternatives such as clean cookstoves are among the most challenging technologies to approach their target goals through sustainable adoption due to a lack of systematic market-driven design for adoption.","offset":2,"pro":0.2,"labels":"BAC"},{"idx":3,"sentence":"Thus, a method is needed to provide insight regarding how target customers evaluate and perceive causes for adopting a clean technology.","offset":3,"pro":0.3,"labels":"GAP"},{"idx":3,"sentence":"The holistic approach of this study captures technology adoption through lenses of social networks, individual and society scale beliefs, and rational decision-making behavior.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":3,"sentence":"Based on the data collected in the Apac region in Northern Uganda, an agent-based model is developed to simulate emerging adoption behavior in a community.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":3,"sentence":"Then, four different scenarios investigate how adoption patterns change due to the potential changes in technology or intervention strategy.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":3,"sentence":"These scenarios include influence of stove malfunctions, price elasticity, information campaigns, and strength of a social network.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":3,"sentence":"Results suggest that higher adoption rates are achievable if designed technologies are more durable, information campaigns provide realistic expectations for users, policymakers, and education programs work toward women's empowerment, and communal social ties are recognized for influence maximization.","offset":8,"pro":0.8,"labels":"CLN"},{"idx":3,"sentence":"The application of this study provides insight for technology designers, project implementers, and policymakers to update their practices for achieving sustainable and to the scale clean technology adoption rates.","offset":9,"pro":0.9,"labels":"CTN"},{"idx":4,"sentence":"Design, development, and delivery of a new product to fulfill the needs and aspirations of marginalized people living at the bottom of the pyramid (BoP) is a challenging endeavor.","offset":0,"pro":0,"labels":"GAP"},{"idx":4,"sentence":"This is challenging, as the dynamic integration of sociocognitive aspects of the BoP with technological factors is a complicated task.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":4,"sentence":"Therefore, the participatory design approach is essential, as it facilitates dialogue among experts from multiple domains.","offset":2,"pro":0.2857142857142857,"labels":"BAC"},{"idx":4,"sentence":"This article presents the case of improvised pedal-operated Chaak (IPC) to appraise a realistic view of a participatory approach in the design and delivery of a new product in the BoP.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":4,"sentence":"The role played by Rural Technology Action Group as an intermediary to facilitate co-creation of knowledge during design and delivery of IPC is also discussed.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":4,"sentence":"This study infers that the IPC will improve productivity and has the capability to reduce the health drudgery of pottery artisans in Assam, India.","offset":5,"pro":0.7142857142857143,"labels":"CLN"},{"idx":4,"sentence":"This study demonstrates how nonbusiness actors (e.g., academia, non-government organizations, etc.) and local fabricators can constructively participate in the product design and delivery for sustainable community development at the BoP.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":5,"sentence":"An approach is proposed to quantify the uncertainty in probability of failure using a Gaussian process (GP) and to estimate uncertainty change before actually adding samples to GP.","offset":0,"pro":0,"labels":"PUR"},{"idx":5,"sentence":"The approach estimates the coefficient of variation (CV) of failure probability due to prediction variance of GP.","offset":1,"pro":0.125,"labels":"MTD"},{"idx":5,"sentence":"The CV is estimated using single-loop Monte Carlo simulation (MCS), which integrates the probabilistic classification function while replacing expensive multi-loop MCS.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":5,"sentence":"The methodology ensures a conservative estimate of CV, in order to compensate for sampling uncertainty in MCS.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":5,"sentence":"Uncertainty change is estimated by adding a virtual sample from the current GP and calculating the change in CV, which is called expected uncertainty change (EUC).","offset":4,"pro":0.5,"labels":"MTD"},{"idx":5,"sentence":"The proposed method can help adaptive sampling schemes to determine when to stop before adding a sample.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":5,"sentence":"In numerical examples, the proposed method is used in conjunction with the efficient local reliability analysis to calculate the reliability of analytical function as well as the battery drop test simulation.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":5,"sentence":"It is shown that the EUC converges to the true uncertainty change as the model becomes accurate.","offset":7,"pro":0.875,"labels":"RST"},{"idx":6,"sentence":"Compliant constant-force mechanisms (CCFMs), which provide a near constant-force output over a range of displacement, can benefit many applications.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"This work proposes a novel large-stroke CCFM (abbreviated as B2CCFM) that utilizes the second buckling mode of flexible beams.","offset":1,"pro":0.125,"labels":"PUR"},{"idx":6,"sentence":"Two general nondimensionalized metrics, one describing the variation of output force and the other describing the operational displacement, are proposed to effectively characterize the performances of various CCFMs.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":6,"sentence":"Based on the general metrics, design formulas that can help designers quickly find suitable B2CCFM design for a specific application are obtained.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":6,"sentence":"A kinetostatic model for B2CCFM is also provided based on the chained beam constrain model to verify B2CCFM designs.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":6,"sentence":"An example accompanied with a prototype is presented to verify this novel CCFM and the effectiveness of the design formulas.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":6,"sentence":"The experimental results show that the B2CCFM example outputs a constant-force in a range as large as 45\\% of the beam length with variation less than 4.7\\%.","offset":6,"pro":0.75,"labels":"RST"},{"idx":6,"sentence":"The nondimensionalized metrics were demonstrated in comparison of several CCFMs, and the comparison results show the superior performances of B2CCFMs.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":7,"sentence":"Classical dynamic balancing techniques do not consider the linkage elastic behavior.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"For mechanism or robot design purpose, taking into account the flexibility of the multibody system is of utmost importance, in order to be able to manufacture a mechanism/robot which is stiff enough for a given task.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":7,"sentence":"This paper deals with a novel approach that allows to design mechanisms by means of structural topology optimization while specific dynamic balancing conditions are considered.","offset":2,"pro":0.2,"labels":"PUR"},{"idx":7,"sentence":"In our work, the links are treated as three-dimensional flexible bodies, and the optimization process is performed for all the bodies simultaneously.","offset":3,"pro":0.3,"labels":"MTD"},{"idx":7,"sentence":"Applying this methodology, the optimal design of a dynamically balanced four-bar linkage is accomplished while its compliance is minimized.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":7,"sentence":"Numerical validations of the optimized linkage properties are carried out using commercial software.","offset":5,"pro":0.5,"labels":"MTD"},{"idx":7,"sentence":"The dynamic balancing performance of the optimized four-bar linkage is numerically validated using adams.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":7,"sentence":"Besides, ansys software was used in order to perform the linkage stiffness analysis and to compare it with the results of the optimization solver.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":7,"sentence":"In order to verify the feasibility of the proposed methodology, a prototype is built.","offset":8,"pro":0.8,"labels":"MTD"},{"idx":7,"sentence":"Experimental studies are carried out in order to evaluate its dynamic balancing performance.","offset":9,"pro":0.9,"labels":"MTD"},{"idx":8,"sentence":"In this paper, we present a predictive and generative design approach for supporting the conceptual design of product shapes in 3D meshes.","offset":0,"pro":0,"labels":"PUR"},{"idx":8,"sentence":"We develop a target-embedding variational autoencoder (TEVAE) neural network architecture, which consists of two modules: (1) a training module with two encoders and one decoder ((ED)-D-2 network) and (2) an application module performing the generative design of new 3D shapes and the prediction of a 3D shape from its silhouette.","offset":1,"pro":0.2,"labels":"MTD"},{"idx":8,"sentence":"We demonstrate the utility and effectiveness of the proposed approach in the design of 3D car body and mugs.","offset":2,"pro":0.4,"labels":"CLN"},{"idx":8,"sentence":"The results show that our approach can generate a large number of novel 3D shapes and successfully predict a 3D shape based on a single silhouette sketch.","offset":3,"pro":0.6,"labels":"RST"},{"idx":8,"sentence":"The resulting 3D shapes are watertight polygon meshes with high-quality surface details, which have better visualization than voxels and point clouds, and are ready for downstream engineering evaluation (e.g., drag coefficient) and prototyping (e.g., 3D printing).","offset":4,"pro":0.8,"labels":"RST"},{"idx":9,"sentence":"Design and optimization of hull shapes for optimal hydrodynamic performance have been a major challenge for naval architectures.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"Deep learning bears the promise of comprehensive geometric representation and new design synthesis.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":9,"sentence":"In this work, we develop a deep neural network (DNN)-based approach to encode the hull designs to condensed representations, synthesize novel designs, and optimize the synthetic design based on the hydrodynamic performance.","offset":2,"pro":0.2222222222222222,"labels":"PUR"},{"idx":9,"sentence":"A variational autoencoder (VAE) with the hydro-predictor is developed to learn the representation through reconstructing the Laplacian parameterized hulls and encode the geometry-drag function simulated through computational fluid dynamics (CFD).","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":9,"sentence":"Two data augmentation techniques, Perlin noise mapping and free-form deformation (FFD), are implemented to create the training set from a parent hull.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":9,"sentence":"The trained VAE is leveraged to efficiently optimize from massive synthetic hull vessels toward the optimal predicted drag performance.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":9,"sentence":"The selected geometries are further investigated and virtually screened under CFD simulations.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":9,"sentence":"Experiments show that our convolutional neural network (CNN) model accurately reconstructs the input vessels and predicts the corresponding drag coefficients.","offset":7,"pro":0.7777777777777778,"labels":"CLN"},{"idx":9,"sentence":"The proposed framework is demonstrated to synthesize realistic hull designs and optimize toward new hull designs with the drag coefficient decreased by 35\\% comparing to the parent design.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":10,"sentence":"The nonlinear stiffness actuator (NSA) is a type of compliant joint with variable stiffness that is necessary for human-machine collaboration and has been the focus of extensive research attention.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"This academic work presents the principle of a torsion spring-connected nonlinear stiffness actuator (TSNSA), a NSA suitable for implementing revolute motions, which contrasts with existing NSAs that often have bulky mechanisms and are affected by a limited range of movement.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":10,"sentence":"It has a split sleeve, disc springs, and cam mechanisms (CMs) that when correctly configured, can passively and actively adjust stiffness, where the speed of stiffness regulation can be effectively improved.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":10,"sentence":"The mathematical model of a split sleeve inserted into a torsion spring is designed to analyze the behavior of mechanical components.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":10,"sentence":"Furthermore, a detailed CM profile has been designed to obtain the desired nonlinear stiffness.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":10,"sentence":"Its theoretical analysis includes numerical simulations with results consistent with the desired deflection-torque profile.","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":11,"sentence":"The paper presents an analytical approach for designing grooved cam mechanisms with a modified arrangement of the common translating follower.","offset":0,"pro":0,"labels":"PUR"},{"idx":11,"sentence":"That is, an intermediate link having three rollers is added between the cam and the common follower.","offset":1,"pro":0.1111111111111111,"labels":"MTD"},{"idx":11,"sentence":"On the basis of an existing cam mechanism with a common roller follower, an intermediate link that has three rollers is added between the cam and the common follower.","offset":2,"pro":0.2222222222222222,"labels":"MTD"},{"idx":11,"sentence":"Such a cam mechanism has two sets of profile and can create multiple contact points between the cam and the follower at any instant.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":11,"sentence":"The two sets of profiles of such a cam mechanism can serve as the grooved types.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":11,"sentence":"Since the follower has three rollers that can simultaneously contact the cam at any instant, it can be positively driven along the guided groove of the cam contour.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":11,"sentence":"The contact forces and contact stresses of such a cam mechanism are analyzed to illustrate the advantage of spreading the force transmission and reducing the contact stress of this uncommon follower.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":11,"sentence":"The obtained results indicate that the contact stress at the surface of the cam and the follower for such a cam mechanism can be reduced by 30 to 47\\% in comparison with those of the cam mechanism with a common translating roller follower.","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":11,"sentence":"In conclusion, the cam mechanism with a translating follower having an added ternary-roller intermediate link can be a preferable choice for the applications that follower is against heavy loads or moves at high speed.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":12,"sentence":"This paper addresses the synthesis of one degree-of-freedom (1DOF) linkages that can exactly transmit angular motion between coplanar axes (i.e., parallel axes or intersectant axes) with arbitrarily prescribed constant velocity ratios.","offset":0,"pro":0,"labels":"PUR"},{"idx":12,"sentence":"According to motion polynomials over dual quaternions and pure rolling models between two circles, an algebraic approach is presented to precisely synthesize new 1DOF linkages with arbitrarily prescribed constant velocity ratios.","offset":1,"pro":0.125,"labels":"PUR"},{"idx":12,"sentence":"The approach includes four steps: (a) formulate a characteristic curve occurred by the pure rolling, (b) compute the motion polynomial of the minimal degree that can generate the curve, (c) deal with the factorization of the motion polynomial to construct an open chain, and (d) convert the open chain to a 1DOF linkage.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":12,"sentence":"Using this approach, several 1DOF planar, spherical, and spatial linkages for angular motion transmission between parallel axes or intersectant ones are constructed by designating various velocity ratios.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":12,"sentence":"Taking the planar and spherical linkages with a constant 1:2 velocity ratio as examples, kinematics analysis is implemented to prove their motion characteristics.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":12,"sentence":"The result shows that the generated linkages indeed can transmit angular motion between two coplanar axes with constant velocity ratios.","offset":5,"pro":0.625,"labels":"CLN"},{"idx":12,"sentence":"Meanwhile, three-dimensional (3D)-printed prototypes of these linkages also demonstrate such a conclusion.","offset":6,"pro":0.75,"labels":"CLN"},{"idx":12,"sentence":"This work provides a framework for synthesizing linkages that have great application potential to transmit motion in robotic systems that require low inertia to achieve reciprocating motion with high speed and accuracy.","offset":7,"pro":0.875,"labels":"PUR"},{"idx":13,"sentence":"Remanufacturing is a representative product recovery strategy that can improve economic profitability and sustainability, but many companies are struggling because of the lack of understanding of the market and the design strategies for remanufactured products.","offset":0,"pro":0,"labels":"GAP"},{"idx":13,"sentence":"Unlike the production process of new products, remanufacturing requires unique production processes, such as collecting used products and dis(re)assembly.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":13,"sentence":"Therefore, several factors need to be considered for the design of remanufactured products.","offset":2,"pro":0.2857142857142857,"labels":"GAP"},{"idx":13,"sentence":"First, when designing a remanufactured product, it is crucial to ensure that the specifications of components meet the customer's requirements because the remanufacturing uses relatively outdated components or modules.","offset":3,"pro":0.42857142857142855,"labels":"BAC"},{"idx":13,"sentence":"In addition, it is necessary to consider the disassembly level and order to facilitate the disassembly process to obtain the desired parts.","offset":4,"pro":0.5714285714285714,"labels":"BAC"},{"idx":13,"sentence":"This study proposes an integrated model to (i) find configuration design suitable for remanufactured products that can maximize customer utility based on customer online review analysis regarding End-of-Life products, and (ii) establish a harvest plan that determines the optimal disassembly operations and levels.","offset":5,"pro":0.7142857142857143,"labels":"PUR"},{"idx":13,"sentence":"This proposed model can be used as a decision-making tool that helps product designers find the appropriate design of remanufactured products while increasing the efficiency of the remanufacturing process.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":14,"sentence":"Design changes and change propagation have been recognized as ubiquitous in the engineering design process.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"But why are some design changes propagated while others are absorbed?","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":14,"sentence":"This paper reports on a study to investigate the specific properties of a mechanical design that influence whether a change is either propagated or absorbed.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":14,"sentence":"Student participants in the study were asked to complete a well-defined mechanical design task and then to introduce several design changes.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":14,"sentence":"Analysis of the recorded design processes reveals new insight into the mechanisms of change propagation in terms of properties of the design.","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":14,"sentence":"The insights suggest avenues for future research to make designs more tolerant to potential future change and to develop improved methods to predict change propagation.","offset":5,"pro":0.8333333333333334,"labels":"IMP"},{"idx":15,"sentence":"Standard life cycle techniques such as life cycle warranty cost (LCWC) analysis and life cycle analysis (LCA) are used to respectively quantify the relative economical and environmental advantages of remanufactured goods while simultaneously identifying avenues for improvement.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"In this paper, we contribute to the literature on life cycle studies by incorporating reliability into LCWC analysis and LCA with the goal of improving long-term/multiple life cycle decision making.","offset":1,"pro":0.125,"labels":"PUR"},{"idx":15,"sentence":"We develop a branched power-law model to incorporate the physical degradation mechanisms leading to reduced reuse rates of system parts over multiple life cycles.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":15,"sentence":"We then follow a standard LCA protocol to quantify the difference between a new unit and its remanufactured version in terms of environmental impact items such as abiotic depletion potential, global warming potential, and energy consumption.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":15,"sentence":"We then devise four practical warranty policies that vary in the choice of replacement and/or provision for extended warranty.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":15,"sentence":"All possible replacement scenarios for multiple life cycles are explored for each policy and a mathematically rigorous framework is provided, where the reliability information is used to calculate probabilistic LCWC and life cycle impact items.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":15,"sentence":"This reliability-informed LCWC analysis and LCA framework enables design engineers to compare design options and warranty policies by quantifying both economical and environmental impacts to aid in decision making.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":15,"sentence":"Although the framework is presented in a general form applicable to any engineered system, we demonstrate the utility of this framework by using a case study of an infinitely variable transmission used in agricultural equipment.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":16,"sentence":"Planetary gear set (PGS) has been one of the best components to constitute a transmission configuration, including the dedicated hybrid transmission (DHT).","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"Using different synthesis approaches, the DHT configurations can be obtained through algorithms.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":16,"sentence":"However, different synthesis results correspond to different connection states of the planetary gear system.","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":16,"sentence":"There are a certain number of results that violate the motion requirements of the mechanical principal need to be detected and removed.","offset":3,"pro":0.3333333333333333,"labels":"GAP"},{"idx":16,"sentence":"Therefore, this paper presents a novel modeling method to systematically remove the interference structures, with graph theory in structural synthesis.","offset":4,"pro":0.4444444444444444,"labels":"PUR"},{"idx":16,"sentence":"Based on the original graph theory, this paper proposes an equivalent replacement modeling method to convert the motor graph model into a brake-like graph model.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":16,"sentence":"Based on the conversion, avoid the appearance of the hanging points in the graph model.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":16,"sentence":"By applying the proposed approach, a DHT structure proves the feasibility of the method.","offset":7,"pro":0.7777777777777778,"labels":"CLN"},{"idx":16,"sentence":"The proposed detection approach can systematically detect all the PGS-based transmission with multi-PGSs, multi-shifting elements, and multi-power sources.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":17,"sentence":"This paper presents an approach for designing symmetric (effective cylinder area during extension is the same as that during retraction) multichamber cylinders with discretely variable piston area.","offset":0,"pro":0,"labels":"PUR"},{"idx":17,"sentence":"The design methodology is presented in a generalizable manner and is demonstrated on an example five-chamber cylinder design.","offset":1,"pro":0.14285714285714285,"labels":"MTD"},{"idx":17,"sentence":"A method for finding symmetric multichamber cylinder configurations from a given cylinder topology is presented, and subsequently, a method for discretely varying the effective piston area is developed, subject to a cylinder symmetry constraint.","offset":2,"pro":0.2857142857142857,"labels":"MTD"},{"idx":17,"sentence":"Furthermore, an algorithm is presented to optimally switch the effective cylinder area of an electrohydrostatic actuation system either to minimize the magnitude of motor torque or to minimize resistive power losses in the system.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":17,"sentence":"Additionally, a method for optimizing standard (constant area) hydraulic cylinders to minimize motor torque magnitude or resistive power losses is presented.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":17,"sentence":"These methods are then demonstrated on an example electrohydrostatic actuation system via simulation.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":17,"sentence":"Results indicate that this multichamber cylinder approach with discretely variable piston area may allow for the design of compact and efficient actuators relative to standard methods.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":18,"sentence":"The objective of this study is to position speculative fiction as a broader framework to stimulate, facilitate, and study engineering design ideation.","offset":0,"pro":0,"labels":"PUR"},{"idx":18,"sentence":"For this, we first present a comprehensive and detailed review of the literature on how fiction, especially science fiction, has played a role in design and decision-making.","offset":1,"pro":0.1111111111111111,"labels":"MTD"},{"idx":18,"sentence":"To further strengthen the need for speculative fiction for idea stimulation, we further prototype and study a prototype workflow that utilizes excerpts from speculative fiction books as textual stimuli for design ideation.","offset":2,"pro":0.2222222222222222,"labels":"MTD"},{"idx":18,"sentence":"Through a qualitative study of this workflow, we gain insights into the effect of textual stimuli from science fiction narratives on design concepts.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":18,"sentence":"Our study reveals that the texts consisting of the terms from the design statement or closely related to the problem boost the idea generation process.","offset":4,"pro":0.4444444444444444,"labels":"CLN"},{"idx":18,"sentence":"We further discover that less directly related stimuli may encourage out-of-the-box and divergent thinking.","offset":5,"pro":0.5555555555555556,"labels":"RST"},{"idx":18,"sentence":"Using the insights gained from our study, we pose critical questions to initiate speculative fiction-based design ideation as a new research direction in engineering design.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":18,"sentence":"Subsequently, we discuss current research directions and domains necessary to take the technical, technological, and methodological steps needed for future research on design methodologies based on speculative fictional inspiration.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":18,"sentence":"Finally, we present a practical case to demonstrate how an engineering design workflow could be operationalized by investigating a concrete example of the design of automotive user interfaces (automotive-UI) through the lens of speculative fiction.","offset":8,"pro":0.8888888888888888,"labels":"MTD"},{"idx":19,"sentence":"Industry 4.0, the fourth industrial revolution, puts forward new requirements for the sustainable service of products.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"With the recent advances in measurement technologies, global and local deformations in inaccessible areas can be monitored.","offset":1,"pro":0.07692307692307693,"labels":"BAC"},{"idx":19,"sentence":"Product usage data such as geometric deviation, position deviation, and angular deviation that lead to product functional performance degradation can be continuously collected during the product usage stage.","offset":2,"pro":0.15384615384615385,"labels":"BAC"},{"idx":19,"sentence":"These technologies provide opportunities to improve tolerance design by improving tolerance allocation using product usage data.","offset":3,"pro":0.23076923076923078,"labels":"BAC"},{"idx":19,"sentence":"The challenge lies in how to assess these deviations for identifying relevant field factors and reallocate the tolerance value.","offset":4,"pro":0.3076923076923077,"labels":"GAP"},{"idx":19,"sentence":"In this paper, a data-driven methodology based on the deviation for tolerance analysis is proposed to improve the tolerance allocation.","offset":5,"pro":0.38461538461538464,"labels":"PUR"},{"idx":19,"sentence":"A feature graph of a mechanical assembly is established based on the assembly relationship.","offset":6,"pro":0.46153846153846156,"labels":"MTD"},{"idx":19,"sentence":"The node representation in the feature graph is defined based on the unified Jacobian-torsor model and the node label is calculated by a synthetic evaluation method.","offset":7,"pro":0.5384615384615384,"labels":"MTD"},{"idx":19,"sentence":"A novel hierarchical graph attention networks (HGAT) is proposed to investigate hidden relations between nodes in the feature graph and calculate labels of all nodes.","offset":8,"pro":0.6153846153846154,"labels":"PUR"},{"idx":19,"sentence":"A modification necessity index (MNI) is defined for each tolerance between two nodes based on their labels.","offset":9,"pro":0.6923076923076923,"labels":"MTD"},{"idx":19,"sentence":"An identification of the to-be-modified tolerance method is proposed to specify the tolerance analysis target.","offset":10,"pro":0.7692307692307693,"labels":"PUR"},{"idx":19,"sentence":"A deviation difference matrix is constructed to calculate the MNI of each tolerance for identifying the to-be-modified tolerance value with high priorities for product improvement.","offset":11,"pro":0.8461538461538461,"labels":"MTD"},{"idx":19,"sentence":"The effectiveness of the proposed methodology is demonstrated through a case study for improving tolerance allocation of a press machine.","offset":12,"pro":0.9230769230769231,"labels":"CLN"},{"idx":0,"sentence":"Urgent societal problems, including climate change, require innovation and can benefit from interdisciplinary solutions.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"A small body of research has demonstrated the potential of positive emotions (e.g., gratitude, awe) to promote creativity and prosocial behavior, which may help address these problems.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":0,"sentence":"This study integrates, for the first time, psychology research on a positive prosocial emotion (i.e., gratitude) with engineering-design creativity research.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":0,"sentence":"In a preregistered study design, engineering students and working engineers (pilot N = 49; full study N = 329) completed gratitude, positive-emotion-control, or neutral-control inductions.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":0,"sentence":"Design creativity was assessed through rated scores of responses to an Alternate Uses Task (AUT) and a Wind-Turbine-Blade Repurposing Task (WRT).","offset":4,"pro":0.5,"labels":"MTD"},{"idx":0,"sentence":"No significant differences among AUT scores emerged across conditions in either sample.","offset":5,"pro":0.625,"labels":"RST"},{"idx":0,"sentence":"As only the pilot-study manipulation of gratitude was successful, recommendations are provided for further studies on the effect of gratitude on engineering-design creativity.","offset":6,"pro":0.75,"labels":"CLN"},{"idx":0,"sentence":"The reported work may also inform other strategies to incorporate prosocial emotion to help engineers arrive at more original and effective concepts to tackle environmental sustainability, and in the future, other problems facing society.","offset":7,"pro":0.875,"labels":"IMP"},{"idx":1,"sentence":"Mission design is a challenging problem, requiring designers to consider complex design spaces and dynamically evolving mission environments.","offset":0,"pro":0,"labels":"BAC"},{"idx":1,"sentence":"In this paper, we adapt computational design approaches, widely used by the engineering design community, to address unique challenges associated with mission design.","offset":1,"pro":0.09090909090909091,"labels":"PUR"},{"idx":1,"sentence":"We present a framework to enable efficient mission design by efficiently building a surrogate model of the mission simulation environment to assist with design tasks.","offset":2,"pro":0.18181818181818182,"labels":"MTD"},{"idx":1,"sentence":"This framework combines design of experiments (DOEs) techniques for data collection, meta-modeling with machine learning models, and uncertainty quantification (UQ) and explainable AI (XAI) techniques to validate the model and explore the mission design space.","offset":3,"pro":0.2727272727272727,"labels":"MTD"},{"idx":1,"sentence":"We demonstrate this framework using an open-source real-time strategy (RTS) game called microRTS as our mission environment.","offset":4,"pro":0.36363636363636365,"labels":"MTD"},{"idx":1,"sentence":"The objective considered in this use case is game balance, observed through the probability of each player winning.","offset":5,"pro":0.45454545454545453,"labels":"MTD"},{"idx":1,"sentence":"Mission parameters are varied according to a DOE over chosen player bots and possible initial conditions of the microRTS game.","offset":6,"pro":0.5454545454545454,"labels":"MTD"},{"idx":1,"sentence":"A neural network model is then trained based on gameplay data obtained from the specified experiments to predict the probability of a player winning given any game state.","offset":7,"pro":0.6363636363636364,"labels":"MTD"},{"idx":1,"sentence":"The model confidence is evaluated using Monte Carlo Dropout Networks (MCDN), and an explanation model is built using SHapley Additive exPlanations (SHAP).","offset":8,"pro":0.7272727272727273,"labels":"MTD"},{"idx":1,"sentence":"Design changes to a sample game are introduced based on important features of the game identified by SHAP analysis.","offset":9,"pro":0.8181818181818182,"labels":"MTD"},{"idx":1,"sentence":"Results show that this analysis can successfully capture feature importance and uncertainty in predictions to guide additional data collection for mission design exploration.","offset":10,"pro":0.9090909090909091,"labels":"CLN"},{"idx":2,"sentence":"In the past two decades, there has been increasing use of semantic networks in engineering design for supporting various activities, such as knowledge extraction, prior art search, idea generation, and evaluation.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"Leveraging large-scale pre-trained graph knowledge databases to support engineering design-related natural language processing (NLP) tasks has attracted a growing interest in the engineering design research community.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":2,"sentence":"Therefore, this study aims to provide a survey of the state-of-the-art semantic networks for engineering design and propositions of future research to build and utilize large-scale semantic networks as knowledge bases to support engineering design research and practice.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":2,"sentence":"The survey shows that WordNet, ConceptNet, and other semantic networks, which contain common-sense knowledge or are trained on non-engineering data sources, are primarily used by engineering design researchers to develop methods and tools.","offset":3,"pro":0.5,"labels":"GAP"},{"idx":2,"sentence":"Meanwhile, there are emerging efforts in constructing engineering and technical-contextualized semantic network databases, such as B-Link and TechNet, through retrieving data from technical data sources and employing unsupervised machine learning approaches.","offset":4,"pro":0.6666666666666666,"labels":"BAC"},{"idx":2,"sentence":"On this basis, we recommend six strategic future research directions to advance the development and uses of large-scale semantic networks for artificial intelligence applications in engineering design.","offset":5,"pro":0.8333333333333334,"labels":"IMP"},{"idx":3,"sentence":"Implementation of biomimetics in practical innovation strategies still faces various impediments.","offset":0,"pro":0,"labels":"GAP"},{"idx":3,"sentence":"Multidisciplinary communication is one of the most recognized one.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":3,"sentence":"Enabling teammates having various cognitive and conceptual frameworks to properly exchange information is a key lever for optimization.","offset":2,"pro":0.2222222222222222,"labels":"BAC"},{"idx":3,"sentence":"In a previous study, we performed a comparative analysis of biologists' and engineers' cognitive and conceptual frameworks in order to support the establishment of a shared framework of reference within biomimetic teams.","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":3,"sentence":"This theoretical work led us to consider various guidelines, embodied in a tool, LINKAGE, guiding the team along the biomimetic process, and more specifically during analysis and abstraction steps.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":3,"sentence":"This article presents a first version of this free access digital tool, LINKAGE 1.2.","offset":5,"pro":0.5555555555555556,"labels":"PUR"},{"idx":3,"sentence":"After the description and positioning of LINKAGE, comparing with other existing tools, a testing phase involving 19 professionals divided into five interdisciplinary teams is presented.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":3,"sentence":"The results of this evaluation lead to the validation of some of the tool's objectives while underlining some lines of improvements.","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":3,"sentence":"Various perspectives on the tool's development are also presented.","offset":8,"pro":0.8888888888888888,"labels":"MTD"},{"idx":4,"sentence":"A dynamically balanced robotic manipulator does not exert forces or moments onto the base on which it is fixed; this can be important for the performance of parallel robots as they are able to move at very high speeds, albeit usually have a reduced workspace.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"In recent years, kinematically redundant architectures have been proposed to mitigate the workspace limitations of parallel manipulators and increase their rotational capabilities; however, dynamically balanced versions of these architectures have not yet been presented.","offset":1,"pro":0.16666666666666666,"labels":"GAP"},{"idx":4,"sentence":"In this paper, a dynamically balanced kinematically redundant planar parallel architecture is introduced.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":4,"sentence":"The manipulator is composed of parallelogram linkages that reduce the number of counter rotary elements required to moment balance the mechanism.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":4,"sentence":"The balancing conditions are derived, and the balancing parameters are optimized using Lagrange multipliers, such that the total mass and inertia of the system is minimized.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":4,"sentence":"The elimination of the shaking forces and moments is then verified via a simulation in the multi-body dynamic simulation software msc adams.","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":5,"sentence":"In the age of Industry 4.0, the capability of health management is critical to the design and maintenance of gas turbines.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"This study presents a probabilistic method to estimate the low-cycle fatigue (LCF) life of a gas turbine compressor vane carrier (CVC) under varying operating conditions.","offset":1,"pro":0.125,"labels":"PUR"},{"idx":5,"sentence":"Sensitivity analysis based on the finite element analysis (FEA) indicates that an operating cycle can be characterized by three predominant contributors to the LCF damage of the CVC among multiple parameters of an operating cycle.","offset":2,"pro":0.25,"labels":"CLN"},{"idx":5,"sentence":"Two surrogate models mapping these three features to equivalent stresses are then built for fast computation of the LCF damage.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":5,"sentence":"Miner's rule is applied in a probabilistic way to calculate the distribution of accumulated LCF damage over varying operating cycles.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":5,"sentence":"Finally, the probabilistic LCF life of the CVC is assessed using real operational data.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":5,"sentence":"The proposed approach includes two novel solutions: (1) a new data processing technique inspired by the cumulative sum (CUSUM) control chart to identify the first ramp-up period as well as the shutdown period of each cycle from noisy operational data; (2) the sequential convolution strategy adapted from Miner's rule to compute the probability distribution of accumulated LCF damage (and hence LCF life) from the single-cycle damage distribution, and an approximative quick estimation method to reduce computational expense.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":5,"sentence":"Both the offline application for design and online implementation for predictive maintenance show that the expected LCF life at a critical location of the CVC is significantly longer than the deterministically assessed life.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":6,"sentence":"In the past few decades, multidisciplinary design optimization (MDO) has become a very important research topic along with the increase of the system complexity.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"In an MDO problem, it is very typical that multiple disciplines are involved, making the problem coupled and complex.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":6,"sentence":"Monolithic and distributed architectures have been proposed for solving MDO problems.","offset":2,"pro":0.2,"labels":"BAC"},{"idx":6,"sentence":"However, efficient architectures are still needed.","offset":3,"pro":0.3,"labels":"GAP"},{"idx":6,"sentence":"In the prior work, a sequential multidisciplinary design optimization (S-MDO) architecture was proposed that has a distributed structure that decomposes the original MDO problem into several subproblems.","offset":4,"pro":0.4,"labels":"BAC"},{"idx":6,"sentence":"However, in the original S-MDO work, the theoretical behaviors were not analyzed because its mathematical representations were not clear.","offset":5,"pro":0.5,"labels":"GAP"},{"idx":6,"sentence":"In this article, we present a clear mathematical representation of the S-MDO architecture and conduct theoretical analysis on the S-MDO architecture to explain its performance in solving MDO problems.","offset":6,"pro":0.6,"labels":"PUR"},{"idx":6,"sentence":"The optimality condition of the S-MDO architecture is derived and summarized as a theorem and a proposition.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":6,"sentence":"To demonstrate the general formulation of solving an MDO problem using the S-MDO architecture and validate the correctness of the optimality condition, we use it to obtain the Pareto frontier of a benchmark MDO problem.","offset":8,"pro":0.8,"labels":"PUR"},{"idx":6,"sentence":"From the spread of the obtained Pareto frontier, we can conclude that the S-MDO architecture performs well, as long as the global optimum of each disciplinary subproblem can be found.","offset":9,"pro":0.9,"labels":"RST"},{"idx":7,"sentence":"Architecture selection for systems undergoing rapid technological and market change is challenging.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"It is desirable to select architectures that can provide cost-effective possibilities for future changes and avoid architecture lock-in.","offset":1,"pro":0.1,"labels":"BAC"},{"idx":7,"sentence":"However, optimal architectures for prevailing conditions may not be changeable for future adaptation.","offset":2,"pro":0.2,"labels":"GAP"},{"idx":7,"sentence":"This tension between objectives for system (product) development for both short-term and long-term competitiveness has been an enduring challenge for system architects.","offset":3,"pro":0.3,"labels":"BAC"},{"idx":7,"sentence":"Here, we use time-expanded decision networks (TDNs) with time-varying costs and demands to systematically explore future architecture transition pathways and strategically identify useful designs.","offset":4,"pro":0.4,"labels":"PUR"},{"idx":7,"sentence":"We demonstrate a new application for autonomous driving (AD) systems, a nascent technology, where the design and capabilities of constituent components (such as sensors, processors, and data communication links) are still evolving and significant market and regulatory uncertainties persist.","offset":5,"pro":0.5,"labels":"PUR"},{"idx":7,"sentence":"In this case, we model technology costs with time-based factors to explicitly include future trends.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":7,"sentence":"The results show that as cost differences between architectures increase and demand for new functionality changes with time, the approach is able to identify potential transition points between architecture choices that optimize the net present value (NPV) of the system.","offset":7,"pro":0.7,"labels":"RST"},{"idx":7,"sentence":"For some of the specific scenarios analyzed in this study, the NPV with optimal architecture transitions is at least 10-20\\% larger as compared with fixed cases.","offset":8,"pro":0.8,"labels":"RST"},{"idx":7,"sentence":"Overall, this work presents a case for planning and partly constructing architecture transition roadmaps for new systems wherein dominant architectures have not emerged.","offset":9,"pro":0.9,"labels":"CTN"},{"idx":8,"sentence":"Research in engineering system evolution studies the technical performance (e.g., speed, capacity, and energy efficiency) and the functional and architectural changes of engineering systems (e.g., automobiles, aircrafts, laptops, and smartphones) over time.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"The research results of engineering system evolution help designers, R\\&D managers, investors, and policy makers to generate innovative design concepts, set reasonable R\\&D targets, invest in promising technologies, and develop effective incentive policies.","offset":1,"pro":0.16666666666666666,"labels":"BAC"},{"idx":8,"sentence":"In this paper, we introduce engineering system evolution as an emerging research area.","offset":2,"pro":0.3333333333333333,"labels":"PUR"},{"idx":8,"sentence":"We develop a cyclic model to understand the general structure of engineering system evolution and summarize seven basic research questions accordingly.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":8,"sentence":"A review and analysis of prior research related to engineering system evolution is provided to identify the pioneering works in this promising research area.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":8,"sentence":"We also discuss the challenges and opportunities in the quantitative and qualitative study of engineering system evolution for future research.","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":9,"sentence":"The four Ps of the marketing mix (Product, Price, Place, and Promotion) serve as a framework for characterizing the marketing decisions made during the product development process.","offset":0,"pro":0,"labels":"MTD"},{"idx":9,"sentence":"In this paper, we describe how the last 40 years of engineering design research has increasingly incorporated representations of preference as a means of addressing the decisions that come with each P.'} We argue that this incorporation began with problem formulations based on Product only, with surrogates of preference posed as objectives (such as minimizing weight, minimizing part count) representing a firm's desire for offering a mix of products while reducing cost and maximizing profit.","offset":1,"pro":0.14285714285714285,"labels":"PUR"},{"idx":9,"sentence":"As the complexity of problem formulations progressed, researchers began representing preferences of the designer (using decision theory techniques) and of the customer (often in the form of random utility models).","offset":2,"pro":0.2857142857142857,"labels":"BAC"},{"idx":9,"sentence":"The Design for Market Systems special session was created specifically in the Design Automation Conference for advancing our understanding of design in the content of a market, extending from the decision-based design framework introduced by Hazelrigg.","offset":3,"pro":0.42857142857142855,"labels":"BAC"},{"idx":9,"sentence":"Since then, researchers have explored the engineering design problem formulation challenges associated with the marketing decisions of Price, Place, and Promotion.","offset":4,"pro":0.5714285714285714,"labels":"BAC"},{"idx":9,"sentence":"This paper highlights the advancements of the design community in each of the Ps and shows how the marketing decisions of Place and Promotion extend from the central hub of considering Price in an engineering design problem.","offset":5,"pro":0.7142857142857143,"labels":"PUR"},{"idx":9,"sentence":"We also highlight the exciting research opportunities that exist as the community considers more complicated, and interconnected, problem formulations that encompass the entirety of the Marketing Mix.","offset":6,"pro":0.8571428571428571,"labels":"CTN"},{"idx":10,"sentence":"Standardized design approaches such as those embodied by concurrent design facilities have many benefits, such as increased efficiency of the design process, but may also have hidden costs.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"Specifically, when their standardized organizational decomposition is a poor fit for the particular design problem, important design trades might be missed or poor decisions made.","offset":1,"pro":0.2,"labels":"GAP"},{"idx":10,"sentence":"Before we can understand how this lack of fit impacts the design process, we must be able to empirically observe and measure it.","offset":2,"pro":0.4,"labels":"GAP"},{"idx":10,"sentence":"To that end, this paper identifies measures of fit.from the literature along with attributes likely to impact design process performance, then evaluates the measures to determine how well the measures can detect and diagnose potential issues.","offset":3,"pro":0.6,"labels":"PUR"},{"idx":10,"sentence":"The results provide comparative insights into the capabilities of existing fit measures, and also build guidance for how the systems engineering and design community can use insights from the fit'} literature to inform process improvement.","offset":4,"pro":0.8,"labels":"RST"},{"idx":11,"sentence":"The critical problem of reliability design is how to obtain a more accurate failure probability with a smaller number of evaluations of actual complex and nonlinear performance function.","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"To achieve this objective, an adaptive subset simulation method with a deep neural network (DNN) is proposed for accurate estimation of small failure probability.","offset":1,"pro":0.125,"labels":"PUR"},{"idx":11,"sentence":"A determinate criterion for threshold values is developed, and the subset number is adaptively quantified according to the initial estimated value of small failure probability.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":11,"sentence":"Therefore, the estimation of small failure probability is converted to estimation problem of multiple large conditional probabilities.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":11,"sentence":"An adaptive deep neural network model is constructed in every subset to predict the conditional probability with a smaller number of evaluations of the actual performance function.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":11,"sentence":"Furthermore, the sampling points for the next subset can be adaptively selected according to the constructed DNN model, which can decrease the number of invalid sampling points and evaluations of actual performance function, then the computational efficiency for estimating the conditional probability in every subset is increased.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":11,"sentence":"The sampling points with high probability density functions are recalculated with actual performance function values to replace the predicted values of the DNN model, which can verify the accuracy of DNN model and increase the estimation accuracy of small failure probability.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":11,"sentence":"By analyzing a nonlinear problem, a multiple failure domain problem and two engineering examples, the effectiveness and accuracy of the proposed methodology for estimating small failure probability are verified.","offset":7,"pro":0.875,"labels":"RST"},{"idx":12,"sentence":"Designers should adequately develop user considerations such as usability, safety, and comfort during the design process of new systems.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"Nevertheless, incorporating human factors engineering principles during early design phases is not simple.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":12,"sentence":"The objective of this work is to assist designers in implementing human factors engineering principles during early design phases using a functional model framework.","offset":2,"pro":0.2222222222222222,"labels":"PUR"},{"idx":12,"sentence":"This effort expands our previous work on automating the function-human error design method (FHEDM) implementation.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":12,"sentence":"In this work, we use data mining techniques in a design repository to explore the construction of association rules between components, functions, flows, and user interactions.","offset":4,"pro":0.4444444444444444,"labels":"PUR"},{"idx":12,"sentence":"Such association rules can support designers assessing user-system interactions during the early design stages.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":12,"sentence":"To validate this approach, we compare the associations generated by expert designers using the FHEDM while designing a new product to those generated by an algorithm using the repository data.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":12,"sentence":"The results show notable similarities between the associations extracted by the algorithm and the associations identified by designers.","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":12,"sentence":"Thus, the overall results show that association rules extracted from a rich dataset can be used to distinguish user-product interactions, demonstrating the potential of automating the identification of user-product interactions from a functional model.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":13,"sentence":"Automated design synthesis has the potential to revolutionize the modern engineering design process and improve access to highly optimized and customized products across countless industries.","offset":0,"pro":0,"labels":"BAC"},{"idx":13,"sentence":"Successfully adapting generative machine learning to design engineering may enable such automated design synthesis and is a research subject of great importance.","offset":1,"pro":0.09090909090909091,"labels":"BAC"},{"idx":13,"sentence":"We present a review and analysis of deep generative machine learning models in engineering design.","offset":2,"pro":0.18181818181818182,"labels":"PUR"},{"idx":13,"sentence":"Deep generative models (DGMs) typically leverage deep networks to learn from an input dataset and synthesize new designs.","offset":3,"pro":0.2727272727272727,"labels":"BAC"},{"idx":13,"sentence":"Recently, DGMs such as feedforward neural networks (NNs), generative adversarial networks (GANs), variational autoencoders (VAEs), and certain deep reinforcement learning (DRL) frameworks have shown promising results in design applications like structural optimization, materials design, and shape synthesis.","offset":4,"pro":0.36363636363636365,"labels":"BAC"},{"idx":13,"sentence":"The prevalence of DGMs in engineering design has skyrocketed since 2016.","offset":5,"pro":0.45454545454545453,"labels":"BAC"},{"idx":13,"sentence":"Anticipating the continued growth, we conduct a review of recent advances to benefit researchers interested in DGMs for design.","offset":6,"pro":0.5454545454545454,"labels":"PUR"},{"idx":13,"sentence":"We structure our review as an exposition of the algorithms, datasets, representation methods, and applications commonly used in the current literature.","offset":7,"pro":0.6363636363636364,"labels":"MTD"},{"idx":13,"sentence":"In particular, we discuss key works that have introduced new techniques and methods in DGMs, successfully applied DGMs to a design-related domain, or directly supported the development of DGMs through datasets or auxiliary methods.","offset":8,"pro":0.7272727272727273,"labels":"MTD"},{"idx":13,"sentence":"We further identify key challenges and limitations currently seen in DGMs across design fields, such as design creativity, handling constraints and objectives, and modeling both form and functional performance simultaneously.","offset":9,"pro":0.8181818181818182,"labels":"MTD"},{"idx":13,"sentence":"In our discussion, we identify possible solution pathways as key areas on which to target the future work.","offset":10,"pro":0.9090909090909091,"labels":"MTD"},{"idx":14,"sentence":"Although artificial intelligence (AI) systems which support composition using predictive text are well established, there are no analogous technologies for mechanical design.","offset":0,"pro":0,"labels":"GAP"},{"idx":14,"sentence":"Motivated by the vision of a predictive system that learns from previous designs and can interactively provide a list of established feature alternatives to the designer as design progresses, this paper describes the theory, implementation, and assessment of an intelligent system that learns from a family of previous designs and generates inferences using a form of spatial statistics.","offset":1,"pro":0.1,"labels":"PUR"},{"idx":14,"sentence":"The formalism presented models 3D design activity as a marked point process.that enables the probability of specific features being added at particular locations to be calculated.","offset":2,"pro":0.2,"labels":"MTD"},{"idx":14,"sentence":"Because the resulting probabilities are updated every time a new feature is added, the predictions will become more accurate as a design develops.","offset":3,"pro":0.3,"labels":"MTD"},{"idx":14,"sentence":"This approach allows the cursor position on a CAD model to implicitly define a spatial focus for every query made to the statistical model.","offset":4,"pro":0.4,"labels":"MTD"},{"idx":14,"sentence":"The authors describe the mathematics underlying a statistical model that amalgamates the frequency of occurrence of the features in the existing designs of a product family.","offset":5,"pro":0.5,"labels":"PUR"},{"idx":14,"sentence":"Having established the theoretical foundations of the work, a generic six-step implementation process is described.","offset":6,"pro":0.6,"labels":"MTD"},{"idx":14,"sentence":"This process is then illustrated for circular hole features using a statistical model generated from a dataset of hydraulic valves.","offset":7,"pro":0.7,"labels":"MTD"},{"idx":14,"sentence":"The paper describes how the positions of each design's extracted hole features can be homogenized through rotation and scaling.","offset":8,"pro":0.8,"labels":"PUR"},{"idx":14,"sentence":"Results suggest that within generic part families (i.e., designs with common structure), a marked point process can be effective at predicting incremental steps in the development of new designs.","offset":9,"pro":0.9,"labels":"CLN"},{"idx":15,"sentence":"High trafficability and stability are the most two significant features of the forestry chassis.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"In this study, in order to improve surface trafficability, a novel articulated wheel-legged forestry chassis (AWLFC) is presented.","offset":1,"pro":0.1111111111111111,"labels":"PUR"},{"idx":15,"sentence":"To balance the trafficability and stability, a serial suspension system which is a combination with the active four-bar linkage articulated suspension (AFLAS) and passive V shape rocker-bogie is proposed.","offset":2,"pro":0.2222222222222222,"labels":"MTD"},{"idx":15,"sentence":"Then, parameter optimization with a comprehensive object function is implemented not only to enhance the trafficability and stability benefit of the structure but also to reduce the wheel slip.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":15,"sentence":"After that, through the flexible kinematic model based on screw theory, characteristics such as leveling ability and surface profile accessibility of the chassis are analyzed.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":15,"sentence":"The minimum accessible radius is obtained as 3088 mm, and the longitudinal and lateral leveling angle reaches to 22 deg and 28.7 deg separately.","offset":5,"pro":0.5555555555555556,"labels":"RST"},{"idx":15,"sentence":"The new chassis performs better on leveling ability and surface profile accessibility than the forestry chassis in the current literature.","offset":6,"pro":0.6666666666666666,"labels":"CLN"},{"idx":15,"sentence":"Finally, through the results of simulation and prototype experiment, error rates related to the flexible analysis are reduced by 12.2\\% and 8.6\\% compared with the rigid model.","offset":7,"pro":0.7777777777777778,"labels":"RST"},{"idx":15,"sentence":"Previously inaccessible forestry working environments can be available with the development of AWLFC.","offset":8,"pro":0.8888888888888888,"labels":"CTN"},{"idx":16,"sentence":"Multi-fidelity surrogate model-based engineering optimization has received much attention because it alleviates the computational burdens of expensive simulations or experiments.","offset":0,"pro":0,"labels":"BAC"},{"idx":16,"sentence":"However, due to the nonlinearity of practical engineering problems, the initial sample set selected to produce the first set of data will almost inevitably miss certain features of the landscape, and thus, the construction of a useful surrogate often requires further, judicious infilling of some new samples.","offset":1,"pro":0.1111111111111111,"labels":"GAP"},{"idx":16,"sentence":"Sequential sampling strategies used to select new infilling samples during each iteration can gradually extend the data set and improve the accuracy of the initial model with an acceptable cost.","offset":2,"pro":0.2222222222222222,"labels":"BAC"},{"idx":16,"sentence":"In this paper, a sequential sampling generation method based on the Voronoi region and the sample density (SSGM-VRDS) is proposed.","offset":3,"pro":0.3333333333333333,"labels":"PUR"},{"idx":16,"sentence":"First, with a Monte Carlo-based approximation of a Voronoi tessellation for region division, Pearson correlation coefficients and cross-validation (CV) are employed to determine the candidate Voronoi region for infilling a new sample.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":16,"sentence":"Then, a relative sample density is defined to identify the position of the new infilling point at which the sample is the sparsest within the selected Voronoi region.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":16,"sentence":"A correction of this density is carried out concurrently through an expansion coefficient.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":16,"sentence":"The proposed method is applied to three numerical functions and a lightweight design problem via finite element analysis (FEA).","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":16,"sentence":"Results suggest that the SSGM-VRDS strategy has outstanding effectiveness and efficiency in selecting a new sample for improving the accuracy of a surrogate model, as well as practicality for solving practical optimization problems.","offset":8,"pro":0.8888888888888888,"labels":"CLN"},{"idx":17,"sentence":"For customized design of a hybrid manipulator for a specific application, selection of an appropriate configuration is always a challenge.","offset":0,"pro":0,"labels":"GAP"},{"idx":17,"sentence":"To assist in this foremost decision in data-driven synthesis, a novel approach is proposed for modular formation of quick configurations.","offset":1,"pro":0.1111111111111111,"labels":"PUR"},{"idx":17,"sentence":"Majorly, a unified methodology is presented for the development of respective kinematic models and differential relations for their performance analyses.","offset":2,"pro":0.2222222222222222,"labels":"MTD"},{"idx":17,"sentence":"This unified modular approach utilizes modular primitives to define planar hybrid configurations.","offset":3,"pro":0.3333333333333333,"labels":"MTD"},{"idx":17,"sentence":"Three types of primitives are introduced as modular components, and the pattern study is detailed.","offset":4,"pro":0.4444444444444444,"labels":"MTD"},{"idx":17,"sentence":"Modeling results from the proposed approach are also compared with normally used partial differentiation with respect to the computational efforts, streamlined modular implementations, and applicability in optimal design approaches.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":17,"sentence":"The comparison highlights how the column-wise approach is appropriate for modular methodology.","offset":6,"pro":0.6666666666666666,"labels":"CLN"},{"idx":17,"sentence":"The strategy will help a designer as a tool for analyzing several configurations.","offset":7,"pro":0.7777777777777778,"labels":"CTN"},{"idx":17,"sentence":"Two realistic case studies are demonstrated in this article for application of the methodology in the medical robotics field.","offset":8,"pro":0.8888888888888888,"labels":"RST"},{"idx":18,"sentence":"Reconstruction of gear tooth surfaces from point clouds obtained by noncontact metrology machines constitutes a promising step forward not only for a fast gear inspection but also for reverse engineering and virtual testing and analysis of gear drives.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"In this article, a new methodology to reconstruct spiral bevel gear tooth surfaces from point clouds obtained by noncontact metrology machines is proposed.","offset":1,"pro":0.125,"labels":"PUR"},{"idx":18,"sentence":"The need of application of a filtering process to the point clouds before the process of reconstruction of the gear tooth surfaces has been revealed.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":18,"sentence":"Hence, the bilateral filter commonly used for 3D object recognition has been applied and integrated in the proposed methodology.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":18,"sentence":"The shape of the contact patterns and the level of the unloaded functions of transmission errors are considered as the criteria to select the appropriate settings of the bilateral filter.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":18,"sentence":"The results of the tooth contact analysis of the reconstructed gear tooth surfaces show a good agreement with the design ones.","offset":5,"pro":0.625,"labels":"CLN"},{"idx":18,"sentence":"However, stress analyses performed with reconstructed gear tooth surfaces reveal that the maximum level of contact pressures is overestimated.","offset":6,"pro":0.75,"labels":"CLN"},{"idx":18,"sentence":"A numerical example based on a spiral bevel gear drive is presented.","offset":7,"pro":0.875,"labels":"PUR"},{"idx":19,"sentence":"Prior research suggests that excess (purposeful inclusion of margin beyond what is required for known system uncertainties) can limit change propagation and reduce system modifications.","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"Reducing change costs increases system flexibility, permitting adaptions that satisfy uncertain future requirements.","offset":1,"pro":0.07692307692307693,"labels":"BAC"},{"idx":19,"sentence":"The benefits of excess, however, must be traded against higher costs of the initial system and likely performance decreases.","offset":2,"pro":0.15384615384615385,"labels":"GAP"},{"idx":19,"sentence":"Assessing the benefits and costs of excess requires evaluating what forms, locations, and magnitudes of excess inclusion are optimal.","offset":3,"pro":0.23076923076923078,"labels":"BAC"},{"idx":19,"sentence":"This paper improves the state-of-the-art in two ways.","offset":4,"pro":0.3076923076923077,"labels":"PUR"},{"idx":19,"sentence":"First, prior research has generally assessed excess in system-level properties (aggregating component properties into a single metric).","offset":5,"pro":0.38461538461538464,"labels":"BAC"},{"idx":19,"sentence":"The approach presented in this paper extends excess assessment to the component level so that the effects of excess on change propagation may be explicitly captured.","offset":6,"pro":0.46153846153846156,"labels":"PUR"},{"idx":19,"sentence":"Second, this approach holistically assesses the value of excess by evaluating both its costs and benefits.","offset":7,"pro":0.5384615384615384,"labels":"MTD"},{"idx":19,"sentence":"The approach borrows from Decision-Based Design and Model Based System Engineering (MBSE) in creating a generic modeling method capable of excess valuation.","offset":8,"pro":0.6153846153846154,"labels":"MTD"},{"idx":19,"sentence":"A desktop computer example is used for demonstrating how excess is valued in a system and the potential gains associated with excess inclusion when mining cryptocurrency.","offset":9,"pro":0.6923076923076923,"labels":"MTD"},{"idx":19,"sentence":"A single component optimization of the power supply capacity for the desktop is assessed to be 750 W, which balances the initial cost against the future flexibility.","offset":10,"pro":0.7692307692307693,"labels":"MTD"},{"idx":19,"sentence":"A system-level optimization then demonstrates the identification of critical change propagation pathways and illuminates both where and how excess may be included to inhibit change propagation.","offset":11,"pro":0.8461538461538461,"labels":"MTD"},{"idx":19,"sentence":"This key component was identified as the motherboard-central processing unit (CPU) slot in the tested systems.","offset":12,"pro":0.9230769230769231,"labels":"MTD"},{"idx":0,"sentence":"Clean technologies aim to address climatic, environmental, and health concerns associated with their conventional counterparts.","offset":0,"pro":0,"labels":"BAC"},{"idx":0,"sentence":"However, such technologies achieve these goals only if they are adopted by users and effectively replace conventional practices.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":0,"sentence":"Despite the important role that users play to accomplish these goals by making decisions whether to adopt such clean alternatives or not, currently, there is no systematic framework for quantitative integration of the behavioral motivations of users during the design process for these technologies.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":0,"sentence":"In this study, the theory of planned behavior (TPB) is integrated with usage-context-based design to provide a holistic approach for predicting the market share of clean versus conventional product alternatives based on users' personal beliefs, social norms, and perception of behavioral control.","offset":3,"pro":0.375,"labels":"PUR"},{"idx":0,"sentence":"Based on the mathematical linkage of the model components, technology design attributes can then be adjusted, resulting in the design of products that are more in line with users' behavioral intentions, which can lead to higher adoption rates.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":0,"sentence":"The developed framework is applied in a case study of adoption of improved cookstoves in a community in Northern Uganda.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":0,"sentence":"Results indicate that incorporating TPB attributes into utility functions improves the prediction power of the model and that the attributes that users in the subject community prioritize in a clean cookstove are elicited through the TPB.","offset":6,"pro":0.75,"labels":"CLN"},{"idx":0,"sentence":"Households' decision-making behavior before and after a trial period suggests that design and marketing strategy should systematically integrate user's behavioral tendencies prior to interventions to improve the outcomes of clean technology implementation projects.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":1,"sentence":"This paper presents a new trajectory planning method based on the improved quintic B-splines curves for a three degrees-of-freedom (3-DOF) cable-driven parallel robot (CDPR).","offset":0,"pro":0,"labels":"PUR"},{"idx":1,"sentence":"First, the conditions of positive cables' tension are expressed in terms of the position and acceleration constraints of the end-effector.","offset":1,"pro":0.125,"labels":"MTD"},{"idx":1,"sentence":"Then, an improved B-spline curve is introduced, which is employed for generating a pick-and-place path by interpolating a set of given via-points.","offset":2,"pro":0.25,"labels":"MTD"},{"idx":1,"sentence":"Meanwhile, by expressing the position and acceleration of the end-effector in terms of the first and second derivatives of the improved B-spline, the cable tension constraints are described in the form of B-spline parameters.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":1,"sentence":"According to the properties of the defined pick-and-place path, the proposed motion profile is dominated by two factors: the time taken for the end-effector to pass through all the via-points and the ratio between the nodes of B-spline.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":1,"sentence":"The two factors are determined through multi-objective optimization based on the efficiency coefficient method.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":1,"sentence":"Finally, experimental results on a 3-DOF CDPR show that the improved B-spline exhibits overall superior behavior in terms of velocity, acceleration, and cables force compared with the traditional B-spline.","offset":6,"pro":0.75,"labels":"RST"},{"idx":1,"sentence":"The validity of the proposed trajectory planning method is proved through the experiments.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":2,"sentence":"Roots rotors dramatically determine the performance of Roots vacuum pumps.","offset":0,"pro":0,"labels":"BAC"},{"idx":2,"sentence":"In order to develop new rotors with higher performance, a pair of novel asymmetrical rotors or eccentric involute rotors were proposed based on the conventional symmetrical involute rotors.","offset":1,"pro":0.16666666666666666,"labels":"PUR"},{"idx":2,"sentence":"Profiles of one rotor consist of four eccentric involutes and four circular arcs; profiles of the other rotor consist of four conjugate curves of eccentric involutes and four circular arcs.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":2,"sentence":"A model of the proposed asymmetrical rotors was established, and effects of geometric parameters on the area-utilizing ratio of rotors were analyzed.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":2,"sentence":"The study results show that the proposed asymmetrical rotors have more number of independent geometric parameters than the conventional rotors of Roots vacuum pumps, so that the asymmetrical rotors are superior to the conventional rotors in terms of design flexibility.","offset":4,"pro":0.6666666666666666,"labels":"CLN"},{"idx":2,"sentence":"The contents of this study can be applied in the design of Roots vacuum pumps.","offset":5,"pro":0.8333333333333334,"labels":"CTN"},{"idx":3,"sentence":"In vehicle collision accidents, an occupant restraint system (ORS) is crucial to protect the human body from injury, and it commonly involves a large number of design parameters.","offset":0,"pro":0,"labels":"BAC"},{"idx":3,"sentence":"However, it is very difficult to quantify the importance of design parameters and determine them in the ORS design process.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":3,"sentence":"Therefore, an approach of the combination of the proposed approximate sensitivity analysis (SA) method and the interval multi-objective optimization design is presented to reduce craniocerebral injury and improve ORS protection performance.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":3,"sentence":"First, to simulate the vehicle collision process and obtain the craniocerebral injury responses, the integrated finite element model of vehicle -occupant (IFEM-VO) is established by integrating the vehicle, dummy, seatbelt, airbag, etc. Then, the proposed approximate SA method is used to quantify the importance ranking of design parameters and ignore the effects of some nonessential parameters.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":3,"sentence":"In the SA process, the Kriging meta model characterizing the relationships between design parameters and injury responses is fitted to overcome the time-consuming disadvantage of IFEM-VO.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":3,"sentence":"Finally, according to the results of SA, considering the influence of uncertainty, an interval multiobjective optimization design is implemented by treating the brain injury criteria (BRIC, BrIC) as the objectives and regarding the head injury criterion (HIC) and the rotational injury criterion (RIC) as the constraints.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":3,"sentence":"Comparison of the results before and after optimization indicates that the maximum values of the translational and rotational accelerations are greatly reduced, and the ORS protection performance is significantly improved.","offset":6,"pro":0.75,"labels":"CLN"},{"idx":3,"sentence":"This study provides an effective way to improve the protection performance of vehicle ORS under uncertainty.","offset":7,"pro":0.875,"labels":"CTN"},{"idx":4,"sentence":"Practicing design engineers often have certain knowledge about a design problem.","offset":0,"pro":0,"labels":"BAC"},{"idx":4,"sentence":"However, in the last decades, the design optimization community largely treats design functions as black-boxes.","offset":1,"pro":0.14285714285714285,"labels":"GAP"},{"idx":4,"sentence":"This paper discusses whether and how knowledge can help with optimization, especially for large-scale optimization problems.","offset":2,"pro":0.2857142857142857,"labels":"PUR"},{"idx":4,"sentence":"Existing large-scale optimization methods based on black-box functions are first reviewed, and the drawbacks of those methods are briefly discussed.","offset":3,"pro":0.42857142857142855,"labels":"MTD"},{"idx":4,"sentence":"To understand what knowledge is and what kinds of knowledge can be obtained and applied in a design, the concepts of knowledge in both artificial intelligence (AI) and in the area of the product design are reviewed.","offset":4,"pro":0.5714285714285714,"labels":"MTD"},{"idx":4,"sentence":"Existing applications of knowledge in optimization are reviewed and categorized.","offset":5,"pro":0.7142857142857143,"labels":"MTD"},{"idx":4,"sentence":"Potential applications of knowledge for optimization are discussed in more detail, in hope to identify possible directions for future research in knowledge-assisted optimization (KAO).","offset":6,"pro":0.8571428571428571,"labels":"MTD"},{"idx":5,"sentence":"Herringbone gear transmission system is widely used in high-speed overloaded fields such as ships.","offset":0,"pro":0,"labels":"BAC"},{"idx":5,"sentence":"Tooth deformation and installation error, which can cause meshing impact, load mutation, and load uneven, will seriously affect dynamic performance of the whole transmission system.","offset":1,"pro":0.14285714285714285,"labels":"BAC"},{"idx":5,"sentence":"Modification technology is the most effective way to achieve damping and noise reduction.","offset":2,"pro":0.2857142857142857,"labels":"BAC"},{"idx":5,"sentence":"In this article, we propose a three-dimensional (3D) modified tooth surface by grinding wheel along axial to the spiral rise and along radial to the parabolic movement based on the grinding principle and a multiobjective ant lion optimizer model for optimizing meshing performance and dynamic characteristics of herringbone gear transmission system and analyze the impact of optimized modifications of tooth profile, axial, and three dimension on meshing performance, loaded transmission error, load distribution coefficient and meshing impact, meshing stiffness, and vibration acceleration by the example.","offset":3,"pro":0.42857142857142855,"labels":"PUR"},{"idx":5,"sentence":"The results show that the 3D modification of optimization can not only eliminate the contact between the tooth side edge and tooth top edge but also eliminate the influence of installation error on contact performance.","offset":4,"pro":0.5714285714285714,"labels":"RST"},{"idx":5,"sentence":"The root mean square values of the relative comprehensive vibration acceleration of tooth profile modification, axial modification, and 3D modification are reduced by 30.11\\%, 49.24\\%, and 61.41\\% compared with the standard, respectively.","offset":5,"pro":0.7142857142857143,"labels":"RST"},{"idx":5,"sentence":"The 3D modification can greatly reduce tooth vibration, reduce resonance peak, and achieve the goal of noise reduction.","offset":6,"pro":0.8571428571428571,"labels":"CLN"},{"idx":6,"sentence":"Mobility prediction of off-road autonomous ground vehicles (AGV) in uncertain environments is essential for their model-based mission planning, especially in the early design stage.","offset":0,"pro":0,"labels":"BAC"},{"idx":6,"sentence":"While surrogate modeling methods have been developed to overcome the computational challenge in simulation-based mobility prediction, it is very challenging for a single surrogate model to accurately capture the complicated vehicle dynamics.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":6,"sentence":"With a focus on vertical acceleration of an AGV under off-road conditions, this article proposes a surrogate modeling approach for AGV mobility prediction using a dynamic ensemble of nonlinear autoregressive models with exogenous inputs (NARX) over time.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":6,"sentence":"Synthetic vehicle mobility data of an AGV are first collected using a limited number of high-fidelity simulations.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":6,"sentence":"The data are then partitioned into different segments using a variational Gaussian mixture model to represent different vehicle dynamic behaviors.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":6,"sentence":"Based on the partitioned data, multiple surrogate models are constructed under the NARX framework with different numbers of lags.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":6,"sentence":"The NARX models are then assembled together dynamically over time to predict the mobility of the AGV under new conditions.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":6,"sentence":"A case study demonstrates the advantages of the proposed method over the classical NARX models for AGV mobility prediction.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":7,"sentence":"With limited time and resources available to carry out Engineering for Global Development (EGD) projects, it can be difficult to know where those resources should be allocated to have greater potential for meaningful impact.","offset":0,"pro":0,"labels":"BAC"},{"idx":7,"sentence":"It is easy to assume that projects should occur in a particular location based on personal experience or where other development projects are taking place.","offset":1,"pro":0.1111111111111111,"labels":"BAC"},{"idx":7,"sentence":"This can be a consideration, but it may not lead to the greatest social impact.","offset":2,"pro":0.2222222222222222,"labels":"GAP"},{"idx":7,"sentence":"Where to work on a project and what problem to work on are key questions in the early stages of product development in the context of EGD.","offset":3,"pro":0.3333333333333333,"labels":"GAP"},{"idx":7,"sentence":"To aid in this process, this article presents a method for assessing global needs to ensure thoughtful use of limited EGD resources.","offset":4,"pro":0.4444444444444444,"labels":"PUR"},{"idx":7,"sentence":"We introduce a method for identifying locations where there is human need, gaps in technological achievement, and what the work environment is in a country.","offset":5,"pro":0.5555555555555556,"labels":"MTD"},{"idx":7,"sentence":"Results of the method are compared to what countries receive the most foreign aid dollars per capita.","offset":6,"pro":0.6666666666666666,"labels":"MTD"},{"idx":7,"sentence":"Measures were calculated using the principal component analysis on data from development agencies.","offset":7,"pro":0.7777777777777778,"labels":"MTD"},{"idx":7,"sentence":"These results can help practitioners in selecting where to undertake development projects with an eye toward targeting locations that may yield high levels of social impact.","offset":8,"pro":0.8888888888888888,"labels":"CTN"},{"idx":8,"sentence":"The fuzzy front end of engineering design can present a difficult challenge, and as such, recent engineering design research has focused on guiding and influencing the way a designer ideates.","offset":0,"pro":0,"labels":"BAC"},{"idx":8,"sentence":"Early ideation can be especially difficult when attempting to integrate specific design objectives in product design, called Design for X (DfX).","offset":1,"pro":0.125,"labels":"BAC"},{"idx":8,"sentence":"This paper presents two experiments exploring the efficacy of a structured Design for the Environment (DfE) design method called the Guidelines and Regulations for Early design for the Environment (GREEn) Quiz that provides designers with sustainable design knowledge during the conceptual design phase.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":8,"sentence":"The GREEn Quiz operates on a web-based platform and queries the designer about their design concepts; an end-of-quiz report provides abstract DfE knowledge to designers.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":8,"sentence":"While this abstract knowledge was able to be applied by designers in a former study, we hypothesize that providing targeted, specific design strategies during conceptual design will enable novice designers to better integrate DfE.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":8,"sentence":"In this study, we created these DfE strategies, integrated these into the GREEn Quiz, and studied the efficacy of these strategies when presented to designers at both the expert and novice levels.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":8,"sentence":"Results suggest that respondents with access to the strategy-based GREEn Quiz produced concepts with evidence of a broader range of sustainable design decisions and higher solution quality scores.","offset":6,"pro":0.75,"labels":"CLN"},{"idx":8,"sentence":"This work shows the promise of supplemental DfE methods for concept generation to enable the design of more environmentally sustainable products.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":9,"sentence":"A number of machine learning methods have been recently proposed to circumvent the high computational cost of the gradient-based topology optimization solvers.","offset":0,"pro":0,"labels":"BAC"},{"idx":9,"sentence":"By and large, these methods show tight generalizability to unseen boundary and external loading conditions, require prohibitively large datasets for training, and do not take into consideration topological constraints of the predictions, which results in solutions with unpredictable connectivity.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":9,"sentence":"To address these limitations, we propose a design exploration framework for topology optimization that exploits the knowledge transfer capability of the transfer learning methods and the generative power of conditional generative adversarial networks (GANs).","offset":2,"pro":0.25,"labels":"PUR"},{"idx":9,"sentence":"We show that the proposed framework significantly exceeds the generalization ability of current methods.","offset":3,"pro":0.375,"labels":"RST"},{"idx":9,"sentence":"Moreover, the proposed architecture is capable of reusing the knowledge learned on low-resolution and computationally inexpensive samples, which notably reduces both the size of the required high-resolution training datasets and the demand on the computational infrastructure needed to generate the training data.","offset":4,"pro":0.5,"labels":"RST"},{"idx":9,"sentence":"Finally, we propose and evaluate novel approaches to improve the structural connectivity of the predicted optimal topology by including topological metrics into the loss function.","offset":5,"pro":0.625,"labels":"MTD"},{"idx":9,"sentence":"We show that by including the bottleneck distance between the persistence diagrams of the predicted and ground truth structures, we significantly improve the connectivity of the prediction.","offset":6,"pro":0.75,"labels":"RST"},{"idx":9,"sentence":"Together, our results reveal the ability of generative adversarial networks implemented in a transfer learning environment to serve as powerful and practical real-time design exploration tools in topology optimization.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":10,"sentence":"High power density in-wheel motor could be achieved by integrating a high-speed ratio (larger than 25) compound power-split mechanism (CPSM) with small motors.","offset":0,"pro":0,"labels":"BAC"},{"idx":10,"sentence":"However, due to the exhaustive searching method adopted by the traditional lever analogy method, it is time-consuming to design high-speed ratio compound power-split mechanism configurations satisfying the high power density of in-wheel motor.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":10,"sentence":"In this paper, an improved lever analogy method is proposed to find the optimal configurations with a high-speed ratio to satisfy the high power density in-wheel motor.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":10,"sentence":"In this method, a judgment algorithm about the rank of structure matrix is proposed to identify three-node compound lever models of the CPSM.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":10,"sentence":"The improved lever analogy method can filter out useless configurations that significantly improve the calculation efficiency.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":10,"sentence":"The simulation results show that the calculation efficiency is improved by 215 times higher than that of the lever analogy method.","offset":5,"pro":0.625,"labels":"RST"},{"idx":10,"sentence":"Finally, 16 reasonable and 14 new configurations are obtained.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":10,"sentence":"This indicates that the improved lever analogy method can provide an effective way to design the high-speed ratio CPSM, which is widely used in-wheel motor-driven vehicles.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":11,"sentence":"Reliability-based design (RBD) employs optimization to identify design variables that satisfy the reliability requirement.","offset":0,"pro":0,"labels":"BAC"},{"idx":11,"sentence":"For many routine component design jobs that do not need optimization, however, RBD may not be applicable, especially for those design jobs which are performed manually or with a spreadsheet.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":11,"sentence":"This work develops a modified RBD approach to component design so that the reliability target can be achieved by conducting traditional component design repeatedly using a deterministic safety factor.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":11,"sentence":"The new component design is based on the first-order reliability method (FORM), which iteratively assigns the safety factor during the design process until the reliability requirement is satisfied.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":11,"sentence":"In addition to several iterations of deterministic component design, the other additional work is the calculation of the derivatives of the design margin with respect to the random input variables.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":11,"sentence":"The proposed method can be used for a wide range of component design applications.","offset":5,"pro":0.625,"labels":"CTN"},{"idx":11,"sentence":"For example, if a deterministic component design is performed manually or with a spreadsheet, so is the reliability-based component design.","offset":6,"pro":0.75,"labels":"CTN"},{"idx":11,"sentence":"Three examples are used to demonstrate the practicality of the new design method.","offset":7,"pro":0.875,"labels":"CTN"},{"idx":12,"sentence":"The bamboo weevil, Cyrtotrachelus buqueti, has excellent flight ability and strong environmental adaptability.","offset":0,"pro":0,"labels":"BAC"},{"idx":12,"sentence":"When it flies, its fore wings and hind wings are unfolded, whereas when it crawls, its fore wings are closed, and its flexible hind wings are regularly folded under the fore wings.","offset":1,"pro":0.09090909090909091,"labels":"BAC"},{"idx":12,"sentence":"In this paper, the hind wing folding/unfolding pattern of C. buqueti is analyzed and a new bionic foldable wing with rigid-flexible coupling consisting of a link mechanism and a wing membrane is constructed.","offset":2,"pro":0.18181818181818182,"labels":"PUR"},{"idx":12,"sentence":"The movement of the link at the wing base mimics the contraction of a muscle in the thorax that triggers scissor-like motion and the deployment of the veins.","offset":3,"pro":0.2727272727272727,"labels":"MTD"},{"idx":12,"sentence":"Elastic hinges are used to mimic the rotational motion of the wing base and the vein joints.","offset":4,"pro":0.36363636363636365,"labels":"MTD"},{"idx":12,"sentence":"The static/dynamic characteristics of bionic foldable wings are further analyzed, and the LS-DYNA software is used to investigate rigid-flexible coupling dynamics.","offset":5,"pro":0.45454545454545453,"labels":"MTD"},{"idx":12,"sentence":"The elastic deformation of the wing membrane, kinematic characteristics of the linkage mechanism, and modes of the whole system are calculated.","offset":6,"pro":0.5454545454545454,"labels":"MTD"},{"idx":12,"sentence":"Static analysis of the structure reveals that the foldable wing has excellent stiffness characteristics and load-bearing capacity.","offset":7,"pro":0.6363636363636364,"labels":"RST"},{"idx":12,"sentence":"The bionic foldable wing is constructed using three-dimensional (3D) printing technology, and its folding and unfolding performance is tested.","offset":8,"pro":0.7272727272727273,"labels":"MTD"},{"idx":12,"sentence":"Evaluation of its performance shows that the bionic wing has a large fold ratio and can achieve stable folding and unfolding motions.","offset":9,"pro":0.8181818181818182,"labels":"RST"},{"idx":12,"sentence":"A slightly tighter assembly between the pin and the hinge hole ensures that the wing does not fold back during flapping.","offset":10,"pro":0.9090909090909091,"labels":"MTD"},{"idx":13,"sentence":"For the reliability-oriented sensitivity analysis with respect to the parameters of input variables, by introducing the copula function to describe the joint probability distribution with dependent input variables, the reliability-oriented sensitivity can be decomposed into independent sensitivity and dependent sensitivity, which can be used to measure the influence of distribution parameters separately.","offset":0,"pro":0,"labels":"MTD"},{"idx":13,"sentence":"Since the parameters of multivariate copula function are difficult to be estimated and not flexible in high dimension, the bivariate copulas are preferred in practice.","offset":1,"pro":0.16666666666666666,"labels":"MTD"},{"idx":13,"sentence":"Then, the vine copula model is employed to transform the multivariate joint probability density function (PDF) into the product of multiple bivariate copulas and marginal PDF of all variables.","offset":2,"pro":0.3333333333333333,"labels":"MTD"},{"idx":13,"sentence":"Based on copula theory, the computation of reliability-oriented sensitivity with dependent variables can be transformed into the computation of a score function for each marginal PDF and the computation of a copula score function for each pair-copula PDF involved in the vine factorization.","offset":3,"pro":0.5,"labels":"MTD"},{"idx":13,"sentence":"A general numerical approach is proposed to compute the separate sensitivity.","offset":4,"pro":0.6666666666666666,"labels":"MTD"},{"idx":13,"sentence":"Then, some numerical examples and engineering applications are employed to validate the rationality of the proposed method.","offset":5,"pro":0.8333333333333334,"labels":"MTD"},{"idx":14,"sentence":"The transition to Industry 4.0 poses new challenges for sensor integration design.","offset":0,"pro":0,"labels":"BAC"},{"idx":14,"sentence":"The foundation of any intelligent system is the data, and the data quality depends largely on the integration of the sensor generating it.","offset":1,"pro":0.2,"labels":"BAC"},{"idx":14,"sentence":"In this study, the barriers for robust sensor system design are explored through an interview study among practitioners from different industrial contexts.","offset":2,"pro":0.4,"labels":"PUR"},{"idx":14,"sentence":"The aim is to explore potential challenges within different contexts and suggest possible directions for research within the field of sensor integration design.","offset":3,"pro":0.6,"labels":"PUR"},{"idx":14,"sentence":"Beyond the question of new sensing technologies, the study highlights an increasing challenge of physical integration tasks and illustrates the varying requirements for development support in different industry sectors.","offset":4,"pro":0.8,"labels":"CLN"},{"idx":15,"sentence":"Biomimetic practice requires a diverse set of knowledge from both biology and engineering.","offset":0,"pro":0,"labels":"BAC"},{"idx":15,"sentence":"Several researchers have been supporting the integration of biologists within biomimetic design teams in order to meet those biological requirements and improve the effectiveness of biomimetic processes.","offset":1,"pro":0.125,"labels":"BAC"},{"idx":15,"sentence":"However, interdisciplinarity practices create well-known communication challenges.","offset":2,"pro":0.25,"labels":"GAP"},{"idx":15,"sentence":"Based on functional representations (like SAPPhIRE or function behavior structure (FBS)), several approaches to model biological information have been investigated in the literature.","offset":3,"pro":0.375,"labels":"GAP"},{"idx":15,"sentence":"Nonetheless, actual communication processes within interdisciplinary biomimetic design teams are yet to be studied.","offset":4,"pro":0.5,"labels":"GAP"},{"idx":15,"sentence":"Following this research axis, this publication focuses on communication noises and wonders if a shared framework of reference can be defined to improve communication between biologists and engineers?","offset":5,"pro":0.625,"labels":"PUR"},{"idx":15,"sentence":"Through the comparison of processes and graphic representations between biology and engineering design, a set of guidelines is defined to structure a shared framework of reference.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":15,"sentence":"Within this framework, a new tool referred to as LINKAGE is then proposed to assist interdisciplinary communication during the biomimetic process.","offset":7,"pro":0.875,"labels":"MTD"},{"idx":16,"sentence":"The concept of a statically balanced mechanism with a single rotational degree-of-freedom is presented.","offset":0,"pro":0,"labels":"PUR"},{"idx":16,"sentence":"The proposed device achieves static balancing by combining positive stiffness elements and negative stiffness elements within an annular domain.","offset":1,"pro":0.09090909090909091,"labels":"MTD"},{"idx":16,"sentence":"Two designs are discussed.","offset":2,"pro":0.18181818181818182,"labels":"MTD"},{"idx":16,"sentence":"The first is composed of an Archimedean spiral and two pinned-pinned pre-buckled beams.","offset":3,"pro":0.2727272727272727,"labels":"MTD"},{"idx":16,"sentence":"The overall mechanism is modeled via an analytical approach and the element dimensions are optimized.","offset":4,"pro":0.36363636363636365,"labels":"MTD"},{"idx":16,"sentence":"The optimal configuration is then tested through finite element analysis (FEA).","offset":5,"pro":0.45454545454545453,"labels":"MTD"},{"idx":16,"sentence":"A second approach replaces the spiral beam with elastic custom-shaped spline beams.","offset":6,"pro":0.5454545454545454,"labels":"MTD"},{"idx":16,"sentence":"A FEA optimization is performed to determine the shape and size of such spline beams.","offset":7,"pro":0.6363636363636364,"labels":"MTD"},{"idx":16,"sentence":"The behavior of the negators is used as reference for the optimization so as to achieve a complete balancing.","offset":8,"pro":0.7272727272727273,"labels":"MTD"},{"idx":16,"sentence":"A physical prototype of each configuration is machined and tested.","offset":9,"pro":0.8181818181818182,"labels":"MTD"},{"idx":16,"sentence":"The comparison between predicted and acquired data confirmed the efficacy of the design methods.","offset":10,"pro":0.9090909090909091,"labels":"CLN"},{"idx":17,"sentence":"Due to the growing need for gearboxes to be as lightweight and efficient as possible, it is most important that the gear mesh's potential is utilized as well as possible.","offset":0,"pro":0,"labels":"BAC"},{"idx":17,"sentence":"One way of doing that is to define a flank modification that optimally distributes the load over the flank.","offset":1,"pro":0.0625,"labels":"BAC"},{"idx":17,"sentence":"Best practice for defining a flank modification is to manually check out the load distribution and to define a value of the flank modification.","offset":2,"pro":0.125,"labels":"BAC"},{"idx":17,"sentence":"In general, this is an iterative method to get an optimally distributed load.","offset":3,"pro":0.1875,"labels":"BAC"},{"idx":17,"sentence":"This method can also be automated.","offset":4,"pro":0.25,"labels":"BAC"},{"idx":17,"sentence":"To do this, the deformations of the gearbox (shafts, bearings, gear mesh) are calculated.","offset":5,"pro":0.3125,"labels":"BAC"},{"idx":17,"sentence":"With those results, a modification proposal is calculated and applied to the calculation model.","offset":6,"pro":0.375,"labels":"BAC"},{"idx":17,"sentence":"As soon as the values for the next additional modification proposal drop under a certain limit, the iteration is finished.","offset":7,"pro":0.4375,"labels":"BAC"},{"idx":17,"sentence":"This method consumes time and computing power.","offset":8,"pro":0.5,"labels":"BAC"},{"idx":17,"sentence":"Additionally, since it is an iteration, it does not always converge.","offset":9,"pro":0.5625,"labels":"BAC"},{"idx":17,"sentence":"A new method for calculating the lead flank modification for all gear stages in the gearbox to be calculated is presented in this paper.","offset":10,"pro":0.625,"labels":"PUR"},{"idx":17,"sentence":"The method shown in this paper uses additional degrees of freedom and equations, which are integrated into the linear equation system of the gearbox model.","offset":11,"pro":0.6875,"labels":"MTD"},{"idx":17,"sentence":"Those degrees of freedom and the equations apply the boundary condition to the model of a constant load distribution.","offset":12,"pro":0.75,"labels":"MTD"},{"idx":17,"sentence":"By introducing additional factors in the equations, it is possible to calculate a lead flank modification for an arbitrary load distribution.","offset":13,"pro":0.8125,"labels":"MTD"},{"idx":17,"sentence":"By integrating these additional degrees of freedom and the equations, only one additional calculation is needed to get a modification proposal.","offset":14,"pro":0.875,"labels":"MTD"},{"idx":17,"sentence":"Examples throughout this paper show the results of this method.","offset":15,"pro":0.9375,"labels":"MTD"},{"idx":18,"sentence":"Existing literature on information sharing in contests has established that sharing contest-specific information influences contestant behaviors, and thereby, the outcomes of a contest.","offset":0,"pro":0,"labels":"BAC"},{"idx":18,"sentence":"However, in the context of engineering design contests, there is a gap in knowledge about how contest-specific information such as competitors' historical performance influences designers' actions and the resulting design outcomes.","offset":1,"pro":0.125,"labels":"GAP"},{"idx":18,"sentence":"To address this gap, the objective of this study is to quantify the influence of information about competitors' past performance on designers' belief about the outcomes of a contest, which influences their design decisions, and the resulting design outcomes.","offset":2,"pro":0.25,"labels":"PUR"},{"idx":18,"sentence":"We focus on a single-stage design competition where an objective figure of merit is available to the contestants for assessing the performance of their design.","offset":3,"pro":0.375,"labels":"MTD"},{"idx":18,"sentence":"Our approach includes (i) developing a behavioral model of sequential decision making that accounts for information about competitors' historical performance and (ii) using the model in conjunction with a human-subject experiment where participants make design decisions given controlled strong or weak performance records of past competitors.","offset":4,"pro":0.5,"labels":"MTD"},{"idx":18,"sentence":"Our results indicate that participants spend greater efforts when they know that the contest history reflects that past competitors had a strong performance record than when it reflects a weak performance record.","offset":5,"pro":0.625,"labels":"RST"},{"idx":18,"sentence":"Moreover, we quantify cognitive underpinnings of such informational influence via our model parameters.","offset":6,"pro":0.75,"labels":"MTD"},{"idx":18,"sentence":"Based on the parametric inferences about participants' cognition, we suggest that contest designers are better off not providing historical performance records if past contest outcomes do not match their expectations setup for a given design contest.","offset":7,"pro":0.875,"labels":"CLN"},{"idx":19,"sentence":"Growing trends towards increased complexity and prolonged useful lives of engineering systems present challenges for system designers in accounting for the impacts of post-design activities (e.g., manufacturing, condition monitoring, remaining life prediction, maintenance, service logistics, end-of-life options, etc.) on system performance (e.g., costs, reliability, customer satisfaction, environmental impacts, etc.).","offset":0,"pro":0,"labels":"BAC"},{"idx":19,"sentence":"It is very difficult to develop accredited lifecycle system performance models because these activities only occur after the system is built and operated.","offset":1,"pro":0.1,"labels":"GAP"},{"idx":19,"sentence":"Thus, system design and post-design decision-making have traditionally been addressed separately, leading to suboptimal performance over the systems lifecycle.","offset":2,"pro":0.2,"labels":"GAP"},{"idx":19,"sentence":"With significant advances in computational modeling, simulation, sensing \\& condition monitoring, and machine learning \\& artificial intelligence, the capability of predictive modeling has grown prominently over the past decade, leading to demonstrated benefits such as improved system availability and reduced operation and maintenance costs.","offset":3,"pro":0.3,"labels":"BAC"},{"idx":19,"sentence":"Predictive modeling can bridge system design and post-design stages and provide an optimal pathway for system designers to effectively account for future system operations at the design stage.","offset":4,"pro":0.4,"labels":"BAC"},{"idx":19,"sentence":"In order to achieve optimal performance over the system's lifecycle, post-design decisions and system operating performance can be incorporated into the initial design with the aid of state-of-the-art predictive modeling approaches.","offset":5,"pro":0.5,"labels":"BAC"},{"idx":19,"sentence":"Therefore, optimized design and operation decisions can be explored jointly in an enlarged system design space.","offset":6,"pro":0.6,"labels":"BAC"},{"idx":19,"sentence":"This article conducted a literature review for the integrated design and operation of engineering systems with predictive modeling, where not only the predictive modeling approaches but also the strategies of integrating predictive models into the system design processes are categorized.","offset":7,"pro":0.7,"labels":"PUR"},{"idx":19,"sentence":"Although predictive modeling has been handled from data-driven, statistical, analytical, and empirical aspects, and recent design problems have started to evaluate the lifecycle performance, there are still challenges in the field that require active investigation and exploration.","offset":8,"pro":0.8,"labels":"RST"},{"idx":19,"sentence":"So, in the end, this article provides a summary of the future directions that encourages research collaborations among the various communities interested in the optimal system lifecycle design.","offset":9,"pro":0.9,"labels":"CLN"}]